{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x15e14129970>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = xlrd.open_workbook('data.xlsx')\n",
    "table = data.sheet_by_index(0)\n",
    "dataFile = []\n",
    "for rowNum in range(table.nrows):\n",
    "    dataFile.append(table.row_values(rowNum))\n",
    "    \n",
    "data_len = len(dataFile)\n",
    "par_len = len(dataFile[0])\n",
    "y = []\n",
    "for i in range(data_len):\n",
    "    y.append(dataFile[i][par_len - 1])\n",
    "    dataFile[i].pop()\n",
    "y = np.trunc(y).astype(int).tolist()\n",
    "X = dataFile\n",
    "plt.scatter([x[0] for x in X], [x[1] for x in X], c=y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68807785\n",
      "Iteration 2, loss = 0.68795384\n",
      "Iteration 3, loss = 0.68777986\n",
      "Iteration 4, loss = 0.68756379\n",
      "Iteration 5, loss = 0.68731317\n",
      "Iteration 6, loss = 0.68703513\n",
      "Iteration 7, loss = 0.68673622\n",
      "Iteration 8, loss = 0.68642245\n",
      "Iteration 9, loss = 0.68609919\n",
      "Iteration 10, loss = 0.68577121\n",
      "Iteration 11, loss = 0.68544267\n",
      "Iteration 12, loss = 0.68511713\n",
      "Iteration 13, loss = 0.68479761\n",
      "Iteration 14, loss = 0.68448659\n",
      "Iteration 15, loss = 0.68418610\n",
      "Iteration 16, loss = 0.68389771\n",
      "Iteration 17, loss = 0.68362261\n",
      "Iteration 18, loss = 0.68336164\n",
      "Iteration 19, loss = 0.68311534\n",
      "Iteration 20, loss = 0.68288398\n",
      "Iteration 21, loss = 0.68266760\n",
      "Iteration 22, loss = 0.68246608\n",
      "Iteration 23, loss = 0.68227910\n",
      "Iteration 24, loss = 0.68210625\n",
      "Iteration 25, loss = 0.68194699\n",
      "Iteration 26, loss = 0.68180073\n",
      "Iteration 27, loss = 0.68166680\n",
      "Iteration 28, loss = 0.68154450\n",
      "Iteration 29, loss = 0.68143313\n",
      "Iteration 30, loss = 0.68133193\n",
      "Iteration 31, loss = 0.68124020\n",
      "Iteration 32, loss = 0.68115720\n",
      "Iteration 33, loss = 0.68108225\n",
      "Iteration 34, loss = 0.68101467\n",
      "Iteration 35, loss = 0.68095382\n",
      "Iteration 36, loss = 0.68089907\n",
      "Iteration 37, loss = 0.68084985\n",
      "Iteration 38, loss = 0.68080563\n",
      "Iteration 39, loss = 0.68076588\n",
      "Iteration 40, loss = 0.68073015\n",
      "Iteration 41, loss = 0.68069799\n",
      "Iteration 42, loss = 0.68066901\n",
      "Iteration 43, loss = 0.68064284\n",
      "Iteration 44, loss = 0.68061915\n",
      "Iteration 45, loss = 0.68059764\n",
      "Iteration 46, loss = 0.68057804\n",
      "Iteration 47, loss = 0.68056009\n",
      "Iteration 48, loss = 0.68054359\n",
      "Iteration 49, loss = 0.68052833\n",
      "Iteration 50, loss = 0.68051414\n",
      "Iteration 51, loss = 0.68050087\n",
      "Iteration 52, loss = 0.68048837\n",
      "Iteration 53, loss = 0.68047654\n",
      "Iteration 54, loss = 0.68046526\n",
      "Iteration 55, loss = 0.68045445\n",
      "Iteration 56, loss = 0.68044403\n",
      "Iteration 57, loss = 0.68043392\n",
      "Iteration 58, loss = 0.68042408\n",
      "Iteration 59, loss = 0.68041445\n",
      "Iteration 60, loss = 0.68040499\n",
      "Iteration 61, loss = 0.68039566\n",
      "Iteration 62, loss = 0.68038644\n",
      "Iteration 63, loss = 0.68037730\n",
      "Iteration 64, loss = 0.68036821\n",
      "Iteration 65, loss = 0.68035918\n",
      "Iteration 66, loss = 0.68035017\n",
      "Iteration 67, loss = 0.68034118\n",
      "Iteration 68, loss = 0.68033221\n",
      "Iteration 69, loss = 0.68032324\n",
      "Iteration 70, loss = 0.68031427\n",
      "Iteration 71, loss = 0.68030530\n",
      "Iteration 72, loss = 0.68029633\n",
      "Iteration 73, loss = 0.68028735\n",
      "Iteration 74, loss = 0.68027837\n",
      "Iteration 75, loss = 0.68026938\n",
      "Iteration 76, loss = 0.68026039\n",
      "Iteration 77, loss = 0.68025139\n",
      "Iteration 78, loss = 0.68024238\n",
      "Iteration 79, loss = 0.68023338\n",
      "Iteration 80, loss = 0.68022437\n",
      "Iteration 81, loss = 0.68021537\n",
      "Iteration 82, loss = 0.68020636\n",
      "Iteration 83, loss = 0.68019736\n",
      "Iteration 84, loss = 0.68018836\n",
      "Iteration 85, loss = 0.68017936\n",
      "Iteration 86, loss = 0.68017037\n",
      "Iteration 87, loss = 0.68016138\n",
      "Iteration 88, loss = 0.68015240\n",
      "Iteration 89, loss = 0.68014343\n",
      "Iteration 90, loss = 0.68013447\n",
      "Iteration 91, loss = 0.68012551\n",
      "Iteration 92, loss = 0.68011657\n",
      "Iteration 93, loss = 0.68010763\n",
      "Iteration 94, loss = 0.68009870\n",
      "Iteration 95, loss = 0.68008978\n",
      "Iteration 96, loss = 0.68008088\n",
      "Iteration 97, loss = 0.68007198\n",
      "Iteration 98, loss = 0.68006309\n",
      "Iteration 99, loss = 0.68005421\n",
      "Iteration 100, loss = 0.68004534\n",
      "Iteration 101, loss = 0.68003648\n",
      "Iteration 102, loss = 0.68002763\n",
      "Iteration 103, loss = 0.68001879\n",
      "Iteration 104, loss = 0.68000996\n",
      "Iteration 105, loss = 0.68000114\n",
      "Iteration 106, loss = 0.67999233\n",
      "Iteration 107, loss = 0.67998352\n",
      "Iteration 108, loss = 0.67997473\n",
      "Iteration 109, loss = 0.67996594\n",
      "Iteration 110, loss = 0.67995716\n",
      "Iteration 111, loss = 0.67994839\n",
      "Iteration 112, loss = 0.67993963\n",
      "Iteration 113, loss = 0.67993088\n",
      "Iteration 114, loss = 0.67992213\n",
      "Iteration 115, loss = 0.67991339\n",
      "Iteration 116, loss = 0.67990466\n",
      "Iteration 117, loss = 0.67989594\n",
      "Iteration 118, loss = 0.67988722\n",
      "Iteration 119, loss = 0.67987851\n",
      "Iteration 120, loss = 0.67986981\n",
      "Iteration 121, loss = 0.67986112\n",
      "Iteration 122, loss = 0.67985243\n",
      "Iteration 123, loss = 0.67984374\n",
      "Iteration 124, loss = 0.67983507\n",
      "Iteration 125, loss = 0.67982640\n",
      "Iteration 126, loss = 0.67981774\n",
      "Iteration 127, loss = 0.67980908\n",
      "Iteration 128, loss = 0.67980043\n",
      "Iteration 129, loss = 0.67979178\n",
      "Iteration 130, loss = 0.67978314\n",
      "Iteration 131, loss = 0.67977451\n",
      "Iteration 132, loss = 0.67976588\n",
      "Iteration 133, loss = 0.67975726\n",
      "Iteration 134, loss = 0.67974865\n",
      "Iteration 135, loss = 0.67974004\n",
      "Iteration 136, loss = 0.67973143\n",
      "Iteration 137, loss = 0.67972283\n",
      "Iteration 138, loss = 0.67971424\n",
      "Iteration 139, loss = 0.67970565\n",
      "Iteration 140, loss = 0.67969706\n",
      "Iteration 141, loss = 0.67968848\n",
      "Iteration 142, loss = 0.67967991\n",
      "Iteration 143, loss = 0.67967134\n",
      "Iteration 144, loss = 0.67966278\n",
      "Iteration 145, loss = 0.67965422\n",
      "Iteration 146, loss = 0.67964567\n",
      "Iteration 147, loss = 0.67963712\n",
      "Iteration 148, loss = 0.67962857\n",
      "Iteration 149, loss = 0.67962003\n",
      "Iteration 150, loss = 0.67961150\n",
      "Iteration 151, loss = 0.67960297\n",
      "Iteration 152, loss = 0.67959444\n",
      "Iteration 153, loss = 0.67958592\n",
      "Iteration 154, loss = 0.67957740\n",
      "Iteration 155, loss = 0.67956889\n",
      "Iteration 156, loss = 0.67956038\n",
      "Iteration 157, loss = 0.67955188\n",
      "Iteration 158, loss = 0.67954338\n",
      "Iteration 159, loss = 0.67953488\n",
      "Iteration 160, loss = 0.67952639\n",
      "Iteration 161, loss = 0.67951790\n",
      "Iteration 162, loss = 0.67950942\n",
      "Iteration 163, loss = 0.67950094\n",
      "Iteration 164, loss = 0.67949246\n",
      "Iteration 165, loss = 0.67948399\n",
      "Iteration 166, loss = 0.67947552\n",
      "Iteration 167, loss = 0.67946705\n",
      "Iteration 168, loss = 0.67945859\n",
      "Iteration 169, loss = 0.67945013\n",
      "Iteration 170, loss = 0.67944168\n",
      "Iteration 171, loss = 0.67943323\n",
      "Iteration 172, loss = 0.67942478\n",
      "Iteration 173, loss = 0.67941634\n",
      "Iteration 174, loss = 0.67940790\n",
      "Iteration 175, loss = 0.67939946\n",
      "Iteration 176, loss = 0.67939103\n",
      "Iteration 177, loss = 0.67938260\n",
      "Iteration 178, loss = 0.67937417\n",
      "Iteration 179, loss = 0.67936574\n",
      "Iteration 180, loss = 0.67935732\n",
      "Iteration 181, loss = 0.67934890\n",
      "Iteration 182, loss = 0.67934049\n",
      "Iteration 183, loss = 0.67933207\n",
      "Iteration 184, loss = 0.67932366\n",
      "Iteration 185, loss = 0.67931526\n",
      "Iteration 186, loss = 0.67930685\n",
      "Iteration 187, loss = 0.67929845\n",
      "Iteration 188, loss = 0.67929005\n",
      "Iteration 189, loss = 0.67928165\n",
      "Iteration 190, loss = 0.67927326\n",
      "Iteration 191, loss = 0.67926486\n",
      "Iteration 192, loss = 0.67925647\n",
      "Iteration 193, loss = 0.67924809\n",
      "Iteration 194, loss = 0.67923970\n",
      "Iteration 195, loss = 0.67923132\n",
      "Iteration 196, loss = 0.67922294\n",
      "Iteration 197, loss = 0.67921456\n",
      "Iteration 198, loss = 0.67920618\n",
      "Iteration 199, loss = 0.67919781\n",
      "Iteration 200, loss = 0.67918943\n",
      "Iteration 201, loss = 0.67918106\n",
      "Iteration 202, loss = 0.67917269\n",
      "Iteration 203, loss = 0.67916433\n",
      "Iteration 204, loss = 0.67915596\n",
      "Iteration 205, loss = 0.67914760\n",
      "Iteration 206, loss = 0.67913923\n",
      "Iteration 207, loss = 0.67913087\n",
      "Iteration 208, loss = 0.67912251\n",
      "Iteration 209, loss = 0.67911416\n",
      "Iteration 210, loss = 0.67910580\n",
      "Iteration 211, loss = 0.67909744\n",
      "Iteration 212, loss = 0.67908909\n",
      "Iteration 213, loss = 0.67908074\n",
      "Iteration 214, loss = 0.67907239\n",
      "Iteration 215, loss = 0.67906404\n",
      "Iteration 216, loss = 0.67905569\n",
      "Iteration 217, loss = 0.67904734\n",
      "Iteration 218, loss = 0.67903899\n",
      "Iteration 219, loss = 0.67903065\n",
      "Iteration 220, loss = 0.67902230\n",
      "Iteration 221, loss = 0.67901396\n",
      "Iteration 222, loss = 0.67900561\n",
      "Iteration 223, loss = 0.67899727\n",
      "Iteration 224, loss = 0.67898893\n",
      "Iteration 225, loss = 0.67898059\n",
      "Iteration 226, loss = 0.67897225\n",
      "Iteration 227, loss = 0.67896391\n",
      "Iteration 228, loss = 0.67895557\n",
      "Iteration 229, loss = 0.67894723\n",
      "Iteration 230, loss = 0.67893889\n",
      "Iteration 231, loss = 0.67893055\n",
      "Iteration 232, loss = 0.67892221\n",
      "Iteration 233, loss = 0.67891387\n",
      "Iteration 234, loss = 0.67890553\n",
      "Iteration 235, loss = 0.67889720\n",
      "Iteration 236, loss = 0.67888886\n",
      "Iteration 237, loss = 0.67888052\n",
      "Iteration 238, loss = 0.67887218\n",
      "Iteration 239, loss = 0.67886384\n",
      "Iteration 240, loss = 0.67885551\n",
      "Iteration 241, loss = 0.67884717\n",
      "Iteration 242, loss = 0.67883883\n",
      "Iteration 243, loss = 0.67883049\n",
      "Iteration 244, loss = 0.67882215\n",
      "Iteration 245, loss = 0.67881381\n",
      "Iteration 246, loss = 0.67880547\n",
      "Iteration 247, loss = 0.67879713\n",
      "Iteration 248, loss = 0.67878879\n",
      "Iteration 249, loss = 0.67878045\n",
      "Iteration 250, loss = 0.67877210\n",
      "Iteration 251, loss = 0.67876376\n",
      "Iteration 252, loss = 0.67875542\n",
      "Iteration 253, loss = 0.67874707\n",
      "Iteration 254, loss = 0.67873873\n",
      "Iteration 255, loss = 0.67873038\n",
      "Iteration 256, loss = 0.67872203\n",
      "Iteration 257, loss = 0.67871369\n",
      "Iteration 258, loss = 0.67870534\n",
      "Iteration 259, loss = 0.67869699\n",
      "Iteration 260, loss = 0.67868863\n",
      "Iteration 261, loss = 0.67868028\n",
      "Iteration 262, loss = 0.67867193\n",
      "Iteration 263, loss = 0.67866357\n",
      "Iteration 264, loss = 0.67865521\n",
      "Iteration 265, loss = 0.67864685\n",
      "Iteration 266, loss = 0.67863850\n",
      "Iteration 267, loss = 0.67863013\n",
      "Iteration 268, loss = 0.67862177\n",
      "Iteration 269, loss = 0.67861341\n",
      "Iteration 270, loss = 0.67860504\n",
      "Iteration 271, loss = 0.67859667\n",
      "Iteration 272, loss = 0.67858830\n",
      "Iteration 273, loss = 0.67857993\n",
      "Iteration 274, loss = 0.67857156\n",
      "Iteration 275, loss = 0.67856318\n",
      "Iteration 276, loss = 0.67855480\n",
      "Iteration 277, loss = 0.67854642\n",
      "Iteration 278, loss = 0.67853804\n",
      "Iteration 279, loss = 0.67852966\n",
      "Iteration 280, loss = 0.67852127\n",
      "Iteration 281, loss = 0.67851288\n",
      "Iteration 282, loss = 0.67850449\n",
      "Iteration 283, loss = 0.67849610\n",
      "Iteration 284, loss = 0.67848771\n",
      "Iteration 285, loss = 0.67847931\n",
      "Iteration 286, loss = 0.67847091\n",
      "Iteration 287, loss = 0.67846251\n",
      "Iteration 288, loss = 0.67845410\n",
      "Iteration 289, loss = 0.67844569\n",
      "Iteration 290, loss = 0.67843728\n",
      "Iteration 291, loss = 0.67842887\n",
      "Iteration 292, loss = 0.67842045\n",
      "Iteration 293, loss = 0.67841203\n",
      "Iteration 294, loss = 0.67840361\n",
      "Iteration 295, loss = 0.67839519\n",
      "Iteration 296, loss = 0.67838676\n",
      "Iteration 297, loss = 0.67837833\n",
      "Iteration 298, loss = 0.67836989\n",
      "Iteration 299, loss = 0.67836146\n",
      "Iteration 300, loss = 0.67835302\n",
      "Iteration 301, loss = 0.67834457\n",
      "Iteration 302, loss = 0.67833613\n",
      "Iteration 303, loss = 0.67832768\n",
      "Iteration 304, loss = 0.67831922\n",
      "Iteration 305, loss = 0.67831076\n",
      "Iteration 306, loss = 0.67830230\n",
      "Iteration 307, loss = 0.67829384\n",
      "Iteration 308, loss = 0.67828537\n",
      "Iteration 309, loss = 0.67827690\n",
      "Iteration 310, loss = 0.67826842\n",
      "Iteration 311, loss = 0.67825995\n",
      "Iteration 312, loss = 0.67825146\n",
      "Iteration 313, loss = 0.67824298\n",
      "Iteration 314, loss = 0.67823449\n",
      "Iteration 315, loss = 0.67822599\n",
      "Iteration 316, loss = 0.67821749\n",
      "Iteration 317, loss = 0.67820899\n",
      "Iteration 318, loss = 0.67820048\n",
      "Iteration 319, loss = 0.67819197\n",
      "Iteration 320, loss = 0.67818346\n",
      "Iteration 321, loss = 0.67817494\n",
      "Iteration 322, loss = 0.67816641\n",
      "Iteration 323, loss = 0.67815788\n",
      "Iteration 324, loss = 0.67814935\n",
      "Iteration 325, loss = 0.67814081\n",
      "Iteration 326, loss = 0.67813227\n",
      "Iteration 327, loss = 0.67812373\n",
      "Iteration 328, loss = 0.67811517\n",
      "Iteration 329, loss = 0.67810662\n",
      "Iteration 330, loss = 0.67809806\n",
      "Iteration 331, loss = 0.67808949\n",
      "Iteration 332, loss = 0.67808092\n",
      "Iteration 333, loss = 0.67807235\n",
      "Iteration 334, loss = 0.67806377\n",
      "Iteration 335, loss = 0.67805518\n",
      "Iteration 336, loss = 0.67804659\n",
      "Iteration 337, loss = 0.67803800\n",
      "Iteration 338, loss = 0.67802940\n",
      "Iteration 339, loss = 0.67802079\n",
      "Iteration 340, loss = 0.67801218\n",
      "Iteration 341, loss = 0.67800357\n",
      "Iteration 342, loss = 0.67799494\n",
      "Iteration 343, loss = 0.67798632\n",
      "Iteration 344, loss = 0.67797769\n",
      "Iteration 345, loss = 0.67796905\n",
      "Iteration 346, loss = 0.67796040\n",
      "Iteration 347, loss = 0.67795176\n",
      "Iteration 348, loss = 0.67794310\n",
      "Iteration 349, loss = 0.67793444\n",
      "Iteration 350, loss = 0.67792577\n",
      "Iteration 351, loss = 0.67791710\n",
      "Iteration 352, loss = 0.67790842\n",
      "Iteration 353, loss = 0.67789974\n",
      "Iteration 354, loss = 0.67789105\n",
      "Iteration 355, loss = 0.67788236\n",
      "Iteration 356, loss = 0.67787365\n",
      "Iteration 357, loss = 0.67786495\n",
      "Iteration 358, loss = 0.67785623\n",
      "Iteration 359, loss = 0.67784751\n",
      "Iteration 360, loss = 0.67783878\n",
      "Iteration 361, loss = 0.67783005\n",
      "Iteration 362, loss = 0.67782131\n",
      "Iteration 363, loss = 0.67781257\n",
      "Iteration 364, loss = 0.67780381\n",
      "Iteration 365, loss = 0.67779505\n",
      "Iteration 366, loss = 0.67778629\n",
      "Iteration 367, loss = 0.67777752\n",
      "Iteration 368, loss = 0.67776874\n",
      "Iteration 369, loss = 0.67775995\n",
      "Iteration 370, loss = 0.67775116\n",
      "Iteration 371, loss = 0.67774236\n",
      "Iteration 372, loss = 0.67773355\n",
      "Iteration 373, loss = 0.67772474\n",
      "Iteration 374, loss = 0.67771592\n",
      "Iteration 375, loss = 0.67770709\n",
      "Iteration 376, loss = 0.67769826\n",
      "Iteration 377, loss = 0.67768942\n",
      "Iteration 378, loss = 0.67768057\n",
      "Iteration 379, loss = 0.67767171\n",
      "Iteration 380, loss = 0.67766285\n",
      "Iteration 381, loss = 0.67765398\n",
      "Iteration 382, loss = 0.67764510\n",
      "Iteration 383, loss = 0.67763621\n",
      "Iteration 384, loss = 0.67762732\n",
      "Iteration 385, loss = 0.67761842\n",
      "Iteration 386, loss = 0.67760951\n",
      "Iteration 387, loss = 0.67760059\n",
      "Iteration 388, loss = 0.67759167\n",
      "Iteration 389, loss = 0.67758274\n",
      "Iteration 390, loss = 0.67757380\n",
      "Iteration 391, loss = 0.67756485\n",
      "Iteration 392, loss = 0.67755589\n",
      "Iteration 393, loss = 0.67754693\n",
      "Iteration 394, loss = 0.67753796\n",
      "Iteration 395, loss = 0.67752898\n",
      "Iteration 396, loss = 0.67751999\n",
      "Iteration 397, loss = 0.67751099\n",
      "Iteration 398, loss = 0.67750199\n",
      "Iteration 399, loss = 0.67749298\n",
      "Iteration 400, loss = 0.67748395\n",
      "Iteration 401, loss = 0.67747492\n",
      "Iteration 402, loss = 0.67746589\n",
      "Iteration 403, loss = 0.67745684\n",
      "Iteration 404, loss = 0.67744778\n",
      "Iteration 405, loss = 0.67743872\n",
      "Iteration 406, loss = 0.67742965\n",
      "Iteration 407, loss = 0.67742056\n",
      "Iteration 408, loss = 0.67741147\n",
      "Iteration 409, loss = 0.67740238\n",
      "Iteration 410, loss = 0.67739327\n",
      "Iteration 411, loss = 0.67738415\n",
      "Iteration 412, loss = 0.67737502\n",
      "Iteration 413, loss = 0.67736589\n",
      "Iteration 414, loss = 0.67735674\n",
      "Iteration 415, loss = 0.67734759\n",
      "Iteration 416, loss = 0.67733843\n",
      "Iteration 417, loss = 0.67732925\n",
      "Iteration 418, loss = 0.67732007\n",
      "Iteration 419, loss = 0.67731088\n",
      "Iteration 420, loss = 0.67730168\n",
      "Iteration 421, loss = 0.67729247\n",
      "Iteration 422, loss = 0.67728325\n",
      "Iteration 423, loss = 0.67727402\n",
      "Iteration 424, loss = 0.67726478\n",
      "Iteration 425, loss = 0.67725553\n",
      "Iteration 426, loss = 0.67724627\n",
      "Iteration 427, loss = 0.67723701\n",
      "Iteration 428, loss = 0.67722773\n",
      "Iteration 429, loss = 0.67721844\n",
      "Iteration 430, loss = 0.67720914\n",
      "Iteration 431, loss = 0.67719983\n",
      "Iteration 432, loss = 0.67719051\n",
      "Iteration 433, loss = 0.67718118\n",
      "Iteration 434, loss = 0.67717185\n",
      "Iteration 435, loss = 0.67716250\n",
      "Iteration 436, loss = 0.67715314\n",
      "Iteration 437, loss = 0.67714377\n",
      "Iteration 438, loss = 0.67713439\n",
      "Iteration 439, loss = 0.67712499\n",
      "Iteration 440, loss = 0.67711559\n",
      "Iteration 441, loss = 0.67710618\n",
      "Iteration 442, loss = 0.67709676\n",
      "Iteration 443, loss = 0.67708732\n",
      "Iteration 444, loss = 0.67707788\n",
      "Iteration 445, loss = 0.67706842\n",
      "Iteration 446, loss = 0.67705896\n",
      "Iteration 447, loss = 0.67704948\n",
      "Iteration 448, loss = 0.67703999\n",
      "Iteration 449, loss = 0.67703049\n",
      "Iteration 450, loss = 0.67702098\n",
      "Iteration 451, loss = 0.67701146\n",
      "Iteration 452, loss = 0.67700193\n",
      "Iteration 453, loss = 0.67699238\n",
      "Iteration 454, loss = 0.67698283\n",
      "Iteration 455, loss = 0.67697326\n",
      "Iteration 456, loss = 0.67696368\n",
      "Iteration 457, loss = 0.67695409\n",
      "Iteration 458, loss = 0.67694449\n",
      "Iteration 459, loss = 0.67693488\n",
      "Iteration 460, loss = 0.67692525\n",
      "Iteration 461, loss = 0.67691562\n",
      "Iteration 462, loss = 0.67690597\n",
      "Iteration 463, loss = 0.67689631\n",
      "Iteration 464, loss = 0.67688664\n",
      "Iteration 465, loss = 0.67687695\n",
      "Iteration 466, loss = 0.67686726\n",
      "Iteration 467, loss = 0.67685755\n",
      "Iteration 468, loss = 0.67684783\n",
      "Iteration 469, loss = 0.67683809\n",
      "Iteration 470, loss = 0.67682835\n",
      "Iteration 471, loss = 0.67681859\n",
      "Iteration 472, loss = 0.67680882\n",
      "Iteration 473, loss = 0.67679904\n",
      "Iteration 474, loss = 0.67678925\n",
      "Iteration 475, loss = 0.67677944\n",
      "Iteration 476, loss = 0.67676962\n",
      "Iteration 477, loss = 0.67675979\n",
      "Iteration 478, loss = 0.67674994\n",
      "Iteration 479, loss = 0.67674008\n",
      "Iteration 480, loss = 0.67673021\n",
      "Iteration 481, loss = 0.67672033\n",
      "Iteration 482, loss = 0.67671043\n",
      "Iteration 483, loss = 0.67670052\n",
      "Iteration 484, loss = 0.67669060\n",
      "Iteration 485, loss = 0.67668066\n",
      "Iteration 486, loss = 0.67667072\n",
      "Iteration 487, loss = 0.67666075\n",
      "Iteration 488, loss = 0.67665078\n",
      "Iteration 489, loss = 0.67664079\n",
      "Iteration 490, loss = 0.67663079\n",
      "Iteration 491, loss = 0.67662077\n",
      "Iteration 492, loss = 0.67661074\n",
      "Iteration 493, loss = 0.67660070\n",
      "Iteration 494, loss = 0.67659064\n",
      "Iteration 495, loss = 0.67658057\n",
      "Iteration 496, loss = 0.67657049\n",
      "Iteration 497, loss = 0.67656039\n",
      "Iteration 498, loss = 0.67655028\n",
      "Iteration 499, loss = 0.67654015\n",
      "Iteration 500, loss = 0.67653001\n",
      "Iteration 501, loss = 0.67651986\n",
      "Iteration 502, loss = 0.67650969\n",
      "Iteration 503, loss = 0.67649951\n",
      "Iteration 504, loss = 0.67648931\n",
      "Iteration 505, loss = 0.67647910\n",
      "Iteration 506, loss = 0.67646887\n",
      "Iteration 507, loss = 0.67645863\n",
      "Iteration 508, loss = 0.67644838\n",
      "Iteration 509, loss = 0.67643811\n",
      "Iteration 510, loss = 0.67642783\n",
      "Iteration 511, loss = 0.67641753\n",
      "Iteration 512, loss = 0.67640722\n",
      "Iteration 513, loss = 0.67639689\n",
      "Iteration 514, loss = 0.67638655\n",
      "Iteration 515, loss = 0.67637619\n",
      "Iteration 516, loss = 0.67636582\n",
      "Iteration 517, loss = 0.67635543\n",
      "Iteration 518, loss = 0.67634503\n",
      "Iteration 519, loss = 0.67633461\n",
      "Iteration 520, loss = 0.67632417\n",
      "Iteration 521, loss = 0.67631373\n",
      "Iteration 522, loss = 0.67630326\n",
      "Iteration 523, loss = 0.67629278\n",
      "Iteration 524, loss = 0.67628229\n",
      "Iteration 525, loss = 0.67627178\n",
      "Iteration 526, loss = 0.67626125\n",
      "Iteration 527, loss = 0.67625071\n",
      "Iteration 528, loss = 0.67624015\n",
      "Iteration 529, loss = 0.67622958\n",
      "Iteration 530, loss = 0.67621899\n",
      "Iteration 531, loss = 0.67620839\n",
      "Iteration 532, loss = 0.67619777\n",
      "Iteration 533, loss = 0.67618713\n",
      "Iteration 534, loss = 0.67617648\n",
      "Iteration 535, loss = 0.67616581\n",
      "Iteration 536, loss = 0.67615512\n",
      "Iteration 537, loss = 0.67614442\n",
      "Iteration 538, loss = 0.67613370\n",
      "Iteration 539, loss = 0.67612297\n",
      "Iteration 540, loss = 0.67611222\n",
      "Iteration 541, loss = 0.67610145\n",
      "Iteration 542, loss = 0.67609067\n",
      "Iteration 543, loss = 0.67607987\n",
      "Iteration 544, loss = 0.67606905\n",
      "Iteration 545, loss = 0.67605822\n",
      "Iteration 546, loss = 0.67604736\n",
      "Iteration 547, loss = 0.67603650\n",
      "Iteration 548, loss = 0.67602561\n",
      "Iteration 549, loss = 0.67601471\n",
      "Iteration 550, loss = 0.67600379\n",
      "Iteration 551, loss = 0.67599285\n",
      "Iteration 552, loss = 0.67598190\n",
      "Iteration 553, loss = 0.67597093\n",
      "Iteration 554, loss = 0.67595994\n",
      "Iteration 555, loss = 0.67594894\n",
      "Iteration 556, loss = 0.67593791\n",
      "Iteration 557, loss = 0.67592687\n",
      "Iteration 558, loss = 0.67591581\n",
      "Iteration 559, loss = 0.67590474\n",
      "Iteration 560, loss = 0.67589364\n",
      "Iteration 561, loss = 0.67588253\n",
      "Iteration 562, loss = 0.67587140\n",
      "Iteration 563, loss = 0.67586025\n",
      "Iteration 564, loss = 0.67584909\n",
      "Iteration 565, loss = 0.67583790\n",
      "Iteration 566, loss = 0.67582670\n",
      "Iteration 567, loss = 0.67581548\n",
      "Iteration 568, loss = 0.67580424\n",
      "Iteration 569, loss = 0.67579299\n",
      "Iteration 570, loss = 0.67578171\n",
      "Iteration 571, loss = 0.67577042\n",
      "Iteration 572, loss = 0.67575911\n",
      "Iteration 573, loss = 0.67574777\n",
      "Iteration 574, loss = 0.67573642\n",
      "Iteration 575, loss = 0.67572506\n",
      "Iteration 576, loss = 0.67571367\n",
      "Iteration 577, loss = 0.67570226\n",
      "Iteration 578, loss = 0.67569084\n",
      "Iteration 579, loss = 0.67567939\n",
      "Iteration 580, loss = 0.67566793\n",
      "Iteration 581, loss = 0.67565645\n",
      "Iteration 582, loss = 0.67564494\n",
      "Iteration 583, loss = 0.67563342\n",
      "Iteration 584, loss = 0.67562188\n",
      "Iteration 585, loss = 0.67561032\n",
      "Iteration 586, loss = 0.67559874\n",
      "Iteration 587, loss = 0.67558714\n",
      "Iteration 588, loss = 0.67557553\n",
      "Iteration 589, loss = 0.67556389\n",
      "Iteration 590, loss = 0.67555223\n",
      "Iteration 591, loss = 0.67554055\n",
      "Iteration 592, loss = 0.67552885\n",
      "Iteration 593, loss = 0.67551714\n",
      "Iteration 594, loss = 0.67550540\n",
      "Iteration 595, loss = 0.67549364\n",
      "Iteration 596, loss = 0.67548186\n",
      "Iteration 597, loss = 0.67547006\n",
      "Iteration 598, loss = 0.67545824\n",
      "Iteration 599, loss = 0.67544640\n",
      "Iteration 600, loss = 0.67543454\n",
      "Iteration 601, loss = 0.67542266\n",
      "Iteration 602, loss = 0.67541076\n",
      "Iteration 603, loss = 0.67539884\n",
      "Iteration 604, loss = 0.67538690\n",
      "Iteration 605, loss = 0.67537493\n",
      "Iteration 606, loss = 0.67536295\n",
      "Iteration 607, loss = 0.67535094\n",
      "Iteration 608, loss = 0.67533892\n",
      "Iteration 609, loss = 0.67532687\n",
      "Iteration 610, loss = 0.67531480\n",
      "Iteration 611, loss = 0.67530271\n",
      "Iteration 612, loss = 0.67529060\n",
      "Iteration 613, loss = 0.67527847\n",
      "Iteration 614, loss = 0.67526631\n",
      "Iteration 615, loss = 0.67525414\n",
      "Iteration 616, loss = 0.67524194\n",
      "Iteration 617, loss = 0.67522972\n",
      "Iteration 618, loss = 0.67521748\n",
      "Iteration 619, loss = 0.67520522\n",
      "Iteration 620, loss = 0.67519293\n",
      "Iteration 621, loss = 0.67518062\n",
      "Iteration 622, loss = 0.67516829\n",
      "Iteration 623, loss = 0.67515594\n",
      "Iteration 624, loss = 0.67514357\n",
      "Iteration 625, loss = 0.67513117\n",
      "Iteration 626, loss = 0.67511876\n",
      "Iteration 627, loss = 0.67510632\n",
      "Iteration 628, loss = 0.67509385\n",
      "Iteration 629, loss = 0.67508137\n",
      "Iteration 630, loss = 0.67506886\n",
      "Iteration 631, loss = 0.67505633\n",
      "Iteration 632, loss = 0.67504378\n",
      "Iteration 633, loss = 0.67503120\n",
      "Iteration 634, loss = 0.67501860\n",
      "Iteration 635, loss = 0.67500598\n",
      "Iteration 636, loss = 0.67499333\n",
      "Iteration 637, loss = 0.67498066\n",
      "Iteration 638, loss = 0.67496797\n",
      "Iteration 639, loss = 0.67495525\n",
      "Iteration 640, loss = 0.67494252\n",
      "Iteration 641, loss = 0.67492975\n",
      "Iteration 642, loss = 0.67491697\n",
      "Iteration 643, loss = 0.67490416\n",
      "Iteration 644, loss = 0.67489132\n",
      "Iteration 645, loss = 0.67487847\n",
      "Iteration 646, loss = 0.67486559\n",
      "Iteration 647, loss = 0.67485268\n",
      "Iteration 648, loss = 0.67483975\n",
      "Iteration 649, loss = 0.67482680\n",
      "Iteration 650, loss = 0.67481382\n",
      "Iteration 651, loss = 0.67480082\n",
      "Iteration 652, loss = 0.67478779\n",
      "Iteration 653, loss = 0.67477474\n",
      "Iteration 654, loss = 0.67476167\n",
      "Iteration 655, loss = 0.67474857\n",
      "Iteration 656, loss = 0.67473545\n",
      "Iteration 657, loss = 0.67472230\n",
      "Iteration 658, loss = 0.67470912\n",
      "Iteration 659, loss = 0.67469592\n",
      "Iteration 660, loss = 0.67468270\n",
      "Iteration 661, loss = 0.67466945\n",
      "Iteration 662, loss = 0.67465618\n",
      "Iteration 663, loss = 0.67464288\n",
      "Iteration 664, loss = 0.67462956\n",
      "Iteration 665, loss = 0.67461621\n",
      "Iteration 666, loss = 0.67460283\n",
      "Iteration 667, loss = 0.67458943\n",
      "Iteration 668, loss = 0.67457601\n",
      "Iteration 669, loss = 0.67456255\n",
      "Iteration 670, loss = 0.67454908\n",
      "Iteration 671, loss = 0.67453557\n",
      "Iteration 672, loss = 0.67452204\n",
      "Iteration 673, loss = 0.67450849\n",
      "Iteration 674, loss = 0.67449491\n",
      "Iteration 675, loss = 0.67448130\n",
      "Iteration 676, loss = 0.67446767\n",
      "Iteration 677, loss = 0.67445400\n",
      "Iteration 678, loss = 0.67444032\n",
      "Iteration 679, loss = 0.67442661\n",
      "Iteration 680, loss = 0.67441287\n",
      "Iteration 681, loss = 0.67439910\n",
      "Iteration 682, loss = 0.67438531\n",
      "Iteration 683, loss = 0.67437149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 684, loss = 0.67435764\n",
      "Iteration 685, loss = 0.67434377\n",
      "Iteration 686, loss = 0.67432986\n",
      "Iteration 687, loss = 0.67431594\n",
      "Iteration 688, loss = 0.67430198\n",
      "Iteration 689, loss = 0.67428800\n",
      "Iteration 690, loss = 0.67427399\n",
      "Iteration 691, loss = 0.67425995\n",
      "Iteration 692, loss = 0.67424589\n",
      "Iteration 693, loss = 0.67423179\n",
      "Iteration 694, loss = 0.67421767\n",
      "Iteration 695, loss = 0.67420352\n",
      "Iteration 696, loss = 0.67418935\n",
      "Iteration 697, loss = 0.67417514\n",
      "Iteration 698, loss = 0.67416091\n",
      "Iteration 699, loss = 0.67414665\n",
      "Iteration 700, loss = 0.67413236\n",
      "Iteration 701, loss = 0.67411804\n",
      "Iteration 702, loss = 0.67410370\n",
      "Iteration 703, loss = 0.67408932\n",
      "Iteration 704, loss = 0.67407492\n",
      "Iteration 705, loss = 0.67406049\n",
      "Iteration 706, loss = 0.67404603\n",
      "Iteration 707, loss = 0.67403154\n",
      "Iteration 708, loss = 0.67401702\n",
      "Iteration 709, loss = 0.67400247\n",
      "Iteration 710, loss = 0.67398790\n",
      "Iteration 711, loss = 0.67397329\n",
      "Iteration 712, loss = 0.67395866\n",
      "Iteration 713, loss = 0.67394399\n",
      "Iteration 714, loss = 0.67392930\n",
      "Iteration 715, loss = 0.67391457\n",
      "Iteration 716, loss = 0.67389982\n",
      "Iteration 717, loss = 0.67388504\n",
      "Iteration 718, loss = 0.67387023\n",
      "Iteration 719, loss = 0.67385538\n",
      "Iteration 720, loss = 0.67384051\n",
      "Iteration 721, loss = 0.67382561\n",
      "Iteration 722, loss = 0.67381067\n",
      "Iteration 723, loss = 0.67379571\n",
      "Iteration 724, loss = 0.67378072\n",
      "Iteration 725, loss = 0.67376569\n",
      "Iteration 726, loss = 0.67375064\n",
      "Iteration 727, loss = 0.67373555\n",
      "Iteration 728, loss = 0.67372044\n",
      "Iteration 729, loss = 0.67370529\n",
      "Iteration 730, loss = 0.67369011\n",
      "Iteration 731, loss = 0.67367490\n",
      "Iteration 732, loss = 0.67365966\n",
      "Iteration 733, loss = 0.67364439\n",
      "Iteration 734, loss = 0.67362909\n",
      "Iteration 735, loss = 0.67361375\n",
      "Iteration 736, loss = 0.67359839\n",
      "Iteration 737, loss = 0.67358299\n",
      "Iteration 738, loss = 0.67356756\n",
      "Iteration 739, loss = 0.67355210\n",
      "Iteration 740, loss = 0.67353661\n",
      "Iteration 741, loss = 0.67352109\n",
      "Iteration 742, loss = 0.67350553\n",
      "Iteration 743, loss = 0.67348994\n",
      "Iteration 744, loss = 0.67347432\n",
      "Iteration 745, loss = 0.67345867\n",
      "Iteration 746, loss = 0.67344298\n",
      "Iteration 747, loss = 0.67342726\n",
      "Iteration 748, loss = 0.67341151\n",
      "Iteration 749, loss = 0.67339573\n",
      "Iteration 750, loss = 0.67337991\n",
      "Iteration 751, loss = 0.67336406\n",
      "Iteration 752, loss = 0.67334818\n",
      "Iteration 753, loss = 0.67333227\n",
      "Iteration 754, loss = 0.67331632\n",
      "Iteration 755, loss = 0.67330034\n",
      "Iteration 756, loss = 0.67328432\n",
      "Iteration 757, loss = 0.67326827\n",
      "Iteration 758, loss = 0.67325219\n",
      "Iteration 759, loss = 0.67323607\n",
      "Iteration 760, loss = 0.67321992\n",
      "Iteration 761, loss = 0.67320374\n",
      "Iteration 762, loss = 0.67318752\n",
      "Iteration 763, loss = 0.67317127\n",
      "Iteration 764, loss = 0.67315499\n",
      "Iteration 765, loss = 0.67313867\n",
      "Iteration 766, loss = 0.67312231\n",
      "Iteration 767, loss = 0.67310592\n",
      "Iteration 768, loss = 0.67308950\n",
      "Iteration 769, loss = 0.67307304\n",
      "Iteration 770, loss = 0.67305655\n",
      "Iteration 771, loss = 0.67304002\n",
      "Iteration 772, loss = 0.67302346\n",
      "Iteration 773, loss = 0.67300686\n",
      "Iteration 774, loss = 0.67299023\n",
      "Iteration 775, loss = 0.67297356\n",
      "Iteration 776, loss = 0.67295685\n",
      "Iteration 777, loss = 0.67294011\n",
      "Iteration 778, loss = 0.67292334\n",
      "Iteration 779, loss = 0.67290653\n",
      "Iteration 780, loss = 0.67288968\n",
      "Iteration 781, loss = 0.67287280\n",
      "Iteration 782, loss = 0.67285588\n",
      "Iteration 783, loss = 0.67283893\n",
      "Iteration 784, loss = 0.67282194\n",
      "Iteration 785, loss = 0.67280491\n",
      "Iteration 786, loss = 0.67278785\n",
      "Iteration 787, loss = 0.67277075\n",
      "Iteration 788, loss = 0.67275361\n",
      "Iteration 789, loss = 0.67273644\n",
      "Iteration 790, loss = 0.67271923\n",
      "Iteration 791, loss = 0.67270198\n",
      "Iteration 792, loss = 0.67268470\n",
      "Iteration 793, loss = 0.67266738\n",
      "Iteration 794, loss = 0.67265002\n",
      "Iteration 795, loss = 0.67263262\n",
      "Iteration 796, loss = 0.67261519\n",
      "Iteration 797, loss = 0.67259772\n",
      "Iteration 798, loss = 0.67258021\n",
      "Iteration 799, loss = 0.67256266\n",
      "Iteration 800, loss = 0.67254508\n",
      "Iteration 801, loss = 0.67252746\n",
      "Iteration 802, loss = 0.67250980\n",
      "Iteration 803, loss = 0.67249210\n",
      "Iteration 804, loss = 0.67247436\n",
      "Iteration 805, loss = 0.67245659\n",
      "Iteration 806, loss = 0.67243877\n",
      "Iteration 807, loss = 0.67242092\n",
      "Iteration 808, loss = 0.67240303\n",
      "Iteration 809, loss = 0.67238510\n",
      "Iteration 810, loss = 0.67236713\n",
      "Iteration 811, loss = 0.67234912\n",
      "Iteration 812, loss = 0.67233108\n",
      "Iteration 813, loss = 0.67231299\n",
      "Iteration 814, loss = 0.67229486\n",
      "Iteration 815, loss = 0.67227670\n",
      "Iteration 816, loss = 0.67225849\n",
      "Iteration 817, loss = 0.67224025\n",
      "Iteration 818, loss = 0.67222196\n",
      "Iteration 819, loss = 0.67220364\n",
      "Iteration 820, loss = 0.67218527\n",
      "Iteration 821, loss = 0.67216687\n",
      "Iteration 822, loss = 0.67214842\n",
      "Iteration 823, loss = 0.67212994\n",
      "Iteration 824, loss = 0.67211141\n",
      "Iteration 825, loss = 0.67209284\n",
      "Iteration 826, loss = 0.67207423\n",
      "Iteration 827, loss = 0.67205559\n",
      "Iteration 828, loss = 0.67203690\n",
      "Iteration 829, loss = 0.67201817\n",
      "Iteration 830, loss = 0.67199939\n",
      "Iteration 831, loss = 0.67198058\n",
      "Iteration 832, loss = 0.67196172\n",
      "Iteration 833, loss = 0.67194283\n",
      "Iteration 834, loss = 0.67192389\n",
      "Iteration 835, loss = 0.67190491\n",
      "Iteration 836, loss = 0.67188589\n",
      "Iteration 837, loss = 0.67186682\n",
      "Iteration 838, loss = 0.67184772\n",
      "Iteration 839, loss = 0.67182857\n",
      "Iteration 840, loss = 0.67180938\n",
      "Iteration 841, loss = 0.67179015\n",
      "Iteration 842, loss = 0.67177087\n",
      "Iteration 843, loss = 0.67175155\n",
      "Iteration 844, loss = 0.67173219\n",
      "Iteration 845, loss = 0.67171278\n",
      "Iteration 846, loss = 0.67169334\n",
      "Iteration 847, loss = 0.67167385\n",
      "Iteration 848, loss = 0.67165431\n",
      "Iteration 849, loss = 0.67163473\n",
      "Iteration 850, loss = 0.67161511\n",
      "Iteration 851, loss = 0.67159545\n",
      "Iteration 852, loss = 0.67157574\n",
      "Iteration 853, loss = 0.67155598\n",
      "Iteration 854, loss = 0.67153619\n",
      "Iteration 855, loss = 0.67151635\n",
      "Iteration 856, loss = 0.67149646\n",
      "Iteration 857, loss = 0.67147653\n",
      "Iteration 858, loss = 0.67145656\n",
      "Iteration 859, loss = 0.67143654\n",
      "Iteration 860, loss = 0.67141647\n",
      "Iteration 861, loss = 0.67139636\n",
      "Iteration 862, loss = 0.67137621\n",
      "Iteration 863, loss = 0.67135601\n",
      "Iteration 864, loss = 0.67133577\n",
      "Iteration 865, loss = 0.67131548\n",
      "Iteration 866, loss = 0.67129514\n",
      "Iteration 867, loss = 0.67127476\n",
      "Iteration 868, loss = 0.67125433\n",
      "Iteration 869, loss = 0.67123386\n",
      "Iteration 870, loss = 0.67121334\n",
      "Iteration 871, loss = 0.67119277\n",
      "Iteration 872, loss = 0.67117216\n",
      "Iteration 873, loss = 0.67115150\n",
      "Iteration 874, loss = 0.67113080\n",
      "Iteration 875, loss = 0.67111005\n",
      "Iteration 876, loss = 0.67108925\n",
      "Iteration 877, loss = 0.67106840\n",
      "Iteration 878, loss = 0.67104751\n",
      "Iteration 879, loss = 0.67102657\n",
      "Iteration 880, loss = 0.67100559\n",
      "Iteration 881, loss = 0.67098455\n",
      "Iteration 882, loss = 0.67096347\n",
      "Iteration 883, loss = 0.67094234\n",
      "Iteration 884, loss = 0.67092116\n",
      "Iteration 885, loss = 0.67089994\n",
      "Iteration 886, loss = 0.67087866\n",
      "Iteration 887, loss = 0.67085734\n",
      "Iteration 888, loss = 0.67083597\n",
      "Iteration 889, loss = 0.67081455\n",
      "Iteration 890, loss = 0.67079308\n",
      "Iteration 891, loss = 0.67077157\n",
      "Iteration 892, loss = 0.67075000\n",
      "Iteration 893, loss = 0.67072839\n",
      "Iteration 894, loss = 0.67070672\n",
      "Iteration 895, loss = 0.67068501\n",
      "Iteration 896, loss = 0.67066325\n",
      "Iteration 897, loss = 0.67064144\n",
      "Iteration 898, loss = 0.67061957\n",
      "Iteration 899, loss = 0.67059766\n",
      "Iteration 900, loss = 0.67057570\n",
      "Iteration 901, loss = 0.67055369\n",
      "Iteration 902, loss = 0.67053163\n",
      "Iteration 903, loss = 0.67050951\n",
      "Iteration 904, loss = 0.67048735\n",
      "Iteration 905, loss = 0.67046514\n",
      "Iteration 906, loss = 0.67044287\n",
      "Iteration 907, loss = 0.67042056\n",
      "Iteration 908, loss = 0.67039819\n",
      "Iteration 909, loss = 0.67037577\n",
      "Iteration 910, loss = 0.67035330\n",
      "Iteration 911, loss = 0.67033078\n",
      "Iteration 912, loss = 0.67030821\n",
      "Iteration 913, loss = 0.67028558\n",
      "Iteration 914, loss = 0.67026291\n",
      "Iteration 915, loss = 0.67024018\n",
      "Iteration 916, loss = 0.67021740\n",
      "Iteration 917, loss = 0.67019457\n",
      "Iteration 918, loss = 0.67017168\n",
      "Iteration 919, loss = 0.67014874\n",
      "Iteration 920, loss = 0.67012575\n",
      "Iteration 921, loss = 0.67010271\n",
      "Iteration 922, loss = 0.67007961\n",
      "Iteration 923, loss = 0.67005646\n",
      "Iteration 924, loss = 0.67003326\n",
      "Iteration 925, loss = 0.67001000\n",
      "Iteration 926, loss = 0.66998669\n",
      "Iteration 927, loss = 0.66996332\n",
      "Iteration 928, loss = 0.66993990\n",
      "Iteration 929, loss = 0.66991643\n",
      "Iteration 930, loss = 0.66989290\n",
      "Iteration 931, loss = 0.66986932\n",
      "Iteration 932, loss = 0.66984569\n",
      "Iteration 933, loss = 0.66982200\n",
      "Iteration 934, loss = 0.66979825\n",
      "Iteration 935, loss = 0.66977445\n",
      "Iteration 936, loss = 0.66975059\n",
      "Iteration 937, loss = 0.66972668\n",
      "Iteration 938, loss = 0.66970272\n",
      "Iteration 939, loss = 0.66967869\n",
      "Iteration 940, loss = 0.66965462\n",
      "Iteration 941, loss = 0.66963048\n",
      "Iteration 942, loss = 0.66960629\n",
      "Iteration 943, loss = 0.66958205\n",
      "Iteration 944, loss = 0.66955775\n",
      "Iteration 945, loss = 0.66953339\n",
      "Iteration 946, loss = 0.66950897\n",
      "Iteration 947, loss = 0.66948450\n",
      "Iteration 948, loss = 0.66945997\n",
      "Iteration 949, loss = 0.66943538\n",
      "Iteration 950, loss = 0.66941074\n",
      "Iteration 951, loss = 0.66938604\n",
      "Iteration 952, loss = 0.66936128\n",
      "Iteration 953, loss = 0.66933647\n",
      "Iteration 954, loss = 0.66931159\n",
      "Iteration 955, loss = 0.66928666\n",
      "Iteration 956, loss = 0.66926167\n",
      "Iteration 957, loss = 0.66923662\n",
      "Iteration 958, loss = 0.66921151\n",
      "Iteration 959, loss = 0.66918635\n",
      "Iteration 960, loss = 0.66916112\n",
      "Iteration 961, loss = 0.66913584\n",
      "Iteration 962, loss = 0.66911050\n",
      "Iteration 963, loss = 0.66908510\n",
      "Iteration 964, loss = 0.66905964\n",
      "Iteration 965, loss = 0.66903412\n",
      "Iteration 966, loss = 0.66900853\n",
      "Iteration 967, loss = 0.66898289\n",
      "Iteration 968, loss = 0.66895719\n",
      "Iteration 969, loss = 0.66893143\n",
      "Iteration 970, loss = 0.66890561\n",
      "Iteration 971, loss = 0.66887973\n",
      "Iteration 972, loss = 0.66885379\n",
      "Iteration 973, loss = 0.66882778\n",
      "Iteration 974, loss = 0.66880172\n",
      "Iteration 975, loss = 0.66877559\n",
      "Iteration 976, loss = 0.66874941\n",
      "Iteration 977, loss = 0.66872316\n",
      "Iteration 978, loss = 0.66869685\n",
      "Iteration 979, loss = 0.66867048\n",
      "Iteration 980, loss = 0.66864404\n",
      "Iteration 981, loss = 0.66861755\n",
      "Iteration 982, loss = 0.66859099\n",
      "Iteration 983, loss = 0.66856437\n",
      "Iteration 984, loss = 0.66853768\n",
      "Iteration 985, loss = 0.66851093\n",
      "Iteration 986, loss = 0.66848412\n",
      "Iteration 987, loss = 0.66845725\n",
      "Iteration 988, loss = 0.66843032\n",
      "Iteration 989, loss = 0.66840332\n",
      "Iteration 990, loss = 0.66837625\n",
      "Iteration 991, loss = 0.66834912\n",
      "Iteration 992, loss = 0.66832193\n",
      "Iteration 993, loss = 0.66829468\n",
      "Iteration 994, loss = 0.66826736\n",
      "Iteration 995, loss = 0.66823997\n",
      "Iteration 996, loss = 0.66821252\n",
      "Iteration 997, loss = 0.66818501\n",
      "Iteration 998, loss = 0.66815743\n",
      "Iteration 999, loss = 0.66812978\n",
      "Iteration 1000, loss = 0.66810207\n",
      "Iteration 1001, loss = 0.66807430\n",
      "Iteration 1002, loss = 0.66804646\n",
      "Iteration 1003, loss = 0.66801855\n",
      "Iteration 1004, loss = 0.66799058\n",
      "Iteration 1005, loss = 0.66796254\n",
      "Iteration 1006, loss = 0.66793443\n",
      "Iteration 1007, loss = 0.66790626\n",
      "Iteration 1008, loss = 0.66787802\n",
      "Iteration 1009, loss = 0.66784971\n",
      "Iteration 1010, loss = 0.66782134\n",
      "Iteration 1011, loss = 0.66779289\n",
      "Iteration 1012, loss = 0.66776438\n",
      "Iteration 1013, loss = 0.66773581\n",
      "Iteration 1014, loss = 0.66770716\n",
      "Iteration 1015, loss = 0.66767845\n",
      "Iteration 1016, loss = 0.66764967\n",
      "Iteration 1017, loss = 0.66762082\n",
      "Iteration 1018, loss = 0.66759190\n",
      "Iteration 1019, loss = 0.66756292\n",
      "Iteration 1020, loss = 0.66753386\n",
      "Iteration 1021, loss = 0.66750474\n",
      "Iteration 1022, loss = 0.66747554\n",
      "Iteration 1023, loss = 0.66744628\n",
      "Iteration 1024, loss = 0.66741694\n",
      "Iteration 1025, loss = 0.66738754\n",
      "Iteration 1026, loss = 0.66735807\n",
      "Iteration 1027, loss = 0.66732852\n",
      "Iteration 1028, loss = 0.66729891\n",
      "Iteration 1029, loss = 0.66726922\n",
      "Iteration 1030, loss = 0.66723947\n",
      "Iteration 1031, loss = 0.66720964\n",
      "Iteration 1032, loss = 0.66717974\n",
      "Iteration 1033, loss = 0.66714978\n",
      "Iteration 1034, loss = 0.66711973\n",
      "Iteration 1035, loss = 0.66708962\n",
      "Iteration 1036, loss = 0.66705944\n",
      "Iteration 1037, loss = 0.66702918\n",
      "Iteration 1038, loss = 0.66699885\n",
      "Iteration 1039, loss = 0.66696845\n",
      "Iteration 1040, loss = 0.66693798\n",
      "Iteration 1041, loss = 0.66690743\n",
      "Iteration 1042, loss = 0.66687681\n",
      "Iteration 1043, loss = 0.66684611\n",
      "Iteration 1044, loss = 0.66681535\n",
      "Iteration 1045, loss = 0.66678451\n",
      "Iteration 1046, loss = 0.66675359\n",
      "Iteration 1047, loss = 0.66672260\n",
      "Iteration 1048, loss = 0.66669154\n",
      "Iteration 1049, loss = 0.66666040\n",
      "Iteration 1050, loss = 0.66662919\n",
      "Iteration 1051, loss = 0.66659790\n",
      "Iteration 1052, loss = 0.66656654\n",
      "Iteration 1053, loss = 0.66653510\n",
      "Iteration 1054, loss = 0.66650359\n",
      "Iteration 1055, loss = 0.66647200\n",
      "Iteration 1056, loss = 0.66644034\n",
      "Iteration 1057, loss = 0.66640860\n",
      "Iteration 1058, loss = 0.66637678\n",
      "Iteration 1059, loss = 0.66634489\n",
      "Iteration 1060, loss = 0.66631292\n",
      "Iteration 1061, loss = 0.66628087\n",
      "Iteration 1062, loss = 0.66624875\n",
      "Iteration 1063, loss = 0.66621655\n",
      "Iteration 1064, loss = 0.66618427\n",
      "Iteration 1065, loss = 0.66615191\n",
      "Iteration 1066, loss = 0.66611948\n",
      "Iteration 1067, loss = 0.66608697\n",
      "Iteration 1068, loss = 0.66605438\n",
      "Iteration 1069, loss = 0.66602171\n",
      "Iteration 1070, loss = 0.66598896\n",
      "Iteration 1071, loss = 0.66595614\n",
      "Iteration 1072, loss = 0.66592323\n",
      "Iteration 1073, loss = 0.66589025\n",
      "Iteration 1074, loss = 0.66585718\n",
      "Iteration 1075, loss = 0.66582404\n",
      "Iteration 1076, loss = 0.66579082\n",
      "Iteration 1077, loss = 0.66575751\n",
      "Iteration 1078, loss = 0.66572413\n",
      "Iteration 1079, loss = 0.66569066\n",
      "Iteration 1080, loss = 0.66565712\n",
      "Iteration 1081, loss = 0.66562349\n",
      "Iteration 1082, loss = 0.66558978\n",
      "Iteration 1083, loss = 0.66555599\n",
      "Iteration 1084, loss = 0.66552212\n",
      "Iteration 1085, loss = 0.66548817\n",
      "Iteration 1086, loss = 0.66545414\n",
      "Iteration 1087, loss = 0.66542002\n",
      "Iteration 1088, loss = 0.66538582\n",
      "Iteration 1089, loss = 0.66535154\n",
      "Iteration 1090, loss = 0.66531717\n",
      "Iteration 1091, loss = 0.66528272\n",
      "Iteration 1092, loss = 0.66524819\n",
      "Iteration 1093, loss = 0.66521358\n",
      "Iteration 1094, loss = 0.66517888\n",
      "Iteration 1095, loss = 0.66514410\n",
      "Iteration 1096, loss = 0.66510923\n",
      "Iteration 1097, loss = 0.66507428\n",
      "Iteration 1098, loss = 0.66503924\n",
      "Iteration 1099, loss = 0.66500412\n",
      "Iteration 1100, loss = 0.66496892\n",
      "Iteration 1101, loss = 0.66493362\n",
      "Iteration 1102, loss = 0.66489825\n",
      "Iteration 1103, loss = 0.66486278\n",
      "Iteration 1104, loss = 0.66482724\n",
      "Iteration 1105, loss = 0.66479160\n",
      "Iteration 1106, loss = 0.66475588\n",
      "Iteration 1107, loss = 0.66472007\n",
      "Iteration 1108, loss = 0.66468418\n",
      "Iteration 1109, loss = 0.66464819\n",
      "Iteration 1110, loss = 0.66461213\n",
      "Iteration 1111, loss = 0.66457597\n",
      "Iteration 1112, loss = 0.66453972\n",
      "Iteration 1113, loss = 0.66450339\n",
      "Iteration 1114, loss = 0.66446697\n",
      "Iteration 1115, loss = 0.66443046\n",
      "Iteration 1116, loss = 0.66439386\n",
      "Iteration 1117, loss = 0.66435717\n",
      "Iteration 1118, loss = 0.66432040\n",
      "Iteration 1119, loss = 0.66428353\n",
      "Iteration 1120, loss = 0.66424658\n",
      "Iteration 1121, loss = 0.66420953\n",
      "Iteration 1122, loss = 0.66417240\n",
      "Iteration 1123, loss = 0.66413517\n",
      "Iteration 1124, loss = 0.66409785\n",
      "Iteration 1125, loss = 0.66406045\n",
      "Iteration 1126, loss = 0.66402295\n",
      "Iteration 1127, loss = 0.66398536\n",
      "Iteration 1128, loss = 0.66394768\n",
      "Iteration 1129, loss = 0.66390991\n",
      "Iteration 1130, loss = 0.66387204\n",
      "Iteration 1131, loss = 0.66383408\n",
      "Iteration 1132, loss = 0.66379603\n",
      "Iteration 1133, loss = 0.66375789\n",
      "Iteration 1134, loss = 0.66371966\n",
      "Iteration 1135, loss = 0.66368133\n",
      "Iteration 1136, loss = 0.66364291\n",
      "Iteration 1137, loss = 0.66360439\n",
      "Iteration 1138, loss = 0.66356578\n",
      "Iteration 1139, loss = 0.66352708\n",
      "Iteration 1140, loss = 0.66348828\n",
      "Iteration 1141, loss = 0.66344939\n",
      "Iteration 1142, loss = 0.66341040\n",
      "Iteration 1143, loss = 0.66337132\n",
      "Iteration 1144, loss = 0.66333214\n",
      "Iteration 1145, loss = 0.66329286\n",
      "Iteration 1146, loss = 0.66325349\n",
      "Iteration 1147, loss = 0.66321403\n",
      "Iteration 1148, loss = 0.66317446\n",
      "Iteration 1149, loss = 0.66313481\n",
      "Iteration 1150, loss = 0.66309505\n",
      "Iteration 1151, loss = 0.66305520\n",
      "Iteration 1152, loss = 0.66301525\n",
      "Iteration 1153, loss = 0.66297520\n",
      "Iteration 1154, loss = 0.66293505\n",
      "Iteration 1155, loss = 0.66289481\n",
      "Iteration 1156, loss = 0.66285447\n",
      "Iteration 1157, loss = 0.66281402\n",
      "Iteration 1158, loss = 0.66277348\n",
      "Iteration 1159, loss = 0.66273284\n",
      "Iteration 1160, loss = 0.66269211\n",
      "Iteration 1161, loss = 0.66265127\n",
      "Iteration 1162, loss = 0.66261033\n",
      "Iteration 1163, loss = 0.66256929\n",
      "Iteration 1164, loss = 0.66252815\n",
      "Iteration 1165, loss = 0.66248691\n",
      "Iteration 1166, loss = 0.66244557\n",
      "Iteration 1167, loss = 0.66240413\n",
      "Iteration 1168, loss = 0.66236258\n",
      "Iteration 1169, loss = 0.66232094\n",
      "Iteration 1170, loss = 0.66227919\n",
      "Iteration 1171, loss = 0.66223734\n",
      "Iteration 1172, loss = 0.66219539\n",
      "Iteration 1173, loss = 0.66215333\n",
      "Iteration 1174, loss = 0.66211118\n",
      "Iteration 1175, loss = 0.66206891\n",
      "Iteration 1176, loss = 0.66202655\n",
      "Iteration 1177, loss = 0.66198408\n",
      "Iteration 1178, loss = 0.66194151\n",
      "Iteration 1179, loss = 0.66189883\n",
      "Iteration 1180, loss = 0.66185605\n",
      "Iteration 1181, loss = 0.66181316\n",
      "Iteration 1182, loss = 0.66177017\n",
      "Iteration 1183, loss = 0.66172707\n",
      "Iteration 1184, loss = 0.66168386\n",
      "Iteration 1185, loss = 0.66164055\n",
      "Iteration 1186, loss = 0.66159714\n",
      "Iteration 1187, loss = 0.66155362\n",
      "Iteration 1188, loss = 0.66150999\n",
      "Iteration 1189, loss = 0.66146625\n",
      "Iteration 1190, loss = 0.66142241\n",
      "Iteration 1191, loss = 0.66137845\n",
      "Iteration 1192, loss = 0.66133439\n",
      "Iteration 1193, loss = 0.66129023\n",
      "Iteration 1194, loss = 0.66124595\n",
      "Iteration 1195, loss = 0.66120156\n",
      "Iteration 1196, loss = 0.66115707\n",
      "Iteration 1197, loss = 0.66111246\n",
      "Iteration 1198, loss = 0.66106775\n",
      "Iteration 1199, loss = 0.66102293\n",
      "Iteration 1200, loss = 0.66097799\n",
      "Iteration 1201, loss = 0.66093295\n",
      "Iteration 1202, loss = 0.66088779\n",
      "Iteration 1203, loss = 0.66084253\n",
      "Iteration 1204, loss = 0.66079715\n",
      "Iteration 1205, loss = 0.66075166\n",
      "Iteration 1206, loss = 0.66070606\n",
      "Iteration 1207, loss = 0.66066035\n",
      "Iteration 1208, loss = 0.66061452\n",
      "Iteration 1209, loss = 0.66056858\n",
      "Iteration 1210, loss = 0.66052253\n",
      "Iteration 1211, loss = 0.66047637\n",
      "Iteration 1212, loss = 0.66043009\n",
      "Iteration 1213, loss = 0.66038370\n",
      "Iteration 1214, loss = 0.66033719\n",
      "Iteration 1215, loss = 0.66029057\n",
      "Iteration 1216, loss = 0.66024383\n",
      "Iteration 1217, loss = 0.66019698\n",
      "Iteration 1218, loss = 0.66015002\n",
      "Iteration 1219, loss = 0.66010293\n",
      "Iteration 1220, loss = 0.66005574\n",
      "Iteration 1221, loss = 0.66000842\n",
      "Iteration 1222, loss = 0.65996099\n",
      "Iteration 1223, loss = 0.65991344\n",
      "Iteration 1224, loss = 0.65986578\n",
      "Iteration 1225, loss = 0.65981800\n",
      "Iteration 1226, loss = 0.65977010\n",
      "Iteration 1227, loss = 0.65972208\n",
      "Iteration 1228, loss = 0.65967394\n",
      "Iteration 1229, loss = 0.65962569\n",
      "Iteration 1230, loss = 0.65957731\n",
      "Iteration 1231, loss = 0.65952882\n",
      "Iteration 1232, loss = 0.65948020\n",
      "Iteration 1233, loss = 0.65943147\n",
      "Iteration 1234, loss = 0.65938262\n",
      "Iteration 1235, loss = 0.65933364\n",
      "Iteration 1236, loss = 0.65928455\n",
      "Iteration 1237, loss = 0.65923533\n",
      "Iteration 1238, loss = 0.65918600\n",
      "Iteration 1239, loss = 0.65913654\n",
      "Iteration 1240, loss = 0.65908696\n",
      "Iteration 1241, loss = 0.65903725\n",
      "Iteration 1242, loss = 0.65898743\n",
      "Iteration 1243, loss = 0.65893748\n",
      "Iteration 1244, loss = 0.65888741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1245, loss = 0.65883721\n",
      "Iteration 1246, loss = 0.65878689\n",
      "Iteration 1247, loss = 0.65873645\n",
      "Iteration 1248, loss = 0.65868588\n",
      "Iteration 1249, loss = 0.65863519\n",
      "Iteration 1250, loss = 0.65858437\n",
      "Iteration 1251, loss = 0.65853342\n",
      "Iteration 1252, loss = 0.65848235\n",
      "Iteration 1253, loss = 0.65843116\n",
      "Iteration 1254, loss = 0.65837984\n",
      "Iteration 1255, loss = 0.65832839\n",
      "Iteration 1256, loss = 0.65827681\n",
      "Iteration 1257, loss = 0.65822511\n",
      "Iteration 1258, loss = 0.65817328\n",
      "Iteration 1259, loss = 0.65812132\n",
      "Iteration 1260, loss = 0.65806923\n",
      "Iteration 1261, loss = 0.65801701\n",
      "Iteration 1262, loss = 0.65796467\n",
      "Iteration 1263, loss = 0.65791219\n",
      "Iteration 1264, loss = 0.65785959\n",
      "Iteration 1265, loss = 0.65780686\n",
      "Iteration 1266, loss = 0.65775399\n",
      "Iteration 1267, loss = 0.65770100\n",
      "Iteration 1268, loss = 0.65764787\n",
      "Iteration 1269, loss = 0.65759461\n",
      "Iteration 1270, loss = 0.65754122\n",
      "Iteration 1271, loss = 0.65748770\n",
      "Iteration 1272, loss = 0.65743405\n",
      "Iteration 1273, loss = 0.65738026\n",
      "Iteration 1274, loss = 0.65732634\n",
      "Iteration 1275, loss = 0.65727229\n",
      "Iteration 1276, loss = 0.65721810\n",
      "Iteration 1277, loss = 0.65716378\n",
      "Iteration 1278, loss = 0.65710933\n",
      "Iteration 1279, loss = 0.65705474\n",
      "Iteration 1280, loss = 0.65700002\n",
      "Iteration 1281, loss = 0.65694516\n",
      "Iteration 1282, loss = 0.65689016\n",
      "Iteration 1283, loss = 0.65683503\n",
      "Iteration 1284, loss = 0.65677977\n",
      "Iteration 1285, loss = 0.65672436\n",
      "Iteration 1286, loss = 0.65666882\n",
      "Iteration 1287, loss = 0.65661314\n",
      "Iteration 1288, loss = 0.65655733\n",
      "Iteration 1289, loss = 0.65650137\n",
      "Iteration 1290, loss = 0.65644528\n",
      "Iteration 1291, loss = 0.65638905\n",
      "Iteration 1292, loss = 0.65633268\n",
      "Iteration 1293, loss = 0.65627617\n",
      "Iteration 1294, loss = 0.65621952\n",
      "Iteration 1295, loss = 0.65616273\n",
      "Iteration 1296, loss = 0.65610580\n",
      "Iteration 1297, loss = 0.65604873\n",
      "Iteration 1298, loss = 0.65599152\n",
      "Iteration 1299, loss = 0.65593417\n",
      "Iteration 1300, loss = 0.65587667\n",
      "Iteration 1301, loss = 0.65581903\n",
      "Iteration 1302, loss = 0.65576125\n",
      "Iteration 1303, loss = 0.65570333\n",
      "Iteration 1304, loss = 0.65564527\n",
      "Iteration 1305, loss = 0.65558706\n",
      "Iteration 1306, loss = 0.65552870\n",
      "Iteration 1307, loss = 0.65547020\n",
      "Iteration 1308, loss = 0.65541156\n",
      "Iteration 1309, loss = 0.65535277\n",
      "Iteration 1310, loss = 0.65529384\n",
      "Iteration 1311, loss = 0.65523476\n",
      "Iteration 1312, loss = 0.65517554\n",
      "Iteration 1313, loss = 0.65511616\n",
      "Iteration 1314, loss = 0.65505665\n",
      "Iteration 1315, loss = 0.65499698\n",
      "Iteration 1316, loss = 0.65493717\n",
      "Iteration 1317, loss = 0.65487720\n",
      "Iteration 1318, loss = 0.65481710\n",
      "Iteration 1319, loss = 0.65475684\n",
      "Iteration 1320, loss = 0.65469643\n",
      "Iteration 1321, loss = 0.65463587\n",
      "Iteration 1322, loss = 0.65457517\n",
      "Iteration 1323, loss = 0.65451431\n",
      "Iteration 1324, loss = 0.65445330\n",
      "Iteration 1325, loss = 0.65439214\n",
      "Iteration 1326, loss = 0.65433084\n",
      "Iteration 1327, loss = 0.65426937\n",
      "Iteration 1328, loss = 0.65420776\n",
      "Iteration 1329, loss = 0.65414600\n",
      "Iteration 1330, loss = 0.65408408\n",
      "Iteration 1331, loss = 0.65402201\n",
      "Iteration 1332, loss = 0.65395978\n",
      "Iteration 1333, loss = 0.65389740\n",
      "Iteration 1334, loss = 0.65383487\n",
      "Iteration 1335, loss = 0.65377218\n",
      "Iteration 1336, loss = 0.65370934\n",
      "Iteration 1337, loss = 0.65364634\n",
      "Iteration 1338, loss = 0.65358319\n",
      "Iteration 1339, loss = 0.65351988\n",
      "Iteration 1340, loss = 0.65345642\n",
      "Iteration 1341, loss = 0.65339279\n",
      "Iteration 1342, loss = 0.65332901\n",
      "Iteration 1343, loss = 0.65326508\n",
      "Iteration 1344, loss = 0.65320098\n",
      "Iteration 1345, loss = 0.65313673\n",
      "Iteration 1346, loss = 0.65307231\n",
      "Iteration 1347, loss = 0.65300774\n",
      "Iteration 1348, loss = 0.65294301\n",
      "Iteration 1349, loss = 0.65287812\n",
      "Iteration 1350, loss = 0.65281307\n",
      "Iteration 1351, loss = 0.65274785\n",
      "Iteration 1352, loss = 0.65268248\n",
      "Iteration 1353, loss = 0.65261695\n",
      "Iteration 1354, loss = 0.65255125\n",
      "Iteration 1355, loss = 0.65248539\n",
      "Iteration 1356, loss = 0.65241937\n",
      "Iteration 1357, loss = 0.65235318\n",
      "Iteration 1358, loss = 0.65228684\n",
      "Iteration 1359, loss = 0.65222032\n",
      "Iteration 1360, loss = 0.65215365\n",
      "Iteration 1361, loss = 0.65208681\n",
      "Iteration 1362, loss = 0.65201980\n",
      "Iteration 1363, loss = 0.65195263\n",
      "Iteration 1364, loss = 0.65188529\n",
      "Iteration 1365, loss = 0.65181779\n",
      "Iteration 1366, loss = 0.65175012\n",
      "Iteration 1367, loss = 0.65168228\n",
      "Iteration 1368, loss = 0.65161428\n",
      "Iteration 1369, loss = 0.65154611\n",
      "Iteration 1370, loss = 0.65147777\n",
      "Iteration 1371, loss = 0.65140926\n",
      "Iteration 1372, loss = 0.65134058\n",
      "Iteration 1373, loss = 0.65127174\n",
      "Iteration 1374, loss = 0.65120272\n",
      "Iteration 1375, loss = 0.65113353\n",
      "Iteration 1376, loss = 0.65106418\n",
      "Iteration 1377, loss = 0.65099465\n",
      "Iteration 1378, loss = 0.65092495\n",
      "Iteration 1379, loss = 0.65085508\n",
      "Iteration 1380, loss = 0.65078503\n",
      "Iteration 1381, loss = 0.65071482\n",
      "Iteration 1382, loss = 0.65064443\n",
      "Iteration 1383, loss = 0.65057387\n",
      "Iteration 1384, loss = 0.65050313\n",
      "Iteration 1385, loss = 0.65043222\n",
      "Iteration 1386, loss = 0.65036114\n",
      "Iteration 1387, loss = 0.65028988\n",
      "Iteration 1388, loss = 0.65021844\n",
      "Iteration 1389, loss = 0.65014683\n",
      "Iteration 1390, loss = 0.65007504\n",
      "Iteration 1391, loss = 0.65000308\n",
      "Iteration 1392, loss = 0.64993094\n",
      "Iteration 1393, loss = 0.64985862\n",
      "Iteration 1394, loss = 0.64978612\n",
      "Iteration 1395, loss = 0.64971345\n",
      "Iteration 1396, loss = 0.64964059\n",
      "Iteration 1397, loss = 0.64956756\n",
      "Iteration 1398, loss = 0.64949435\n",
      "Iteration 1399, loss = 0.64942096\n",
      "Iteration 1400, loss = 0.64934739\n",
      "Iteration 1401, loss = 0.64927363\n",
      "Iteration 1402, loss = 0.64919970\n",
      "Iteration 1403, loss = 0.64912558\n",
      "Iteration 1404, loss = 0.64905129\n",
      "Iteration 1405, loss = 0.64897681\n",
      "Iteration 1406, loss = 0.64890214\n",
      "Iteration 1407, loss = 0.64882730\n",
      "Iteration 1408, loss = 0.64875227\n",
      "Iteration 1409, loss = 0.64867705\n",
      "Iteration 1410, loss = 0.64860165\n",
      "Iteration 1411, loss = 0.64852607\n",
      "Iteration 1412, loss = 0.64845030\n",
      "Iteration 1413, loss = 0.64837435\n",
      "Iteration 1414, loss = 0.64829820\n",
      "Iteration 1415, loss = 0.64822188\n",
      "Iteration 1416, loss = 0.64814536\n",
      "Iteration 1417, loss = 0.64806866\n",
      "Iteration 1418, loss = 0.64799177\n",
      "Iteration 1419, loss = 0.64791469\n",
      "Iteration 1420, loss = 0.64783743\n",
      "Iteration 1421, loss = 0.64775997\n",
      "Iteration 1422, loss = 0.64768232\n",
      "Iteration 1423, loss = 0.64760449\n",
      "Iteration 1424, loss = 0.64752646\n",
      "Iteration 1425, loss = 0.64744824\n",
      "Iteration 1426, loss = 0.64736984\n",
      "Iteration 1427, loss = 0.64729124\n",
      "Iteration 1428, loss = 0.64721244\n",
      "Iteration 1429, loss = 0.64713346\n",
      "Iteration 1430, loss = 0.64705428\n",
      "Iteration 1431, loss = 0.64697491\n",
      "Iteration 1432, loss = 0.64689534\n",
      "Iteration 1433, loss = 0.64681558\n",
      "Iteration 1434, loss = 0.64673563\n",
      "Iteration 1435, loss = 0.64665548\n",
      "Iteration 1436, loss = 0.64657514\n",
      "Iteration 1437, loss = 0.64649459\n",
      "Iteration 1438, loss = 0.64641386\n",
      "Iteration 1439, loss = 0.64633292\n",
      "Iteration 1440, loss = 0.64625179\n",
      "Iteration 1441, loss = 0.64617046\n",
      "Iteration 1442, loss = 0.64608893\n",
      "Iteration 1443, loss = 0.64600721\n",
      "Iteration 1444, loss = 0.64592528\n",
      "Iteration 1445, loss = 0.64584316\n",
      "Iteration 1446, loss = 0.64576083\n",
      "Iteration 1447, loss = 0.64567831\n",
      "Iteration 1448, loss = 0.64559558\n",
      "Iteration 1449, loss = 0.64551266\n",
      "Iteration 1450, loss = 0.64542953\n",
      "Iteration 1451, loss = 0.64534620\n",
      "Iteration 1452, loss = 0.64526267\n",
      "Iteration 1453, loss = 0.64517893\n",
      "Iteration 1454, loss = 0.64509499\n",
      "Iteration 1455, loss = 0.64501085\n",
      "Iteration 1456, loss = 0.64492650\n",
      "Iteration 1457, loss = 0.64484195\n",
      "Iteration 1458, loss = 0.64475719\n",
      "Iteration 1459, loss = 0.64467223\n",
      "Iteration 1460, loss = 0.64458706\n",
      "Iteration 1461, loss = 0.64450168\n",
      "Iteration 1462, loss = 0.64441610\n",
      "Iteration 1463, loss = 0.64433031\n",
      "Iteration 1464, loss = 0.64424432\n",
      "Iteration 1465, loss = 0.64415811\n",
      "Iteration 1466, loss = 0.64407170\n",
      "Iteration 1467, loss = 0.64398508\n",
      "Iteration 1468, loss = 0.64389824\n",
      "Iteration 1469, loss = 0.64381120\n",
      "Iteration 1470, loss = 0.64372395\n",
      "Iteration 1471, loss = 0.64363649\n",
      "Iteration 1472, loss = 0.64354881\n",
      "Iteration 1473, loss = 0.64346092\n",
      "Iteration 1474, loss = 0.64337283\n",
      "Iteration 1475, loss = 0.64328452\n",
      "Iteration 1476, loss = 0.64319599\n",
      "Iteration 1477, loss = 0.64310725\n",
      "Iteration 1478, loss = 0.64301830\n",
      "Iteration 1479, loss = 0.64292914\n",
      "Iteration 1480, loss = 0.64283976\n",
      "Iteration 1481, loss = 0.64275016\n",
      "Iteration 1482, loss = 0.64266035\n",
      "Iteration 1483, loss = 0.64257032\n",
      "Iteration 1484, loss = 0.64248008\n",
      "Iteration 1485, loss = 0.64238962\n",
      "Iteration 1486, loss = 0.64229894\n",
      "Iteration 1487, loss = 0.64220805\n",
      "Iteration 1488, loss = 0.64211693\n",
      "Iteration 1489, loss = 0.64202560\n",
      "Iteration 1490, loss = 0.64193405\n",
      "Iteration 1491, loss = 0.64184228\n",
      "Iteration 1492, loss = 0.64175029\n",
      "Iteration 1493, loss = 0.64165807\n",
      "Iteration 1494, loss = 0.64156564\n",
      "Iteration 1495, loss = 0.64147299\n",
      "Iteration 1496, loss = 0.64138011\n",
      "Iteration 1497, loss = 0.64128701\n",
      "Iteration 1498, loss = 0.64119369\n",
      "Iteration 1499, loss = 0.64110015\n",
      "Iteration 1500, loss = 0.64100638\n",
      "Iteration 1501, loss = 0.64091239\n",
      "Iteration 1502, loss = 0.64081817\n",
      "Iteration 1503, loss = 0.64072373\n",
      "Iteration 1504, loss = 0.64062907\n",
      "Iteration 1505, loss = 0.64053418\n",
      "Iteration 1506, loss = 0.64043906\n",
      "Iteration 1507, loss = 0.64034371\n",
      "Iteration 1508, loss = 0.64024814\n",
      "Iteration 1509, loss = 0.64015234\n",
      "Iteration 1510, loss = 0.64005632\n",
      "Iteration 1511, loss = 0.63996006\n",
      "Iteration 1512, loss = 0.63986358\n",
      "Iteration 1513, loss = 0.63976686\n",
      "Iteration 1514, loss = 0.63966992\n",
      "Iteration 1515, loss = 0.63957275\n",
      "Iteration 1516, loss = 0.63947534\n",
      "Iteration 1517, loss = 0.63937771\n",
      "Iteration 1518, loss = 0.63927984\n",
      "Iteration 1519, loss = 0.63918175\n",
      "Iteration 1520, loss = 0.63908342\n",
      "Iteration 1521, loss = 0.63898485\n",
      "Iteration 1522, loss = 0.63888606\n",
      "Iteration 1523, loss = 0.63878703\n",
      "Iteration 1524, loss = 0.63868776\n",
      "Iteration 1525, loss = 0.63858827\n",
      "Iteration 1526, loss = 0.63848853\n",
      "Iteration 1527, loss = 0.63838856\n",
      "Iteration 1528, loss = 0.63828836\n",
      "Iteration 1529, loss = 0.63818792\n",
      "Iteration 1530, loss = 0.63808724\n",
      "Iteration 1531, loss = 0.63798633\n",
      "Iteration 1532, loss = 0.63788518\n",
      "Iteration 1533, loss = 0.63778379\n",
      "Iteration 1534, loss = 0.63768216\n",
      "Iteration 1535, loss = 0.63758029\n",
      "Iteration 1536, loss = 0.63747819\n",
      "Iteration 1537, loss = 0.63737584\n",
      "Iteration 1538, loss = 0.63727326\n",
      "Iteration 1539, loss = 0.63717043\n",
      "Iteration 1540, loss = 0.63706736\n",
      "Iteration 1541, loss = 0.63696406\n",
      "Iteration 1542, loss = 0.63686051\n",
      "Iteration 1543, loss = 0.63675671\n",
      "Iteration 1544, loss = 0.63665268\n",
      "Iteration 1545, loss = 0.63654840\n",
      "Iteration 1546, loss = 0.63644388\n",
      "Iteration 1547, loss = 0.63633911\n",
      "Iteration 1548, loss = 0.63623410\n",
      "Iteration 1549, loss = 0.63612885\n",
      "Iteration 1550, loss = 0.63602335\n",
      "Iteration 1551, loss = 0.63591760\n",
      "Iteration 1552, loss = 0.63581161\n",
      "Iteration 1553, loss = 0.63570537\n",
      "Iteration 1554, loss = 0.63559889\n",
      "Iteration 1555, loss = 0.63549216\n",
      "Iteration 1556, loss = 0.63538517\n",
      "Iteration 1557, loss = 0.63527795\n",
      "Iteration 1558, loss = 0.63517047\n",
      "Iteration 1559, loss = 0.63506274\n",
      "Iteration 1560, loss = 0.63495477\n",
      "Iteration 1561, loss = 0.63484654\n",
      "Iteration 1562, loss = 0.63473807\n",
      "Iteration 1563, loss = 0.63462934\n",
      "Iteration 1564, loss = 0.63452036\n",
      "Iteration 1565, loss = 0.63441113\n",
      "Iteration 1566, loss = 0.63430165\n",
      "Iteration 1567, loss = 0.63419192\n",
      "Iteration 1568, loss = 0.63408193\n",
      "Iteration 1569, loss = 0.63397169\n",
      "Iteration 1570, loss = 0.63386120\n",
      "Iteration 1571, loss = 0.63375045\n",
      "Iteration 1572, loss = 0.63363945\n",
      "Iteration 1573, loss = 0.63352819\n",
      "Iteration 1574, loss = 0.63341668\n",
      "Iteration 1575, loss = 0.63330491\n",
      "Iteration 1576, loss = 0.63319289\n",
      "Iteration 1577, loss = 0.63308061\n",
      "Iteration 1578, loss = 0.63296807\n",
      "Iteration 1579, loss = 0.63285528\n",
      "Iteration 1580, loss = 0.63274222\n",
      "Iteration 1581, loss = 0.63262891\n",
      "Iteration 1582, loss = 0.63251535\n",
      "Iteration 1583, loss = 0.63240152\n",
      "Iteration 1584, loss = 0.63228743\n",
      "Iteration 1585, loss = 0.63217308\n",
      "Iteration 1586, loss = 0.63205848\n",
      "Iteration 1587, loss = 0.63194361\n",
      "Iteration 1588, loss = 0.63182848\n",
      "Iteration 1589, loss = 0.63171309\n",
      "Iteration 1590, loss = 0.63159744\n",
      "Iteration 1591, loss = 0.63148152\n",
      "Iteration 1592, loss = 0.63136534\n",
      "Iteration 1593, loss = 0.63124890\n",
      "Iteration 1594, loss = 0.63113220\n",
      "Iteration 1595, loss = 0.63101523\n",
      "Iteration 1596, loss = 0.63089800\n",
      "Iteration 1597, loss = 0.63078051\n",
      "Iteration 1598, loss = 0.63066274\n",
      "Iteration 1599, loss = 0.63054472\n",
      "Iteration 1600, loss = 0.63042643\n",
      "Iteration 1601, loss = 0.63030787\n",
      "Iteration 1602, loss = 0.63018904\n",
      "Iteration 1603, loss = 0.63006995\n",
      "Iteration 1604, loss = 0.62995059\n",
      "Iteration 1605, loss = 0.62983096\n",
      "Iteration 1606, loss = 0.62971107\n",
      "Iteration 1607, loss = 0.62959091\n",
      "Iteration 1608, loss = 0.62947047\n",
      "Iteration 1609, loss = 0.62934977\n",
      "Iteration 1610, loss = 0.62922880\n",
      "Iteration 1611, loss = 0.62910756\n",
      "Iteration 1612, loss = 0.62898604\n",
      "Iteration 1613, loss = 0.62886426\n",
      "Iteration 1614, loss = 0.62874221\n",
      "Iteration 1615, loss = 0.62861988\n",
      "Iteration 1616, loss = 0.62849728\n",
      "Iteration 1617, loss = 0.62837441\n",
      "Iteration 1618, loss = 0.62825127\n",
      "Iteration 1619, loss = 0.62812786\n",
      "Iteration 1620, loss = 0.62800417\n",
      "Iteration 1621, loss = 0.62788020\n",
      "Iteration 1622, loss = 0.62775596\n",
      "Iteration 1623, loss = 0.62763145\n",
      "Iteration 1624, loss = 0.62750667\n",
      "Iteration 1625, loss = 0.62738160\n",
      "Iteration 1626, loss = 0.62725626\n",
      "Iteration 1627, loss = 0.62713065\n",
      "Iteration 1628, loss = 0.62700476\n",
      "Iteration 1629, loss = 0.62687859\n",
      "Iteration 1630, loss = 0.62675215\n",
      "Iteration 1631, loss = 0.62662542\n",
      "Iteration 1632, loss = 0.62649842\n",
      "Iteration 1633, loss = 0.62637114\n",
      "Iteration 1634, loss = 0.62624359\n",
      "Iteration 1635, loss = 0.62611575\n",
      "Iteration 1636, loss = 0.62598763\n",
      "Iteration 1637, loss = 0.62585924\n",
      "Iteration 1638, loss = 0.62573056\n",
      "Iteration 1639, loss = 0.62560160\n",
      "Iteration 1640, loss = 0.62547237\n",
      "Iteration 1641, loss = 0.62534285\n",
      "Iteration 1642, loss = 0.62521305\n",
      "Iteration 1643, loss = 0.62508297\n",
      "Iteration 1644, loss = 0.62495260\n",
      "Iteration 1645, loss = 0.62482196\n",
      "Iteration 1646, loss = 0.62469103\n",
      "Iteration 1647, loss = 0.62455981\n",
      "Iteration 1648, loss = 0.62442832\n",
      "Iteration 1649, loss = 0.62429653\n",
      "Iteration 1650, loss = 0.62416447\n",
      "Iteration 1651, loss = 0.62403212\n",
      "Iteration 1652, loss = 0.62389948\n",
      "Iteration 1653, loss = 0.62376656\n",
      "Iteration 1654, loss = 0.62363336\n",
      "Iteration 1655, loss = 0.62349986\n",
      "Iteration 1656, loss = 0.62336608\n",
      "Iteration 1657, loss = 0.62323202\n",
      "Iteration 1658, loss = 0.62309767\n",
      "Iteration 1659, loss = 0.62296303\n",
      "Iteration 1660, loss = 0.62282810\n",
      "Iteration 1661, loss = 0.62269288\n",
      "Iteration 1662, loss = 0.62255738\n",
      "Iteration 1663, loss = 0.62242158\n",
      "Iteration 1664, loss = 0.62228550\n",
      "Iteration 1665, loss = 0.62214913\n",
      "Iteration 1666, loss = 0.62201246\n",
      "Iteration 1667, loss = 0.62187551\n",
      "Iteration 1668, loss = 0.62173827\n",
      "Iteration 1669, loss = 0.62160073\n",
      "Iteration 1670, loss = 0.62146291\n",
      "Iteration 1671, loss = 0.62132479\n",
      "Iteration 1672, loss = 0.62118639\n",
      "Iteration 1673, loss = 0.62104768\n",
      "Iteration 1674, loss = 0.62090869\n",
      "Iteration 1675, loss = 0.62076941\n",
      "Iteration 1676, loss = 0.62062983\n",
      "Iteration 1677, loss = 0.62048996\n",
      "Iteration 1678, loss = 0.62034979\n",
      "Iteration 1679, loss = 0.62020933\n",
      "Iteration 1680, loss = 0.62006858\n",
      "Iteration 1681, loss = 0.61992753\n",
      "Iteration 1682, loss = 0.61978618\n",
      "Iteration 1683, loss = 0.61964454\n",
      "Iteration 1684, loss = 0.61950261\n",
      "Iteration 1685, loss = 0.61936038\n",
      "Iteration 1686, loss = 0.61921785\n",
      "Iteration 1687, loss = 0.61907503\n",
      "Iteration 1688, loss = 0.61893190\n",
      "Iteration 1689, loss = 0.61878849\n",
      "Iteration 1690, loss = 0.61864477\n",
      "Iteration 1691, loss = 0.61850076\n",
      "Iteration 1692, loss = 0.61835645\n",
      "Iteration 1693, loss = 0.61821184\n",
      "Iteration 1694, loss = 0.61806693\n",
      "Iteration 1695, loss = 0.61792172\n",
      "Iteration 1696, loss = 0.61777621\n",
      "Iteration 1697, loss = 0.61763041\n",
      "Iteration 1698, loss = 0.61748430\n",
      "Iteration 1699, loss = 0.61733789\n",
      "Iteration 1700, loss = 0.61719119\n",
      "Iteration 1701, loss = 0.61704418\n",
      "Iteration 1702, loss = 0.61689687\n",
      "Iteration 1703, loss = 0.61674926\n",
      "Iteration 1704, loss = 0.61660135\n",
      "Iteration 1705, loss = 0.61645313\n",
      "Iteration 1706, loss = 0.61630461\n",
      "Iteration 1707, loss = 0.61615580\n",
      "Iteration 1708, loss = 0.61600667\n",
      "Iteration 1709, loss = 0.61585725\n",
      "Iteration 1710, loss = 0.61570752\n",
      "Iteration 1711, loss = 0.61555749\n",
      "Iteration 1712, loss = 0.61540715\n",
      "Iteration 1713, loss = 0.61525651\n",
      "Iteration 1714, loss = 0.61510556\n",
      "Iteration 1715, loss = 0.61495431\n",
      "Iteration 1716, loss = 0.61480276\n",
      "Iteration 1717, loss = 0.61465090\n",
      "Iteration 1718, loss = 0.61449873\n",
      "Iteration 1719, loss = 0.61434626\n",
      "Iteration 1720, loss = 0.61419348\n",
      "Iteration 1721, loss = 0.61404039\n",
      "Iteration 1722, loss = 0.61388700\n",
      "Iteration 1723, loss = 0.61373330\n",
      "Iteration 1724, loss = 0.61357930\n",
      "Iteration 1725, loss = 0.61342498\n",
      "Iteration 1726, loss = 0.61327036\n",
      "Iteration 1727, loss = 0.61311543\n",
      "Iteration 1728, loss = 0.61296019\n",
      "Iteration 1729, loss = 0.61280465\n",
      "Iteration 1730, loss = 0.61264879\n",
      "Iteration 1731, loss = 0.61249262\n",
      "Iteration 1732, loss = 0.61233615\n",
      "Iteration 1733, loss = 0.61217936\n",
      "Iteration 1734, loss = 0.61202227\n",
      "Iteration 1735, loss = 0.61186486\n",
      "Iteration 1736, loss = 0.61170715\n",
      "Iteration 1737, loss = 0.61154912\n",
      "Iteration 1738, loss = 0.61139079\n",
      "Iteration 1739, loss = 0.61123214\n",
      "Iteration 1740, loss = 0.61107318\n",
      "Iteration 1741, loss = 0.61091390\n",
      "Iteration 1742, loss = 0.61075432\n",
      "Iteration 1743, loss = 0.61059442\n",
      "Iteration 1744, loss = 0.61043422\n",
      "Iteration 1745, loss = 0.61027369\n",
      "Iteration 1746, loss = 0.61011286\n",
      "Iteration 1747, loss = 0.60995171\n",
      "Iteration 1748, loss = 0.60979025\n",
      "Iteration 1749, loss = 0.60962847\n",
      "Iteration 1750, loss = 0.60946638\n",
      "Iteration 1751, loss = 0.60930398\n",
      "Iteration 1752, loss = 0.60914126\n",
      "Iteration 1753, loss = 0.60897823\n",
      "Iteration 1754, loss = 0.60881488\n",
      "Iteration 1755, loss = 0.60865122\n",
      "Iteration 1756, loss = 0.60848724\n",
      "Iteration 1757, loss = 0.60832294\n",
      "Iteration 1758, loss = 0.60815833\n",
      "Iteration 1759, loss = 0.60799341\n",
      "Iteration 1760, loss = 0.60782816\n",
      "Iteration 1761, loss = 0.60766260\n",
      "Iteration 1762, loss = 0.60749673\n",
      "Iteration 1763, loss = 0.60733053\n",
      "Iteration 1764, loss = 0.60716402\n",
      "Iteration 1765, loss = 0.60699719\n",
      "Iteration 1766, loss = 0.60683004\n",
      "Iteration 1767, loss = 0.60666258\n",
      "Iteration 1768, loss = 0.60649480\n",
      "Iteration 1769, loss = 0.60632669\n",
      "Iteration 1770, loss = 0.60615827\n",
      "Iteration 1771, loss = 0.60598953\n",
      "Iteration 1772, loss = 0.60582048\n",
      "Iteration 1773, loss = 0.60565110\n",
      "Iteration 1774, loss = 0.60548140\n",
      "Iteration 1775, loss = 0.60531138\n",
      "Iteration 1776, loss = 0.60514105\n",
      "Iteration 1777, loss = 0.60497039\n",
      "Iteration 1778, loss = 0.60479941\n",
      "Iteration 1779, loss = 0.60462811\n",
      "Iteration 1780, loss = 0.60445649\n",
      "Iteration 1781, loss = 0.60428455\n",
      "Iteration 1782, loss = 0.60411229\n",
      "Iteration 1783, loss = 0.60393970\n",
      "Iteration 1784, loss = 0.60376680\n",
      "Iteration 1785, loss = 0.60359357\n",
      "Iteration 1786, loss = 0.60342002\n",
      "Iteration 1787, loss = 0.60324615\n",
      "Iteration 1788, loss = 0.60307195\n",
      "Iteration 1789, loss = 0.60289744\n",
      "Iteration 1790, loss = 0.60272260\n",
      "Iteration 1791, loss = 0.60254743\n",
      "Iteration 1792, loss = 0.60237195\n",
      "Iteration 1793, loss = 0.60219613\n",
      "Iteration 1794, loss = 0.60202000\n",
      "Iteration 1795, loss = 0.60184354\n",
      "Iteration 1796, loss = 0.60166676\n",
      "Iteration 1797, loss = 0.60148965\n",
      "Iteration 1798, loss = 0.60131222\n",
      "Iteration 1799, loss = 0.60113446\n",
      "Iteration 1800, loss = 0.60095638\n",
      "Iteration 1801, loss = 0.60077798\n",
      "Iteration 1802, loss = 0.60059924\n",
      "Iteration 1803, loss = 0.60042019\n",
      "Iteration 1804, loss = 0.60024080\n",
      "Iteration 1805, loss = 0.60006109\n",
      "Iteration 1806, loss = 0.59988106\n",
      "Iteration 1807, loss = 0.59970070\n",
      "Iteration 1808, loss = 0.59952001\n",
      "Iteration 1809, loss = 0.59933900\n",
      "Iteration 1810, loss = 0.59915765\n",
      "Iteration 1811, loss = 0.59897599\n",
      "Iteration 1812, loss = 0.59879399\n",
      "Iteration 1813, loss = 0.59861167\n",
      "Iteration 1814, loss = 0.59842902\n",
      "Iteration 1815, loss = 0.59824604\n",
      "Iteration 1816, loss = 0.59806273\n",
      "Iteration 1817, loss = 0.59787910\n",
      "Iteration 1818, loss = 0.59769514\n",
      "Iteration 1819, loss = 0.59751085\n",
      "Iteration 1820, loss = 0.59732623\n",
      "Iteration 1821, loss = 0.59714128\n",
      "Iteration 1822, loss = 0.59695600\n",
      "Iteration 1823, loss = 0.59677040\n",
      "Iteration 1824, loss = 0.59658446\n",
      "Iteration 1825, loss = 0.59639820\n",
      "Iteration 1826, loss = 0.59621160\n",
      "Iteration 1827, loss = 0.59602468\n",
      "Iteration 1828, loss = 0.59583742\n",
      "Iteration 1829, loss = 0.59564984\n",
      "Iteration 1830, loss = 0.59546192\n",
      "Iteration 1831, loss = 0.59527368\n",
      "Iteration 1832, loss = 0.59508511\n",
      "Iteration 1833, loss = 0.59489620\n",
      "Iteration 1834, loss = 0.59470696\n",
      "Iteration 1835, loss = 0.59451740\n",
      "Iteration 1836, loss = 0.59432750\n",
      "Iteration 1837, loss = 0.59413727\n",
      "Iteration 1838, loss = 0.59394671\n",
      "Iteration 1839, loss = 0.59375581\n",
      "Iteration 1840, loss = 0.59356459\n",
      "Iteration 1841, loss = 0.59337303\n",
      "Iteration 1842, loss = 0.59318114\n",
      "Iteration 1843, loss = 0.59298892\n",
      "Iteration 1844, loss = 0.59279637\n",
      "Iteration 1845, loss = 0.59260348\n",
      "Iteration 1846, loss = 0.59241026\n",
      "Iteration 1847, loss = 0.59221671\n",
      "Iteration 1848, loss = 0.59202283\n",
      "Iteration 1849, loss = 0.59182861\n",
      "Iteration 1850, loss = 0.59163406\n",
      "Iteration 1851, loss = 0.59143918\n",
      "Iteration 1852, loss = 0.59124396\n",
      "Iteration 1853, loss = 0.59104841\n",
      "Iteration 1854, loss = 0.59085253\n",
      "Iteration 1855, loss = 0.59065631\n",
      "Iteration 1856, loss = 0.59045976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1857, loss = 0.59026288\n",
      "Iteration 1858, loss = 0.59006566\n",
      "Iteration 1859, loss = 0.58986810\n",
      "Iteration 1860, loss = 0.58967022\n",
      "Iteration 1861, loss = 0.58947199\n",
      "Iteration 1862, loss = 0.58927344\n",
      "Iteration 1863, loss = 0.58907454\n",
      "Iteration 1864, loss = 0.58887532\n",
      "Iteration 1865, loss = 0.58867576\n",
      "Iteration 1866, loss = 0.58847586\n",
      "Iteration 1867, loss = 0.58827563\n",
      "Iteration 1868, loss = 0.58807506\n",
      "Iteration 1869, loss = 0.58787416\n",
      "Iteration 1870, loss = 0.58767292\n",
      "Iteration 1871, loss = 0.58747135\n",
      "Iteration 1872, loss = 0.58726944\n",
      "Iteration 1873, loss = 0.58706719\n",
      "Iteration 1874, loss = 0.58686461\n",
      "Iteration 1875, loss = 0.58666169\n",
      "Iteration 1876, loss = 0.58645844\n",
      "Iteration 1877, loss = 0.58625485\n",
      "Iteration 1878, loss = 0.58605093\n",
      "Iteration 1879, loss = 0.58584666\n",
      "Iteration 1880, loss = 0.58564207\n",
      "Iteration 1881, loss = 0.58543713\n",
      "Iteration 1882, loss = 0.58523186\n",
      "Iteration 1883, loss = 0.58502625\n",
      "Iteration 1884, loss = 0.58482031\n",
      "Iteration 1885, loss = 0.58461402\n",
      "Iteration 1886, loss = 0.58440740\n",
      "Iteration 1887, loss = 0.58420045\n",
      "Iteration 1888, loss = 0.58399316\n",
      "Iteration 1889, loss = 0.58378553\n",
      "Iteration 1890, loss = 0.58357756\n",
      "Iteration 1891, loss = 0.58336925\n",
      "Iteration 1892, loss = 0.58316061\n",
      "Iteration 1893, loss = 0.58295163\n",
      "Iteration 1894, loss = 0.58274231\n",
      "Iteration 1895, loss = 0.58253266\n",
      "Iteration 1896, loss = 0.58232266\n",
      "Iteration 1897, loss = 0.58211233\n",
      "Iteration 1898, loss = 0.58190166\n",
      "Iteration 1899, loss = 0.58169066\n",
      "Iteration 1900, loss = 0.58147931\n",
      "Iteration 1901, loss = 0.58126763\n",
      "Iteration 1902, loss = 0.58105561\n",
      "Iteration 1903, loss = 0.58084325\n",
      "Iteration 1904, loss = 0.58063056\n",
      "Iteration 1905, loss = 0.58041752\n",
      "Iteration 1906, loss = 0.58020415\n",
      "Iteration 1907, loss = 0.57999044\n",
      "Iteration 1908, loss = 0.57977639\n",
      "Iteration 1909, loss = 0.57956200\n",
      "Iteration 1910, loss = 0.57934727\n",
      "Iteration 1911, loss = 0.57913221\n",
      "Iteration 1912, loss = 0.57891680\n",
      "Iteration 1913, loss = 0.57870106\n",
      "Iteration 1914, loss = 0.57848498\n",
      "Iteration 1915, loss = 0.57826856\n",
      "Iteration 1916, loss = 0.57805180\n",
      "Iteration 1917, loss = 0.57783470\n",
      "Iteration 1918, loss = 0.57761727\n",
      "Iteration 1919, loss = 0.57739950\n",
      "Iteration 1920, loss = 0.57718138\n",
      "Iteration 1921, loss = 0.57696293\n",
      "Iteration 1922, loss = 0.57674414\n",
      "Iteration 1923, loss = 0.57652501\n",
      "Iteration 1924, loss = 0.57630554\n",
      "Iteration 1925, loss = 0.57608574\n",
      "Iteration 1926, loss = 0.57586559\n",
      "Iteration 1927, loss = 0.57564511\n",
      "Iteration 1928, loss = 0.57542428\n",
      "Iteration 1929, loss = 0.57520312\n",
      "Iteration 1930, loss = 0.57498162\n",
      "Iteration 1931, loss = 0.57475978\n",
      "Iteration 1932, loss = 0.57453760\n",
      "Iteration 1933, loss = 0.57431509\n",
      "Iteration 1934, loss = 0.57409223\n",
      "Iteration 1935, loss = 0.57386903\n",
      "Iteration 1936, loss = 0.57364550\n",
      "Iteration 1937, loss = 0.57342163\n",
      "Iteration 1938, loss = 0.57319742\n",
      "Iteration 1939, loss = 0.57297287\n",
      "Iteration 1940, loss = 0.57274798\n",
      "Iteration 1941, loss = 0.57252275\n",
      "Iteration 1942, loss = 0.57229718\n",
      "Iteration 1943, loss = 0.57207128\n",
      "Iteration 1944, loss = 0.57184503\n",
      "Iteration 1945, loss = 0.57161845\n",
      "Iteration 1946, loss = 0.57139153\n",
      "Iteration 1947, loss = 0.57116427\n",
      "Iteration 1948, loss = 0.57093667\n",
      "Iteration 1949, loss = 0.57070874\n",
      "Iteration 1950, loss = 0.57048046\n",
      "Iteration 1951, loss = 0.57025185\n",
      "Iteration 1952, loss = 0.57002290\n",
      "Iteration 1953, loss = 0.56979361\n",
      "Iteration 1954, loss = 0.56956398\n",
      "Iteration 1955, loss = 0.56933401\n",
      "Iteration 1956, loss = 0.56910371\n",
      "Iteration 1957, loss = 0.56887306\n",
      "Iteration 1958, loss = 0.56864208\n",
      "Iteration 1959, loss = 0.56841076\n",
      "Iteration 1960, loss = 0.56817911\n",
      "Iteration 1961, loss = 0.56794711\n",
      "Iteration 1962, loss = 0.56771478\n",
      "Iteration 1963, loss = 0.56748211\n",
      "Iteration 1964, loss = 0.56724910\n",
      "Iteration 1965, loss = 0.56701576\n",
      "Iteration 1966, loss = 0.56678208\n",
      "Iteration 1967, loss = 0.56654806\n",
      "Iteration 1968, loss = 0.56631370\n",
      "Iteration 1969, loss = 0.56607900\n",
      "Iteration 1970, loss = 0.56584397\n",
      "Iteration 1971, loss = 0.56560860\n",
      "Iteration 1972, loss = 0.56537290\n",
      "Iteration 1973, loss = 0.56513685\n",
      "Iteration 1974, loss = 0.56490047\n",
      "Iteration 1975, loss = 0.56466376\n",
      "Iteration 1976, loss = 0.56442671\n",
      "Iteration 1977, loss = 0.56418932\n",
      "Iteration 1978, loss = 0.56395159\n",
      "Iteration 1979, loss = 0.56371353\n",
      "Iteration 1980, loss = 0.56347513\n",
      "Iteration 1981, loss = 0.56323640\n",
      "Iteration 1982, loss = 0.56299733\n",
      "Iteration 1983, loss = 0.56275792\n",
      "Iteration 1984, loss = 0.56251818\n",
      "Iteration 1985, loss = 0.56227810\n",
      "Iteration 1986, loss = 0.56203769\n",
      "Iteration 1987, loss = 0.56179694\n",
      "Iteration 1988, loss = 0.56155586\n",
      "Iteration 1989, loss = 0.56131444\n",
      "Iteration 1990, loss = 0.56107269\n",
      "Iteration 1991, loss = 0.56083060\n",
      "Iteration 1992, loss = 0.56058818\n",
      "Iteration 1993, loss = 0.56034543\n",
      "Iteration 1994, loss = 0.56010233\n",
      "Iteration 1995, loss = 0.55985891\n",
      "Iteration 1996, loss = 0.55961515\n",
      "Iteration 1997, loss = 0.55937106\n",
      "Iteration 1998, loss = 0.55912663\n",
      "Iteration 1999, loss = 0.55888187\n",
      "Iteration 2000, loss = 0.55863678\n",
      "Iteration 2001, loss = 0.55839135\n",
      "Iteration 2002, loss = 0.55814559\n",
      "Iteration 2003, loss = 0.55789950\n",
      "Iteration 2004, loss = 0.55765308\n",
      "Iteration 2005, loss = 0.55740632\n",
      "Iteration 2006, loss = 0.55715923\n",
      "Iteration 2007, loss = 0.55691181\n",
      "Iteration 2008, loss = 0.55666405\n",
      "Iteration 2009, loss = 0.55641597\n",
      "Iteration 2010, loss = 0.55616755\n",
      "Iteration 2011, loss = 0.55591880\n",
      "Iteration 2012, loss = 0.55566972\n",
      "Iteration 2013, loss = 0.55542031\n",
      "Iteration 2014, loss = 0.55517057\n",
      "Iteration 2015, loss = 0.55492050\n",
      "Iteration 2016, loss = 0.55467010\n",
      "Iteration 2017, loss = 0.55441937\n",
      "Iteration 2018, loss = 0.55416830\n",
      "Iteration 2019, loss = 0.55391691\n",
      "Iteration 2020, loss = 0.55366519\n",
      "Iteration 2021, loss = 0.55341314\n",
      "Iteration 2022, loss = 0.55316076\n",
      "Iteration 2023, loss = 0.55290805\n",
      "Iteration 2024, loss = 0.55265502\n",
      "Iteration 2025, loss = 0.55240165\n",
      "Iteration 2026, loss = 0.55214796\n",
      "Iteration 2027, loss = 0.55189394\n",
      "Iteration 2028, loss = 0.55163959\n",
      "Iteration 2029, loss = 0.55138491\n",
      "Iteration 2030, loss = 0.55112991\n",
      "Iteration 2031, loss = 0.55087458\n",
      "Iteration 2032, loss = 0.55061892\n",
      "Iteration 2033, loss = 0.55036294\n",
      "Iteration 2034, loss = 0.55010663\n",
      "Iteration 2035, loss = 0.54985000\n",
      "Iteration 2036, loss = 0.54959304\n",
      "Iteration 2037, loss = 0.54933575\n",
      "Iteration 2038, loss = 0.54907814\n",
      "Iteration 2039, loss = 0.54882021\n",
      "Iteration 2040, loss = 0.54856195\n",
      "Iteration 2041, loss = 0.54830336\n",
      "Iteration 2042, loss = 0.54804446\n",
      "Iteration 2043, loss = 0.54778522\n",
      "Iteration 2044, loss = 0.54752567\n",
      "Iteration 2045, loss = 0.54726579\n",
      "Iteration 2046, loss = 0.54700559\n",
      "Iteration 2047, loss = 0.54674507\n",
      "Iteration 2048, loss = 0.54648423\n",
      "Iteration 2049, loss = 0.54622306\n",
      "Iteration 2050, loss = 0.54596157\n",
      "Iteration 2051, loss = 0.54569976\n",
      "Iteration 2052, loss = 0.54543763\n",
      "Iteration 2053, loss = 0.54517518\n",
      "Iteration 2054, loss = 0.54491241\n",
      "Iteration 2055, loss = 0.54464932\n",
      "Iteration 2056, loss = 0.54438591\n",
      "Iteration 2057, loss = 0.54412218\n",
      "Iteration 2058, loss = 0.54385814\n",
      "Iteration 2059, loss = 0.54359377\n",
      "Iteration 2060, loss = 0.54332908\n",
      "Iteration 2061, loss = 0.54306408\n",
      "Iteration 2062, loss = 0.54279876\n",
      "Iteration 2063, loss = 0.54253313\n",
      "Iteration 2064, loss = 0.54226717\n",
      "Iteration 2065, loss = 0.54200090\n",
      "Iteration 2066, loss = 0.54173432\n",
      "Iteration 2067, loss = 0.54146741\n",
      "Iteration 2068, loss = 0.54120020\n",
      "Iteration 2069, loss = 0.54093266\n",
      "Iteration 2070, loss = 0.54066482\n",
      "Iteration 2071, loss = 0.54039666\n",
      "Iteration 2072, loss = 0.54012818\n",
      "Iteration 2073, loss = 0.53985939\n",
      "Iteration 2074, loss = 0.53959029\n",
      "Iteration 2075, loss = 0.53932088\n",
      "Iteration 2076, loss = 0.53905115\n",
      "Iteration 2077, loss = 0.53878112\n",
      "Iteration 2078, loss = 0.53851077\n",
      "Iteration 2079, loss = 0.53824011\n",
      "Iteration 2080, loss = 0.53796913\n",
      "Iteration 2081, loss = 0.53769785\n",
      "Iteration 2082, loss = 0.53742626\n",
      "Iteration 2083, loss = 0.53715436\n",
      "Iteration 2084, loss = 0.53688215\n",
      "Iteration 2085, loss = 0.53660963\n",
      "Iteration 2086, loss = 0.53633681\n",
      "Iteration 2087, loss = 0.53606367\n",
      "Iteration 2088, loss = 0.53579023\n",
      "Iteration 2089, loss = 0.53551648\n",
      "Iteration 2090, loss = 0.53524243\n",
      "Iteration 2091, loss = 0.53496807\n",
      "Iteration 2092, loss = 0.53469340\n",
      "Iteration 2093, loss = 0.53441843\n",
      "Iteration 2094, loss = 0.53414315\n",
      "Iteration 2095, loss = 0.53386757\n",
      "Iteration 2096, loss = 0.53359169\n",
      "Iteration 2097, loss = 0.53331550\n",
      "Iteration 2098, loss = 0.53303901\n",
      "Iteration 2099, loss = 0.53276222\n",
      "Iteration 2100, loss = 0.53248512\n",
      "Iteration 2101, loss = 0.53220773\n",
      "Iteration 2102, loss = 0.53193003\n",
      "Iteration 2103, loss = 0.53165203\n",
      "Iteration 2104, loss = 0.53137373\n",
      "Iteration 2105, loss = 0.53109514\n",
      "Iteration 2106, loss = 0.53081624\n",
      "Iteration 2107, loss = 0.53053705\n",
      "Iteration 2108, loss = 0.53025756\n",
      "Iteration 2109, loss = 0.52997777\n",
      "Iteration 2110, loss = 0.52969768\n",
      "Iteration 2111, loss = 0.52941730\n",
      "Iteration 2112, loss = 0.52913662\n",
      "Iteration 2113, loss = 0.52885564\n",
      "Iteration 2114, loss = 0.52857437\n",
      "Iteration 2115, loss = 0.52829281\n",
      "Iteration 2116, loss = 0.52801095\n",
      "Iteration 2117, loss = 0.52772880\n",
      "Iteration 2118, loss = 0.52744635\n",
      "Iteration 2119, loss = 0.52716362\n",
      "Iteration 2120, loss = 0.52688059\n",
      "Iteration 2121, loss = 0.52659727\n",
      "Iteration 2122, loss = 0.52631366\n",
      "Iteration 2123, loss = 0.52602976\n",
      "Iteration 2124, loss = 0.52574557\n",
      "Iteration 2125, loss = 0.52546109\n",
      "Iteration 2126, loss = 0.52517632\n",
      "Iteration 2127, loss = 0.52489126\n",
      "Iteration 2128, loss = 0.52460592\n",
      "Iteration 2129, loss = 0.52432029\n",
      "Iteration 2130, loss = 0.52403437\n",
      "Iteration 2131, loss = 0.52374817\n",
      "Iteration 2132, loss = 0.52346168\n",
      "Iteration 2133, loss = 0.52317491\n",
      "Iteration 2134, loss = 0.52288786\n",
      "Iteration 2135, loss = 0.52260052\n",
      "Iteration 2136, loss = 0.52231290\n",
      "Iteration 2137, loss = 0.52202499\n",
      "Iteration 2138, loss = 0.52173681\n",
      "Iteration 2139, loss = 0.52144834\n",
      "Iteration 2140, loss = 0.52115959\n",
      "Iteration 2141, loss = 0.52087056\n",
      "Iteration 2142, loss = 0.52058126\n",
      "Iteration 2143, loss = 0.52029167\n",
      "Iteration 2144, loss = 0.52000181\n",
      "Iteration 2145, loss = 0.51971167\n",
      "Iteration 2146, loss = 0.51942125\n",
      "Iteration 2147, loss = 0.51913056\n",
      "Iteration 2148, loss = 0.51883959\n",
      "Iteration 2149, loss = 0.51854834\n",
      "Iteration 2150, loss = 0.51825683\n",
      "Iteration 2151, loss = 0.51796503\n",
      "Iteration 2152, loss = 0.51767297\n",
      "Iteration 2153, loss = 0.51738063\n",
      "Iteration 2154, loss = 0.51708802\n",
      "Iteration 2155, loss = 0.51679514\n",
      "Iteration 2156, loss = 0.51650199\n",
      "Iteration 2157, loss = 0.51620857\n",
      "Iteration 2158, loss = 0.51591488\n",
      "Iteration 2159, loss = 0.51562092\n",
      "Iteration 2160, loss = 0.51532670\n",
      "Iteration 2161, loss = 0.51503220\n",
      "Iteration 2162, loss = 0.51473744\n",
      "Iteration 2163, loss = 0.51444242\n",
      "Iteration 2164, loss = 0.51414713\n",
      "Iteration 2165, loss = 0.51385157\n",
      "Iteration 2166, loss = 0.51355576\n",
      "Iteration 2167, loss = 0.51325967\n",
      "Iteration 2168, loss = 0.51296333\n",
      "Iteration 2169, loss = 0.51266672\n",
      "Iteration 2170, loss = 0.51236986\n",
      "Iteration 2171, loss = 0.51207273\n",
      "Iteration 2172, loss = 0.51177534\n",
      "Iteration 2173, loss = 0.51147770\n",
      "Iteration 2174, loss = 0.51117979\n",
      "Iteration 2175, loss = 0.51088163\n",
      "Iteration 2176, loss = 0.51058321\n",
      "Iteration 2177, loss = 0.51028454\n",
      "Iteration 2178, loss = 0.50998561\n",
      "Iteration 2179, loss = 0.50968642\n",
      "Iteration 2180, loss = 0.50938698\n",
      "Iteration 2181, loss = 0.50908729\n",
      "Iteration 2182, loss = 0.50878735\n",
      "Iteration 2183, loss = 0.50848715\n",
      "Iteration 2184, loss = 0.50818671\n",
      "Iteration 2185, loss = 0.50788601\n",
      "Iteration 2186, loss = 0.50758506\n",
      "Iteration 2187, loss = 0.50728387\n",
      "Iteration 2188, loss = 0.50698242\n",
      "Iteration 2189, loss = 0.50668073\n",
      "Iteration 2190, loss = 0.50637879\n",
      "Iteration 2191, loss = 0.50607661\n",
      "Iteration 2192, loss = 0.50577418\n",
      "Iteration 2193, loss = 0.50547151\n",
      "Iteration 2194, loss = 0.50516859\n",
      "Iteration 2195, loss = 0.50486543\n",
      "Iteration 2196, loss = 0.50456203\n",
      "Iteration 2197, loss = 0.50425839\n",
      "Iteration 2198, loss = 0.50395450\n",
      "Iteration 2199, loss = 0.50365038\n",
      "Iteration 2200, loss = 0.50334602\n",
      "Iteration 2201, loss = 0.50304142\n",
      "Iteration 2202, loss = 0.50273658\n",
      "Iteration 2203, loss = 0.50243151\n",
      "Iteration 2204, loss = 0.50212620\n",
      "Iteration 2205, loss = 0.50182066\n",
      "Iteration 2206, loss = 0.50151488\n",
      "Iteration 2207, loss = 0.50120886\n",
      "Iteration 2208, loss = 0.50090262\n",
      "Iteration 2209, loss = 0.50059614\n",
      "Iteration 2210, loss = 0.50028944\n",
      "Iteration 2211, loss = 0.49998250\n",
      "Iteration 2212, loss = 0.49967533\n",
      "Iteration 2213, loss = 0.49936794\n",
      "Iteration 2214, loss = 0.49906031\n",
      "Iteration 2215, loss = 0.49875246\n",
      "Iteration 2216, loss = 0.49844439\n",
      "Iteration 2217, loss = 0.49813609\n",
      "Iteration 2218, loss = 0.49782756\n",
      "Iteration 2219, loss = 0.49751881\n",
      "Iteration 2220, loss = 0.49720984\n",
      "Iteration 2221, loss = 0.49690065\n",
      "Iteration 2222, loss = 0.49659123\n",
      "Iteration 2223, loss = 0.49628160\n",
      "Iteration 2224, loss = 0.49597174\n",
      "Iteration 2225, loss = 0.49566167\n",
      "Iteration 2226, loss = 0.49535138\n",
      "Iteration 2227, loss = 0.49504087\n",
      "Iteration 2228, loss = 0.49473015\n",
      "Iteration 2229, loss = 0.49441921\n",
      "Iteration 2230, loss = 0.49410805\n",
      "Iteration 2231, loss = 0.49379669\n",
      "Iteration 2232, loss = 0.49348511\n",
      "Iteration 2233, loss = 0.49317332\n",
      "Iteration 2234, loss = 0.49286132\n",
      "Iteration 2235, loss = 0.49254910\n",
      "Iteration 2236, loss = 0.49223668\n",
      "Iteration 2237, loss = 0.49192405\n",
      "Iteration 2238, loss = 0.49161122\n",
      "Iteration 2239, loss = 0.49129818\n",
      "Iteration 2240, loss = 0.49098493\n",
      "Iteration 2241, loss = 0.49067147\n",
      "Iteration 2242, loss = 0.49035782\n",
      "Iteration 2243, loss = 0.49004396\n",
      "Iteration 2244, loss = 0.48972990\n",
      "Iteration 2245, loss = 0.48941563\n",
      "Iteration 2246, loss = 0.48910117\n",
      "Iteration 2247, loss = 0.48878651\n",
      "Iteration 2248, loss = 0.48847165\n",
      "Iteration 2249, loss = 0.48815659\n",
      "Iteration 2250, loss = 0.48784133\n",
      "Iteration 2251, loss = 0.48752588\n",
      "Iteration 2252, loss = 0.48721024\n",
      "Iteration 2253, loss = 0.48689440\n",
      "Iteration 2254, loss = 0.48657837\n",
      "Iteration 2255, loss = 0.48626214\n",
      "Iteration 2256, loss = 0.48594573\n",
      "Iteration 2257, loss = 0.48562912\n",
      "Iteration 2258, loss = 0.48531233\n",
      "Iteration 2259, loss = 0.48499534\n",
      "Iteration 2260, loss = 0.48467817\n",
      "Iteration 2261, loss = 0.48436082\n",
      "Iteration 2262, loss = 0.48404327\n",
      "Iteration 2263, loss = 0.48372555\n",
      "Iteration 2264, loss = 0.48340764\n",
      "Iteration 2265, loss = 0.48308954\n",
      "Iteration 2266, loss = 0.48277127\n",
      "Iteration 2267, loss = 0.48245281\n",
      "Iteration 2268, loss = 0.48213417\n",
      "Iteration 2269, loss = 0.48181536\n",
      "Iteration 2270, loss = 0.48149637\n",
      "Iteration 2271, loss = 0.48117719\n",
      "Iteration 2272, loss = 0.48085785\n",
      "Iteration 2273, loss = 0.48053833\n",
      "Iteration 2274, loss = 0.48021863\n",
      "Iteration 2275, loss = 0.47989876\n",
      "Iteration 2276, loss = 0.47957872\n",
      "Iteration 2277, loss = 0.47925851\n",
      "Iteration 2278, loss = 0.47893812\n",
      "Iteration 2279, loss = 0.47861757\n",
      "Iteration 2280, loss = 0.47829685\n",
      "Iteration 2281, loss = 0.47797596\n",
      "Iteration 2282, loss = 0.47765491\n",
      "Iteration 2283, loss = 0.47733368\n",
      "Iteration 2284, loss = 0.47701230\n",
      "Iteration 2285, loss = 0.47669075\n",
      "Iteration 2286, loss = 0.47636904\n",
      "Iteration 2287, loss = 0.47604717\n",
      "Iteration 2288, loss = 0.47572513\n",
      "Iteration 2289, loss = 0.47540294\n",
      "Iteration 2290, loss = 0.47508059\n",
      "Iteration 2291, loss = 0.47475808\n",
      "Iteration 2292, loss = 0.47443541\n",
      "Iteration 2293, loss = 0.47411259\n",
      "Iteration 2294, loss = 0.47378961\n",
      "Iteration 2295, loss = 0.47346648\n",
      "Iteration 2296, loss = 0.47314320\n",
      "Iteration 2297, loss = 0.47281976\n",
      "Iteration 2298, loss = 0.47249618\n",
      "Iteration 2299, loss = 0.47217244\n",
      "Iteration 2300, loss = 0.47184856\n",
      "Iteration 2301, loss = 0.47152452\n",
      "Iteration 2302, loss = 0.47120034\n",
      "Iteration 2303, loss = 0.47087602\n",
      "Iteration 2304, loss = 0.47055155\n",
      "Iteration 2305, loss = 0.47022694\n",
      "Iteration 2306, loss = 0.46990218\n",
      "Iteration 2307, loss = 0.46957728\n",
      "Iteration 2308, loss = 0.46925224\n",
      "Iteration 2309, loss = 0.46892707\n",
      "Iteration 2310, loss = 0.46860175\n",
      "Iteration 2311, loss = 0.46827630\n",
      "Iteration 2312, loss = 0.46795071\n",
      "Iteration 2313, loss = 0.46762498\n",
      "Iteration 2314, loss = 0.46729912\n",
      "Iteration 2315, loss = 0.46697312\n",
      "Iteration 2316, loss = 0.46664700\n",
      "Iteration 2317, loss = 0.46632074\n",
      "Iteration 2318, loss = 0.46599435\n",
      "Iteration 2319, loss = 0.46566783\n",
      "Iteration 2320, loss = 0.46534118\n",
      "Iteration 2321, loss = 0.46501441\n",
      "Iteration 2322, loss = 0.46468751\n",
      "Iteration 2323, loss = 0.46436048\n",
      "Iteration 2324, loss = 0.46403333\n",
      "Iteration 2325, loss = 0.46370606\n",
      "Iteration 2326, loss = 0.46337866\n",
      "Iteration 2327, loss = 0.46305114\n",
      "Iteration 2328, loss = 0.46272350\n",
      "Iteration 2329, loss = 0.46239575\n",
      "Iteration 2330, loss = 0.46206787\n",
      "Iteration 2331, loss = 0.46173988\n",
      "Iteration 2332, loss = 0.46141177\n",
      "Iteration 2333, loss = 0.46108355\n",
      "Iteration 2334, loss = 0.46075521\n",
      "Iteration 2335, loss = 0.46042676\n",
      "Iteration 2336, loss = 0.46009820\n",
      "Iteration 2337, loss = 0.45976953\n",
      "Iteration 2338, loss = 0.45944075\n",
      "Iteration 2339, loss = 0.45911186\n",
      "Iteration 2340, loss = 0.45878286\n",
      "Iteration 2341, loss = 0.45845375\n",
      "Iteration 2342, loss = 0.45812454\n",
      "Iteration 2343, loss = 0.45779523\n",
      "Iteration 2344, loss = 0.45746581\n",
      "Iteration 2345, loss = 0.45713629\n",
      "Iteration 2346, loss = 0.45680667\n",
      "Iteration 2347, loss = 0.45647694\n",
      "Iteration 2348, loss = 0.45614712\n",
      "Iteration 2349, loss = 0.45581720\n",
      "Iteration 2350, loss = 0.45548719\n",
      "Iteration 2351, loss = 0.45515707\n",
      "Iteration 2352, loss = 0.45482686\n",
      "Iteration 2353, loss = 0.45449656\n",
      "Iteration 2354, loss = 0.45416617\n",
      "Iteration 2355, loss = 0.45383568\n",
      "Iteration 2356, loss = 0.45350510\n",
      "Iteration 2357, loss = 0.45317444\n",
      "Iteration 2358, loss = 0.45284368\n",
      "Iteration 2359, loss = 0.45251284\n",
      "Iteration 2360, loss = 0.45218191\n",
      "Iteration 2361, loss = 0.45185089\n",
      "Iteration 2362, loss = 0.45151979\n",
      "Iteration 2363, loss = 0.45118861\n",
      "Iteration 2364, loss = 0.45085734\n",
      "Iteration 2365, loss = 0.45052600\n",
      "Iteration 2366, loss = 0.45019457\n",
      "Iteration 2367, loss = 0.44986306\n",
      "Iteration 2368, loss = 0.44953148\n",
      "Iteration 2369, loss = 0.44919982\n",
      "Iteration 2370, loss = 0.44886808\n",
      "Iteration 2371, loss = 0.44853627\n",
      "Iteration 2372, loss = 0.44820438\n",
      "Iteration 2373, loss = 0.44787243\n",
      "Iteration 2374, loss = 0.44754040\n",
      "Iteration 2375, loss = 0.44720830\n",
      "Iteration 2376, loss = 0.44687613\n",
      "Iteration 2377, loss = 0.44654389\n",
      "Iteration 2378, loss = 0.44621158\n",
      "Iteration 2379, loss = 0.44587921\n",
      "Iteration 2380, loss = 0.44554678\n",
      "Iteration 2381, loss = 0.44521428\n",
      "Iteration 2382, loss = 0.44488171\n",
      "Iteration 2383, loss = 0.44454909\n",
      "Iteration 2384, loss = 0.44421640\n",
      "Iteration 2385, loss = 0.44388365\n",
      "Iteration 2386, loss = 0.44355085\n",
      "Iteration 2387, loss = 0.44321798\n",
      "Iteration 2388, loss = 0.44288507\n",
      "Iteration 2389, loss = 0.44255209\n",
      "Iteration 2390, loss = 0.44221906\n",
      "Iteration 2391, loss = 0.44188598\n",
      "Iteration 2392, loss = 0.44155284\n",
      "Iteration 2393, loss = 0.44121966\n",
      "Iteration 2394, loss = 0.44088642\n",
      "Iteration 2395, loss = 0.44055313\n",
      "Iteration 2396, loss = 0.44021980\n",
      "Iteration 2397, loss = 0.43988642\n",
      "Iteration 2398, loss = 0.43955299\n",
      "Iteration 2399, loss = 0.43921952\n",
      "Iteration 2400, loss = 0.43888600\n",
      "Iteration 2401, loss = 0.43855245\n",
      "Iteration 2402, loss = 0.43821885\n",
      "Iteration 2403, loss = 0.43788520\n",
      "Iteration 2404, loss = 0.43755152\n",
      "Iteration 2405, loss = 0.43721781\n",
      "Iteration 2406, loss = 0.43688405\n",
      "Iteration 2407, loss = 0.43655026\n",
      "Iteration 2408, loss = 0.43621643\n",
      "Iteration 2409, loss = 0.43588257\n",
      "Iteration 2410, loss = 0.43554868\n",
      "Iteration 2411, loss = 0.43521475\n",
      "Iteration 2412, loss = 0.43488079\n",
      "Iteration 2413, loss = 0.43454680\n",
      "Iteration 2414, loss = 0.43421279\n",
      "Iteration 2415, loss = 0.43387874\n",
      "Iteration 2416, loss = 0.43354467\n",
      "Iteration 2417, loss = 0.43321058\n",
      "Iteration 2418, loss = 0.43287646\n",
      "Iteration 2419, loss = 0.43254231\n",
      "Iteration 2420, loss = 0.43220815\n",
      "Iteration 2421, loss = 0.43187396\n",
      "Iteration 2422, loss = 0.43153975\n",
      "Iteration 2423, loss = 0.43120552\n",
      "Iteration 2424, loss = 0.43087128\n",
      "Iteration 2425, loss = 0.43053701\n",
      "Iteration 2426, loss = 0.43020274\n",
      "Iteration 2427, loss = 0.42986844\n",
      "Iteration 2428, loss = 0.42953413\n",
      "Iteration 2429, loss = 0.42919981\n",
      "Iteration 2430, loss = 0.42886548\n",
      "Iteration 2431, loss = 0.42853114\n",
      "Iteration 2432, loss = 0.42819678\n",
      "Iteration 2433, loss = 0.42786242\n",
      "Iteration 2434, loss = 0.42752805\n",
      "Iteration 2435, loss = 0.42719368\n",
      "Iteration 2436, loss = 0.42685930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2437, loss = 0.42652491\n",
      "Iteration 2438, loss = 0.42619052\n",
      "Iteration 2439, loss = 0.42585613\n",
      "Iteration 2440, loss = 0.42552174\n",
      "Iteration 2441, loss = 0.42518734\n",
      "Iteration 2442, loss = 0.42485295\n",
      "Iteration 2443, loss = 0.42451856\n",
      "Iteration 2444, loss = 0.42418417\n",
      "Iteration 2445, loss = 0.42384979\n",
      "Iteration 2446, loss = 0.42351541\n",
      "Iteration 2447, loss = 0.42318104\n",
      "Iteration 2448, loss = 0.42284668\n",
      "Iteration 2449, loss = 0.42251232\n",
      "Iteration 2450, loss = 0.42217797\n",
      "Iteration 2451, loss = 0.42184363\n",
      "Iteration 2452, loss = 0.42150931\n",
      "Iteration 2453, loss = 0.42117499\n",
      "Iteration 2454, loss = 0.42084069\n",
      "Iteration 2455, loss = 0.42050640\n",
      "Iteration 2456, loss = 0.42017213\n",
      "Iteration 2457, loss = 0.41983788\n",
      "Iteration 2458, loss = 0.41950364\n",
      "Iteration 2459, loss = 0.41916942\n",
      "Iteration 2460, loss = 0.41883522\n",
      "Iteration 2461, loss = 0.41850105\n",
      "Iteration 2462, loss = 0.41816689\n",
      "Iteration 2463, loss = 0.41783275\n",
      "Iteration 2464, loss = 0.41749864\n",
      "Iteration 2465, loss = 0.41716456\n",
      "Iteration 2466, loss = 0.41683050\n",
      "Iteration 2467, loss = 0.41649646\n",
      "Iteration 2468, loss = 0.41616246\n",
      "Iteration 2469, loss = 0.41582848\n",
      "Iteration 2470, loss = 0.41549453\n",
      "Iteration 2471, loss = 0.41516062\n",
      "Iteration 2472, loss = 0.41482673\n",
      "Iteration 2473, loss = 0.41449288\n",
      "Iteration 2474, loss = 0.41415906\n",
      "Iteration 2475, loss = 0.41382528\n",
      "Iteration 2476, loss = 0.41349153\n",
      "Iteration 2477, loss = 0.41315782\n",
      "Iteration 2478, loss = 0.41282415\n",
      "Iteration 2479, loss = 0.41249052\n",
      "Iteration 2480, loss = 0.41215692\n",
      "Iteration 2481, loss = 0.41182337\n",
      "Iteration 2482, loss = 0.41148986\n",
      "Iteration 2483, loss = 0.41115639\n",
      "Iteration 2484, loss = 0.41082297\n",
      "Iteration 2485, loss = 0.41048959\n",
      "Iteration 2486, loss = 0.41015625\n",
      "Iteration 2487, loss = 0.40982297\n",
      "Iteration 2488, loss = 0.40948973\n",
      "Iteration 2489, loss = 0.40915654\n",
      "Iteration 2490, loss = 0.40882340\n",
      "Iteration 2491, loss = 0.40849031\n",
      "Iteration 2492, loss = 0.40815727\n",
      "Iteration 2493, loss = 0.40782429\n",
      "Iteration 2494, loss = 0.40749136\n",
      "Iteration 2495, loss = 0.40715848\n",
      "Iteration 2496, loss = 0.40682566\n",
      "Iteration 2497, loss = 0.40649290\n",
      "Iteration 2498, loss = 0.40616020\n",
      "Iteration 2499, loss = 0.40582755\n",
      "Iteration 2500, loss = 0.40549496\n",
      "Iteration 2501, loss = 0.40516244\n",
      "Iteration 2502, loss = 0.40482997\n",
      "Iteration 2503, loss = 0.40449757\n",
      "Iteration 2504, loss = 0.40416523\n",
      "Iteration 2505, loss = 0.40383296\n",
      "Iteration 2506, loss = 0.40350075\n",
      "Iteration 2507, loss = 0.40316861\n",
      "Iteration 2508, loss = 0.40283653\n",
      "Iteration 2509, loss = 0.40250453\n",
      "Iteration 2510, loss = 0.40217259\n",
      "Iteration 2511, loss = 0.40184073\n",
      "Iteration 2512, loss = 0.40150893\n",
      "Iteration 2513, loss = 0.40117721\n",
      "Iteration 2514, loss = 0.40084556\n",
      "Iteration 2515, loss = 0.40051398\n",
      "Iteration 2516, loss = 0.40018248\n",
      "Iteration 2517, loss = 0.39985106\n",
      "Iteration 2518, loss = 0.39951971\n",
      "Iteration 2519, loss = 0.39918844\n",
      "Iteration 2520, loss = 0.39885725\n",
      "Iteration 2521, loss = 0.39852614\n",
      "Iteration 2522, loss = 0.39819511\n",
      "Iteration 2523, loss = 0.39786416\n",
      "Iteration 2524, loss = 0.39753329\n",
      "Iteration 2525, loss = 0.39720251\n",
      "Iteration 2526, loss = 0.39687181\n",
      "Iteration 2527, loss = 0.39654120\n",
      "Iteration 2528, loss = 0.39621067\n",
      "Iteration 2529, loss = 0.39588023\n",
      "Iteration 2530, loss = 0.39554988\n",
      "Iteration 2531, loss = 0.39521962\n",
      "Iteration 2532, loss = 0.39488945\n",
      "Iteration 2533, loss = 0.39455937\n",
      "Iteration 2534, loss = 0.39422938\n",
      "Iteration 2535, loss = 0.39389948\n",
      "Iteration 2536, loss = 0.39356968\n",
      "Iteration 2537, loss = 0.39323997\n",
      "Iteration 2538, loss = 0.39291035\n",
      "Iteration 2539, loss = 0.39258084\n",
      "Iteration 2540, loss = 0.39225142\n",
      "Iteration 2541, loss = 0.39192209\n",
      "Iteration 2542, loss = 0.39159287\n",
      "Iteration 2543, loss = 0.39126375\n",
      "Iteration 2544, loss = 0.39093473\n",
      "Iteration 2545, loss = 0.39060580\n",
      "Iteration 2546, loss = 0.39027699\n",
      "Iteration 2547, loss = 0.38994827\n",
      "Iteration 2548, loss = 0.38961966\n",
      "Iteration 2549, loss = 0.38929115\n",
      "Iteration 2550, loss = 0.38896276\n",
      "Iteration 2551, loss = 0.38863446\n",
      "Iteration 2552, loss = 0.38830628\n",
      "Iteration 2553, loss = 0.38797820\n",
      "Iteration 2554, loss = 0.38765024\n",
      "Iteration 2555, loss = 0.38732238\n",
      "Iteration 2556, loss = 0.38699463\n",
      "Iteration 2557, loss = 0.38666700\n",
      "Iteration 2558, loss = 0.38633948\n",
      "Iteration 2559, loss = 0.38601208\n",
      "Iteration 2560, loss = 0.38568478\n",
      "Iteration 2561, loss = 0.38535761\n",
      "Iteration 2562, loss = 0.38503055\n",
      "Iteration 2563, loss = 0.38470361\n",
      "Iteration 2564, loss = 0.38437678\n",
      "Iteration 2565, loss = 0.38405008\n",
      "Iteration 2566, loss = 0.38372349\n",
      "Iteration 2567, loss = 0.38339703\n",
      "Iteration 2568, loss = 0.38307068\n",
      "Iteration 2569, loss = 0.38274446\n",
      "Iteration 2570, loss = 0.38241836\n",
      "Iteration 2571, loss = 0.38209239\n",
      "Iteration 2572, loss = 0.38176654\n",
      "Iteration 2573, loss = 0.38144082\n",
      "Iteration 2574, loss = 0.38111522\n",
      "Iteration 2575, loss = 0.38078975\n",
      "Iteration 2576, loss = 0.38046441\n",
      "Iteration 2577, loss = 0.38013919\n",
      "Iteration 2578, loss = 0.37981411\n",
      "Iteration 2579, loss = 0.37948916\n",
      "Iteration 2580, loss = 0.37916433\n",
      "Iteration 2581, loss = 0.37883964\n",
      "Iteration 2582, loss = 0.37851509\n",
      "Iteration 2583, loss = 0.37819066\n",
      "Iteration 2584, loss = 0.37786637\n",
      "Iteration 2585, loss = 0.37754222\n",
      "Iteration 2586, loss = 0.37721820\n",
      "Iteration 2587, loss = 0.37689432\n",
      "Iteration 2588, loss = 0.37657058\n",
      "Iteration 2589, loss = 0.37624697\n",
      "Iteration 2590, loss = 0.37592351\n",
      "Iteration 2591, loss = 0.37560018\n",
      "Iteration 2592, loss = 0.37527700\n",
      "Iteration 2593, loss = 0.37495395\n",
      "Iteration 2594, loss = 0.37463105\n",
      "Iteration 2595, loss = 0.37430829\n",
      "Iteration 2596, loss = 0.37398568\n",
      "Iteration 2597, loss = 0.37366321\n",
      "Iteration 2598, loss = 0.37334089\n",
      "Iteration 2599, loss = 0.37301871\n",
      "Iteration 2600, loss = 0.37269668\n",
      "Iteration 2601, loss = 0.37237479\n",
      "Iteration 2602, loss = 0.37205306\n",
      "Iteration 2603, loss = 0.37173147\n",
      "Iteration 2604, loss = 0.37141003\n",
      "Iteration 2605, loss = 0.37108875\n",
      "Iteration 2606, loss = 0.37076761\n",
      "Iteration 2607, loss = 0.37044663\n",
      "Iteration 2608, loss = 0.37012580\n",
      "Iteration 2609, loss = 0.36980512\n",
      "Iteration 2610, loss = 0.36948460\n",
      "Iteration 2611, loss = 0.36916424\n",
      "Iteration 2612, loss = 0.36884402\n",
      "Iteration 2613, loss = 0.36852397\n",
      "Iteration 2614, loss = 0.36820407\n",
      "Iteration 2615, loss = 0.36788433\n",
      "Iteration 2616, loss = 0.36756475\n",
      "Iteration 2617, loss = 0.36724533\n",
      "Iteration 2618, loss = 0.36692607\n",
      "Iteration 2619, loss = 0.36660697\n",
      "Iteration 2620, loss = 0.36628803\n",
      "Iteration 2621, loss = 0.36596925\n",
      "Iteration 2622, loss = 0.36565063\n",
      "Iteration 2623, loss = 0.36533218\n",
      "Iteration 2624, loss = 0.36501389\n",
      "Iteration 2625, loss = 0.36469577\n",
      "Iteration 2626, loss = 0.36437781\n",
      "Iteration 2627, loss = 0.36406002\n",
      "Iteration 2628, loss = 0.36374240\n",
      "Iteration 2629, loss = 0.36342494\n",
      "Iteration 2630, loss = 0.36310766\n",
      "Iteration 2631, loss = 0.36279054\n",
      "Iteration 2632, loss = 0.36247359\n",
      "Iteration 2633, loss = 0.36215681\n",
      "Iteration 2634, loss = 0.36184020\n",
      "Iteration 2635, loss = 0.36152377\n",
      "Iteration 2636, loss = 0.36120750\n",
      "Iteration 2637, loss = 0.36089141\n",
      "Iteration 2638, loss = 0.36057549\n",
      "Iteration 2639, loss = 0.36025975\n",
      "Iteration 2640, loss = 0.35994418\n",
      "Iteration 2641, loss = 0.35962879\n",
      "Iteration 2642, loss = 0.35931357\n",
      "Iteration 2643, loss = 0.35899853\n",
      "Iteration 2644, loss = 0.35868367\n",
      "Iteration 2645, loss = 0.35836899\n",
      "Iteration 2646, loss = 0.35805448\n",
      "Iteration 2647, loss = 0.35774015\n",
      "Iteration 2648, loss = 0.35742601\n",
      "Iteration 2649, loss = 0.35711204\n",
      "Iteration 2650, loss = 0.35679826\n",
      "Iteration 2651, loss = 0.35648466\n",
      "Iteration 2652, loss = 0.35617124\n",
      "Iteration 2653, loss = 0.35585800\n",
      "Iteration 2654, loss = 0.35554495\n",
      "Iteration 2655, loss = 0.35523208\n",
      "Iteration 2656, loss = 0.35491939\n",
      "Iteration 2657, loss = 0.35460689\n",
      "Iteration 2658, loss = 0.35429458\n",
      "Iteration 2659, loss = 0.35398246\n",
      "Iteration 2660, loss = 0.35367052\n",
      "Iteration 2661, loss = 0.35335877\n",
      "Iteration 2662, loss = 0.35304721\n",
      "Iteration 2663, loss = 0.35273583\n",
      "Iteration 2664, loss = 0.35242465\n",
      "Iteration 2665, loss = 0.35211366\n",
      "Iteration 2666, loss = 0.35180285\n",
      "Iteration 2667, loss = 0.35149224\n",
      "Iteration 2668, loss = 0.35118182\n",
      "Iteration 2669, loss = 0.35087160\n",
      "Iteration 2670, loss = 0.35056156\n",
      "Iteration 2671, loss = 0.35025172\n",
      "Iteration 2672, loss = 0.34994208\n",
      "Iteration 2673, loss = 0.34963263\n",
      "Iteration 2674, loss = 0.34932337\n",
      "Iteration 2675, loss = 0.34901431\n",
      "Iteration 2676, loss = 0.34870545\n",
      "Iteration 2677, loss = 0.34839678\n",
      "Iteration 2678, loss = 0.34808831\n",
      "Iteration 2679, loss = 0.34778004\n",
      "Iteration 2680, loss = 0.34747197\n",
      "Iteration 2681, loss = 0.34716410\n",
      "Iteration 2682, loss = 0.34685642\n",
      "Iteration 2683, loss = 0.34654895\n",
      "Iteration 2684, loss = 0.34624168\n",
      "Iteration 2685, loss = 0.34593461\n",
      "Iteration 2686, loss = 0.34562774\n",
      "Iteration 2687, loss = 0.34532107\n",
      "Iteration 2688, loss = 0.34501461\n",
      "Iteration 2689, loss = 0.34470834\n",
      "Iteration 2690, loss = 0.34440229\n",
      "Iteration 2691, loss = 0.34409643\n",
      "Iteration 2692, loss = 0.34379079\n",
      "Iteration 2693, loss = 0.34348534\n",
      "Iteration 2694, loss = 0.34318011\n",
      "Iteration 2695, loss = 0.34287507\n",
      "Iteration 2696, loss = 0.34257025\n",
      "Iteration 2697, loss = 0.34226563\n",
      "Iteration 2698, loss = 0.34196123\n",
      "Iteration 2699, loss = 0.34165702\n",
      "Iteration 2700, loss = 0.34135303\n",
      "Iteration 2701, loss = 0.34104925\n",
      "Iteration 2702, loss = 0.34074568\n",
      "Iteration 2703, loss = 0.34044232\n",
      "Iteration 2704, loss = 0.34013916\n",
      "Iteration 2705, loss = 0.33983622\n",
      "Iteration 2706, loss = 0.33953349\n",
      "Iteration 2707, loss = 0.33923098\n",
      "Iteration 2708, loss = 0.33892867\n",
      "Iteration 2709, loss = 0.33862658\n",
      "Iteration 2710, loss = 0.33832470\n",
      "Iteration 2711, loss = 0.33802304\n",
      "Iteration 2712, loss = 0.33772159\n",
      "Iteration 2713, loss = 0.33742035\n",
      "Iteration 2714, loss = 0.33711933\n",
      "Iteration 2715, loss = 0.33681853\n",
      "Iteration 2716, loss = 0.33651794\n",
      "Iteration 2717, loss = 0.33621757\n",
      "Iteration 2718, loss = 0.33591742\n",
      "Iteration 2719, loss = 0.33561748\n",
      "Iteration 2720, loss = 0.33531776\n",
      "Iteration 2721, loss = 0.33501826\n",
      "Iteration 2722, loss = 0.33471898\n",
      "Iteration 2723, loss = 0.33441992\n",
      "Iteration 2724, loss = 0.33412107\n",
      "Iteration 2725, loss = 0.33382245\n",
      "Iteration 2726, loss = 0.33352405\n",
      "Iteration 2727, loss = 0.33322586\n",
      "Iteration 2728, loss = 0.33292790\n",
      "Iteration 2729, loss = 0.33263016\n",
      "Iteration 2730, loss = 0.33233265\n",
      "Iteration 2731, loss = 0.33203535\n",
      "Iteration 2732, loss = 0.33173828\n",
      "Iteration 2733, loss = 0.33144143\n",
      "Iteration 2734, loss = 0.33114480\n",
      "Iteration 2735, loss = 0.33084840\n",
      "Iteration 2736, loss = 0.33055223\n",
      "Iteration 2737, loss = 0.33025627\n",
      "Iteration 2738, loss = 0.32996055\n",
      "Iteration 2739, loss = 0.32966504\n",
      "Iteration 2740, loss = 0.32936977\n",
      "Iteration 2741, loss = 0.32907472\n",
      "Iteration 2742, loss = 0.32877990\n",
      "Iteration 2743, loss = 0.32848530\n",
      "Iteration 2744, loss = 0.32819093\n",
      "Iteration 2745, loss = 0.32789679\n",
      "Iteration 2746, loss = 0.32760288\n",
      "Iteration 2747, loss = 0.32730919\n",
      "Iteration 2748, loss = 0.32701574\n",
      "Iteration 2749, loss = 0.32672251\n",
      "Iteration 2750, loss = 0.32642951\n",
      "Iteration 2751, loss = 0.32613675\n",
      "Iteration 2752, loss = 0.32584421\n",
      "Iteration 2753, loss = 0.32555190\n",
      "Iteration 2754, loss = 0.32525983\n",
      "Iteration 2755, loss = 0.32496798\n",
      "Iteration 2756, loss = 0.32467637\n",
      "Iteration 2757, loss = 0.32438499\n",
      "Iteration 2758, loss = 0.32409384\n",
      "Iteration 2759, loss = 0.32380293\n",
      "Iteration 2760, loss = 0.32351224\n",
      "Iteration 2761, loss = 0.32322179\n",
      "Iteration 2762, loss = 0.32293158\n",
      "Iteration 2763, loss = 0.32264159\n",
      "Iteration 2764, loss = 0.32235184\n",
      "Iteration 2765, loss = 0.32206233\n",
      "Iteration 2766, loss = 0.32177305\n",
      "Iteration 2767, loss = 0.32148400\n",
      "Iteration 2768, loss = 0.32119520\n",
      "Iteration 2769, loss = 0.32090662\n",
      "Iteration 2770, loss = 0.32061828\n",
      "Iteration 2771, loss = 0.32033018\n",
      "Iteration 2772, loss = 0.32004232\n",
      "Iteration 2773, loss = 0.31975469\n",
      "Iteration 2774, loss = 0.31946730\n",
      "Iteration 2775, loss = 0.31918014\n",
      "Iteration 2776, loss = 0.31889323\n",
      "Iteration 2777, loss = 0.31860655\n",
      "Iteration 2778, loss = 0.31832011\n",
      "Iteration 2779, loss = 0.31803391\n",
      "Iteration 2780, loss = 0.31774795\n",
      "Iteration 2781, loss = 0.31746222\n",
      "Iteration 2782, loss = 0.31717674\n",
      "Iteration 2783, loss = 0.31689150\n",
      "Iteration 2784, loss = 0.31660649\n",
      "Iteration 2785, loss = 0.31632173\n",
      "Iteration 2786, loss = 0.31603720\n",
      "Iteration 2787, loss = 0.31575292\n",
      "Iteration 2788, loss = 0.31546888\n",
      "Iteration 2789, loss = 0.31518508\n",
      "Iteration 2790, loss = 0.31490152\n",
      "Iteration 2791, loss = 0.31461820\n",
      "Iteration 2792, loss = 0.31433512\n",
      "Iteration 2793, loss = 0.31405229\n",
      "Iteration 2794, loss = 0.31376969\n",
      "Iteration 2795, loss = 0.31348734\n",
      "Iteration 2796, loss = 0.31320524\n",
      "Iteration 2797, loss = 0.31292337\n",
      "Iteration 2798, loss = 0.31264175\n",
      "Iteration 2799, loss = 0.31236038\n",
      "Iteration 2800, loss = 0.31207925\n",
      "Iteration 2801, loss = 0.31179836\n",
      "Iteration 2802, loss = 0.31151771\n",
      "Iteration 2803, loss = 0.31123731\n",
      "Iteration 2804, loss = 0.31095716\n",
      "Iteration 2805, loss = 0.31067724\n",
      "Iteration 2806, loss = 0.31039758\n",
      "Iteration 2807, loss = 0.31011816\n",
      "Iteration 2808, loss = 0.30983898\n",
      "Iteration 2809, loss = 0.30956005\n",
      "Iteration 2810, loss = 0.30928137\n",
      "Iteration 2811, loss = 0.30900293\n",
      "Iteration 2812, loss = 0.30872474\n",
      "Iteration 2813, loss = 0.30844680\n",
      "Iteration 2814, loss = 0.30816910\n",
      "Iteration 2815, loss = 0.30789165\n",
      "Iteration 2816, loss = 0.30761445\n",
      "Iteration 2817, loss = 0.30733749\n",
      "Iteration 2818, loss = 0.30706078\n",
      "Iteration 2819, loss = 0.30678432\n",
      "Iteration 2820, loss = 0.30650810\n",
      "Iteration 2821, loss = 0.30623214\n",
      "Iteration 2822, loss = 0.30595642\n",
      "Iteration 2823, loss = 0.30568095\n",
      "Iteration 2824, loss = 0.30540573\n",
      "Iteration 2825, loss = 0.30513076\n",
      "Iteration 2826, loss = 0.30485603\n",
      "Iteration 2827, loss = 0.30458156\n",
      "Iteration 2828, loss = 0.30430733\n",
      "Iteration 2829, loss = 0.30403336\n",
      "Iteration 2830, loss = 0.30375963\n",
      "Iteration 2831, loss = 0.30348615\n",
      "Iteration 2832, loss = 0.30321293\n",
      "Iteration 2833, loss = 0.30293995\n",
      "Iteration 2834, loss = 0.30266722\n",
      "Iteration 2835, loss = 0.30239475\n",
      "Iteration 2836, loss = 0.30212252\n",
      "Iteration 2837, loss = 0.30185054\n",
      "Iteration 2838, loss = 0.30157882\n",
      "Iteration 2839, loss = 0.30130734\n",
      "Iteration 2840, loss = 0.30103612\n",
      "Iteration 2841, loss = 0.30076515\n",
      "Iteration 2842, loss = 0.30049443\n",
      "Iteration 2843, loss = 0.30022396\n",
      "Iteration 2844, loss = 0.29995374\n",
      "Iteration 2845, loss = 0.29968377\n",
      "Iteration 2846, loss = 0.29941406\n",
      "Iteration 2847, loss = 0.29914459\n",
      "Iteration 2848, loss = 0.29887538\n",
      "Iteration 2849, loss = 0.29860642\n",
      "Iteration 2850, loss = 0.29833772\n",
      "Iteration 2851, loss = 0.29806926\n",
      "Iteration 2852, loss = 0.29780106\n",
      "Iteration 2853, loss = 0.29753311\n",
      "Iteration 2854, loss = 0.29726541\n",
      "Iteration 2855, loss = 0.29699797\n",
      "Iteration 2856, loss = 0.29673077\n",
      "Iteration 2857, loss = 0.29646384\n",
      "Iteration 2858, loss = 0.29619715\n",
      "Iteration 2859, loss = 0.29593072\n",
      "Iteration 2860, loss = 0.29566454\n",
      "Iteration 2861, loss = 0.29539861\n",
      "Iteration 2862, loss = 0.29513294\n",
      "Iteration 2863, loss = 0.29486752\n",
      "Iteration 2864, loss = 0.29460236\n",
      "Iteration 2865, loss = 0.29433744\n",
      "Iteration 2866, loss = 0.29407279\n",
      "Iteration 2867, loss = 0.29380838\n",
      "Iteration 2868, loss = 0.29354423\n",
      "Iteration 2869, loss = 0.29328034\n",
      "Iteration 2870, loss = 0.29301670\n",
      "Iteration 2871, loss = 0.29275331\n",
      "Iteration 2872, loss = 0.29249017\n",
      "Iteration 2873, loss = 0.29222730\n",
      "Iteration 2874, loss = 0.29196467\n",
      "Iteration 2875, loss = 0.29170230\n",
      "Iteration 2876, loss = 0.29144018\n",
      "Iteration 2877, loss = 0.29117832\n",
      "Iteration 2878, loss = 0.29091672\n",
      "Iteration 2879, loss = 0.29065537\n",
      "Iteration 2880, loss = 0.29039427\n",
      "Iteration 2881, loss = 0.29013343\n",
      "Iteration 2882, loss = 0.28987284\n",
      "Iteration 2883, loss = 0.28961251\n",
      "Iteration 2884, loss = 0.28935243\n",
      "Iteration 2885, loss = 0.28909261\n",
      "Iteration 2886, loss = 0.28883304\n",
      "Iteration 2887, loss = 0.28857373\n",
      "Iteration 2888, loss = 0.28831467\n",
      "Iteration 2889, loss = 0.28805587\n",
      "Iteration 2890, loss = 0.28779732\n",
      "Iteration 2891, loss = 0.28753903\n",
      "Iteration 2892, loss = 0.28728100\n",
      "Iteration 2893, loss = 0.28702322\n",
      "Iteration 2894, loss = 0.28676569\n",
      "Iteration 2895, loss = 0.28650842\n",
      "Iteration 2896, loss = 0.28625141\n",
      "Iteration 2897, loss = 0.28599465\n",
      "Iteration 2898, loss = 0.28573815\n",
      "Iteration 2899, loss = 0.28548190\n",
      "Iteration 2900, loss = 0.28522591\n",
      "Iteration 2901, loss = 0.28497017\n",
      "Iteration 2902, loss = 0.28471469\n",
      "Iteration 2903, loss = 0.28445947\n",
      "Iteration 2904, loss = 0.28420450\n",
      "Iteration 2905, loss = 0.28394978\n",
      "Iteration 2906, loss = 0.28369533\n",
      "Iteration 2907, loss = 0.28344112\n",
      "Iteration 2908, loss = 0.28318718\n",
      "Iteration 2909, loss = 0.28293349\n",
      "Iteration 2910, loss = 0.28268005\n",
      "Iteration 2911, loss = 0.28242687\n",
      "Iteration 2912, loss = 0.28217395\n",
      "Iteration 2913, loss = 0.28192128\n",
      "Iteration 2914, loss = 0.28166887\n",
      "Iteration 2915, loss = 0.28141671\n",
      "Iteration 2916, loss = 0.28116481\n",
      "Iteration 2917, loss = 0.28091316\n",
      "Iteration 2918, loss = 0.28066178\n",
      "Iteration 2919, loss = 0.28041064\n",
      "Iteration 2920, loss = 0.28015976\n",
      "Iteration 2921, loss = 0.27990914\n",
      "Iteration 2922, loss = 0.27965878\n",
      "Iteration 2923, loss = 0.27940866\n",
      "Iteration 2924, loss = 0.27915881\n",
      "Iteration 2925, loss = 0.27890921\n",
      "Iteration 2926, loss = 0.27865987\n",
      "Iteration 2927, loss = 0.27841078\n",
      "Iteration 2928, loss = 0.27816194\n",
      "Iteration 2929, loss = 0.27791337\n",
      "Iteration 2930, loss = 0.27766505\n",
      "Iteration 2931, loss = 0.27741698\n",
      "Iteration 2932, loss = 0.27716917\n",
      "Iteration 2933, loss = 0.27692161\n",
      "Iteration 2934, loss = 0.27667431\n",
      "Iteration 2935, loss = 0.27642727\n",
      "Iteration 2936, loss = 0.27618048\n",
      "Iteration 2937, loss = 0.27593395\n",
      "Iteration 2938, loss = 0.27568767\n",
      "Iteration 2939, loss = 0.27544164\n",
      "Iteration 2940, loss = 0.27519588\n",
      "Iteration 2941, loss = 0.27495036\n",
      "Iteration 2942, loss = 0.27470511\n",
      "Iteration 2943, loss = 0.27446010\n",
      "Iteration 2944, loss = 0.27421535\n",
      "Iteration 2945, loss = 0.27397086\n",
      "Iteration 2946, loss = 0.27372662\n",
      "Iteration 2947, loss = 0.27348264\n",
      "Iteration 2948, loss = 0.27323891\n",
      "Iteration 2949, loss = 0.27299544\n",
      "Iteration 2950, loss = 0.27275222\n",
      "Iteration 2951, loss = 0.27250926\n",
      "Iteration 2952, loss = 0.27226655\n",
      "Iteration 2953, loss = 0.27202409\n",
      "Iteration 2954, loss = 0.27178189\n",
      "Iteration 2955, loss = 0.27153995\n",
      "Iteration 2956, loss = 0.27129826\n",
      "Iteration 2957, loss = 0.27105682\n",
      "Iteration 2958, loss = 0.27081564\n",
      "Iteration 2959, loss = 0.27057471\n",
      "Iteration 2960, loss = 0.27033404\n",
      "Iteration 2961, loss = 0.27009362\n",
      "Iteration 2962, loss = 0.26985345\n",
      "Iteration 2963, loss = 0.26961354\n",
      "Iteration 2964, loss = 0.26937388\n",
      "Iteration 2965, loss = 0.26913447\n",
      "Iteration 2966, loss = 0.26889532\n",
      "Iteration 2967, loss = 0.26865643\n",
      "Iteration 2968, loss = 0.26841778\n",
      "Iteration 2969, loss = 0.26817939\n",
      "Iteration 2970, loss = 0.26794126\n",
      "Iteration 2971, loss = 0.26770337\n",
      "Iteration 2972, loss = 0.26746574\n",
      "Iteration 2973, loss = 0.26722837\n",
      "Iteration 2974, loss = 0.26699124\n",
      "Iteration 2975, loss = 0.26675437\n",
      "Iteration 2976, loss = 0.26651775\n",
      "Iteration 2977, loss = 0.26628139\n",
      "Iteration 2978, loss = 0.26604528\n",
      "Iteration 2979, loss = 0.26580942\n",
      "Iteration 2980, loss = 0.26557381\n",
      "Iteration 2981, loss = 0.26533846\n",
      "Iteration 2982, loss = 0.26510335\n",
      "Iteration 2983, loss = 0.26486850\n",
      "Iteration 2984, loss = 0.26463391\n",
      "Iteration 2985, loss = 0.26439956\n",
      "Iteration 2986, loss = 0.26416547\n",
      "Iteration 2987, loss = 0.26393162\n",
      "Iteration 2988, loss = 0.26369803\n",
      "Iteration 2989, loss = 0.26346470\n",
      "Iteration 2990, loss = 0.26323161\n",
      "Iteration 2991, loss = 0.26299877\n",
      "Iteration 2992, loss = 0.26276619\n",
      "Iteration 2993, loss = 0.26253386\n",
      "Iteration 2994, loss = 0.26230178\n",
      "Iteration 2995, loss = 0.26206995\n",
      "Iteration 2996, loss = 0.26183837\n",
      "Iteration 2997, loss = 0.26160704\n",
      "Iteration 2998, loss = 0.26137596\n",
      "Iteration 2999, loss = 0.26114514\n",
      "Iteration 3000, loss = 0.26091456\n",
      "Iteration 3001, loss = 0.26068423\n",
      "Iteration 3002, loss = 0.26045416\n",
      "Iteration 3003, loss = 0.26022433\n",
      "Iteration 3004, loss = 0.25999476\n",
      "Iteration 3005, loss = 0.25976543\n",
      "Iteration 3006, loss = 0.25953635\n",
      "Iteration 3007, loss = 0.25930753\n",
      "Iteration 3008, loss = 0.25907895\n",
      "Iteration 3009, loss = 0.25885063\n",
      "Iteration 3010, loss = 0.25862255\n",
      "Iteration 3011, loss = 0.25839472\n",
      "Iteration 3012, loss = 0.25816714\n",
      "Iteration 3013, loss = 0.25793981\n",
      "Iteration 3014, loss = 0.25771273\n",
      "Iteration 3015, loss = 0.25748590\n",
      "Iteration 3016, loss = 0.25725931\n",
      "Iteration 3017, loss = 0.25703298\n",
      "Iteration 3018, loss = 0.25680689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3019, loss = 0.25658105\n",
      "Iteration 3020, loss = 0.25635546\n",
      "Iteration 3021, loss = 0.25613012\n",
      "Iteration 3022, loss = 0.25590502\n",
      "Iteration 3023, loss = 0.25568017\n",
      "Iteration 3024, loss = 0.25545557\n",
      "Iteration 3025, loss = 0.25523122\n",
      "Iteration 3026, loss = 0.25500712\n",
      "Iteration 3027, loss = 0.25478326\n",
      "Iteration 3028, loss = 0.25455965\n",
      "Iteration 3029, loss = 0.25433628\n",
      "Iteration 3030, loss = 0.25411316\n",
      "Iteration 3031, loss = 0.25389029\n",
      "Iteration 3032, loss = 0.25366767\n",
      "Iteration 3033, loss = 0.25344529\n",
      "Iteration 3034, loss = 0.25322316\n",
      "Iteration 3035, loss = 0.25300127\n",
      "Iteration 3036, loss = 0.25277963\n",
      "Iteration 3037, loss = 0.25255824\n",
      "Iteration 3038, loss = 0.25233709\n",
      "Iteration 3039, loss = 0.25211618\n",
      "Iteration 3040, loss = 0.25189552\n",
      "Iteration 3041, loss = 0.25167511\n",
      "Iteration 3042, loss = 0.25145494\n",
      "Iteration 3043, loss = 0.25123502\n",
      "Iteration 3044, loss = 0.25101534\n",
      "Iteration 3045, loss = 0.25079590\n",
      "Iteration 3046, loss = 0.25057671\n",
      "Iteration 3047, loss = 0.25035777\n",
      "Iteration 3048, loss = 0.25013906\n",
      "Iteration 3049, loss = 0.24992061\n",
      "Iteration 3050, loss = 0.24970239\n",
      "Iteration 3051, loss = 0.24948442\n",
      "Iteration 3052, loss = 0.24926669\n",
      "Iteration 3053, loss = 0.24904921\n",
      "Iteration 3054, loss = 0.24883197\n",
      "Iteration 3055, loss = 0.24861497\n",
      "Iteration 3056, loss = 0.24839821\n",
      "Iteration 3057, loss = 0.24818170\n",
      "Iteration 3058, loss = 0.24796543\n",
      "Iteration 3059, loss = 0.24774940\n",
      "Iteration 3060, loss = 0.24753361\n",
      "Iteration 3061, loss = 0.24731807\n",
      "Iteration 3062, loss = 0.24710277\n",
      "Iteration 3063, loss = 0.24688771\n",
      "Iteration 3064, loss = 0.24667289\n",
      "Iteration 3065, loss = 0.24645831\n",
      "Iteration 3066, loss = 0.24624397\n",
      "Iteration 3067, loss = 0.24602988\n",
      "Iteration 3068, loss = 0.24581602\n",
      "Iteration 3069, loss = 0.24560240\n",
      "Iteration 3070, loss = 0.24538903\n",
      "Iteration 3071, loss = 0.24517590\n",
      "Iteration 3072, loss = 0.24496300\n",
      "Iteration 3073, loss = 0.24475035\n",
      "Iteration 3074, loss = 0.24453793\n",
      "Iteration 3075, loss = 0.24432576\n",
      "Iteration 3076, loss = 0.24411382\n",
      "Iteration 3077, loss = 0.24390212\n",
      "Iteration 3078, loss = 0.24369067\n",
      "Iteration 3079, loss = 0.24347945\n",
      "Iteration 3080, loss = 0.24326847\n",
      "Iteration 3081, loss = 0.24305773\n",
      "Iteration 3082, loss = 0.24284722\n",
      "Iteration 3083, loss = 0.24263696\n",
      "Iteration 3084, loss = 0.24242693\n",
      "Iteration 3085, loss = 0.24221714\n",
      "Iteration 3086, loss = 0.24200759\n",
      "Iteration 3087, loss = 0.24179828\n",
      "Iteration 3088, loss = 0.24158920\n",
      "Iteration 3089, loss = 0.24138036\n",
      "Iteration 3090, loss = 0.24117175\n",
      "Iteration 3091, loss = 0.24096339\n",
      "Iteration 3092, loss = 0.24075526\n",
      "Iteration 3093, loss = 0.24054736\n",
      "Iteration 3094, loss = 0.24033971\n",
      "Iteration 3095, loss = 0.24013228\n",
      "Iteration 3096, loss = 0.23992510\n",
      "Iteration 3097, loss = 0.23971815\n",
      "Iteration 3098, loss = 0.23951143\n",
      "Iteration 3099, loss = 0.23930495\n",
      "Iteration 3100, loss = 0.23909871\n",
      "Iteration 3101, loss = 0.23889270\n",
      "Iteration 3102, loss = 0.23868692\n",
      "Iteration 3103, loss = 0.23848138\n",
      "Iteration 3104, loss = 0.23827607\n",
      "Iteration 3105, loss = 0.23807100\n",
      "Iteration 3106, loss = 0.23786616\n",
      "Iteration 3107, loss = 0.23766155\n",
      "Iteration 3108, loss = 0.23745718\n",
      "Iteration 3109, loss = 0.23725304\n",
      "Iteration 3110, loss = 0.23704914\n",
      "Iteration 3111, loss = 0.23684546\n",
      "Iteration 3112, loss = 0.23664202\n",
      "Iteration 3113, loss = 0.23643882\n",
      "Iteration 3114, loss = 0.23623584\n",
      "Iteration 3115, loss = 0.23603310\n",
      "Iteration 3116, loss = 0.23583058\n",
      "Iteration 3117, loss = 0.23562830\n",
      "Iteration 3118, loss = 0.23542626\n",
      "Iteration 3119, loss = 0.23522444\n",
      "Iteration 3120, loss = 0.23502285\n",
      "Iteration 3121, loss = 0.23482150\n",
      "Iteration 3122, loss = 0.23462037\n",
      "Iteration 3123, loss = 0.23441948\n",
      "Iteration 3124, loss = 0.23421882\n",
      "Iteration 3125, loss = 0.23401838\n",
      "Iteration 3126, loss = 0.23381818\n",
      "Iteration 3127, loss = 0.23361820\n",
      "Iteration 3128, loss = 0.23341846\n",
      "Iteration 3129, loss = 0.23321894\n",
      "Iteration 3130, loss = 0.23301966\n",
      "Iteration 3131, loss = 0.23282060\n",
      "Iteration 3132, loss = 0.23262177\n",
      "Iteration 3133, loss = 0.23242317\n",
      "Iteration 3134, loss = 0.23222480\n",
      "Iteration 3135, loss = 0.23202666\n",
      "Iteration 3136, loss = 0.23182874\n",
      "Iteration 3137, loss = 0.23163105\n",
      "Iteration 3138, loss = 0.23143359\n",
      "Iteration 3139, loss = 0.23123636\n",
      "Iteration 3140, loss = 0.23103935\n",
      "Iteration 3141, loss = 0.23084257\n",
      "Iteration 3142, loss = 0.23064602\n",
      "Iteration 3143, loss = 0.23044969\n",
      "Iteration 3144, loss = 0.23025359\n",
      "Iteration 3145, loss = 0.23005771\n",
      "Iteration 3146, loss = 0.22986207\n",
      "Iteration 3147, loss = 0.22966664\n",
      "Iteration 3148, loss = 0.22947144\n",
      "Iteration 3149, loss = 0.22927647\n",
      "Iteration 3150, loss = 0.22908172\n",
      "Iteration 3151, loss = 0.22888720\n",
      "Iteration 3152, loss = 0.22869290\n",
      "Iteration 3153, loss = 0.22849883\n",
      "Iteration 3154, loss = 0.22830498\n",
      "Iteration 3155, loss = 0.22811135\n",
      "Iteration 3156, loss = 0.22791795\n",
      "Iteration 3157, loss = 0.22772477\n",
      "Iteration 3158, loss = 0.22753181\n",
      "Iteration 3159, loss = 0.22733908\n",
      "Iteration 3160, loss = 0.22714657\n",
      "Iteration 3161, loss = 0.22695428\n",
      "Iteration 3162, loss = 0.22676222\n",
      "Iteration 3163, loss = 0.22657037\n",
      "Iteration 3164, loss = 0.22637875\n",
      "Iteration 3165, loss = 0.22618735\n",
      "Iteration 3166, loss = 0.22599617\n",
      "Iteration 3167, loss = 0.22580522\n",
      "Iteration 3168, loss = 0.22561448\n",
      "Iteration 3169, loss = 0.22542397\n",
      "Iteration 3170, loss = 0.22523367\n",
      "Iteration 3171, loss = 0.22504360\n",
      "Iteration 3172, loss = 0.22485375\n",
      "Iteration 3173, loss = 0.22466411\n",
      "Iteration 3174, loss = 0.22447470\n",
      "Iteration 3175, loss = 0.22428551\n",
      "Iteration 3176, loss = 0.22409653\n",
      "Iteration 3177, loss = 0.22390778\n",
      "Iteration 3178, loss = 0.22371924\n",
      "Iteration 3179, loss = 0.22353092\n",
      "Iteration 3180, loss = 0.22334282\n",
      "Iteration 3181, loss = 0.22315494\n",
      "Iteration 3182, loss = 0.22296728\n",
      "Iteration 3183, loss = 0.22277983\n",
      "Iteration 3184, loss = 0.22259261\n",
      "Iteration 3185, loss = 0.22240560\n",
      "Iteration 3186, loss = 0.22221880\n",
      "Iteration 3187, loss = 0.22203223\n",
      "Iteration 3188, loss = 0.22184587\n",
      "Iteration 3189, loss = 0.22165972\n",
      "Iteration 3190, loss = 0.22147380\n",
      "Iteration 3191, loss = 0.22128809\n",
      "Iteration 3192, loss = 0.22110259\n",
      "Iteration 3193, loss = 0.22091731\n",
      "Iteration 3194, loss = 0.22073225\n",
      "Iteration 3195, loss = 0.22054740\n",
      "Iteration 3196, loss = 0.22036277\n",
      "Iteration 3197, loss = 0.22017835\n",
      "Iteration 3198, loss = 0.21999415\n",
      "Iteration 3199, loss = 0.21981016\n",
      "Iteration 3200, loss = 0.21962638\n",
      "Iteration 3201, loss = 0.21944282\n",
      "Iteration 3202, loss = 0.21925947\n",
      "Iteration 3203, loss = 0.21907633\n",
      "Iteration 3204, loss = 0.21889341\n",
      "Iteration 3205, loss = 0.21871070\n",
      "Iteration 3206, loss = 0.21852821\n",
      "Iteration 3207, loss = 0.21834592\n",
      "Iteration 3208, loss = 0.21816385\n",
      "Iteration 3209, loss = 0.21798199\n",
      "Iteration 3210, loss = 0.21780035\n",
      "Iteration 3211, loss = 0.21761891\n",
      "Iteration 3212, loss = 0.21743768\n",
      "Iteration 3213, loss = 0.21725667\n",
      "Iteration 3214, loss = 0.21707587\n",
      "Iteration 3215, loss = 0.21689528\n",
      "Iteration 3216, loss = 0.21671490\n",
      "Iteration 3217, loss = 0.21653472\n",
      "Iteration 3218, loss = 0.21635476\n",
      "Iteration 3219, loss = 0.21617501\n",
      "Iteration 3220, loss = 0.21599547\n",
      "Iteration 3221, loss = 0.21581614\n",
      "Iteration 3222, loss = 0.21563702\n",
      "Iteration 3223, loss = 0.21545810\n",
      "Iteration 3224, loss = 0.21527940\n",
      "Iteration 3225, loss = 0.21510090\n",
      "Iteration 3226, loss = 0.21492261\n",
      "Iteration 3227, loss = 0.21474453\n",
      "Iteration 3228, loss = 0.21456666\n",
      "Iteration 3229, loss = 0.21438899\n",
      "Iteration 3230, loss = 0.21421153\n",
      "Iteration 3231, loss = 0.21403428\n",
      "Iteration 3232, loss = 0.21385724\n",
      "Iteration 3233, loss = 0.21368040\n",
      "Iteration 3234, loss = 0.21350377\n",
      "Iteration 3235, loss = 0.21332734\n",
      "Iteration 3236, loss = 0.21315112\n",
      "Iteration 3237, loss = 0.21297511\n",
      "Iteration 3238, loss = 0.21279930\n",
      "Iteration 3239, loss = 0.21262370\n",
      "Iteration 3240, loss = 0.21244830\n",
      "Iteration 3241, loss = 0.21227311\n",
      "Iteration 3242, loss = 0.21209812\n",
      "Iteration 3243, loss = 0.21192334\n",
      "Iteration 3244, loss = 0.21174876\n",
      "Iteration 3245, loss = 0.21157438\n",
      "Iteration 3246, loss = 0.21140021\n",
      "Iteration 3247, loss = 0.21122624\n",
      "Iteration 3248, loss = 0.21105248\n",
      "Iteration 3249, loss = 0.21087891\n",
      "Iteration 3250, loss = 0.21070555\n",
      "Iteration 3251, loss = 0.21053240\n",
      "Iteration 3252, loss = 0.21035944\n",
      "Iteration 3253, loss = 0.21018669\n",
      "Iteration 3254, loss = 0.21001414\n",
      "Iteration 3255, loss = 0.20984179\n",
      "Iteration 3256, loss = 0.20966964\n",
      "Iteration 3257, loss = 0.20949770\n",
      "Iteration 3258, loss = 0.20932595\n",
      "Iteration 3259, loss = 0.20915440\n",
      "Iteration 3260, loss = 0.20898306\n",
      "Iteration 3261, loss = 0.20881192\n",
      "Iteration 3262, loss = 0.20864097\n",
      "Iteration 3263, loss = 0.20847023\n",
      "Iteration 3264, loss = 0.20829968\n",
      "Iteration 3265, loss = 0.20812934\n",
      "Iteration 3266, loss = 0.20795919\n",
      "Iteration 3267, loss = 0.20778924\n",
      "Iteration 3268, loss = 0.20761949\n",
      "Iteration 3269, loss = 0.20744994\n",
      "Iteration 3270, loss = 0.20728059\n",
      "Iteration 3271, loss = 0.20711144\n",
      "Iteration 3272, loss = 0.20694248\n",
      "Iteration 3273, loss = 0.20677372\n",
      "Iteration 3274, loss = 0.20660516\n",
      "Iteration 3275, loss = 0.20643680\n",
      "Iteration 3276, loss = 0.20626863\n",
      "Iteration 3277, loss = 0.20610066\n",
      "Iteration 3278, loss = 0.20593288\n",
      "Iteration 3279, loss = 0.20576530\n",
      "Iteration 3280, loss = 0.20559792\n",
      "Iteration 3281, loss = 0.20543073\n",
      "Iteration 3282, loss = 0.20526374\n",
      "Iteration 3283, loss = 0.20509694\n",
      "Iteration 3284, loss = 0.20493034\n",
      "Iteration 3285, loss = 0.20476394\n",
      "Iteration 3286, loss = 0.20459772\n",
      "Iteration 3287, loss = 0.20443171\n",
      "Iteration 3288, loss = 0.20426588\n",
      "Iteration 3289, loss = 0.20410025\n",
      "Iteration 3290, loss = 0.20393482\n",
      "Iteration 3291, loss = 0.20376957\n",
      "Iteration 3292, loss = 0.20360452\n",
      "Iteration 3293, loss = 0.20343967\n",
      "Iteration 3294, loss = 0.20327500\n",
      "Iteration 3295, loss = 0.20311053\n",
      "Iteration 3296, loss = 0.20294625\n",
      "Iteration 3297, loss = 0.20278217\n",
      "Iteration 3298, loss = 0.20261827\n",
      "Iteration 3299, loss = 0.20245457\n",
      "Iteration 3300, loss = 0.20229105\n",
      "Iteration 3301, loss = 0.20212773\n",
      "Iteration 3302, loss = 0.20196460\n",
      "Iteration 3303, loss = 0.20180166\n",
      "Iteration 3304, loss = 0.20163891\n",
      "Iteration 3305, loss = 0.20147635\n",
      "Iteration 3306, loss = 0.20131399\n",
      "Iteration 3307, loss = 0.20115181\n",
      "Iteration 3308, loss = 0.20098982\n",
      "Iteration 3309, loss = 0.20082802\n",
      "Iteration 3310, loss = 0.20066640\n",
      "Iteration 3311, loss = 0.20050498\n",
      "Iteration 3312, loss = 0.20034375\n",
      "Iteration 3313, loss = 0.20018270\n",
      "Iteration 3314, loss = 0.20002184\n",
      "Iteration 3315, loss = 0.19986117\n",
      "Iteration 3316, loss = 0.19970069\n",
      "Iteration 3317, loss = 0.19954040\n",
      "Iteration 3318, loss = 0.19938029\n",
      "Iteration 3319, loss = 0.19922037\n",
      "Iteration 3320, loss = 0.19906064\n",
      "Iteration 3321, loss = 0.19890109\n",
      "Iteration 3322, loss = 0.19874173\n",
      "Iteration 3323, loss = 0.19858256\n",
      "Iteration 3324, loss = 0.19842357\n",
      "Iteration 3325, loss = 0.19826477\n",
      "Iteration 3326, loss = 0.19810615\n",
      "Iteration 3327, loss = 0.19794772\n",
      "Iteration 3328, loss = 0.19778947\n",
      "Iteration 3329, loss = 0.19763141\n",
      "Iteration 3330, loss = 0.19747353\n",
      "Iteration 3331, loss = 0.19731584\n",
      "Iteration 3332, loss = 0.19715833\n",
      "Iteration 3333, loss = 0.19700100\n",
      "Iteration 3334, loss = 0.19684386\n",
      "Iteration 3335, loss = 0.19668690\n",
      "Iteration 3336, loss = 0.19653013\n",
      "Iteration 3337, loss = 0.19637353\n",
      "Iteration 3338, loss = 0.19621712\n",
      "Iteration 3339, loss = 0.19606090\n",
      "Iteration 3340, loss = 0.19590485\n",
      "Iteration 3341, loss = 0.19574899\n",
      "Iteration 3342, loss = 0.19559331\n",
      "Iteration 3343, loss = 0.19543781\n",
      "Iteration 3344, loss = 0.19528249\n",
      "Iteration 3345, loss = 0.19512736\n",
      "Iteration 3346, loss = 0.19497240\n",
      "Iteration 3347, loss = 0.19481763\n",
      "Iteration 3348, loss = 0.19466303\n",
      "Iteration 3349, loss = 0.19450862\n",
      "Iteration 3350, loss = 0.19435438\n",
      "Iteration 3351, loss = 0.19420033\n",
      "Iteration 3352, loss = 0.19404645\n",
      "Iteration 3353, loss = 0.19389276\n",
      "Iteration 3354, loss = 0.19373924\n",
      "Iteration 3355, loss = 0.19358590\n",
      "Iteration 3356, loss = 0.19343275\n",
      "Iteration 3357, loss = 0.19327977\n",
      "Iteration 3358, loss = 0.19312696\n",
      "Iteration 3359, loss = 0.19297434\n",
      "Iteration 3360, loss = 0.19282189\n",
      "Iteration 3361, loss = 0.19266962\n",
      "Iteration 3362, loss = 0.19251753\n",
      "Iteration 3363, loss = 0.19236562\n",
      "Iteration 3364, loss = 0.19221388\n",
      "Iteration 3365, loss = 0.19206232\n",
      "Iteration 3366, loss = 0.19191094\n",
      "Iteration 3367, loss = 0.19175973\n",
      "Iteration 3368, loss = 0.19160870\n",
      "Iteration 3369, loss = 0.19145784\n",
      "Iteration 3370, loss = 0.19130716\n",
      "Iteration 3371, loss = 0.19115666\n",
      "Iteration 3372, loss = 0.19100633\n",
      "Iteration 3373, loss = 0.19085617\n",
      "Iteration 3374, loss = 0.19070619\n",
      "Iteration 3375, loss = 0.19055639\n",
      "Iteration 3376, loss = 0.19040676\n",
      "Iteration 3377, loss = 0.19025730\n",
      "Iteration 3378, loss = 0.19010802\n",
      "Iteration 3379, loss = 0.18995891\n",
      "Iteration 3380, loss = 0.18980997\n",
      "Iteration 3381, loss = 0.18966121\n",
      "Iteration 3382, loss = 0.18951262\n",
      "Iteration 3383, loss = 0.18936420\n",
      "Iteration 3384, loss = 0.18921595\n",
      "Iteration 3385, loss = 0.18906788\n",
      "Iteration 3386, loss = 0.18891998\n",
      "Iteration 3387, loss = 0.18877225\n",
      "Iteration 3388, loss = 0.18862470\n",
      "Iteration 3389, loss = 0.18847731\n",
      "Iteration 3390, loss = 0.18833009\n",
      "Iteration 3391, loss = 0.18818305\n",
      "Iteration 3392, loss = 0.18803618\n",
      "Iteration 3393, loss = 0.18788948\n",
      "Iteration 3394, loss = 0.18774294\n",
      "Iteration 3395, loss = 0.18759658\n",
      "Iteration 3396, loss = 0.18745039\n",
      "Iteration 3397, loss = 0.18730437\n",
      "Iteration 3398, loss = 0.18715851\n",
      "Iteration 3399, loss = 0.18701283\n",
      "Iteration 3400, loss = 0.18686732\n",
      "Iteration 3401, loss = 0.18672197\n",
      "Iteration 3402, loss = 0.18657679\n",
      "Iteration 3403, loss = 0.18643178\n",
      "Iteration 3404, loss = 0.18628694\n",
      "Iteration 3405, loss = 0.18614227\n",
      "Iteration 3406, loss = 0.18599777\n",
      "Iteration 3407, loss = 0.18585343\n",
      "Iteration 3408, loss = 0.18570926\n",
      "Iteration 3409, loss = 0.18556525\n",
      "Iteration 3410, loss = 0.18542142\n",
      "Iteration 3411, loss = 0.18527775\n",
      "Iteration 3412, loss = 0.18513425\n",
      "Iteration 3413, loss = 0.18499091\n",
      "Iteration 3414, loss = 0.18484774\n",
      "Iteration 3415, loss = 0.18470473\n",
      "Iteration 3416, loss = 0.18456189\n",
      "Iteration 3417, loss = 0.18441922\n",
      "Iteration 3418, loss = 0.18427671\n",
      "Iteration 3419, loss = 0.18413436\n",
      "Iteration 3420, loss = 0.18399219\n",
      "Iteration 3421, loss = 0.18385017\n",
      "Iteration 3422, loss = 0.18370832\n",
      "Iteration 3423, loss = 0.18356663\n",
      "Iteration 3424, loss = 0.18342511\n",
      "Iteration 3425, loss = 0.18328375\n",
      "Iteration 3426, loss = 0.18314255\n",
      "Iteration 3427, loss = 0.18300152\n",
      "Iteration 3428, loss = 0.18286065\n",
      "Iteration 3429, loss = 0.18271995\n",
      "Iteration 3430, loss = 0.18257940\n",
      "Iteration 3431, loss = 0.18243902\n",
      "Iteration 3432, loss = 0.18229880\n",
      "Iteration 3433, loss = 0.18215874\n",
      "Iteration 3434, loss = 0.18201885\n",
      "Iteration 3435, loss = 0.18187911\n",
      "Iteration 3436, loss = 0.18173954\n",
      "Iteration 3437, loss = 0.18160013\n",
      "Iteration 3438, loss = 0.18146088\n",
      "Iteration 3439, loss = 0.18132179\n",
      "Iteration 3440, loss = 0.18118286\n",
      "Iteration 3441, loss = 0.18104409\n",
      "Iteration 3442, loss = 0.18090548\n",
      "Iteration 3443, loss = 0.18076703\n",
      "Iteration 3444, loss = 0.18062874\n",
      "Iteration 3445, loss = 0.18049061\n",
      "Iteration 3446, loss = 0.18035264\n",
      "Iteration 3447, loss = 0.18021482\n",
      "Iteration 3448, loss = 0.18007717\n",
      "Iteration 3449, loss = 0.17993968\n",
      "Iteration 3450, loss = 0.17980234\n",
      "Iteration 3451, loss = 0.17966516\n",
      "Iteration 3452, loss = 0.17952814\n",
      "Iteration 3453, loss = 0.17939128\n",
      "Iteration 3454, loss = 0.17925457\n",
      "Iteration 3455, loss = 0.17911803\n",
      "Iteration 3456, loss = 0.17898164\n",
      "Iteration 3457, loss = 0.17884540\n",
      "Iteration 3458, loss = 0.17870933\n",
      "Iteration 3459, loss = 0.17857340\n",
      "Iteration 3460, loss = 0.17843764\n",
      "Iteration 3461, loss = 0.17830203\n",
      "Iteration 3462, loss = 0.17816658\n",
      "Iteration 3463, loss = 0.17803129\n",
      "Iteration 3464, loss = 0.17789614\n",
      "Iteration 3465, loss = 0.17776116\n",
      "Iteration 3466, loss = 0.17762633\n",
      "Iteration 3467, loss = 0.17749165\n",
      "Iteration 3468, loss = 0.17735713\n",
      "Iteration 3469, loss = 0.17722277\n",
      "Iteration 3470, loss = 0.17708855\n",
      "Iteration 3471, loss = 0.17695450\n",
      "Iteration 3472, loss = 0.17682059\n",
      "Iteration 3473, loss = 0.17668684\n",
      "Iteration 3474, loss = 0.17655325\n",
      "Iteration 3475, loss = 0.17641980\n",
      "Iteration 3476, loss = 0.17628651\n",
      "Iteration 3477, loss = 0.17615337\n",
      "Iteration 3478, loss = 0.17602039\n",
      "Iteration 3479, loss = 0.17588756\n",
      "Iteration 3480, loss = 0.17575488\n",
      "Iteration 3481, loss = 0.17562235\n",
      "Iteration 3482, loss = 0.17548997\n",
      "Iteration 3483, loss = 0.17535775\n",
      "Iteration 3484, loss = 0.17522567\n",
      "Iteration 3485, loss = 0.17509375\n",
      "Iteration 3486, loss = 0.17496198\n",
      "Iteration 3487, loss = 0.17483036\n",
      "Iteration 3488, loss = 0.17469889\n",
      "Iteration 3489, loss = 0.17456757\n",
      "Iteration 3490, loss = 0.17443640\n",
      "Iteration 3491, loss = 0.17430538\n",
      "Iteration 3492, loss = 0.17417451\n",
      "Iteration 3493, loss = 0.17404380\n",
      "Iteration 3494, loss = 0.17391323\n",
      "Iteration 3495, loss = 0.17378281\n",
      "Iteration 3496, loss = 0.17365253\n",
      "Iteration 3497, loss = 0.17352241\n",
      "Iteration 3498, loss = 0.17339244\n",
      "Iteration 3499, loss = 0.17326261\n",
      "Iteration 3500, loss = 0.17313293\n",
      "Iteration 3501, loss = 0.17300340\n",
      "Iteration 3502, loss = 0.17287402\n",
      "Iteration 3503, loss = 0.17274479\n",
      "Iteration 3504, loss = 0.17261570\n",
      "Iteration 3505, loss = 0.17248676\n",
      "Iteration 3506, loss = 0.17235797\n",
      "Iteration 3507, loss = 0.17222933\n",
      "Iteration 3508, loss = 0.17210083\n",
      "Iteration 3509, loss = 0.17197248\n",
      "Iteration 3510, loss = 0.17184427\n",
      "Iteration 3511, loss = 0.17171621\n",
      "Iteration 3512, loss = 0.17158830\n",
      "Iteration 3513, loss = 0.17146053\n",
      "Iteration 3514, loss = 0.17133291\n",
      "Iteration 3515, loss = 0.17120543\n",
      "Iteration 3516, loss = 0.17107810\n",
      "Iteration 3517, loss = 0.17095091\n",
      "Iteration 3518, loss = 0.17082387\n",
      "Iteration 3519, loss = 0.17069697\n",
      "Iteration 3520, loss = 0.17057022\n",
      "Iteration 3521, loss = 0.17044361\n",
      "Iteration 3522, loss = 0.17031714\n",
      "Iteration 3523, loss = 0.17019082\n",
      "Iteration 3524, loss = 0.17006464\n",
      "Iteration 3525, loss = 0.16993861\n",
      "Iteration 3526, loss = 0.16981271\n",
      "Iteration 3527, loss = 0.16968697\n",
      "Iteration 3528, loss = 0.16956136\n",
      "Iteration 3529, loss = 0.16943590\n",
      "Iteration 3530, loss = 0.16931058\n",
      "Iteration 3531, loss = 0.16918540\n",
      "Iteration 3532, loss = 0.16906036\n",
      "Iteration 3533, loss = 0.16893547\n",
      "Iteration 3534, loss = 0.16881071\n",
      "Iteration 3535, loss = 0.16868610\n",
      "Iteration 3536, loss = 0.16856163\n",
      "Iteration 3537, loss = 0.16843730\n",
      "Iteration 3538, loss = 0.16831312\n",
      "Iteration 3539, loss = 0.16818907\n",
      "Iteration 3540, loss = 0.16806516\n",
      "Iteration 3541, loss = 0.16794139\n",
      "Iteration 3542, loss = 0.16781777\n",
      "Iteration 3543, loss = 0.16769428\n",
      "Iteration 3544, loss = 0.16757093\n",
      "Iteration 3545, loss = 0.16744773\n",
      "Iteration 3546, loss = 0.16732466\n",
      "Iteration 3547, loss = 0.16720173\n",
      "Iteration 3548, loss = 0.16707894\n",
      "Iteration 3549, loss = 0.16695629\n",
      "Iteration 3550, loss = 0.16683378\n",
      "Iteration 3551, loss = 0.16671140\n",
      "Iteration 3552, loss = 0.16658917\n",
      "Iteration 3553, loss = 0.16646707\n",
      "Iteration 3554, loss = 0.16634511\n",
      "Iteration 3555, loss = 0.16622329\n",
      "Iteration 3556, loss = 0.16610161\n",
      "Iteration 3557, loss = 0.16598006\n",
      "Iteration 3558, loss = 0.16585865\n",
      "Iteration 3559, loss = 0.16573738\n",
      "Iteration 3560, loss = 0.16561624\n",
      "Iteration 3561, loss = 0.16549524\n",
      "Iteration 3562, loss = 0.16537438\n",
      "Iteration 3563, loss = 0.16525365\n",
      "Iteration 3564, loss = 0.16513306\n",
      "Iteration 3565, loss = 0.16501261\n",
      "Iteration 3566, loss = 0.16489229\n",
      "Iteration 3567, loss = 0.16477210\n",
      "Iteration 3568, loss = 0.16465205\n",
      "Iteration 3569, loss = 0.16453214\n",
      "Iteration 3570, loss = 0.16441236\n",
      "Iteration 3571, loss = 0.16429272\n",
      "Iteration 3572, loss = 0.16417321\n",
      "Iteration 3573, loss = 0.16405384\n",
      "Iteration 3574, loss = 0.16393460\n",
      "Iteration 3575, loss = 0.16381549\n",
      "Iteration 3576, loss = 0.16369652\n",
      "Iteration 3577, loss = 0.16357768\n",
      "Iteration 3578, loss = 0.16345897\n",
      "Iteration 3579, loss = 0.16334040\n",
      "Iteration 3580, loss = 0.16322196\n",
      "Iteration 3581, loss = 0.16310366\n",
      "Iteration 3582, loss = 0.16298548\n",
      "Iteration 3583, loss = 0.16286744\n",
      "Iteration 3584, loss = 0.16274953\n",
      "Iteration 3585, loss = 0.16263176\n",
      "Iteration 3586, loss = 0.16251412\n",
      "Iteration 3587, loss = 0.16239660\n",
      "Iteration 3588, loss = 0.16227922\n",
      "Iteration 3589, loss = 0.16216197\n",
      "Iteration 3590, loss = 0.16204486\n",
      "Iteration 3591, loss = 0.16192787\n",
      "Iteration 3592, loss = 0.16181102\n",
      "Iteration 3593, loss = 0.16169429\n",
      "Iteration 3594, loss = 0.16157770\n",
      "Iteration 3595, loss = 0.16146123\n",
      "Iteration 3596, loss = 0.16134490\n",
      "Iteration 3597, loss = 0.16122870\n",
      "Iteration 3598, loss = 0.16111263\n",
      "Iteration 3599, loss = 0.16099668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3600, loss = 0.16088087\n",
      "Iteration 3601, loss = 0.16076518\n",
      "Iteration 3602, loss = 0.16064963\n",
      "Iteration 3603, loss = 0.16053420\n",
      "Iteration 3604, loss = 0.16041891\n",
      "Iteration 3605, loss = 0.16030374\n",
      "Iteration 3606, loss = 0.16018870\n",
      "Iteration 3607, loss = 0.16007379\n",
      "Iteration 3608, loss = 0.15995900\n",
      "Iteration 3609, loss = 0.15984435\n",
      "Iteration 3610, loss = 0.15972982\n",
      "Iteration 3611, loss = 0.15961542\n",
      "Iteration 3612, loss = 0.15950115\n",
      "Iteration 3613, loss = 0.15938700\n",
      "Iteration 3614, loss = 0.15927299\n",
      "Iteration 3615, loss = 0.15915909\n",
      "Iteration 3616, loss = 0.15904533\n",
      "Iteration 3617, loss = 0.15893169\n",
      "Iteration 3618, loss = 0.15881818\n",
      "Iteration 3619, loss = 0.15870480\n",
      "Iteration 3620, loss = 0.15859154\n",
      "Iteration 3621, loss = 0.15847841\n",
      "Iteration 3622, loss = 0.15836540\n",
      "Iteration 3623, loss = 0.15825252\n",
      "Iteration 3624, loss = 0.15813976\n",
      "Iteration 3625, loss = 0.15802713\n",
      "Iteration 3626, loss = 0.15791462\n",
      "Iteration 3627, loss = 0.15780224\n",
      "Iteration 3628, loss = 0.15768999\n",
      "Iteration 3629, loss = 0.15757786\n",
      "Iteration 3630, loss = 0.15746585\n",
      "Iteration 3631, loss = 0.15735397\n",
      "Iteration 3632, loss = 0.15724221\n",
      "Iteration 3633, loss = 0.15713057\n",
      "Iteration 3634, loss = 0.15701906\n",
      "Iteration 3635, loss = 0.15690768\n",
      "Iteration 3636, loss = 0.15679641\n",
      "Iteration 3637, loss = 0.15668527\n",
      "Iteration 3638, loss = 0.15657425\n",
      "Iteration 3639, loss = 0.15646336\n",
      "Iteration 3640, loss = 0.15635259\n",
      "Iteration 3641, loss = 0.15624194\n",
      "Iteration 3642, loss = 0.15613141\n",
      "Iteration 3643, loss = 0.15602100\n",
      "Iteration 3644, loss = 0.15591072\n",
      "Iteration 3645, loss = 0.15580056\n",
      "Iteration 3646, loss = 0.15569052\n",
      "Iteration 3647, loss = 0.15558060\n",
      "Iteration 3648, loss = 0.15547081\n",
      "Iteration 3649, loss = 0.15536113\n",
      "Iteration 3650, loss = 0.15525158\n",
      "Iteration 3651, loss = 0.15514214\n",
      "Iteration 3652, loss = 0.15503283\n",
      "Iteration 3653, loss = 0.15492364\n",
      "Iteration 3654, loss = 0.15481457\n",
      "Iteration 3655, loss = 0.15470562\n",
      "Iteration 3656, loss = 0.15459679\n",
      "Iteration 3657, loss = 0.15448807\n",
      "Iteration 3658, loss = 0.15437948\n",
      "Iteration 3659, loss = 0.15427101\n",
      "Iteration 3660, loss = 0.15416266\n",
      "Iteration 3661, loss = 0.15405443\n",
      "Iteration 3662, loss = 0.15394631\n",
      "Iteration 3663, loss = 0.15383832\n",
      "Iteration 3664, loss = 0.15373044\n",
      "Iteration 3665, loss = 0.15362268\n",
      "Iteration 3666, loss = 0.15351504\n",
      "Iteration 3667, loss = 0.15340752\n",
      "Iteration 3668, loss = 0.15330012\n",
      "Iteration 3669, loss = 0.15319284\n",
      "Iteration 3670, loss = 0.15308567\n",
      "Iteration 3671, loss = 0.15297862\n",
      "Iteration 3672, loss = 0.15287169\n",
      "Iteration 3673, loss = 0.15276487\n",
      "Iteration 3674, loss = 0.15265818\n",
      "Iteration 3675, loss = 0.15255160\n",
      "Iteration 3676, loss = 0.15244513\n",
      "Iteration 3677, loss = 0.15233879\n",
      "Iteration 3678, loss = 0.15223256\n",
      "Iteration 3679, loss = 0.15212644\n",
      "Iteration 3680, loss = 0.15202045\n",
      "Iteration 3681, loss = 0.15191456\n",
      "Iteration 3682, loss = 0.15180880\n",
      "Iteration 3683, loss = 0.15170315\n",
      "Iteration 3684, loss = 0.15159762\n",
      "Iteration 3685, loss = 0.15149220\n",
      "Iteration 3686, loss = 0.15138689\n",
      "Iteration 3687, loss = 0.15128170\n",
      "Iteration 3688, loss = 0.15117663\n",
      "Iteration 3689, loss = 0.15107167\n",
      "Iteration 3690, loss = 0.15096683\n",
      "Iteration 3691, loss = 0.15086210\n",
      "Iteration 3692, loss = 0.15075749\n",
      "Iteration 3693, loss = 0.15065298\n",
      "Iteration 3694, loss = 0.15054860\n",
      "Iteration 3695, loss = 0.15044433\n",
      "Iteration 3696, loss = 0.15034017\n",
      "Iteration 3697, loss = 0.15023612\n",
      "Iteration 3698, loss = 0.15013219\n",
      "Iteration 3699, loss = 0.15002837\n",
      "Iteration 3700, loss = 0.14992466\n",
      "Iteration 3701, loss = 0.14982107\n",
      "Iteration 3702, loss = 0.14971759\n",
      "Iteration 3703, loss = 0.14961422\n",
      "Iteration 3704, loss = 0.14951097\n",
      "Iteration 3705, loss = 0.14940783\n",
      "Iteration 3706, loss = 0.14930480\n",
      "Iteration 3707, loss = 0.14920188\n",
      "Iteration 3708, loss = 0.14909907\n",
      "Iteration 3709, loss = 0.14899638\n",
      "Iteration 3710, loss = 0.14889380\n",
      "Iteration 3711, loss = 0.14879132\n",
      "Iteration 3712, loss = 0.14868896\n",
      "Iteration 3713, loss = 0.14858671\n",
      "Iteration 3714, loss = 0.14848458\n",
      "Iteration 3715, loss = 0.14838255\n",
      "Iteration 3716, loss = 0.14828063\n",
      "Iteration 3717, loss = 0.14817883\n",
      "Iteration 3718, loss = 0.14807713\n",
      "Iteration 3719, loss = 0.14797555\n",
      "Iteration 3720, loss = 0.14787407\n",
      "Iteration 3721, loss = 0.14777270\n",
      "Iteration 3722, loss = 0.14767145\n",
      "Iteration 3723, loss = 0.14757030\n",
      "Iteration 3724, loss = 0.14746927\n",
      "Iteration 3725, loss = 0.14736834\n",
      "Iteration 3726, loss = 0.14726752\n",
      "Iteration 3727, loss = 0.14716681\n",
      "Iteration 3728, loss = 0.14706621\n",
      "Iteration 3729, loss = 0.14696572\n",
      "Iteration 3730, loss = 0.14686534\n",
      "Iteration 3731, loss = 0.14676506\n",
      "Iteration 3732, loss = 0.14666490\n",
      "Iteration 3733, loss = 0.14656484\n",
      "Iteration 3734, loss = 0.14646489\n",
      "Iteration 3735, loss = 0.14636505\n",
      "Iteration 3736, loss = 0.14626531\n",
      "Iteration 3737, loss = 0.14616569\n",
      "Iteration 3738, loss = 0.14606617\n",
      "Iteration 3739, loss = 0.14596676\n",
      "Iteration 3740, loss = 0.14586745\n",
      "Iteration 3741, loss = 0.14576825\n",
      "Iteration 3742, loss = 0.14566916\n",
      "Iteration 3743, loss = 0.14557018\n",
      "Iteration 3744, loss = 0.14547130\n",
      "Iteration 3745, loss = 0.14537253\n",
      "Iteration 3746, loss = 0.14527386\n",
      "Iteration 3747, loss = 0.14517531\n",
      "Iteration 3748, loss = 0.14507685\n",
      "Iteration 3749, loss = 0.14497851\n",
      "Iteration 3750, loss = 0.14488026\n",
      "Iteration 3751, loss = 0.14478213\n",
      "Iteration 3752, loss = 0.14468410\n",
      "Iteration 3753, loss = 0.14458617\n",
      "Iteration 3754, loss = 0.14448835\n",
      "Iteration 3755, loss = 0.14439064\n",
      "Iteration 3756, loss = 0.14429303\n",
      "Iteration 3757, loss = 0.14419552\n",
      "Iteration 3758, loss = 0.14409812\n",
      "Iteration 3759, loss = 0.14400082\n",
      "Iteration 3760, loss = 0.14390363\n",
      "Iteration 3761, loss = 0.14380654\n",
      "Iteration 3762, loss = 0.14370956\n",
      "Iteration 3763, loss = 0.14361268\n",
      "Iteration 3764, loss = 0.14351590\n",
      "Iteration 3765, loss = 0.14341923\n",
      "Iteration 3766, loss = 0.14332266\n",
      "Iteration 3767, loss = 0.14322619\n",
      "Iteration 3768, loss = 0.14312983\n",
      "Iteration 3769, loss = 0.14303357\n",
      "Iteration 3770, loss = 0.14293741\n",
      "Iteration 3771, loss = 0.14284136\n",
      "Iteration 3772, loss = 0.14274541\n",
      "Iteration 3773, loss = 0.14264956\n",
      "Iteration 3774, loss = 0.14255381\n",
      "Iteration 3775, loss = 0.14245816\n",
      "Iteration 3776, loss = 0.14236262\n",
      "Iteration 3777, loss = 0.14226718\n",
      "Iteration 3778, loss = 0.14217184\n",
      "Iteration 3779, loss = 0.14207660\n",
      "Iteration 3780, loss = 0.14198147\n",
      "Iteration 3781, loss = 0.14188643\n",
      "Iteration 3782, loss = 0.14179150\n",
      "Iteration 3783, loss = 0.14169667\n",
      "Iteration 3784, loss = 0.14160193\n",
      "Iteration 3785, loss = 0.14150730\n",
      "Iteration 3786, loss = 0.14141277\n",
      "Iteration 3787, loss = 0.14131834\n",
      "Iteration 3788, loss = 0.14122401\n",
      "Iteration 3789, loss = 0.14112979\n",
      "Iteration 3790, loss = 0.14103566\n",
      "Iteration 3791, loss = 0.14094163\n",
      "Iteration 3792, loss = 0.14084770\n",
      "Iteration 3793, loss = 0.14075387\n",
      "Iteration 3794, loss = 0.14066014\n",
      "Iteration 3795, loss = 0.14056651\n",
      "Iteration 3796, loss = 0.14047298\n",
      "Iteration 3797, loss = 0.14037955\n",
      "Iteration 3798, loss = 0.14028621\n",
      "Iteration 3799, loss = 0.14019298\n",
      "Iteration 3800, loss = 0.14009984\n",
      "Iteration 3801, loss = 0.14000681\n",
      "Iteration 3802, loss = 0.13991387\n",
      "Iteration 3803, loss = 0.13982103\n",
      "Iteration 3804, loss = 0.13972829\n",
      "Iteration 3805, loss = 0.13963565\n",
      "Iteration 3806, loss = 0.13954310\n",
      "Iteration 3807, loss = 0.13945065\n",
      "Iteration 3808, loss = 0.13935830\n",
      "Iteration 3809, loss = 0.13926605\n",
      "Iteration 3810, loss = 0.13917390\n",
      "Iteration 3811, loss = 0.13908184\n",
      "Iteration 3812, loss = 0.13898988\n",
      "Iteration 3813, loss = 0.13889802\n",
      "Iteration 3814, loss = 0.13880625\n",
      "Iteration 3815, loss = 0.13871458\n",
      "Iteration 3816, loss = 0.13862301\n",
      "Iteration 3817, loss = 0.13853153\n",
      "Iteration 3818, loss = 0.13844015\n",
      "Iteration 3819, loss = 0.13834887\n",
      "Iteration 3820, loss = 0.13825768\n",
      "Iteration 3821, loss = 0.13816659\n",
      "Iteration 3822, loss = 0.13807560\n",
      "Iteration 3823, loss = 0.13798470\n",
      "Iteration 3824, loss = 0.13789390\n",
      "Iteration 3825, loss = 0.13780319\n",
      "Iteration 3826, loss = 0.13771257\n",
      "Iteration 3827, loss = 0.13762206\n",
      "Iteration 3828, loss = 0.13753163\n",
      "Iteration 3829, loss = 0.13744131\n",
      "Iteration 3830, loss = 0.13735107\n",
      "Iteration 3831, loss = 0.13726094\n",
      "Iteration 3832, loss = 0.13717089\n",
      "Iteration 3833, loss = 0.13708094\n",
      "Iteration 3834, loss = 0.13699109\n",
      "Iteration 3835, loss = 0.13690133\n",
      "Iteration 3836, loss = 0.13681166\n",
      "Iteration 3837, loss = 0.13672209\n",
      "Iteration 3838, loss = 0.13663261\n",
      "Iteration 3839, loss = 0.13654323\n",
      "Iteration 3840, loss = 0.13645394\n",
      "Iteration 3841, loss = 0.13636474\n",
      "Iteration 3842, loss = 0.13627564\n",
      "Iteration 3843, loss = 0.13618663\n",
      "Iteration 3844, loss = 0.13609771\n",
      "Iteration 3845, loss = 0.13600888\n",
      "Iteration 3846, loss = 0.13592015\n",
      "Iteration 3847, loss = 0.13583151\n",
      "Iteration 3848, loss = 0.13574297\n",
      "Iteration 3849, loss = 0.13565451\n",
      "Iteration 3850, loss = 0.13556615\n",
      "Iteration 3851, loss = 0.13547788\n",
      "Iteration 3852, loss = 0.13538970\n",
      "Iteration 3853, loss = 0.13530162\n",
      "Iteration 3854, loss = 0.13521363\n",
      "Iteration 3855, loss = 0.13512572\n",
      "Iteration 3856, loss = 0.13503791\n",
      "Iteration 3857, loss = 0.13495019\n",
      "Iteration 3858, loss = 0.13486257\n",
      "Iteration 3859, loss = 0.13477503\n",
      "Iteration 3860, loss = 0.13468759\n",
      "Iteration 3861, loss = 0.13460023\n",
      "Iteration 3862, loss = 0.13451297\n",
      "Iteration 3863, loss = 0.13442580\n",
      "Iteration 3864, loss = 0.13433872\n",
      "Iteration 3865, loss = 0.13425173\n",
      "Iteration 3866, loss = 0.13416482\n",
      "Iteration 3867, loss = 0.13407801\n",
      "Iteration 3868, loss = 0.13399129\n",
      "Iteration 3869, loss = 0.13390466\n",
      "Iteration 3870, loss = 0.13381812\n",
      "Iteration 3871, loss = 0.13373167\n",
      "Iteration 3872, loss = 0.13364531\n",
      "Iteration 3873, loss = 0.13355904\n",
      "Iteration 3874, loss = 0.13347286\n",
      "Iteration 3875, loss = 0.13338677\n",
      "Iteration 3876, loss = 0.13330076\n",
      "Iteration 3877, loss = 0.13321485\n",
      "Iteration 3878, loss = 0.13312903\n",
      "Iteration 3879, loss = 0.13304329\n",
      "Iteration 3880, loss = 0.13295764\n",
      "Iteration 3881, loss = 0.13287208\n",
      "Iteration 3882, loss = 0.13278661\n",
      "Iteration 3883, loss = 0.13270123\n",
      "Iteration 3884, loss = 0.13261594\n",
      "Iteration 3885, loss = 0.13253073\n",
      "Iteration 3886, loss = 0.13244561\n",
      "Iteration 3887, loss = 0.13236058\n",
      "Iteration 3888, loss = 0.13227564\n",
      "Iteration 3889, loss = 0.13219079\n",
      "Iteration 3890, loss = 0.13210602\n",
      "Iteration 3891, loss = 0.13202134\n",
      "Iteration 3892, loss = 0.13193675\n",
      "Iteration 3893, loss = 0.13185224\n",
      "Iteration 3894, loss = 0.13176783\n",
      "Iteration 3895, loss = 0.13168350\n",
      "Iteration 3896, loss = 0.13159925\n",
      "Iteration 3897, loss = 0.13151509\n",
      "Iteration 3898, loss = 0.13143102\n",
      "Iteration 3899, loss = 0.13134704\n",
      "Iteration 3900, loss = 0.13126314\n",
      "Iteration 3901, loss = 0.13117933\n",
      "Iteration 3902, loss = 0.13109561\n",
      "Iteration 3903, loss = 0.13101197\n",
      "Iteration 3904, loss = 0.13092841\n",
      "Iteration 3905, loss = 0.13084495\n",
      "Iteration 3906, loss = 0.13076156\n",
      "Iteration 3907, loss = 0.13067827\n",
      "Iteration 3908, loss = 0.13059506\n",
      "Iteration 3909, loss = 0.13051193\n",
      "Iteration 3910, loss = 0.13042889\n",
      "Iteration 3911, loss = 0.13034594\n",
      "Iteration 3912, loss = 0.13026307\n",
      "Iteration 3913, loss = 0.13018028\n",
      "Iteration 3914, loss = 0.13009758\n",
      "Iteration 3915, loss = 0.13001497\n",
      "Iteration 3916, loss = 0.12993244\n",
      "Iteration 3917, loss = 0.12984999\n",
      "Iteration 3918, loss = 0.12976763\n",
      "Iteration 3919, loss = 0.12968535\n",
      "Iteration 3920, loss = 0.12960316\n",
      "Iteration 3921, loss = 0.12952105\n",
      "Iteration 3922, loss = 0.12943902\n",
      "Iteration 3923, loss = 0.12935708\n",
      "Iteration 3924, loss = 0.12927522\n",
      "Iteration 3925, loss = 0.12919345\n",
      "Iteration 3926, loss = 0.12911176\n",
      "Iteration 3927, loss = 0.12903015\n",
      "Iteration 3928, loss = 0.12894862\n",
      "Iteration 3929, loss = 0.12886718\n",
      "Iteration 3930, loss = 0.12878582\n",
      "Iteration 3931, loss = 0.12870455\n",
      "Iteration 3932, loss = 0.12862336\n",
      "Iteration 3933, loss = 0.12854225\n",
      "Iteration 3934, loss = 0.12846122\n",
      "Iteration 3935, loss = 0.12838027\n",
      "Iteration 3936, loss = 0.12829941\n",
      "Iteration 3937, loss = 0.12821863\n",
      "Iteration 3938, loss = 0.12813793\n",
      "Iteration 3939, loss = 0.12805732\n",
      "Iteration 3940, loss = 0.12797678\n",
      "Iteration 3941, loss = 0.12789633\n",
      "Iteration 3942, loss = 0.12781596\n",
      "Iteration 3943, loss = 0.12773567\n",
      "Iteration 3944, loss = 0.12765546\n",
      "Iteration 3945, loss = 0.12757534\n",
      "Iteration 3946, loss = 0.12749529\n",
      "Iteration 3947, loss = 0.12741533\n",
      "Iteration 3948, loss = 0.12733545\n",
      "Iteration 3949, loss = 0.12725565\n",
      "Iteration 3950, loss = 0.12717592\n",
      "Iteration 3951, loss = 0.12709628\n",
      "Iteration 3952, loss = 0.12701673\n",
      "Iteration 3953, loss = 0.12693725\n",
      "Iteration 3954, loss = 0.12685785\n",
      "Iteration 3955, loss = 0.12677853\n",
      "Iteration 3956, loss = 0.12669929\n",
      "Iteration 3957, loss = 0.12662014\n",
      "Iteration 3958, loss = 0.12654106\n",
      "Iteration 3959, loss = 0.12646206\n",
      "Iteration 3960, loss = 0.12638314\n",
      "Iteration 3961, loss = 0.12630430\n",
      "Iteration 3962, loss = 0.12622554\n",
      "Iteration 3963, loss = 0.12614687\n",
      "Iteration 3964, loss = 0.12606827\n",
      "Iteration 3965, loss = 0.12598975\n",
      "Iteration 3966, loss = 0.12591130\n",
      "Iteration 3967, loss = 0.12583294\n",
      "Iteration 3968, loss = 0.12575466\n",
      "Iteration 3969, loss = 0.12567645\n",
      "Iteration 3970, loss = 0.12559833\n",
      "Iteration 3971, loss = 0.12552028\n",
      "Iteration 3972, loss = 0.12544231\n",
      "Iteration 3973, loss = 0.12536442\n",
      "Iteration 3974, loss = 0.12528661\n",
      "Iteration 3975, loss = 0.12520888\n",
      "Iteration 3976, loss = 0.12513122\n",
      "Iteration 3977, loss = 0.12505365\n",
      "Iteration 3978, loss = 0.12497615\n",
      "Iteration 3979, loss = 0.12489872\n",
      "Iteration 3980, loss = 0.12482138\n",
      "Iteration 3981, loss = 0.12474412\n",
      "Iteration 3982, loss = 0.12466693\n",
      "Iteration 3983, loss = 0.12458982\n",
      "Iteration 3984, loss = 0.12451278\n",
      "Iteration 3985, loss = 0.12443583\n",
      "Iteration 3986, loss = 0.12435895\n",
      "Iteration 3987, loss = 0.12428214\n",
      "Iteration 3988, loss = 0.12420542\n",
      "Iteration 3989, loss = 0.12412877\n",
      "Iteration 3990, loss = 0.12405220\n",
      "Iteration 3991, loss = 0.12397570\n",
      "Iteration 3992, loss = 0.12389928\n",
      "Iteration 3993, loss = 0.12382294\n",
      "Iteration 3994, loss = 0.12374667\n",
      "Iteration 3995, loss = 0.12367048\n",
      "Iteration 3996, loss = 0.12359437\n",
      "Iteration 3997, loss = 0.12351833\n",
      "Iteration 3998, loss = 0.12344237\n",
      "Iteration 3999, loss = 0.12336649\n",
      "Iteration 4000, loss = 0.12329068\n",
      "Iteration 4001, loss = 0.12321494\n",
      "Iteration 4002, loss = 0.12313928\n",
      "Iteration 4003, loss = 0.12306370\n",
      "Iteration 4004, loss = 0.12298819\n",
      "Iteration 4005, loss = 0.12291276\n",
      "Iteration 4006, loss = 0.12283740\n",
      "Iteration 4007, loss = 0.12276212\n",
      "Iteration 4008, loss = 0.12268691\n",
      "Iteration 4009, loss = 0.12261178\n",
      "Iteration 4010, loss = 0.12253672\n",
      "Iteration 4011, loss = 0.12246173\n",
      "Iteration 4012, loss = 0.12238682\n",
      "Iteration 4013, loss = 0.12231199\n",
      "Iteration 4014, loss = 0.12223723\n",
      "Iteration 4015, loss = 0.12216254\n",
      "Iteration 4016, loss = 0.12208793\n",
      "Iteration 4017, loss = 0.12201339\n",
      "Iteration 4018, loss = 0.12193893\n",
      "Iteration 4019, loss = 0.12186454\n",
      "Iteration 4020, loss = 0.12179022\n",
      "Iteration 4021, loss = 0.12171598\n",
      "Iteration 4022, loss = 0.12164181\n",
      "Iteration 4023, loss = 0.12156772\n",
      "Iteration 4024, loss = 0.12149369\n",
      "Iteration 4025, loss = 0.12141975\n",
      "Iteration 4026, loss = 0.12134587\n",
      "Iteration 4027, loss = 0.12127207\n",
      "Iteration 4028, loss = 0.12119834\n",
      "Iteration 4029, loss = 0.12112468\n",
      "Iteration 4030, loss = 0.12105110\n",
      "Iteration 4031, loss = 0.12097759\n",
      "Iteration 4032, loss = 0.12090415\n",
      "Iteration 4033, loss = 0.12083078\n",
      "Iteration 4034, loss = 0.12075749\n",
      "Iteration 4035, loss = 0.12068427\n",
      "Iteration 4036, loss = 0.12061112\n",
      "Iteration 4037, loss = 0.12053804\n",
      "Iteration 4038, loss = 0.12046504\n",
      "Iteration 4039, loss = 0.12039211\n",
      "Iteration 4040, loss = 0.12031925\n",
      "Iteration 4041, loss = 0.12024646\n",
      "Iteration 4042, loss = 0.12017374\n",
      "Iteration 4043, loss = 0.12010109\n",
      "Iteration 4044, loss = 0.12002852\n",
      "Iteration 4045, loss = 0.11995602\n",
      "Iteration 4046, loss = 0.11988359\n",
      "Iteration 4047, loss = 0.11981123\n",
      "Iteration 4048, loss = 0.11973894\n",
      "Iteration 4049, loss = 0.11966672\n",
      "Iteration 4050, loss = 0.11959457\n",
      "Iteration 4051, loss = 0.11952250\n",
      "Iteration 4052, loss = 0.11945049\n",
      "Iteration 4053, loss = 0.11937856\n",
      "Iteration 4054, loss = 0.11930669\n",
      "Iteration 4055, loss = 0.11923490\n",
      "Iteration 4056, loss = 0.11916318\n",
      "Iteration 4057, loss = 0.11909153\n",
      "Iteration 4058, loss = 0.11901994\n",
      "Iteration 4059, loss = 0.11894843\n",
      "Iteration 4060, loss = 0.11887699\n",
      "Iteration 4061, loss = 0.11880562\n",
      "Iteration 4062, loss = 0.11873432\n",
      "Iteration 4063, loss = 0.11866308\n",
      "Iteration 4064, loss = 0.11859192\n",
      "Iteration 4065, loss = 0.11852083\n",
      "Iteration 4066, loss = 0.11844980\n",
      "Iteration 4067, loss = 0.11837885\n",
      "Iteration 4068, loss = 0.11830796\n",
      "Iteration 4069, loss = 0.11823715\n",
      "Iteration 4070, loss = 0.11816640\n",
      "Iteration 4071, loss = 0.11809572\n",
      "Iteration 4072, loss = 0.11802512\n",
      "Iteration 4073, loss = 0.11795458\n",
      "Iteration 4074, loss = 0.11788411\n",
      "Iteration 4075, loss = 0.11781370\n",
      "Iteration 4076, loss = 0.11774337\n",
      "Iteration 4077, loss = 0.11767311\n",
      "Iteration 4078, loss = 0.11760291\n",
      "Iteration 4079, loss = 0.11753278\n",
      "Iteration 4080, loss = 0.11746272\n",
      "Iteration 4081, loss = 0.11739273\n",
      "Iteration 4082, loss = 0.11732281\n",
      "Iteration 4083, loss = 0.11725295\n",
      "Iteration 4084, loss = 0.11718316\n",
      "Iteration 4085, loss = 0.11711344\n",
      "Iteration 4086, loss = 0.11704379\n",
      "Iteration 4087, loss = 0.11697421\n",
      "Iteration 4088, loss = 0.11690469\n",
      "Iteration 4089, loss = 0.11683524\n",
      "Iteration 4090, loss = 0.11676586\n",
      "Iteration 4091, loss = 0.11669655\n",
      "Iteration 4092, loss = 0.11662730\n",
      "Iteration 4093, loss = 0.11655812\n",
      "Iteration 4094, loss = 0.11648900\n",
      "Iteration 4095, loss = 0.11641996\n",
      "Iteration 4096, loss = 0.11635098\n",
      "Iteration 4097, loss = 0.11628207\n",
      "Iteration 4098, loss = 0.11621322\n",
      "Iteration 4099, loss = 0.11614444\n",
      "Iteration 4100, loss = 0.11607573\n",
      "Iteration 4101, loss = 0.11600708\n",
      "Iteration 4102, loss = 0.11593850\n",
      "Iteration 4103, loss = 0.11586999\n",
      "Iteration 4104, loss = 0.11580154\n",
      "Iteration 4105, loss = 0.11573316\n",
      "Iteration 4106, loss = 0.11566485\n",
      "Iteration 4107, loss = 0.11559660\n",
      "Iteration 4108, loss = 0.11552842\n",
      "Iteration 4109, loss = 0.11546030\n",
      "Iteration 4110, loss = 0.11539225\n",
      "Iteration 4111, loss = 0.11532426\n",
      "Iteration 4112, loss = 0.11525634\n",
      "Iteration 4113, loss = 0.11518849\n",
      "Iteration 4114, loss = 0.11512070\n",
      "Iteration 4115, loss = 0.11505297\n",
      "Iteration 4116, loss = 0.11498531\n",
      "Iteration 4117, loss = 0.11491772\n",
      "Iteration 4118, loss = 0.11485019\n",
      "Iteration 4119, loss = 0.11478273\n",
      "Iteration 4120, loss = 0.11471533\n",
      "Iteration 4121, loss = 0.11464799\n",
      "Iteration 4122, loss = 0.11458072\n",
      "Iteration 4123, loss = 0.11451352\n",
      "Iteration 4124, loss = 0.11444638\n",
      "Iteration 4125, loss = 0.11437930\n",
      "Iteration 4126, loss = 0.11431229\n",
      "Iteration 4127, loss = 0.11424534\n",
      "Iteration 4128, loss = 0.11417846\n",
      "Iteration 4129, loss = 0.11411164\n",
      "Iteration 4130, loss = 0.11404489\n",
      "Iteration 4131, loss = 0.11397820\n",
      "Iteration 4132, loss = 0.11391157\n",
      "Iteration 4133, loss = 0.11384501\n",
      "Iteration 4134, loss = 0.11377851\n",
      "Iteration 4135, loss = 0.11371207\n",
      "Iteration 4136, loss = 0.11364570\n",
      "Iteration 4137, loss = 0.11357939\n",
      "Iteration 4138, loss = 0.11351315\n",
      "Iteration 4139, loss = 0.11344696\n",
      "Iteration 4140, loss = 0.11338084\n",
      "Iteration 4141, loss = 0.11331479\n",
      "Iteration 4142, loss = 0.11324880\n",
      "Iteration 4143, loss = 0.11318287\n",
      "Iteration 4144, loss = 0.11311700\n",
      "Iteration 4145, loss = 0.11305120\n",
      "Iteration 4146, loss = 0.11298546\n",
      "Iteration 4147, loss = 0.11291978\n",
      "Iteration 4148, loss = 0.11285416\n",
      "Iteration 4149, loss = 0.11278861\n",
      "Iteration 4150, loss = 0.11272312\n",
      "Iteration 4151, loss = 0.11265769\n",
      "Iteration 4152, loss = 0.11259233\n",
      "Iteration 4153, loss = 0.11252702\n",
      "Iteration 4154, loss = 0.11246178\n",
      "Iteration 4155, loss = 0.11239660\n",
      "Iteration 4156, loss = 0.11233148\n",
      "Iteration 4157, loss = 0.11226643\n",
      "Iteration 4158, loss = 0.11220143\n",
      "Iteration 4159, loss = 0.11213650\n",
      "Iteration 4160, loss = 0.11207163\n",
      "Iteration 4161, loss = 0.11200682\n",
      "Iteration 4162, loss = 0.11194208\n",
      "Iteration 4163, loss = 0.11187739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4164, loss = 0.11181277\n",
      "Iteration 4165, loss = 0.11174820\n",
      "Iteration 4166, loss = 0.11168370\n",
      "Iteration 4167, loss = 0.11161926\n",
      "Iteration 4168, loss = 0.11155488\n",
      "Iteration 4169, loss = 0.11149056\n",
      "Iteration 4170, loss = 0.11142630\n",
      "Iteration 4171, loss = 0.11136211\n",
      "Iteration 4172, loss = 0.11129797\n",
      "Iteration 4173, loss = 0.11123390\n",
      "Iteration 4174, loss = 0.11116988\n",
      "Iteration 4175, loss = 0.11110593\n",
      "Iteration 4176, loss = 0.11104203\n",
      "Iteration 4177, loss = 0.11097820\n",
      "Iteration 4178, loss = 0.11091442\n",
      "Iteration 4179, loss = 0.11085071\n",
      "Iteration 4180, loss = 0.11078706\n",
      "Iteration 4181, loss = 0.11072347\n",
      "Iteration 4182, loss = 0.11065993\n",
      "Iteration 4183, loss = 0.11059646\n",
      "Iteration 4184, loss = 0.11053305\n",
      "Iteration 4185, loss = 0.11046969\n",
      "Iteration 4186, loss = 0.11040640\n",
      "Iteration 4187, loss = 0.11034317\n",
      "Iteration 4188, loss = 0.11027999\n",
      "Iteration 4189, loss = 0.11021688\n",
      "Iteration 4190, loss = 0.11015382\n",
      "Iteration 4191, loss = 0.11009082\n",
      "Iteration 4192, loss = 0.11002789\n",
      "Iteration 4193, loss = 0.10996501\n",
      "Iteration 4194, loss = 0.10990219\n",
      "Iteration 4195, loss = 0.10983943\n",
      "Iteration 4196, loss = 0.10977673\n",
      "Iteration 4197, loss = 0.10971409\n",
      "Iteration 4198, loss = 0.10965150\n",
      "Iteration 4199, loss = 0.10958898\n",
      "Iteration 4200, loss = 0.10952651\n",
      "Iteration 4201, loss = 0.10946411\n",
      "Iteration 4202, loss = 0.10940176\n",
      "Iteration 4203, loss = 0.10933947\n",
      "Iteration 4204, loss = 0.10927724\n",
      "Iteration 4205, loss = 0.10921506\n",
      "Iteration 4206, loss = 0.10915295\n",
      "Iteration 4207, loss = 0.10909089\n",
      "Iteration 4208, loss = 0.10902889\n",
      "Iteration 4209, loss = 0.10896695\n",
      "Iteration 4210, loss = 0.10890507\n",
      "Iteration 4211, loss = 0.10884324\n",
      "Iteration 4212, loss = 0.10878147\n",
      "Iteration 4213, loss = 0.10871976\n",
      "Iteration 4214, loss = 0.10865811\n",
      "Iteration 4215, loss = 0.10859652\n",
      "Iteration 4216, loss = 0.10853498\n",
      "Iteration 4217, loss = 0.10847350\n",
      "Iteration 4218, loss = 0.10841208\n",
      "Iteration 4219, loss = 0.10835071\n",
      "Iteration 4220, loss = 0.10828941\n",
      "Iteration 4221, loss = 0.10822816\n",
      "Iteration 4222, loss = 0.10816696\n",
      "Iteration 4223, loss = 0.10810583\n",
      "Iteration 4224, loss = 0.10804475\n",
      "Iteration 4225, loss = 0.10798372\n",
      "Iteration 4226, loss = 0.10792276\n",
      "Iteration 4227, loss = 0.10786185\n",
      "Iteration 4228, loss = 0.10780099\n",
      "Iteration 4229, loss = 0.10774020\n",
      "Iteration 4230, loss = 0.10767946\n",
      "Iteration 4231, loss = 0.10761878\n",
      "Iteration 4232, loss = 0.10755815\n",
      "Iteration 4233, loss = 0.10749758\n",
      "Iteration 4234, loss = 0.10743706\n",
      "Iteration 4235, loss = 0.10737661\n",
      "Iteration 4236, loss = 0.10731620\n",
      "Iteration 4237, loss = 0.10725586\n",
      "Iteration 4238, loss = 0.10719557\n",
      "Iteration 4239, loss = 0.10713533\n",
      "Iteration 4240, loss = 0.10707516\n",
      "Iteration 4241, loss = 0.10701503\n",
      "Iteration 4242, loss = 0.10695497\n",
      "Iteration 4243, loss = 0.10689496\n",
      "Iteration 4244, loss = 0.10683500\n",
      "Iteration 4245, loss = 0.10677510\n",
      "Iteration 4246, loss = 0.10671526\n",
      "Iteration 4247, loss = 0.10665547\n",
      "Iteration 4248, loss = 0.10659573\n",
      "Iteration 4249, loss = 0.10653605\n",
      "Iteration 4250, loss = 0.10647643\n",
      "Iteration 4251, loss = 0.10641686\n",
      "Iteration 4252, loss = 0.10635735\n",
      "Iteration 4253, loss = 0.10629789\n",
      "Iteration 4254, loss = 0.10623848\n",
      "Iteration 4255, loss = 0.10617913\n",
      "Iteration 4256, loss = 0.10611984\n",
      "Iteration 4257, loss = 0.10606060\n",
      "Iteration 4258, loss = 0.10600141\n",
      "Iteration 4259, loss = 0.10594228\n",
      "Iteration 4260, loss = 0.10588321\n",
      "Iteration 4261, loss = 0.10582419\n",
      "Iteration 4262, loss = 0.10576522\n",
      "Iteration 4263, loss = 0.10570631\n",
      "Iteration 4264, loss = 0.10564745\n",
      "Iteration 4265, loss = 0.10558864\n",
      "Iteration 4266, loss = 0.10552989\n",
      "Iteration 4267, loss = 0.10547119\n",
      "Iteration 4268, loss = 0.10541255\n",
      "Iteration 4269, loss = 0.10535396\n",
      "Iteration 4270, loss = 0.10529543\n",
      "Iteration 4271, loss = 0.10523694\n",
      "Iteration 4272, loss = 0.10517852\n",
      "Iteration 4273, loss = 0.10512014\n",
      "Iteration 4274, loss = 0.10506182\n",
      "Iteration 4275, loss = 0.10500355\n",
      "Iteration 4276, loss = 0.10494534\n",
      "Iteration 4277, loss = 0.10488718\n",
      "Iteration 4278, loss = 0.10482907\n",
      "Iteration 4279, loss = 0.10477102\n",
      "Iteration 4280, loss = 0.10471302\n",
      "Iteration 4281, loss = 0.10465507\n",
      "Iteration 4282, loss = 0.10459717\n",
      "Iteration 4283, loss = 0.10453933\n",
      "Iteration 4284, loss = 0.10448154\n",
      "Iteration 4285, loss = 0.10442381\n",
      "Iteration 4286, loss = 0.10436612\n",
      "Iteration 4287, loss = 0.10430849\n",
      "Iteration 4288, loss = 0.10425091\n",
      "Iteration 4289, loss = 0.10419339\n",
      "Iteration 4290, loss = 0.10413592\n",
      "Iteration 4291, loss = 0.10407849\n",
      "Iteration 4292, loss = 0.10402113\n",
      "Iteration 4293, loss = 0.10396381\n",
      "Iteration 4294, loss = 0.10390655\n",
      "Iteration 4295, loss = 0.10384933\n",
      "Iteration 4296, loss = 0.10379218\n",
      "Iteration 4297, loss = 0.10373507\n",
      "Iteration 4298, loss = 0.10367801\n",
      "Iteration 4299, loss = 0.10362101\n",
      "Iteration 4300, loss = 0.10356406\n",
      "Iteration 4301, loss = 0.10350716\n",
      "Iteration 4302, loss = 0.10345031\n",
      "Iteration 4303, loss = 0.10339351\n",
      "Iteration 4304, loss = 0.10333677\n",
      "Iteration 4305, loss = 0.10328007\n",
      "Iteration 4306, loss = 0.10322343\n",
      "Iteration 4307, loss = 0.10316684\n",
      "Iteration 4308, loss = 0.10311030\n",
      "Iteration 4309, loss = 0.10305381\n",
      "Iteration 4310, loss = 0.10299738\n",
      "Iteration 4311, loss = 0.10294099\n",
      "Iteration 4312, loss = 0.10288466\n",
      "Iteration 4313, loss = 0.10282838\n",
      "Iteration 4314, loss = 0.10277214\n",
      "Iteration 4315, loss = 0.10271596\n",
      "Iteration 4316, loss = 0.10265983\n",
      "Iteration 4317, loss = 0.10260375\n",
      "Iteration 4318, loss = 0.10254772\n",
      "Iteration 4319, loss = 0.10249174\n",
      "Iteration 4320, loss = 0.10243582\n",
      "Iteration 4321, loss = 0.10237994\n",
      "Iteration 4322, loss = 0.10232411\n",
      "Iteration 4323, loss = 0.10226834\n",
      "Iteration 4324, loss = 0.10221261\n",
      "Iteration 4325, loss = 0.10215694\n",
      "Iteration 4326, loss = 0.10210131\n",
      "Iteration 4327, loss = 0.10204574\n",
      "Iteration 4328, loss = 0.10199021\n",
      "Iteration 4329, loss = 0.10193474\n",
      "Iteration 4330, loss = 0.10187931\n",
      "Iteration 4331, loss = 0.10182394\n",
      "Iteration 4332, loss = 0.10176861\n",
      "Iteration 4333, loss = 0.10171334\n",
      "Iteration 4334, loss = 0.10165811\n",
      "Iteration 4335, loss = 0.10160293\n",
      "Iteration 4336, loss = 0.10154781\n",
      "Iteration 4337, loss = 0.10149273\n",
      "Iteration 4338, loss = 0.10143770\n",
      "Iteration 4339, loss = 0.10138273\n",
      "Iteration 4340, loss = 0.10132780\n",
      "Iteration 4341, loss = 0.10127292\n",
      "Iteration 4342, loss = 0.10121809\n",
      "Iteration 4343, loss = 0.10116331\n",
      "Iteration 4344, loss = 0.10110858\n",
      "Iteration 4345, loss = 0.10105390\n",
      "Iteration 4346, loss = 0.10099926\n",
      "Iteration 4347, loss = 0.10094468\n",
      "Iteration 4348, loss = 0.10089015\n",
      "Iteration 4349, loss = 0.10083566\n",
      "Iteration 4350, loss = 0.10078122\n",
      "Iteration 4351, loss = 0.10072683\n",
      "Iteration 4352, loss = 0.10067249\n",
      "Iteration 4353, loss = 0.10061820\n",
      "Iteration 4354, loss = 0.10056396\n",
      "Iteration 4355, loss = 0.10050977\n",
      "Iteration 4356, loss = 0.10045562\n",
      "Iteration 4357, loss = 0.10040152\n",
      "Iteration 4358, loss = 0.10034748\n",
      "Iteration 4359, loss = 0.10029347\n",
      "Iteration 4360, loss = 0.10023952\n",
      "Iteration 4361, loss = 0.10018562\n",
      "Iteration 4362, loss = 0.10013176\n",
      "Iteration 4363, loss = 0.10007795\n",
      "Iteration 4364, loss = 0.10002419\n",
      "Iteration 4365, loss = 0.09997048\n",
      "Iteration 4366, loss = 0.09991682\n",
      "Iteration 4367, loss = 0.09986320\n",
      "Iteration 4368, loss = 0.09980963\n",
      "Iteration 4369, loss = 0.09975611\n",
      "Iteration 4370, loss = 0.09970264\n",
      "Iteration 4371, loss = 0.09964921\n",
      "Iteration 4372, loss = 0.09959584\n",
      "Iteration 4373, loss = 0.09954251\n",
      "Iteration 4374, loss = 0.09948922\n",
      "Iteration 4375, loss = 0.09943599\n",
      "Iteration 4376, loss = 0.09938280\n",
      "Iteration 4377, loss = 0.09932966\n",
      "Iteration 4378, loss = 0.09927656\n",
      "Iteration 4379, loss = 0.09922352\n",
      "Iteration 4380, loss = 0.09917052\n",
      "Iteration 4381, loss = 0.09911757\n",
      "Iteration 4382, loss = 0.09906466\n",
      "Iteration 4383, loss = 0.09901180\n",
      "Iteration 4384, loss = 0.09895899\n",
      "Iteration 4385, loss = 0.09890622\n",
      "Iteration 4386, loss = 0.09885351\n",
      "Iteration 4387, loss = 0.09880083\n",
      "Iteration 4388, loss = 0.09874821\n",
      "Iteration 4389, loss = 0.09869563\n",
      "Iteration 4390, loss = 0.09864310\n",
      "Iteration 4391, loss = 0.09859061\n",
      "Iteration 4392, loss = 0.09853818\n",
      "Iteration 4393, loss = 0.09848578\n",
      "Iteration 4394, loss = 0.09843344\n",
      "Iteration 4395, loss = 0.09838114\n",
      "Iteration 4396, loss = 0.09832888\n",
      "Iteration 4397, loss = 0.09827668\n",
      "Iteration 4398, loss = 0.09822451\n",
      "Iteration 4399, loss = 0.09817240\n",
      "Iteration 4400, loss = 0.09812033\n",
      "Iteration 4401, loss = 0.09806831\n",
      "Iteration 4402, loss = 0.09801633\n",
      "Iteration 4403, loss = 0.09796440\n",
      "Iteration 4404, loss = 0.09791251\n",
      "Iteration 4405, loss = 0.09786067\n",
      "Iteration 4406, loss = 0.09780888\n",
      "Iteration 4407, loss = 0.09775713\n",
      "Iteration 4408, loss = 0.09770542\n",
      "Iteration 4409, loss = 0.09765377\n",
      "Iteration 4410, loss = 0.09760215\n",
      "Iteration 4411, loss = 0.09755059\n",
      "Iteration 4412, loss = 0.09749907\n",
      "Iteration 4413, loss = 0.09744759\n",
      "Iteration 4414, loss = 0.09739616\n",
      "Iteration 4415, loss = 0.09734477\n",
      "Iteration 4416, loss = 0.09729343\n",
      "Iteration 4417, loss = 0.09724214\n",
      "Iteration 4418, loss = 0.09719089\n",
      "Iteration 4419, loss = 0.09713968\n",
      "Iteration 4420, loss = 0.09708852\n",
      "Iteration 4421, loss = 0.09703740\n",
      "Iteration 4422, loss = 0.09698633\n",
      "Iteration 4423, loss = 0.09693530\n",
      "Iteration 4424, loss = 0.09688432\n",
      "Iteration 4425, loss = 0.09683339\n",
      "Iteration 4426, loss = 0.09678249\n",
      "Iteration 4427, loss = 0.09673165\n",
      "Iteration 4428, loss = 0.09668084\n",
      "Iteration 4429, loss = 0.09663008\n",
      "Iteration 4430, loss = 0.09657937\n",
      "Iteration 4431, loss = 0.09652870\n",
      "Iteration 4432, loss = 0.09647807\n",
      "Iteration 4433, loss = 0.09642749\n",
      "Iteration 4434, loss = 0.09637695\n",
      "Iteration 4435, loss = 0.09632646\n",
      "Iteration 4436, loss = 0.09627601\n",
      "Iteration 4437, loss = 0.09622560\n",
      "Iteration 4438, loss = 0.09617524\n",
      "Iteration 4439, loss = 0.09612493\n",
      "Iteration 4440, loss = 0.09607465\n",
      "Iteration 4441, loss = 0.09602442\n",
      "Iteration 4442, loss = 0.09597424\n",
      "Iteration 4443, loss = 0.09592409\n",
      "Iteration 4444, loss = 0.09587400\n",
      "Iteration 4445, loss = 0.09582394\n",
      "Iteration 4446, loss = 0.09577393\n",
      "Iteration 4447, loss = 0.09572396\n",
      "Iteration 4448, loss = 0.09567404\n",
      "Iteration 4449, loss = 0.09562416\n",
      "Iteration 4450, loss = 0.09557432\n",
      "Iteration 4451, loss = 0.09552452\n",
      "Iteration 4452, loss = 0.09547477\n",
      "Iteration 4453, loss = 0.09542507\n",
      "Iteration 4454, loss = 0.09537540\n",
      "Iteration 4455, loss = 0.09532578\n",
      "Iteration 4456, loss = 0.09527620\n",
      "Iteration 4457, loss = 0.09522666\n",
      "Iteration 4458, loss = 0.09517717\n",
      "Iteration 4459, loss = 0.09512772\n",
      "Iteration 4460, loss = 0.09507832\n",
      "Iteration 4461, loss = 0.09502895\n",
      "Iteration 4462, loss = 0.09497963\n",
      "Iteration 4463, loss = 0.09493035\n",
      "Iteration 4464, loss = 0.09488112\n",
      "Iteration 4465, loss = 0.09483192\n",
      "Iteration 4466, loss = 0.09478277\n",
      "Iteration 4467, loss = 0.09473366\n",
      "Iteration 4468, loss = 0.09468460\n",
      "Iteration 4469, loss = 0.09463557\n",
      "Iteration 4470, loss = 0.09458659\n",
      "Iteration 4471, loss = 0.09453765\n",
      "Iteration 4472, loss = 0.09448876\n",
      "Iteration 4473, loss = 0.09443990\n",
      "Iteration 4474, loss = 0.09439109\n",
      "Iteration 4475, loss = 0.09434232\n",
      "Iteration 4476, loss = 0.09429359\n",
      "Iteration 4477, loss = 0.09424491\n",
      "Iteration 4478, loss = 0.09419626\n",
      "Iteration 4479, loss = 0.09414766\n",
      "Iteration 4480, loss = 0.09409910\n",
      "Iteration 4481, loss = 0.09405058\n",
      "Iteration 4482, loss = 0.09400211\n",
      "Iteration 4483, loss = 0.09395367\n",
      "Iteration 4484, loss = 0.09390528\n",
      "Iteration 4485, loss = 0.09385693\n",
      "Iteration 4486, loss = 0.09380862\n",
      "Iteration 4487, loss = 0.09376035\n",
      "Iteration 4488, loss = 0.09371212\n",
      "Iteration 4489, loss = 0.09366394\n",
      "Iteration 4490, loss = 0.09361579\n",
      "Iteration 4491, loss = 0.09356769\n",
      "Iteration 4492, loss = 0.09351963\n",
      "Iteration 4493, loss = 0.09347161\n",
      "Iteration 4494, loss = 0.09342363\n",
      "Iteration 4495, loss = 0.09337569\n",
      "Iteration 4496, loss = 0.09332780\n",
      "Iteration 4497, loss = 0.09327994\n",
      "Iteration 4498, loss = 0.09323213\n",
      "Iteration 4499, loss = 0.09318435\n",
      "Iteration 4500, loss = 0.09313662\n",
      "Iteration 4501, loss = 0.09308893\n",
      "Iteration 4502, loss = 0.09304128\n",
      "Iteration 4503, loss = 0.09299367\n",
      "Iteration 4504, loss = 0.09294610\n",
      "Iteration 4505, loss = 0.09289857\n",
      "Iteration 4506, loss = 0.09285108\n",
      "Iteration 4507, loss = 0.09280363\n",
      "Iteration 4508, loss = 0.09275622\n",
      "Iteration 4509, loss = 0.09270886\n",
      "Iteration 4510, loss = 0.09266153\n",
      "Iteration 4511, loss = 0.09261425\n",
      "Iteration 4512, loss = 0.09256700\n",
      "Iteration 4513, loss = 0.09251979\n",
      "Iteration 4514, loss = 0.09247263\n",
      "Iteration 4515, loss = 0.09242551\n",
      "Iteration 4516, loss = 0.09237842\n",
      "Iteration 4517, loss = 0.09233138\n",
      "Iteration 4518, loss = 0.09228437\n",
      "Iteration 4519, loss = 0.09223741\n",
      "Iteration 4520, loss = 0.09219048\n",
      "Iteration 4521, loss = 0.09214360\n",
      "Iteration 4522, loss = 0.09209675\n",
      "Iteration 4523, loss = 0.09204995\n",
      "Iteration 4524, loss = 0.09200318\n",
      "Iteration 4525, loss = 0.09195646\n",
      "Iteration 4526, loss = 0.09190977\n",
      "Iteration 4527, loss = 0.09186313\n",
      "Iteration 4528, loss = 0.09181652\n",
      "Iteration 4529, loss = 0.09176996\n",
      "Iteration 4530, loss = 0.09172343\n",
      "Iteration 4531, loss = 0.09167694\n",
      "Iteration 4532, loss = 0.09163049\n",
      "Iteration 4533, loss = 0.09158408\n",
      "Iteration 4534, loss = 0.09153771\n",
      "Iteration 4535, loss = 0.09149138\n",
      "Iteration 4536, loss = 0.09144509\n",
      "Iteration 4537, loss = 0.09139884\n",
      "Iteration 4538, loss = 0.09135263\n",
      "Iteration 4539, loss = 0.09130646\n",
      "Iteration 4540, loss = 0.09126032\n",
      "Iteration 4541, loss = 0.09121423\n",
      "Iteration 4542, loss = 0.09116817\n",
      "Iteration 4543, loss = 0.09112215\n",
      "Iteration 4544, loss = 0.09107617\n",
      "Iteration 4545, loss = 0.09103023\n",
      "Iteration 4546, loss = 0.09098433\n",
      "Iteration 4547, loss = 0.09093847\n",
      "Iteration 4548, loss = 0.09089265\n",
      "Iteration 4549, loss = 0.09084686\n",
      "Iteration 4550, loss = 0.09080112\n",
      "Iteration 4551, loss = 0.09075541\n",
      "Iteration 4552, loss = 0.09070974\n",
      "Iteration 4553, loss = 0.09066411\n",
      "Iteration 4554, loss = 0.09061852\n",
      "Iteration 4555, loss = 0.09057297\n",
      "Iteration 4556, loss = 0.09052745\n",
      "Iteration 4557, loss = 0.09048197\n",
      "Iteration 4558, loss = 0.09043654\n",
      "Iteration 4559, loss = 0.09039114\n",
      "Iteration 4560, loss = 0.09034577\n",
      "Iteration 4561, loss = 0.09030045\n",
      "Iteration 4562, loss = 0.09025516\n",
      "Iteration 4563, loss = 0.09020992\n",
      "Iteration 4564, loss = 0.09016471\n",
      "Iteration 4565, loss = 0.09011953\n",
      "Iteration 4566, loss = 0.09007440\n",
      "Iteration 4567, loss = 0.09002930\n",
      "Iteration 4568, loss = 0.08998425\n",
      "Iteration 4569, loss = 0.08993923\n",
      "Iteration 4570, loss = 0.08989424\n",
      "Iteration 4571, loss = 0.08984930\n",
      "Iteration 4572, loss = 0.08980439\n",
      "Iteration 4573, loss = 0.08975952\n",
      "Iteration 4574, loss = 0.08971469\n",
      "Iteration 4575, loss = 0.08966990\n",
      "Iteration 4576, loss = 0.08962514\n",
      "Iteration 4577, loss = 0.08958042\n",
      "Iteration 4578, loss = 0.08953574\n",
      "Iteration 4579, loss = 0.08949109\n",
      "Iteration 4580, loss = 0.08944649\n",
      "Iteration 4581, loss = 0.08940192\n",
      "Iteration 4582, loss = 0.08935738\n",
      "Iteration 4583, loss = 0.08931289\n",
      "Iteration 4584, loss = 0.08926843\n",
      "Iteration 4585, loss = 0.08922401\n",
      "Iteration 4586, loss = 0.08917963\n",
      "Iteration 4587, loss = 0.08913528\n",
      "Iteration 4588, loss = 0.08909097\n",
      "Iteration 4589, loss = 0.08904670\n",
      "Iteration 4590, loss = 0.08900246\n",
      "Iteration 4591, loss = 0.08895826\n",
      "Iteration 4592, loss = 0.08891410\n",
      "Iteration 4593, loss = 0.08886997\n",
      "Iteration 4594, loss = 0.08882588\n",
      "Iteration 4595, loss = 0.08878183\n",
      "Iteration 4596, loss = 0.08873782\n",
      "Iteration 4597, loss = 0.08869384\n",
      "Iteration 4598, loss = 0.08864989\n",
      "Iteration 4599, loss = 0.08860599\n",
      "Iteration 4600, loss = 0.08856212\n",
      "Iteration 4601, loss = 0.08851829\n",
      "Iteration 4602, loss = 0.08847449\n",
      "Iteration 4603, loss = 0.08843073\n",
      "Iteration 4604, loss = 0.08838701\n",
      "Iteration 4605, loss = 0.08834332\n",
      "Iteration 4606, loss = 0.08829967\n",
      "Iteration 4607, loss = 0.08825605\n",
      "Iteration 4608, loss = 0.08821248\n",
      "Iteration 4609, loss = 0.08816893\n",
      "Iteration 4610, loss = 0.08812543\n",
      "Iteration 4611, loss = 0.08808196\n",
      "Iteration 4612, loss = 0.08803852\n",
      "Iteration 4613, loss = 0.08799512\n",
      "Iteration 4614, loss = 0.08795176\n",
      "Iteration 4615, loss = 0.08790843\n",
      "Iteration 4616, loss = 0.08786514\n",
      "Iteration 4617, loss = 0.08782189\n",
      "Iteration 4618, loss = 0.08777867\n",
      "Iteration 4619, loss = 0.08773549\n",
      "Iteration 4620, loss = 0.08769234\n",
      "Iteration 4621, loss = 0.08764923\n",
      "Iteration 4622, loss = 0.08760615\n",
      "Iteration 4623, loss = 0.08756311\n",
      "Iteration 4624, loss = 0.08752011\n",
      "Iteration 4625, loss = 0.08747714\n",
      "Iteration 4626, loss = 0.08743420\n",
      "Iteration 4627, loss = 0.08739130\n",
      "Iteration 4628, loss = 0.08734844\n",
      "Iteration 4629, loss = 0.08730561\n",
      "Iteration 4630, loss = 0.08726282\n",
      "Iteration 4631, loss = 0.08722006\n",
      "Iteration 4632, loss = 0.08717734\n",
      "Iteration 4633, loss = 0.08713465\n",
      "Iteration 4634, loss = 0.08709200\n",
      "Iteration 4635, loss = 0.08704938\n",
      "Iteration 4636, loss = 0.08700680\n",
      "Iteration 4637, loss = 0.08696425\n",
      "Iteration 4638, loss = 0.08692174\n",
      "Iteration 4639, loss = 0.08687926\n",
      "Iteration 4640, loss = 0.08683682\n",
      "Iteration 4641, loss = 0.08679442\n",
      "Iteration 4642, loss = 0.08675204\n",
      "Iteration 4643, loss = 0.08670971\n",
      "Iteration 4644, loss = 0.08666740\n",
      "Iteration 4645, loss = 0.08662514\n",
      "Iteration 4646, loss = 0.08658290\n",
      "Iteration 4647, loss = 0.08654070\n",
      "Iteration 4648, loss = 0.08649854\n",
      "Iteration 4649, loss = 0.08645641\n",
      "Iteration 4650, loss = 0.08641431\n",
      "Iteration 4651, loss = 0.08637225\n",
      "Iteration 4652, loss = 0.08633023\n",
      "Iteration 4653, loss = 0.08628824\n",
      "Iteration 4654, loss = 0.08624628\n",
      "Iteration 4655, loss = 0.08620436\n",
      "Iteration 4656, loss = 0.08616247\n",
      "Iteration 4657, loss = 0.08612061\n",
      "Iteration 4658, loss = 0.08607879\n",
      "Iteration 4659, loss = 0.08603701\n",
      "Iteration 4660, loss = 0.08599526\n",
      "Iteration 4661, loss = 0.08595354\n",
      "Iteration 4662, loss = 0.08591185\n",
      "Iteration 4663, loss = 0.08587020\n",
      "Iteration 4664, loss = 0.08582859\n",
      "Iteration 4665, loss = 0.08578701\n",
      "Iteration 4666, loss = 0.08574546\n",
      "Iteration 4667, loss = 0.08570394\n",
      "Iteration 4668, loss = 0.08566246\n",
      "Iteration 4669, loss = 0.08562102\n",
      "Iteration 4670, loss = 0.08557961\n",
      "Iteration 4671, loss = 0.08553823\n",
      "Iteration 4672, loss = 0.08549688\n",
      "Iteration 4673, loss = 0.08545557\n",
      "Iteration 4674, loss = 0.08541429\n",
      "Iteration 4675, loss = 0.08537305\n",
      "Iteration 4676, loss = 0.08533184\n",
      "Iteration 4677, loss = 0.08529066\n",
      "Iteration 4678, loss = 0.08524952\n",
      "Iteration 4679, loss = 0.08520841\n",
      "Iteration 4680, loss = 0.08516733\n",
      "Iteration 4681, loss = 0.08512629\n",
      "Iteration 4682, loss = 0.08508528\n",
      "Iteration 4683, loss = 0.08504430\n",
      "Iteration 4684, loss = 0.08500336\n",
      "Iteration 4685, loss = 0.08496245\n",
      "Iteration 4686, loss = 0.08492157\n",
      "Iteration 4687, loss = 0.08488072\n",
      "Iteration 4688, loss = 0.08483991\n",
      "Iteration 4689, loss = 0.08479914\n",
      "Iteration 4690, loss = 0.08475839\n",
      "Iteration 4691, loss = 0.08471768\n",
      "Iteration 4692, loss = 0.08467700\n",
      "Iteration 4693, loss = 0.08463635\n",
      "Iteration 4694, loss = 0.08459574\n",
      "Iteration 4695, loss = 0.08455516\n",
      "Iteration 4696, loss = 0.08451461\n",
      "Iteration 4697, loss = 0.08447410\n",
      "Iteration 4698, loss = 0.08443362\n",
      "Iteration 4699, loss = 0.08439317\n",
      "Iteration 4700, loss = 0.08435275\n",
      "Iteration 4701, loss = 0.08431237\n",
      "Iteration 4702, loss = 0.08427202\n",
      "Iteration 4703, loss = 0.08423170\n",
      "Iteration 4704, loss = 0.08419141\n",
      "Iteration 4705, loss = 0.08415116\n",
      "Iteration 4706, loss = 0.08411094\n",
      "Iteration 4707, loss = 0.08407075\n",
      "Iteration 4708, loss = 0.08403059\n",
      "Iteration 4709, loss = 0.08399047\n",
      "Iteration 4710, loss = 0.08395037\n",
      "Iteration 4711, loss = 0.08391031\n",
      "Iteration 4712, loss = 0.08387029\n",
      "Iteration 4713, loss = 0.08383029\n",
      "Iteration 4714, loss = 0.08379033\n",
      "Iteration 4715, loss = 0.08375040\n",
      "Iteration 4716, loss = 0.08371050\n",
      "Iteration 4717, loss = 0.08367063\n",
      "Iteration 4718, loss = 0.08363080\n",
      "Iteration 4719, loss = 0.08359100\n",
      "Iteration 4720, loss = 0.08355123\n",
      "Iteration 4721, loss = 0.08351149\n",
      "Iteration 4722, loss = 0.08347178\n",
      "Iteration 4723, loss = 0.08343211\n",
      "Iteration 4724, loss = 0.08339246\n",
      "Iteration 4725, loss = 0.08335285\n",
      "Iteration 4726, loss = 0.08331327\n",
      "Iteration 4727, loss = 0.08327372\n",
      "Iteration 4728, loss = 0.08323421\n",
      "Iteration 4729, loss = 0.08319472\n",
      "Iteration 4730, loss = 0.08315527\n",
      "Iteration 4731, loss = 0.08311585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4732, loss = 0.08307646\n",
      "Iteration 4733, loss = 0.08303710\n",
      "Iteration 4734, loss = 0.08299778\n",
      "Iteration 4735, loss = 0.08295848\n",
      "Iteration 4736, loss = 0.08291922\n",
      "Iteration 4737, loss = 0.08287998\n",
      "Iteration 4738, loss = 0.08284078\n",
      "Iteration 4739, loss = 0.08280161\n",
      "Iteration 4740, loss = 0.08276247\n",
      "Iteration 4741, loss = 0.08272337\n",
      "Iteration 4742, loss = 0.08268429\n",
      "Iteration 4743, loss = 0.08264525\n",
      "Iteration 4744, loss = 0.08260623\n",
      "Iteration 4745, loss = 0.08256725\n",
      "Iteration 4746, loss = 0.08252830\n",
      "Iteration 4747, loss = 0.08248938\n",
      "Iteration 4748, loss = 0.08245049\n",
      "Iteration 4749, loss = 0.08241163\n",
      "Iteration 4750, loss = 0.08237280\n",
      "Iteration 4751, loss = 0.08233401\n",
      "Iteration 4752, loss = 0.08229524\n",
      "Iteration 4753, loss = 0.08225651\n",
      "Iteration 4754, loss = 0.08221780\n",
      "Iteration 4755, loss = 0.08217913\n",
      "Iteration 4756, loss = 0.08214049\n",
      "Iteration 4757, loss = 0.08210187\n",
      "Iteration 4758, loss = 0.08206329\n",
      "Iteration 4759, loss = 0.08202474\n",
      "Iteration 4760, loss = 0.08198622\n",
      "Iteration 4761, loss = 0.08194773\n",
      "Iteration 4762, loss = 0.08190927\n",
      "Iteration 4763, loss = 0.08187085\n",
      "Iteration 4764, loss = 0.08183245\n",
      "Iteration 4765, loss = 0.08179408\n",
      "Iteration 4766, loss = 0.08175574\n",
      "Iteration 4767, loss = 0.08171744\n",
      "Iteration 4768, loss = 0.08167916\n",
      "Iteration 4769, loss = 0.08164091\n",
      "Iteration 4770, loss = 0.08160270\n",
      "Iteration 4771, loss = 0.08156451\n",
      "Iteration 4772, loss = 0.08152636\n",
      "Iteration 4773, loss = 0.08148823\n",
      "Iteration 4774, loss = 0.08145014\n",
      "Iteration 4775, loss = 0.08141207\n",
      "Iteration 4776, loss = 0.08137404\n",
      "Iteration 4777, loss = 0.08133603\n",
      "Iteration 4778, loss = 0.08129806\n",
      "Iteration 4779, loss = 0.08126012\n",
      "Iteration 4780, loss = 0.08122220\n",
      "Iteration 4781, loss = 0.08118432\n",
      "Iteration 4782, loss = 0.08114646\n",
      "Iteration 4783, loss = 0.08110864\n",
      "Iteration 4784, loss = 0.08107084\n",
      "Iteration 4785, loss = 0.08103308\n",
      "Iteration 4786, loss = 0.08099534\n",
      "Iteration 4787, loss = 0.08095764\n",
      "Iteration 4788, loss = 0.08091996\n",
      "Iteration 4789, loss = 0.08088231\n",
      "Iteration 4790, loss = 0.08084470\n",
      "Iteration 4791, loss = 0.08080711\n",
      "Iteration 4792, loss = 0.08076955\n",
      "Iteration 4793, loss = 0.08073202\n",
      "Iteration 4794, loss = 0.08069452\n",
      "Iteration 4795, loss = 0.08065706\n",
      "Iteration 4796, loss = 0.08061962\n",
      "Iteration 4797, loss = 0.08058221\n",
      "Iteration 4798, loss = 0.08054483\n",
      "Iteration 4799, loss = 0.08050747\n",
      "Iteration 4800, loss = 0.08047015\n",
      "Iteration 4801, loss = 0.08043286\n",
      "Iteration 4802, loss = 0.08039560\n",
      "Iteration 4803, loss = 0.08035836\n",
      "Iteration 4804, loss = 0.08032116\n",
      "Iteration 4805, loss = 0.08028398\n",
      "Iteration 4806, loss = 0.08024683\n",
      "Iteration 4807, loss = 0.08020972\n",
      "Iteration 4808, loss = 0.08017263\n",
      "Iteration 4809, loss = 0.08013557\n",
      "Iteration 4810, loss = 0.08009854\n",
      "Iteration 4811, loss = 0.08006154\n",
      "Iteration 4812, loss = 0.08002456\n",
      "Iteration 4813, loss = 0.07998762\n",
      "Iteration 4814, loss = 0.07995070\n",
      "Iteration 4815, loss = 0.07991382\n",
      "Iteration 4816, loss = 0.07987696\n",
      "Iteration 4817, loss = 0.07984013\n",
      "Iteration 4818, loss = 0.07980333\n",
      "Iteration 4819, loss = 0.07976656\n",
      "Iteration 4820, loss = 0.07972982\n",
      "Iteration 4821, loss = 0.07969311\n",
      "Iteration 4822, loss = 0.07965642\n",
      "Iteration 4823, loss = 0.07961977\n",
      "Iteration 4824, loss = 0.07958314\n",
      "Iteration 4825, loss = 0.07954654\n",
      "Iteration 4826, loss = 0.07950997\n",
      "Iteration 4827, loss = 0.07947343\n",
      "Iteration 4828, loss = 0.07943691\n",
      "Iteration 4829, loss = 0.07940043\n",
      "Iteration 4830, loss = 0.07936397\n",
      "Iteration 4831, loss = 0.07932754\n",
      "Iteration 4832, loss = 0.07929114\n",
      "Iteration 4833, loss = 0.07925477\n",
      "Iteration 4834, loss = 0.07921843\n",
      "Iteration 4835, loss = 0.07918211\n",
      "Iteration 4836, loss = 0.07914583\n",
      "Iteration 4837, loss = 0.07910957\n",
      "Iteration 4838, loss = 0.07907334\n",
      "Iteration 4839, loss = 0.07903714\n",
      "Iteration 4840, loss = 0.07900096\n",
      "Iteration 4841, loss = 0.07896481\n",
      "Iteration 4842, loss = 0.07892870\n",
      "Iteration 4843, loss = 0.07889261\n",
      "Iteration 4844, loss = 0.07885654\n",
      "Iteration 4845, loss = 0.07882051\n",
      "Iteration 4846, loss = 0.07878450\n",
      "Iteration 4847, loss = 0.07874853\n",
      "Iteration 4848, loss = 0.07871257\n",
      "Iteration 4849, loss = 0.07867665\n",
      "Iteration 4850, loss = 0.07864076\n",
      "Iteration 4851, loss = 0.07860489\n",
      "Iteration 4852, loss = 0.07856905\n",
      "Iteration 4853, loss = 0.07853324\n",
      "Iteration 4854, loss = 0.07849746\n",
      "Iteration 4855, loss = 0.07846170\n",
      "Iteration 4856, loss = 0.07842597\n",
      "Iteration 4857, loss = 0.07839027\n",
      "Iteration 4858, loss = 0.07835460\n",
      "Iteration 4859, loss = 0.07831895\n",
      "Iteration 4860, loss = 0.07828333\n",
      "Iteration 4861, loss = 0.07824774\n",
      "Iteration 4862, loss = 0.07821218\n",
      "Iteration 4863, loss = 0.07817664\n",
      "Iteration 4864, loss = 0.07814113\n",
      "Iteration 4865, loss = 0.07810565\n",
      "Iteration 4866, loss = 0.07807020\n",
      "Iteration 4867, loss = 0.07803477\n",
      "Iteration 4868, loss = 0.07799937\n",
      "Iteration 4869, loss = 0.07796400\n",
      "Iteration 4870, loss = 0.07792866\n",
      "Iteration 4871, loss = 0.07789334\n",
      "Iteration 4872, loss = 0.07785805\n",
      "Iteration 4873, loss = 0.07782279\n",
      "Iteration 4874, loss = 0.07778755\n",
      "Iteration 4875, loss = 0.07775234\n",
      "Iteration 4876, loss = 0.07771716\n",
      "Iteration 4877, loss = 0.07768200\n",
      "Iteration 4878, loss = 0.07764688\n",
      "Iteration 4879, loss = 0.07761178\n",
      "Iteration 4880, loss = 0.07757670\n",
      "Iteration 4881, loss = 0.07754165\n",
      "Iteration 4882, loss = 0.07750663\n",
      "Iteration 4883, loss = 0.07747164\n",
      "Iteration 4884, loss = 0.07743668\n",
      "Iteration 4885, loss = 0.07740174\n",
      "Iteration 4886, loss = 0.07736682\n",
      "Iteration 4887, loss = 0.07733194\n",
      "Iteration 4888, loss = 0.07729708\n",
      "Iteration 4889, loss = 0.07726225\n",
      "Iteration 4890, loss = 0.07722744\n",
      "Iteration 4891, loss = 0.07719266\n",
      "Iteration 4892, loss = 0.07715791\n",
      "Iteration 4893, loss = 0.07712318\n",
      "Iteration 4894, loss = 0.07708848\n",
      "Iteration 4895, loss = 0.07705381\n",
      "Iteration 4896, loss = 0.07701916\n",
      "Iteration 4897, loss = 0.07698454\n",
      "Iteration 4898, loss = 0.07694995\n",
      "Iteration 4899, loss = 0.07691538\n",
      "Iteration 4900, loss = 0.07688084\n",
      "Iteration 4901, loss = 0.07684633\n",
      "Iteration 4902, loss = 0.07681184\n",
      "Iteration 4903, loss = 0.07677738\n",
      "Iteration 4904, loss = 0.07674294\n",
      "Iteration 4905, loss = 0.07670853\n",
      "Iteration 4906, loss = 0.07667415\n",
      "Iteration 4907, loss = 0.07663979\n",
      "Iteration 4908, loss = 0.07660546\n",
      "Iteration 4909, loss = 0.07657116\n",
      "Iteration 4910, loss = 0.07653688\n",
      "Iteration 4911, loss = 0.07650263\n",
      "Iteration 4912, loss = 0.07646840\n",
      "Iteration 4913, loss = 0.07643420\n",
      "Iteration 4914, loss = 0.07640003\n",
      "Iteration 4915, loss = 0.07636588\n",
      "Iteration 4916, loss = 0.07633176\n",
      "Iteration 4917, loss = 0.07629766\n",
      "Iteration 4918, loss = 0.07626359\n",
      "Iteration 4919, loss = 0.07622955\n",
      "Iteration 4920, loss = 0.07619553\n",
      "Iteration 4921, loss = 0.07616153\n",
      "Iteration 4922, loss = 0.07612757\n",
      "Iteration 4923, loss = 0.07609363\n",
      "Iteration 4924, loss = 0.07605971\n",
      "Iteration 4925, loss = 0.07602582\n",
      "Iteration 4926, loss = 0.07599196\n",
      "Iteration 4927, loss = 0.07595812\n",
      "Iteration 4928, loss = 0.07592431\n",
      "Iteration 4929, loss = 0.07589052\n",
      "Iteration 4930, loss = 0.07585676\n",
      "Iteration 4931, loss = 0.07582302\n",
      "Iteration 4932, loss = 0.07578931\n",
      "Iteration 4933, loss = 0.07575563\n",
      "Iteration 4934, loss = 0.07572197\n",
      "Iteration 4935, loss = 0.07568833\n",
      "Iteration 4936, loss = 0.07565472\n",
      "Iteration 4937, loss = 0.07562114\n",
      "Iteration 4938, loss = 0.07558758\n",
      "Iteration 4939, loss = 0.07555405\n",
      "Iteration 4940, loss = 0.07552054\n",
      "Iteration 4941, loss = 0.07548706\n",
      "Iteration 4942, loss = 0.07545361\n",
      "Iteration 4943, loss = 0.07542018\n",
      "Iteration 4944, loss = 0.07538677\n",
      "Iteration 4945, loss = 0.07535339\n",
      "Iteration 4946, loss = 0.07532003\n",
      "Iteration 4947, loss = 0.07528670\n",
      "Iteration 4948, loss = 0.07525340\n",
      "Iteration 4949, loss = 0.07522012\n",
      "Iteration 4950, loss = 0.07518686\n",
      "Iteration 4951, loss = 0.07515363\n",
      "Iteration 4952, loss = 0.07512043\n",
      "Iteration 4953, loss = 0.07508725\n",
      "Iteration 4954, loss = 0.07505409\n",
      "Iteration 4955, loss = 0.07502096\n",
      "Iteration 4956, loss = 0.07498786\n",
      "Iteration 4957, loss = 0.07495477\n",
      "Iteration 4958, loss = 0.07492172\n",
      "Iteration 4959, loss = 0.07488869\n",
      "Iteration 4960, loss = 0.07485568\n",
      "Iteration 4961, loss = 0.07482270\n",
      "Iteration 4962, loss = 0.07478974\n",
      "Iteration 4963, loss = 0.07475681\n",
      "Iteration 4964, loss = 0.07472391\n",
      "Iteration 4965, loss = 0.07469102\n",
      "Iteration 4966, loss = 0.07465817\n",
      "Iteration 4967, loss = 0.07462533\n",
      "Iteration 4968, loss = 0.07459253\n",
      "Iteration 4969, loss = 0.07455974\n",
      "Iteration 4970, loss = 0.07452698\n",
      "Iteration 4971, loss = 0.07449425\n",
      "Iteration 4972, loss = 0.07446154\n",
      "Iteration 4973, loss = 0.07442885\n",
      "Iteration 4974, loss = 0.07439619\n",
      "Iteration 4975, loss = 0.07436355\n",
      "Iteration 4976, loss = 0.07433094\n",
      "Iteration 4977, loss = 0.07429835\n",
      "Iteration 4978, loss = 0.07426579\n",
      "Iteration 4979, loss = 0.07423325\n",
      "Iteration 4980, loss = 0.07420074\n",
      "Iteration 4981, loss = 0.07416824\n",
      "Iteration 4982, loss = 0.07413578\n",
      "Iteration 4983, loss = 0.07410334\n",
      "Iteration 4984, loss = 0.07407092\n",
      "Iteration 4985, loss = 0.07403852\n",
      "Iteration 4986, loss = 0.07400615\n",
      "Iteration 4987, loss = 0.07397381\n",
      "Iteration 4988, loss = 0.07394149\n",
      "Iteration 4989, loss = 0.07390919\n",
      "Iteration 4990, loss = 0.07387692\n",
      "Iteration 4991, loss = 0.07384467\n",
      "Iteration 4992, loss = 0.07381244\n",
      "Iteration 4993, loss = 0.07378024\n",
      "Iteration 4994, loss = 0.07374806\n",
      "Iteration 4995, loss = 0.07371591\n",
      "Iteration 4996, loss = 0.07368378\n",
      "Iteration 4997, loss = 0.07365168\n",
      "Iteration 4998, loss = 0.07361959\n",
      "Iteration 4999, loss = 0.07358754\n",
      "Iteration 5000, loss = 0.07355550\n",
      "Iteration 5001, loss = 0.07352349\n",
      "Iteration 5002, loss = 0.07349151\n",
      "Iteration 5003, loss = 0.07345954\n",
      "Iteration 5004, loss = 0.07342760\n",
      "Iteration 5005, loss = 0.07339569\n",
      "Iteration 5006, loss = 0.07336380\n",
      "Iteration 5007, loss = 0.07333193\n",
      "Iteration 5008, loss = 0.07330009\n",
      "Iteration 5009, loss = 0.07326826\n",
      "Iteration 5010, loss = 0.07323647\n",
      "Iteration 5011, loss = 0.07320469\n",
      "Iteration 5012, loss = 0.07317294\n",
      "Iteration 5013, loss = 0.07314122\n",
      "Iteration 5014, loss = 0.07310951\n",
      "Iteration 5015, loss = 0.07307784\n",
      "Iteration 5016, loss = 0.07304618\n",
      "Iteration 5017, loss = 0.07301455\n",
      "Iteration 5018, loss = 0.07298294\n",
      "Iteration 5019, loss = 0.07295135\n",
      "Iteration 5020, loss = 0.07291979\n",
      "Iteration 5021, loss = 0.07288825\n",
      "Iteration 5022, loss = 0.07285673\n",
      "Iteration 5023, loss = 0.07282524\n",
      "Iteration 5024, loss = 0.07279377\n",
      "Iteration 5025, loss = 0.07276233\n",
      "Iteration 5026, loss = 0.07273090\n",
      "Iteration 5027, loss = 0.07269950\n",
      "Iteration 5028, loss = 0.07266813\n",
      "Iteration 5029, loss = 0.07263677\n",
      "Iteration 5030, loss = 0.07260544\n",
      "Iteration 5031, loss = 0.07257414\n",
      "Iteration 5032, loss = 0.07254285\n",
      "Iteration 5033, loss = 0.07251159\n",
      "Iteration 5034, loss = 0.07248035\n",
      "Iteration 5035, loss = 0.07244914\n",
      "Iteration 5036, loss = 0.07241794\n",
      "Iteration 5037, loss = 0.07238677\n",
      "Iteration 5038, loss = 0.07235563\n",
      "Iteration 5039, loss = 0.07232451\n",
      "Iteration 5040, loss = 0.07229340\n",
      "Iteration 5041, loss = 0.07226233\n",
      "Iteration 5042, loss = 0.07223127\n",
      "Iteration 5043, loss = 0.07220024\n",
      "Iteration 5044, loss = 0.07216923\n",
      "Iteration 5045, loss = 0.07213824\n",
      "Iteration 5046, loss = 0.07210728\n",
      "Iteration 5047, loss = 0.07207634\n",
      "Iteration 5048, loss = 0.07204542\n",
      "Iteration 5049, loss = 0.07201453\n",
      "Iteration 5050, loss = 0.07198365\n",
      "Iteration 5051, loss = 0.07195280\n",
      "Iteration 5052, loss = 0.07192197\n",
      "Iteration 5053, loss = 0.07189117\n",
      "Iteration 5054, loss = 0.07186039\n",
      "Iteration 5055, loss = 0.07182963\n",
      "Iteration 5056, loss = 0.07179889\n",
      "Iteration 5057, loss = 0.07176817\n",
      "Iteration 5058, loss = 0.07173748\n",
      "Iteration 5059, loss = 0.07170681\n",
      "Iteration 5060, loss = 0.07167616\n",
      "Iteration 5061, loss = 0.07164554\n",
      "Iteration 5062, loss = 0.07161494\n",
      "Iteration 5063, loss = 0.07158435\n",
      "Iteration 5064, loss = 0.07155380\n",
      "Iteration 5065, loss = 0.07152326\n",
      "Iteration 5066, loss = 0.07149275\n",
      "Iteration 5067, loss = 0.07146226\n",
      "Iteration 5068, loss = 0.07143179\n",
      "Iteration 5069, loss = 0.07140134\n",
      "Iteration 5070, loss = 0.07137092\n",
      "Iteration 5071, loss = 0.07134051\n",
      "Iteration 5072, loss = 0.07131013\n",
      "Iteration 5073, loss = 0.07127977\n",
      "Iteration 5074, loss = 0.07124944\n",
      "Iteration 5075, loss = 0.07121912\n",
      "Iteration 5076, loss = 0.07118883\n",
      "Iteration 5077, loss = 0.07115856\n",
      "Iteration 5078, loss = 0.07112831\n",
      "Iteration 5079, loss = 0.07109809\n",
      "Iteration 5080, loss = 0.07106788\n",
      "Iteration 5081, loss = 0.07103770\n",
      "Iteration 5082, loss = 0.07100754\n",
      "Iteration 5083, loss = 0.07097741\n",
      "Iteration 5084, loss = 0.07094729\n",
      "Iteration 5085, loss = 0.07091719\n",
      "Iteration 5086, loss = 0.07088712\n",
      "Iteration 5087, loss = 0.07085707\n",
      "Iteration 5088, loss = 0.07082704\n",
      "Iteration 5089, loss = 0.07079704\n",
      "Iteration 5090, loss = 0.07076705\n",
      "Iteration 5091, loss = 0.07073709\n",
      "Iteration 5092, loss = 0.07070715\n",
      "Iteration 5093, loss = 0.07067723\n",
      "Iteration 5094, loss = 0.07064733\n",
      "Iteration 5095, loss = 0.07061745\n",
      "Iteration 5096, loss = 0.07058760\n",
      "Iteration 5097, loss = 0.07055776\n",
      "Iteration 5098, loss = 0.07052795\n",
      "Iteration 5099, loss = 0.07049816\n",
      "Iteration 5100, loss = 0.07046839\n",
      "Iteration 5101, loss = 0.07043864\n",
      "Iteration 5102, loss = 0.07040892\n",
      "Iteration 5103, loss = 0.07037922\n",
      "Iteration 5104, loss = 0.07034953\n",
      "Iteration 5105, loss = 0.07031987\n",
      "Iteration 5106, loss = 0.07029023\n",
      "Iteration 5107, loss = 0.07026061\n",
      "Iteration 5108, loss = 0.07023102\n",
      "Iteration 5109, loss = 0.07020144\n",
      "Iteration 5110, loss = 0.07017189\n",
      "Iteration 5111, loss = 0.07014235\n",
      "Iteration 5112, loss = 0.07011284\n",
      "Iteration 5113, loss = 0.07008335\n",
      "Iteration 5114, loss = 0.07005388\n",
      "Iteration 5115, loss = 0.07002443\n",
      "Iteration 5116, loss = 0.06999501\n",
      "Iteration 5117, loss = 0.06996560\n",
      "Iteration 5118, loss = 0.06993622\n",
      "Iteration 5119, loss = 0.06990685\n",
      "Iteration 5120, loss = 0.06987751\n",
      "Iteration 5121, loss = 0.06984819\n",
      "Iteration 5122, loss = 0.06981889\n",
      "Iteration 5123, loss = 0.06978961\n",
      "Iteration 5124, loss = 0.06976036\n",
      "Iteration 5125, loss = 0.06973112\n",
      "Iteration 5126, loss = 0.06970190\n",
      "Iteration 5127, loss = 0.06967271\n",
      "Iteration 5128, loss = 0.06964353\n",
      "Iteration 5129, loss = 0.06961438\n",
      "Iteration 5130, loss = 0.06958525\n",
      "Iteration 5131, loss = 0.06955614\n",
      "Iteration 5132, loss = 0.06952705\n",
      "Iteration 5133, loss = 0.06949798\n",
      "Iteration 5134, loss = 0.06946893\n",
      "Iteration 5135, loss = 0.06943990\n",
      "Iteration 5136, loss = 0.06941090\n",
      "Iteration 5137, loss = 0.06938191\n",
      "Iteration 5138, loss = 0.06935295\n",
      "Iteration 5139, loss = 0.06932400\n",
      "Iteration 5140, loss = 0.06929508\n",
      "Iteration 5141, loss = 0.06926618\n",
      "Iteration 5142, loss = 0.06923729\n",
      "Iteration 5143, loss = 0.06920843\n",
      "Iteration 5144, loss = 0.06917959\n",
      "Iteration 5145, loss = 0.06915077\n",
      "Iteration 5146, loss = 0.06912197\n",
      "Iteration 5147, loss = 0.06909319\n",
      "Iteration 5148, loss = 0.06906443\n",
      "Iteration 5149, loss = 0.06903570\n",
      "Iteration 5150, loss = 0.06900698\n",
      "Iteration 5151, loss = 0.06897828\n",
      "Iteration 5152, loss = 0.06894960\n",
      "Iteration 5153, loss = 0.06892095\n",
      "Iteration 5154, loss = 0.06889231\n",
      "Iteration 5155, loss = 0.06886370\n",
      "Iteration 5156, loss = 0.06883510\n",
      "Iteration 5157, loss = 0.06880653\n",
      "Iteration 5158, loss = 0.06877797\n",
      "Iteration 5159, loss = 0.06874944\n",
      "Iteration 5160, loss = 0.06872093\n",
      "Iteration 5161, loss = 0.06869243\n",
      "Iteration 5162, loss = 0.06866396\n",
      "Iteration 5163, loss = 0.06863551\n",
      "Iteration 5164, loss = 0.06860708\n",
      "Iteration 5165, loss = 0.06857866\n",
      "Iteration 5166, loss = 0.06855027\n",
      "Iteration 5167, loss = 0.06852190\n",
      "Iteration 5168, loss = 0.06849355\n",
      "Iteration 5169, loss = 0.06846522\n",
      "Iteration 5170, loss = 0.06843691\n",
      "Iteration 5171, loss = 0.06840862\n",
      "Iteration 5172, loss = 0.06838034\n",
      "Iteration 5173, loss = 0.06835209\n",
      "Iteration 5174, loss = 0.06832386\n",
      "Iteration 5175, loss = 0.06829565\n",
      "Iteration 5176, loss = 0.06826746\n",
      "Iteration 5177, loss = 0.06823929\n",
      "Iteration 5178, loss = 0.06821114\n",
      "Iteration 5179, loss = 0.06818301\n",
      "Iteration 5180, loss = 0.06815490\n",
      "Iteration 5181, loss = 0.06812681\n",
      "Iteration 5182, loss = 0.06809873\n",
      "Iteration 5183, loss = 0.06807068\n",
      "Iteration 5184, loss = 0.06804265\n",
      "Iteration 5185, loss = 0.06801464\n",
      "Iteration 5186, loss = 0.06798665\n",
      "Iteration 5187, loss = 0.06795868\n",
      "Iteration 5188, loss = 0.06793072\n",
      "Iteration 5189, loss = 0.06790279\n",
      "Iteration 5190, loss = 0.06787488\n",
      "Iteration 5191, loss = 0.06784699\n",
      "Iteration 5192, loss = 0.06781911\n",
      "Iteration 5193, loss = 0.06779126\n",
      "Iteration 5194, loss = 0.06776342\n",
      "Iteration 5195, loss = 0.06773561\n",
      "Iteration 5196, loss = 0.06770782\n",
      "Iteration 5197, loss = 0.06768004\n",
      "Iteration 5198, loss = 0.06765228\n",
      "Iteration 5199, loss = 0.06762455\n",
      "Iteration 5200, loss = 0.06759683\n",
      "Iteration 5201, loss = 0.06756914\n",
      "Iteration 5202, loss = 0.06754146\n",
      "Iteration 5203, loss = 0.06751380\n",
      "Iteration 5204, loss = 0.06748616\n",
      "Iteration 5205, loss = 0.06745854\n",
      "Iteration 5206, loss = 0.06743094\n",
      "Iteration 5207, loss = 0.06740336\n",
      "Iteration 5208, loss = 0.06737580\n",
      "Iteration 5209, loss = 0.06734826\n",
      "Iteration 5210, loss = 0.06732074\n",
      "Iteration 5211, loss = 0.06729324\n",
      "Iteration 5212, loss = 0.06726575\n",
      "Iteration 5213, loss = 0.06723829\n",
      "Iteration 5214, loss = 0.06721084\n",
      "Iteration 5215, loss = 0.06718342\n",
      "Iteration 5216, loss = 0.06715601\n",
      "Iteration 5217, loss = 0.06712863\n",
      "Iteration 5218, loss = 0.06710126\n",
      "Iteration 5219, loss = 0.06707391\n",
      "Iteration 5220, loss = 0.06704658\n",
      "Iteration 5221, loss = 0.06701927\n",
      "Iteration 5222, loss = 0.06699198\n",
      "Iteration 5223, loss = 0.06696471\n",
      "Iteration 5224, loss = 0.06693746\n",
      "Iteration 5225, loss = 0.06691022\n",
      "Iteration 5226, loss = 0.06688301\n",
      "Iteration 5227, loss = 0.06685581\n",
      "Iteration 5228, loss = 0.06682863\n",
      "Iteration 5229, loss = 0.06680148\n",
      "Iteration 5230, loss = 0.06677434\n",
      "Iteration 5231, loss = 0.06674722\n",
      "Iteration 5232, loss = 0.06672012\n",
      "Iteration 5233, loss = 0.06669304\n",
      "Iteration 5234, loss = 0.06666597\n",
      "Iteration 5235, loss = 0.06663893\n",
      "Iteration 5236, loss = 0.06661191\n",
      "Iteration 5237, loss = 0.06658490\n",
      "Iteration 5238, loss = 0.06655791\n",
      "Iteration 5239, loss = 0.06653094\n",
      "Iteration 5240, loss = 0.06650399\n",
      "Iteration 5241, loss = 0.06647706\n",
      "Iteration 5242, loss = 0.06645015\n",
      "Iteration 5243, loss = 0.06642326\n",
      "Iteration 5244, loss = 0.06639638\n",
      "Iteration 5245, loss = 0.06636953\n",
      "Iteration 5246, loss = 0.06634269\n",
      "Iteration 5247, loss = 0.06631587\n",
      "Iteration 5248, loss = 0.06628907\n",
      "Iteration 5249, loss = 0.06626229\n",
      "Iteration 5250, loss = 0.06623553\n",
      "Iteration 5251, loss = 0.06620879\n",
      "Iteration 5252, loss = 0.06618206\n",
      "Iteration 5253, loss = 0.06615535\n",
      "Iteration 5254, loss = 0.06612867\n",
      "Iteration 5255, loss = 0.06610200\n",
      "Iteration 5256, loss = 0.06607535\n",
      "Iteration 5257, loss = 0.06604871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5258, loss = 0.06602210\n",
      "Iteration 5259, loss = 0.06599550\n",
      "Iteration 5260, loss = 0.06596893\n",
      "Iteration 5261, loss = 0.06594237\n",
      "Iteration 5262, loss = 0.06591583\n",
      "Iteration 5263, loss = 0.06588931\n",
      "Iteration 5264, loss = 0.06586280\n",
      "Iteration 5265, loss = 0.06583632\n",
      "Iteration 5266, loss = 0.06580985\n",
      "Iteration 5267, loss = 0.06578340\n",
      "Iteration 5268, loss = 0.06575697\n",
      "Iteration 5269, loss = 0.06573056\n",
      "Iteration 5270, loss = 0.06570417\n",
      "Iteration 5271, loss = 0.06567779\n",
      "Iteration 5272, loss = 0.06565144\n",
      "Iteration 5273, loss = 0.06562510\n",
      "Iteration 5274, loss = 0.06559878\n",
      "Iteration 5275, loss = 0.06557248\n",
      "Iteration 5276, loss = 0.06554619\n",
      "Iteration 5277, loss = 0.06551993\n",
      "Iteration 5278, loss = 0.06549368\n",
      "Iteration 5279, loss = 0.06546745\n",
      "Iteration 5280, loss = 0.06544124\n",
      "Iteration 5281, loss = 0.06541505\n",
      "Iteration 5282, loss = 0.06538887\n",
      "Iteration 5283, loss = 0.06536271\n",
      "Iteration 5284, loss = 0.06533657\n",
      "Iteration 5285, loss = 0.06531045\n",
      "Iteration 5286, loss = 0.06528435\n",
      "Iteration 5287, loss = 0.06525826\n",
      "Iteration 5288, loss = 0.06523220\n",
      "Iteration 5289, loss = 0.06520615\n",
      "Iteration 5290, loss = 0.06518012\n",
      "Iteration 5291, loss = 0.06515410\n",
      "Iteration 5292, loss = 0.06512811\n",
      "Iteration 5293, loss = 0.06510213\n",
      "Iteration 5294, loss = 0.06507617\n",
      "Iteration 5295, loss = 0.06505023\n",
      "Iteration 5296, loss = 0.06502431\n",
      "Iteration 5297, loss = 0.06499840\n",
      "Iteration 5298, loss = 0.06497251\n",
      "Iteration 5299, loss = 0.06494664\n",
      "Iteration 5300, loss = 0.06492079\n",
      "Iteration 5301, loss = 0.06489495\n",
      "Iteration 5302, loss = 0.06486914\n",
      "Iteration 5303, loss = 0.06484334\n",
      "Iteration 5304, loss = 0.06481756\n",
      "Iteration 5305, loss = 0.06479179\n",
      "Iteration 5306, loss = 0.06476605\n",
      "Iteration 5307, loss = 0.06474032\n",
      "Iteration 5308, loss = 0.06471461\n",
      "Iteration 5309, loss = 0.06468891\n",
      "Iteration 5310, loss = 0.06466324\n",
      "Iteration 5311, loss = 0.06463758\n",
      "Iteration 5312, loss = 0.06461194\n",
      "Iteration 5313, loss = 0.06458632\n",
      "Iteration 5314, loss = 0.06456071\n",
      "Iteration 5315, loss = 0.06453512\n",
      "Iteration 5316, loss = 0.06450955\n",
      "Iteration 5317, loss = 0.06448400\n",
      "Iteration 5318, loss = 0.06445846\n",
      "Iteration 5319, loss = 0.06443294\n",
      "Iteration 5320, loss = 0.06440744\n",
      "Iteration 5321, loss = 0.06438196\n",
      "Iteration 5322, loss = 0.06435650\n",
      "Iteration 5323, loss = 0.06433105\n",
      "Iteration 5324, loss = 0.06430562\n",
      "Iteration 5325, loss = 0.06428020\n",
      "Iteration 5326, loss = 0.06425481\n",
      "Iteration 5327, loss = 0.06422943\n",
      "Iteration 5328, loss = 0.06420407\n",
      "Iteration 5329, loss = 0.06417872\n",
      "Iteration 5330, loss = 0.06415339\n",
      "Iteration 5331, loss = 0.06412808\n",
      "Iteration 5332, loss = 0.06410279\n",
      "Iteration 5333, loss = 0.06407752\n",
      "Iteration 5334, loss = 0.06405226\n",
      "Iteration 5335, loss = 0.06402702\n",
      "Iteration 5336, loss = 0.06400179\n",
      "Iteration 5337, loss = 0.06397659\n",
      "Iteration 5338, loss = 0.06395140\n",
      "Iteration 5339, loss = 0.06392623\n",
      "Iteration 5340, loss = 0.06390107\n",
      "Iteration 5341, loss = 0.06387593\n",
      "Iteration 5342, loss = 0.06385081\n",
      "Iteration 5343, loss = 0.06382571\n",
      "Iteration 5344, loss = 0.06380062\n",
      "Iteration 5345, loss = 0.06377555\n",
      "Iteration 5346, loss = 0.06375050\n",
      "Iteration 5347, loss = 0.06372546\n",
      "Iteration 5348, loss = 0.06370045\n",
      "Iteration 5349, loss = 0.06367544\n",
      "Iteration 5350, loss = 0.06365046\n",
      "Iteration 5351, loss = 0.06362549\n",
      "Iteration 5352, loss = 0.06360054\n",
      "Iteration 5353, loss = 0.06357561\n",
      "Iteration 5354, loss = 0.06355069\n",
      "Iteration 5355, loss = 0.06352579\n",
      "Iteration 5356, loss = 0.06350091\n",
      "Iteration 5357, loss = 0.06347604\n",
      "Iteration 5358, loss = 0.06345119\n",
      "Iteration 5359, loss = 0.06342636\n",
      "Iteration 5360, loss = 0.06340154\n",
      "Iteration 5361, loss = 0.06337674\n",
      "Iteration 5362, loss = 0.06335196\n",
      "Iteration 5363, loss = 0.06332720\n",
      "Iteration 5364, loss = 0.06330245\n",
      "Iteration 5365, loss = 0.06327771\n",
      "Iteration 5366, loss = 0.06325300\n",
      "Iteration 5367, loss = 0.06322830\n",
      "Iteration 5368, loss = 0.06320362\n",
      "Iteration 5369, loss = 0.06317895\n",
      "Iteration 5370, loss = 0.06315430\n",
      "Iteration 5371, loss = 0.06312967\n",
      "Iteration 5372, loss = 0.06310506\n",
      "Iteration 5373, loss = 0.06308046\n",
      "Iteration 5374, loss = 0.06305588\n",
      "Iteration 5375, loss = 0.06303131\n",
      "Iteration 5376, loss = 0.06300676\n",
      "Iteration 5377, loss = 0.06298223\n",
      "Iteration 5378, loss = 0.06295771\n",
      "Iteration 5379, loss = 0.06293321\n",
      "Iteration 5380, loss = 0.06290873\n",
      "Iteration 5381, loss = 0.06288426\n",
      "Iteration 5382, loss = 0.06285981\n",
      "Iteration 5383, loss = 0.06283538\n",
      "Iteration 5384, loss = 0.06281096\n",
      "Iteration 5385, loss = 0.06278656\n",
      "Iteration 5386, loss = 0.06276218\n",
      "Iteration 5387, loss = 0.06273781\n",
      "Iteration 5388, loss = 0.06271346\n",
      "Iteration 5389, loss = 0.06268912\n",
      "Iteration 5390, loss = 0.06266481\n",
      "Iteration 5391, loss = 0.06264050\n",
      "Iteration 5392, loss = 0.06261622\n",
      "Iteration 5393, loss = 0.06259195\n",
      "Iteration 5394, loss = 0.06256769\n",
      "Iteration 5395, loss = 0.06254346\n",
      "Iteration 5396, loss = 0.06251924\n",
      "Iteration 5397, loss = 0.06249503\n",
      "Iteration 5398, loss = 0.06247084\n",
      "Iteration 5399, loss = 0.06244667\n",
      "Iteration 5400, loss = 0.06242251\n",
      "Iteration 5401, loss = 0.06239837\n",
      "Iteration 5402, loss = 0.06237425\n",
      "Iteration 5403, loss = 0.06235014\n",
      "Iteration 5404, loss = 0.06232605\n",
      "Iteration 5405, loss = 0.06230198\n",
      "Iteration 5406, loss = 0.06227792\n",
      "Iteration 5407, loss = 0.06225387\n",
      "Iteration 5408, loss = 0.06222985\n",
      "Iteration 5409, loss = 0.06220584\n",
      "Iteration 5410, loss = 0.06218184\n",
      "Iteration 5411, loss = 0.06215786\n",
      "Iteration 5412, loss = 0.06213390\n",
      "Iteration 5413, loss = 0.06210995\n",
      "Iteration 5414, loss = 0.06208602\n",
      "Iteration 5415, loss = 0.06206211\n",
      "Iteration 5416, loss = 0.06203821\n",
      "Iteration 5417, loss = 0.06201433\n",
      "Iteration 5418, loss = 0.06199046\n",
      "Iteration 5419, loss = 0.06196661\n",
      "Iteration 5420, loss = 0.06194277\n",
      "Iteration 5421, loss = 0.06191895\n",
      "Iteration 5422, loss = 0.06189515\n",
      "Iteration 5423, loss = 0.06187136\n",
      "Iteration 5424, loss = 0.06184759\n",
      "Iteration 5425, loss = 0.06182383\n",
      "Iteration 5426, loss = 0.06180009\n",
      "Iteration 5427, loss = 0.06177637\n",
      "Iteration 5428, loss = 0.06175266\n",
      "Iteration 5429, loss = 0.06172897\n",
      "Iteration 5430, loss = 0.06170529\n",
      "Iteration 5431, loss = 0.06168163\n",
      "Iteration 5432, loss = 0.06165799\n",
      "Iteration 5433, loss = 0.06163436\n",
      "Iteration 5434, loss = 0.06161074\n",
      "Iteration 5435, loss = 0.06158714\n",
      "Iteration 5436, loss = 0.06156356\n",
      "Iteration 5437, loss = 0.06153999\n",
      "Iteration 5438, loss = 0.06151644\n",
      "Iteration 5439, loss = 0.06149291\n",
      "Iteration 5440, loss = 0.06146939\n",
      "Iteration 5441, loss = 0.06144588\n",
      "Iteration 5442, loss = 0.06142239\n",
      "Iteration 5443, loss = 0.06139892\n",
      "Iteration 5444, loss = 0.06137546\n",
      "Iteration 5445, loss = 0.06135202\n",
      "Iteration 5446, loss = 0.06132859\n",
      "Iteration 5447, loss = 0.06130518\n",
      "Iteration 5448, loss = 0.06128179\n",
      "Iteration 5449, loss = 0.06125841\n",
      "Iteration 5450, loss = 0.06123504\n",
      "Iteration 5451, loss = 0.06121169\n",
      "Iteration 5452, loss = 0.06118836\n",
      "Iteration 5453, loss = 0.06116504\n",
      "Iteration 5454, loss = 0.06114174\n",
      "Iteration 5455, loss = 0.06111845\n",
      "Iteration 5456, loss = 0.06109518\n",
      "Iteration 5457, loss = 0.06107192\n",
      "Iteration 5458, loss = 0.06104868\n",
      "Iteration 5459, loss = 0.06102545\n",
      "Iteration 5460, loss = 0.06100224\n",
      "Iteration 5461, loss = 0.06097905\n",
      "Iteration 5462, loss = 0.06095587\n",
      "Iteration 5463, loss = 0.06093270\n",
      "Iteration 5464, loss = 0.06090955\n",
      "Iteration 5465, loss = 0.06088642\n",
      "Iteration 5466, loss = 0.06086330\n",
      "Iteration 5467, loss = 0.06084020\n",
      "Iteration 5468, loss = 0.06081711\n",
      "Iteration 5469, loss = 0.06079404\n",
      "Iteration 5470, loss = 0.06077098\n",
      "Iteration 5471, loss = 0.06074794\n",
      "Iteration 5472, loss = 0.06072491\n",
      "Iteration 5473, loss = 0.06070190\n",
      "Iteration 5474, loss = 0.06067890\n",
      "Iteration 5475, loss = 0.06065592\n",
      "Iteration 5476, loss = 0.06063295\n",
      "Iteration 5477, loss = 0.06061000\n",
      "Iteration 5478, loss = 0.06058706\n",
      "Iteration 5479, loss = 0.06056414\n",
      "Iteration 5480, loss = 0.06054123\n",
      "Iteration 5481, loss = 0.06051834\n",
      "Iteration 5482, loss = 0.06049546\n",
      "Iteration 5483, loss = 0.06047260\n",
      "Iteration 5484, loss = 0.06044975\n",
      "Iteration 5485, loss = 0.06042692\n",
      "Iteration 5486, loss = 0.06040411\n",
      "Iteration 5487, loss = 0.06038130\n",
      "Iteration 5488, loss = 0.06035852\n",
      "Iteration 5489, loss = 0.06033575\n",
      "Iteration 5490, loss = 0.06031299\n",
      "Iteration 5491, loss = 0.06029025\n",
      "Iteration 5492, loss = 0.06026752\n",
      "Iteration 5493, loss = 0.06024481\n",
      "Iteration 5494, loss = 0.06022211\n",
      "Iteration 5495, loss = 0.06019943\n",
      "Iteration 5496, loss = 0.06017676\n",
      "Iteration 5497, loss = 0.06015411\n",
      "Iteration 5498, loss = 0.06013147\n",
      "Iteration 5499, loss = 0.06010885\n",
      "Iteration 5500, loss = 0.06008624\n",
      "Iteration 5501, loss = 0.06006365\n",
      "Iteration 5502, loss = 0.06004107\n",
      "Iteration 5503, loss = 0.06001850\n",
      "Iteration 5504, loss = 0.05999595\n",
      "Iteration 5505, loss = 0.05997342\n",
      "Iteration 5506, loss = 0.05995090\n",
      "Iteration 5507, loss = 0.05992839\n",
      "Iteration 5508, loss = 0.05990590\n",
      "Iteration 5509, loss = 0.05988343\n",
      "Iteration 5510, loss = 0.05986097\n",
      "Iteration 5511, loss = 0.05983852\n",
      "Iteration 5512, loss = 0.05981609\n",
      "Iteration 5513, loss = 0.05979367\n",
      "Iteration 5514, loss = 0.05977127\n",
      "Iteration 5515, loss = 0.05974888\n",
      "Iteration 5516, loss = 0.05972651\n",
      "Iteration 5517, loss = 0.05970415\n",
      "Iteration 5518, loss = 0.05968181\n",
      "Iteration 5519, loss = 0.05965948\n",
      "Iteration 5520, loss = 0.05963716\n",
      "Iteration 5521, loss = 0.05961486\n",
      "Iteration 5522, loss = 0.05959257\n",
      "Iteration 5523, loss = 0.05957030\n",
      "Iteration 5524, loss = 0.05954804\n",
      "Iteration 5525, loss = 0.05952580\n",
      "Iteration 5526, loss = 0.05950357\n",
      "Iteration 5527, loss = 0.05948136\n",
      "Iteration 5528, loss = 0.05945916\n",
      "Iteration 5529, loss = 0.05943698\n",
      "Iteration 5530, loss = 0.05941481\n",
      "Iteration 5531, loss = 0.05939265\n",
      "Iteration 5532, loss = 0.05937051\n",
      "Iteration 5533, loss = 0.05934838\n",
      "Iteration 5534, loss = 0.05932627\n",
      "Iteration 5535, loss = 0.05930417\n",
      "Iteration 5536, loss = 0.05928208\n",
      "Iteration 5537, loss = 0.05926001\n",
      "Iteration 5538, loss = 0.05923796\n",
      "Iteration 5539, loss = 0.05921592\n",
      "Iteration 5540, loss = 0.05919389\n",
      "Iteration 5541, loss = 0.05917188\n",
      "Iteration 5542, loss = 0.05914988\n",
      "Iteration 5543, loss = 0.05912789\n",
      "Iteration 5544, loss = 0.05910592\n",
      "Iteration 5545, loss = 0.05908397\n",
      "Iteration 5546, loss = 0.05906203\n",
      "Iteration 5547, loss = 0.05904010\n",
      "Iteration 5548, loss = 0.05901819\n",
      "Iteration 5549, loss = 0.05899629\n",
      "Iteration 5550, loss = 0.05897440\n",
      "Iteration 5551, loss = 0.05895253\n",
      "Iteration 5552, loss = 0.05893067\n",
      "Iteration 5553, loss = 0.05890883\n",
      "Iteration 5554, loss = 0.05888700\n",
      "Iteration 5555, loss = 0.05886519\n",
      "Iteration 5556, loss = 0.05884339\n",
      "Iteration 5557, loss = 0.05882160\n",
      "Iteration 5558, loss = 0.05879983\n",
      "Iteration 5559, loss = 0.05877807\n",
      "Iteration 5560, loss = 0.05875633\n",
      "Iteration 5561, loss = 0.05873460\n",
      "Iteration 5562, loss = 0.05871288\n",
      "Iteration 5563, loss = 0.05869118\n",
      "Iteration 5564, loss = 0.05866949\n",
      "Iteration 5565, loss = 0.05864781\n",
      "Iteration 5566, loss = 0.05862615\n",
      "Iteration 5567, loss = 0.05860451\n",
      "Iteration 5568, loss = 0.05858288\n",
      "Iteration 5569, loss = 0.05856126\n",
      "Iteration 5570, loss = 0.05853965\n",
      "Iteration 5571, loss = 0.05851806\n",
      "Iteration 5572, loss = 0.05849648\n",
      "Iteration 5573, loss = 0.05847492\n",
      "Iteration 5574, loss = 0.05845337\n",
      "Iteration 5575, loss = 0.05843184\n",
      "Iteration 5576, loss = 0.05841032\n",
      "Iteration 5577, loss = 0.05838881\n",
      "Iteration 5578, loss = 0.05836731\n",
      "Iteration 5579, loss = 0.05834583\n",
      "Iteration 5580, loss = 0.05832437\n",
      "Iteration 5581, loss = 0.05830292\n",
      "Iteration 5582, loss = 0.05828148\n",
      "Iteration 5583, loss = 0.05826005\n",
      "Iteration 5584, loss = 0.05823864\n",
      "Iteration 5585, loss = 0.05821724\n",
      "Iteration 5586, loss = 0.05819586\n",
      "Iteration 5587, loss = 0.05817449\n",
      "Iteration 5588, loss = 0.05815313\n",
      "Iteration 5589, loss = 0.05813179\n",
      "Iteration 5590, loss = 0.05811046\n",
      "Iteration 5591, loss = 0.05808914\n",
      "Iteration 5592, loss = 0.05806784\n",
      "Iteration 5593, loss = 0.05804655\n",
      "Iteration 5594, loss = 0.05802528\n",
      "Iteration 5595, loss = 0.05800402\n",
      "Iteration 5596, loss = 0.05798277\n",
      "Iteration 5597, loss = 0.05796154\n",
      "Iteration 5598, loss = 0.05794032\n",
      "Iteration 5599, loss = 0.05791911\n",
      "Iteration 5600, loss = 0.05789792\n",
      "Iteration 5601, loss = 0.05787674\n",
      "Iteration 5602, loss = 0.05785557\n",
      "Iteration 5603, loss = 0.05783442\n",
      "Iteration 5604, loss = 0.05781328\n",
      "Iteration 5605, loss = 0.05779215\n",
      "Iteration 5606, loss = 0.05777104\n",
      "Iteration 5607, loss = 0.05774994\n",
      "Iteration 5608, loss = 0.05772886\n",
      "Iteration 5609, loss = 0.05770779\n",
      "Iteration 5610, loss = 0.05768673\n",
      "Iteration 5611, loss = 0.05766568\n",
      "Iteration 5612, loss = 0.05764465\n",
      "Iteration 5613, loss = 0.05762363\n",
      "Iteration 5614, loss = 0.05760263\n",
      "Iteration 5615, loss = 0.05758164\n",
      "Iteration 5616, loss = 0.05756066\n",
      "Iteration 5617, loss = 0.05753969\n",
      "Iteration 5618, loss = 0.05751874\n",
      "Iteration 5619, loss = 0.05749780\n",
      "Iteration 5620, loss = 0.05747688\n",
      "Iteration 5621, loss = 0.05745597\n",
      "Iteration 5622, loss = 0.05743507\n",
      "Iteration 5623, loss = 0.05741418\n",
      "Iteration 5624, loss = 0.05739331\n",
      "Iteration 5625, loss = 0.05737245\n",
      "Iteration 5626, loss = 0.05735161\n",
      "Iteration 5627, loss = 0.05733078\n",
      "Iteration 5628, loss = 0.05730996\n",
      "Iteration 5629, loss = 0.05728915\n",
      "Iteration 5630, loss = 0.05726836\n",
      "Iteration 5631, loss = 0.05724758\n",
      "Iteration 5632, loss = 0.05722682\n",
      "Iteration 5633, loss = 0.05720606\n",
      "Iteration 5634, loss = 0.05718532\n",
      "Iteration 5635, loss = 0.05716460\n",
      "Iteration 5636, loss = 0.05714388\n",
      "Iteration 5637, loss = 0.05712318\n",
      "Iteration 5638, loss = 0.05710249\n",
      "Iteration 5639, loss = 0.05708182\n",
      "Iteration 5640, loss = 0.05706116\n",
      "Iteration 5641, loss = 0.05704051\n",
      "Iteration 5642, loss = 0.05701988\n",
      "Iteration 5643, loss = 0.05699925\n",
      "Iteration 5644, loss = 0.05697865\n",
      "Iteration 5645, loss = 0.05695805\n",
      "Iteration 5646, loss = 0.05693747\n",
      "Iteration 5647, loss = 0.05691690\n",
      "Iteration 5648, loss = 0.05689634\n",
      "Iteration 5649, loss = 0.05687580\n",
      "Iteration 5650, loss = 0.05685527\n",
      "Iteration 5651, loss = 0.05683475\n",
      "Iteration 5652, loss = 0.05681424\n",
      "Iteration 5653, loss = 0.05679375\n",
      "Iteration 5654, loss = 0.05677327\n",
      "Iteration 5655, loss = 0.05675280\n",
      "Iteration 5656, loss = 0.05673235\n",
      "Iteration 5657, loss = 0.05671191\n",
      "Iteration 5658, loss = 0.05669148\n",
      "Iteration 5659, loss = 0.05667107\n",
      "Iteration 5660, loss = 0.05665067\n",
      "Iteration 5661, loss = 0.05663028\n",
      "Iteration 5662, loss = 0.05660990\n",
      "Iteration 5663, loss = 0.05658954\n",
      "Iteration 5664, loss = 0.05656919\n",
      "Iteration 5665, loss = 0.05654885\n",
      "Iteration 5666, loss = 0.05652852\n",
      "Iteration 5667, loss = 0.05650821\n",
      "Iteration 5668, loss = 0.05648791\n",
      "Iteration 5669, loss = 0.05646763\n",
      "Iteration 5670, loss = 0.05644735\n",
      "Iteration 5671, loss = 0.05642709\n",
      "Iteration 5672, loss = 0.05640684\n",
      "Iteration 5673, loss = 0.05638661\n",
      "Iteration 5674, loss = 0.05636638\n",
      "Iteration 5675, loss = 0.05634617\n",
      "Iteration 5676, loss = 0.05632598\n",
      "Iteration 5677, loss = 0.05630579\n",
      "Iteration 5678, loss = 0.05628562\n",
      "Iteration 5679, loss = 0.05626546\n",
      "Iteration 5680, loss = 0.05624531\n",
      "Iteration 5681, loss = 0.05622518\n",
      "Iteration 5682, loss = 0.05620505\n",
      "Iteration 5683, loss = 0.05618494\n",
      "Iteration 5684, loss = 0.05616485\n",
      "Iteration 5685, loss = 0.05614476\n",
      "Iteration 5686, loss = 0.05612469\n",
      "Iteration 5687, loss = 0.05610463\n",
      "Iteration 5688, loss = 0.05608459\n",
      "Iteration 5689, loss = 0.05606455\n",
      "Iteration 5690, loss = 0.05604453\n",
      "Iteration 5691, loss = 0.05602452\n",
      "Iteration 5692, loss = 0.05600453\n",
      "Iteration 5693, loss = 0.05598454\n",
      "Iteration 5694, loss = 0.05596457\n",
      "Iteration 5695, loss = 0.05594461\n",
      "Iteration 5696, loss = 0.05592467\n",
      "Iteration 5697, loss = 0.05590473\n",
      "Iteration 5698, loss = 0.05588481\n",
      "Iteration 5699, loss = 0.05586490\n",
      "Iteration 5700, loss = 0.05584500\n",
      "Iteration 5701, loss = 0.05582512\n",
      "Iteration 5702, loss = 0.05580525\n",
      "Iteration 5703, loss = 0.05578539\n",
      "Iteration 5704, loss = 0.05576554\n",
      "Iteration 5705, loss = 0.05574571\n",
      "Iteration 5706, loss = 0.05572588\n",
      "Iteration 5707, loss = 0.05570607\n",
      "Iteration 5708, loss = 0.05568628\n",
      "Iteration 5709, loss = 0.05566649\n",
      "Iteration 5710, loss = 0.05564672\n",
      "Iteration 5711, loss = 0.05562696\n",
      "Iteration 5712, loss = 0.05560721\n",
      "Iteration 5713, loss = 0.05558747\n",
      "Iteration 5714, loss = 0.05556775\n",
      "Iteration 5715, loss = 0.05554804\n",
      "Iteration 5716, loss = 0.05552834\n",
      "Iteration 5717, loss = 0.05550865\n",
      "Iteration 5718, loss = 0.05548897\n",
      "Iteration 5719, loss = 0.05546931\n",
      "Iteration 5720, loss = 0.05544966\n",
      "Iteration 5721, loss = 0.05543002\n",
      "Iteration 5722, loss = 0.05541040\n",
      "Iteration 5723, loss = 0.05539078\n",
      "Iteration 5724, loss = 0.05537118\n",
      "Iteration 5725, loss = 0.05535159\n",
      "Iteration 5726, loss = 0.05533201\n",
      "Iteration 5727, loss = 0.05531245\n",
      "Iteration 5728, loss = 0.05529289\n",
      "Iteration 5729, loss = 0.05527335\n",
      "Iteration 5730, loss = 0.05525382\n",
      "Iteration 5731, loss = 0.05523431\n",
      "Iteration 5732, loss = 0.05521480\n",
      "Iteration 5733, loss = 0.05519531\n",
      "Iteration 5734, loss = 0.05517583\n",
      "Iteration 5735, loss = 0.05515636\n",
      "Iteration 5736, loss = 0.05513690\n",
      "Iteration 5737, loss = 0.05511746\n",
      "Iteration 5738, loss = 0.05509803\n",
      "Iteration 5739, loss = 0.05507861\n",
      "Iteration 5740, loss = 0.05505920\n",
      "Iteration 5741, loss = 0.05503980\n",
      "Iteration 5742, loss = 0.05502042\n",
      "Iteration 5743, loss = 0.05500104\n",
      "Iteration 5744, loss = 0.05498168\n",
      "Iteration 5745, loss = 0.05496233\n",
      "Iteration 5746, loss = 0.05494300\n",
      "Iteration 5747, loss = 0.05492367\n",
      "Iteration 5748, loss = 0.05490436\n",
      "Iteration 5749, loss = 0.05488506\n",
      "Iteration 5750, loss = 0.05486577\n",
      "Iteration 5751, loss = 0.05484649\n",
      "Iteration 5752, loss = 0.05482723\n",
      "Iteration 5753, loss = 0.05480797\n",
      "Iteration 5754, loss = 0.05478873\n",
      "Iteration 5755, loss = 0.05476950\n",
      "Iteration 5756, loss = 0.05475028\n",
      "Iteration 5757, loss = 0.05473108\n",
      "Iteration 5758, loss = 0.05471188\n",
      "Iteration 5759, loss = 0.05469270\n",
      "Iteration 5760, loss = 0.05467353\n",
      "Iteration 5761, loss = 0.05465437\n",
      "Iteration 5762, loss = 0.05463522\n",
      "Iteration 5763, loss = 0.05461609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5764, loss = 0.05459696\n",
      "Iteration 5765, loss = 0.05457785\n",
      "Iteration 5766, loss = 0.05455875\n",
      "Iteration 5767, loss = 0.05453966\n",
      "Iteration 5768, loss = 0.05452059\n",
      "Iteration 5769, loss = 0.05450152\n",
      "Iteration 5770, loss = 0.05448247\n",
      "Iteration 5771, loss = 0.05446343\n",
      "Iteration 5772, loss = 0.05444440\n",
      "Iteration 5773, loss = 0.05442538\n",
      "Iteration 5774, loss = 0.05440637\n",
      "Iteration 5775, loss = 0.05438738\n",
      "Iteration 5776, loss = 0.05436840\n",
      "Iteration 5777, loss = 0.05434942\n",
      "Iteration 5778, loss = 0.05433046\n",
      "Iteration 5779, loss = 0.05431152\n",
      "Iteration 5780, loss = 0.05429258\n",
      "Iteration 5781, loss = 0.05427365\n",
      "Iteration 5782, loss = 0.05425474\n",
      "Iteration 5783, loss = 0.05423584\n",
      "Iteration 5784, loss = 0.05421695\n",
      "Iteration 5785, loss = 0.05419807\n",
      "Iteration 5786, loss = 0.05417920\n",
      "Iteration 5787, loss = 0.05416035\n",
      "Iteration 5788, loss = 0.05414150\n",
      "Iteration 5789, loss = 0.05412267\n",
      "Iteration 5790, loss = 0.05410385\n",
      "Iteration 5791, loss = 0.05408504\n",
      "Iteration 5792, loss = 0.05406624\n",
      "Iteration 5793, loss = 0.05404745\n",
      "Iteration 5794, loss = 0.05402868\n",
      "Iteration 5795, loss = 0.05400991\n",
      "Iteration 5796, loss = 0.05399116\n",
      "Iteration 5797, loss = 0.05397242\n",
      "Iteration 5798, loss = 0.05395369\n",
      "Iteration 5799, loss = 0.05393497\n",
      "Iteration 5800, loss = 0.05391627\n",
      "Iteration 5801, loss = 0.05389757\n",
      "Iteration 5802, loss = 0.05387889\n",
      "Iteration 5803, loss = 0.05386022\n",
      "Iteration 5804, loss = 0.05384155\n",
      "Iteration 5805, loss = 0.05382290\n",
      "Iteration 5806, loss = 0.05380427\n",
      "Iteration 5807, loss = 0.05378564\n",
      "Iteration 5808, loss = 0.05376702\n",
      "Iteration 5809, loss = 0.05374842\n",
      "Iteration 5810, loss = 0.05372983\n",
      "Iteration 5811, loss = 0.05371124\n",
      "Iteration 5812, loss = 0.05369267\n",
      "Iteration 5813, loss = 0.05367411\n",
      "Iteration 5814, loss = 0.05365557\n",
      "Iteration 5815, loss = 0.05363703\n",
      "Iteration 5816, loss = 0.05361850\n",
      "Iteration 5817, loss = 0.05359999\n",
      "Iteration 5818, loss = 0.05358149\n",
      "Iteration 5819, loss = 0.05356300\n",
      "Iteration 5820, loss = 0.05354451\n",
      "Iteration 5821, loss = 0.05352605\n",
      "Iteration 5822, loss = 0.05350759\n",
      "Iteration 5823, loss = 0.05348914\n",
      "Iteration 5824, loss = 0.05347071\n",
      "Iteration 5825, loss = 0.05345228\n",
      "Iteration 5826, loss = 0.05343387\n",
      "Iteration 5827, loss = 0.05341547\n",
      "Iteration 5828, loss = 0.05339707\n",
      "Iteration 5829, loss = 0.05337869\n",
      "Iteration 5830, loss = 0.05336033\n",
      "Iteration 5831, loss = 0.05334197\n",
      "Iteration 5832, loss = 0.05332362\n",
      "Iteration 5833, loss = 0.05330529\n",
      "Iteration 5834, loss = 0.05328696\n",
      "Iteration 5835, loss = 0.05326865\n",
      "Iteration 5836, loss = 0.05325035\n",
      "Iteration 5837, loss = 0.05323206\n",
      "Iteration 5838, loss = 0.05321378\n",
      "Iteration 5839, loss = 0.05319551\n",
      "Iteration 5840, loss = 0.05317725\n",
      "Iteration 5841, loss = 0.05315900\n",
      "Iteration 5842, loss = 0.05314077\n",
      "Iteration 5843, loss = 0.05312254\n",
      "Iteration 5844, loss = 0.05310433\n",
      "Iteration 5845, loss = 0.05308612\n",
      "Iteration 5846, loss = 0.05306793\n",
      "Iteration 5847, loss = 0.05304975\n",
      "Iteration 5848, loss = 0.05303158\n",
      "Iteration 5849, loss = 0.05301342\n",
      "Iteration 5850, loss = 0.05299527\n",
      "Iteration 5851, loss = 0.05297714\n",
      "Iteration 5852, loss = 0.05295901\n",
      "Iteration 5853, loss = 0.05294089\n",
      "Iteration 5854, loss = 0.05292279\n",
      "Iteration 5855, loss = 0.05290470\n",
      "Iteration 5856, loss = 0.05288661\n",
      "Iteration 5857, loss = 0.05286854\n",
      "Iteration 5858, loss = 0.05285048\n",
      "Iteration 5859, loss = 0.05283243\n",
      "Iteration 5860, loss = 0.05281439\n",
      "Iteration 5861, loss = 0.05279636\n",
      "Iteration 5862, loss = 0.05277835\n",
      "Iteration 5863, loss = 0.05276034\n",
      "Iteration 5864, loss = 0.05274234\n",
      "Iteration 5865, loss = 0.05272436\n",
      "Iteration 5866, loss = 0.05270638\n",
      "Iteration 5867, loss = 0.05268842\n",
      "Iteration 5868, loss = 0.05267047\n",
      "Iteration 5869, loss = 0.05265253\n",
      "Iteration 5870, loss = 0.05263460\n",
      "Iteration 5871, loss = 0.05261668\n",
      "Iteration 5872, loss = 0.05259877\n",
      "Iteration 5873, loss = 0.05258087\n",
      "Iteration 5874, loss = 0.05256298\n",
      "Iteration 5875, loss = 0.05254510\n",
      "Iteration 5876, loss = 0.05252723\n",
      "Iteration 5877, loss = 0.05250938\n",
      "Iteration 5878, loss = 0.05249153\n",
      "Iteration 5879, loss = 0.05247370\n",
      "Iteration 5880, loss = 0.05245588\n",
      "Iteration 5881, loss = 0.05243806\n",
      "Iteration 5882, loss = 0.05242026\n",
      "Iteration 5883, loss = 0.05240247\n",
      "Iteration 5884, loss = 0.05238469\n",
      "Iteration 5885, loss = 0.05236692\n",
      "Iteration 5886, loss = 0.05234916\n",
      "Iteration 5887, loss = 0.05233141\n",
      "Iteration 5888, loss = 0.05231367\n",
      "Iteration 5889, loss = 0.05229594\n",
      "Iteration 5890, loss = 0.05227822\n",
      "Iteration 5891, loss = 0.05226051\n",
      "Iteration 5892, loss = 0.05224282\n",
      "Iteration 5893, loss = 0.05222513\n",
      "Iteration 5894, loss = 0.05220746\n",
      "Iteration 5895, loss = 0.05218979\n",
      "Iteration 5896, loss = 0.05217214\n",
      "Iteration 5897, loss = 0.05215449\n",
      "Iteration 5898, loss = 0.05213686\n",
      "Iteration 5899, loss = 0.05211924\n",
      "Iteration 5900, loss = 0.05210163\n",
      "Iteration 5901, loss = 0.05208402\n",
      "Iteration 5902, loss = 0.05206643\n",
      "Iteration 5903, loss = 0.05204885\n",
      "Iteration 5904, loss = 0.05203128\n",
      "Iteration 5905, loss = 0.05201372\n",
      "Iteration 5906, loss = 0.05199617\n",
      "Iteration 5907, loss = 0.05197863\n",
      "Iteration 5908, loss = 0.05196111\n",
      "Iteration 5909, loss = 0.05194359\n",
      "Iteration 5910, loss = 0.05192608\n",
      "Iteration 5911, loss = 0.05190858\n",
      "Iteration 5912, loss = 0.05189110\n",
      "Iteration 5913, loss = 0.05187362\n",
      "Iteration 5914, loss = 0.05185615\n",
      "Iteration 5915, loss = 0.05183870\n",
      "Iteration 5916, loss = 0.05182125\n",
      "Iteration 5917, loss = 0.05180382\n",
      "Iteration 5918, loss = 0.05178639\n",
      "Iteration 5919, loss = 0.05176898\n",
      "Iteration 5920, loss = 0.05175157\n",
      "Iteration 5921, loss = 0.05173418\n",
      "Iteration 5922, loss = 0.05171680\n",
      "Iteration 5923, loss = 0.05169942\n",
      "Iteration 5924, loss = 0.05168206\n",
      "Iteration 5925, loss = 0.05166471\n",
      "Iteration 5926, loss = 0.05164737\n",
      "Iteration 5927, loss = 0.05163004\n",
      "Iteration 5928, loss = 0.05161271\n",
      "Iteration 5929, loss = 0.05159540\n",
      "Iteration 5930, loss = 0.05157810\n",
      "Iteration 5931, loss = 0.05156081\n",
      "Iteration 5932, loss = 0.05154353\n",
      "Iteration 5933, loss = 0.05152626\n",
      "Iteration 5934, loss = 0.05150900\n",
      "Iteration 5935, loss = 0.05149175\n",
      "Iteration 5936, loss = 0.05147451\n",
      "Iteration 5937, loss = 0.05145728\n",
      "Iteration 5938, loss = 0.05144006\n",
      "Iteration 5939, loss = 0.05142285\n",
      "Iteration 5940, loss = 0.05140565\n",
      "Iteration 5941, loss = 0.05138846\n",
      "Iteration 5942, loss = 0.05137129\n",
      "Iteration 5943, loss = 0.05135412\n",
      "Iteration 5944, loss = 0.05133696\n",
      "Iteration 5945, loss = 0.05131981\n",
      "Iteration 5946, loss = 0.05130267\n",
      "Iteration 5947, loss = 0.05128554\n",
      "Iteration 5948, loss = 0.05126843\n",
      "Iteration 5949, loss = 0.05125132\n",
      "Iteration 5950, loss = 0.05123422\n",
      "Iteration 5951, loss = 0.05121713\n",
      "Iteration 5952, loss = 0.05120006\n",
      "Iteration 5953, loss = 0.05118299\n",
      "Iteration 5954, loss = 0.05116593\n",
      "Iteration 5955, loss = 0.05114888\n",
      "Iteration 5956, loss = 0.05113185\n",
      "Iteration 5957, loss = 0.05111482\n",
      "Iteration 5958, loss = 0.05109780\n",
      "Iteration 5959, loss = 0.05108079\n",
      "Iteration 5960, loss = 0.05106380\n",
      "Iteration 5961, loss = 0.05104681\n",
      "Iteration 5962, loss = 0.05102983\n",
      "Iteration 5963, loss = 0.05101286\n",
      "Iteration 5964, loss = 0.05099591\n",
      "Iteration 5965, loss = 0.05097896\n",
      "Iteration 5966, loss = 0.05096202\n",
      "Iteration 5967, loss = 0.05094509\n",
      "Iteration 5968, loss = 0.05092818\n",
      "Iteration 5969, loss = 0.05091127\n",
      "Iteration 5970, loss = 0.05089437\n",
      "Iteration 5971, loss = 0.05087748\n",
      "Iteration 5972, loss = 0.05086061\n",
      "Iteration 5973, loss = 0.05084374\n",
      "Iteration 5974, loss = 0.05082688\n",
      "Iteration 5975, loss = 0.05081003\n",
      "Iteration 5976, loss = 0.05079319\n",
      "Iteration 5977, loss = 0.05077636\n",
      "Iteration 5978, loss = 0.05075955\n",
      "Iteration 5979, loss = 0.05074274\n",
      "Iteration 5980, loss = 0.05072594\n",
      "Iteration 5981, loss = 0.05070915\n",
      "Iteration 5982, loss = 0.05069237\n",
      "Iteration 5983, loss = 0.05067560\n",
      "Iteration 5984, loss = 0.05065884\n",
      "Iteration 5985, loss = 0.05064209\n",
      "Iteration 5986, loss = 0.05062535\n",
      "Iteration 5987, loss = 0.05060862\n",
      "Iteration 5988, loss = 0.05059190\n",
      "Iteration 5989, loss = 0.05057519\n",
      "Iteration 5990, loss = 0.05055849\n",
      "Iteration 5991, loss = 0.05054180\n",
      "Iteration 5992, loss = 0.05052511\n",
      "Iteration 5993, loss = 0.05050844\n",
      "Iteration 5994, loss = 0.05049178\n",
      "Iteration 5995, loss = 0.05047513\n",
      "Iteration 5996, loss = 0.05045849\n",
      "Iteration 5997, loss = 0.05044185\n",
      "Iteration 5998, loss = 0.05042523\n",
      "Iteration 5999, loss = 0.05040862\n",
      "Iteration 6000, loss = 0.05039201\n",
      "Iteration 6001, loss = 0.05037542\n",
      "Iteration 6002, loss = 0.05035883\n",
      "Iteration 6003, loss = 0.05034226\n",
      "Iteration 6004, loss = 0.05032569\n",
      "Iteration 6005, loss = 0.05030914\n",
      "Iteration 6006, loss = 0.05029259\n",
      "Iteration 6007, loss = 0.05027606\n",
      "Iteration 6008, loss = 0.05025953\n",
      "Iteration 6009, loss = 0.05024301\n",
      "Iteration 6010, loss = 0.05022651\n",
      "Iteration 6011, loss = 0.05021001\n",
      "Iteration 6012, loss = 0.05019352\n",
      "Iteration 6013, loss = 0.05017704\n",
      "Iteration 6014, loss = 0.05016057\n",
      "Iteration 6015, loss = 0.05014411\n",
      "Iteration 6016, loss = 0.05012766\n",
      "Iteration 6017, loss = 0.05011122\n",
      "Iteration 6018, loss = 0.05009479\n",
      "Iteration 6019, loss = 0.05007837\n",
      "Iteration 6020, loss = 0.05006196\n",
      "Iteration 6021, loss = 0.05004556\n",
      "Iteration 6022, loss = 0.05002916\n",
      "Iteration 6023, loss = 0.05001278\n",
      "Iteration 6024, loss = 0.04999641\n",
      "Iteration 6025, loss = 0.04998004\n",
      "Iteration 6026, loss = 0.04996369\n",
      "Iteration 6027, loss = 0.04994734\n",
      "Iteration 6028, loss = 0.04993101\n",
      "Iteration 6029, loss = 0.04991468\n",
      "Iteration 6030, loss = 0.04989837\n",
      "Iteration 6031, loss = 0.04988206\n",
      "Iteration 6032, loss = 0.04986576\n",
      "Iteration 6033, loss = 0.04984947\n",
      "Iteration 6034, loss = 0.04983319\n",
      "Iteration 6035, loss = 0.04981692\n",
      "Iteration 6036, loss = 0.04980066\n",
      "Iteration 6037, loss = 0.04978441\n",
      "Iteration 6038, loss = 0.04976817\n",
      "Iteration 6039, loss = 0.04975194\n",
      "Iteration 6040, loss = 0.04973572\n",
      "Iteration 6041, loss = 0.04971950\n",
      "Iteration 6042, loss = 0.04970330\n",
      "Iteration 6043, loss = 0.04968710\n",
      "Iteration 6044, loss = 0.04967092\n",
      "Iteration 6045, loss = 0.04965474\n",
      "Iteration 6046, loss = 0.04963858\n",
      "Iteration 6047, loss = 0.04962242\n",
      "Iteration 6048, loss = 0.04960627\n",
      "Iteration 6049, loss = 0.04959013\n",
      "Iteration 6050, loss = 0.04957400\n",
      "Iteration 6051, loss = 0.04955788\n",
      "Iteration 6052, loss = 0.04954177\n",
      "Iteration 6053, loss = 0.04952567\n",
      "Iteration 6054, loss = 0.04950958\n",
      "Iteration 6055, loss = 0.04949350\n",
      "Iteration 6056, loss = 0.04947742\n",
      "Iteration 6057, loss = 0.04946136\n",
      "Iteration 6058, loss = 0.04944530\n",
      "Iteration 6059, loss = 0.04942926\n",
      "Iteration 6060, loss = 0.04941322\n",
      "Iteration 6061, loss = 0.04939720\n",
      "Iteration 6062, loss = 0.04938118\n",
      "Iteration 6063, loss = 0.04936517\n",
      "Iteration 6064, loss = 0.04934917\n",
      "Iteration 6065, loss = 0.04933318\n",
      "Iteration 6066, loss = 0.04931720\n",
      "Iteration 6067, loss = 0.04930122\n",
      "Iteration 6068, loss = 0.04928526\n",
      "Iteration 6069, loss = 0.04926931\n",
      "Iteration 6070, loss = 0.04925336\n",
      "Iteration 6071, loss = 0.04923743\n",
      "Iteration 6072, loss = 0.04922150\n",
      "Iteration 6073, loss = 0.04920558\n",
      "Iteration 6074, loss = 0.04918968\n",
      "Iteration 6075, loss = 0.04917378\n",
      "Iteration 6076, loss = 0.04915789\n",
      "Iteration 6077, loss = 0.04914201\n",
      "Iteration 6078, loss = 0.04912614\n",
      "Iteration 6079, loss = 0.04911027\n",
      "Iteration 6080, loss = 0.04909442\n",
      "Iteration 6081, loss = 0.04907858\n",
      "Iteration 6082, loss = 0.04906274\n",
      "Iteration 6083, loss = 0.04904691\n",
      "Iteration 6084, loss = 0.04903110\n",
      "Iteration 6085, loss = 0.04901529\n",
      "Iteration 6086, loss = 0.04899949\n",
      "Iteration 6087, loss = 0.04898370\n",
      "Iteration 6088, loss = 0.04896792\n",
      "Iteration 6089, loss = 0.04895215\n",
      "Iteration 6090, loss = 0.04893639\n",
      "Iteration 6091, loss = 0.04892063\n",
      "Iteration 6092, loss = 0.04890489\n",
      "Iteration 6093, loss = 0.04888915\n",
      "Iteration 6094, loss = 0.04887342\n",
      "Iteration 6095, loss = 0.04885771\n",
      "Iteration 6096, loss = 0.04884200\n",
      "Iteration 6097, loss = 0.04882630\n",
      "Iteration 6098, loss = 0.04881061\n",
      "Iteration 6099, loss = 0.04879492\n",
      "Iteration 6100, loss = 0.04877925\n",
      "Iteration 6101, loss = 0.04876359\n",
      "Iteration 6102, loss = 0.04874793\n",
      "Iteration 6103, loss = 0.04873229\n",
      "Iteration 6104, loss = 0.04871665\n",
      "Iteration 6105, loss = 0.04870102\n",
      "Iteration 6106, loss = 0.04868540\n",
      "Iteration 6107, loss = 0.04866979\n",
      "Iteration 6108, loss = 0.04865419\n",
      "Iteration 6109, loss = 0.04863859\n",
      "Iteration 6110, loss = 0.04862301\n",
      "Iteration 6111, loss = 0.04860744\n",
      "Iteration 6112, loss = 0.04859187\n",
      "Iteration 6113, loss = 0.04857631\n",
      "Iteration 6114, loss = 0.04856076\n",
      "Iteration 6115, loss = 0.04854522\n",
      "Iteration 6116, loss = 0.04852969\n",
      "Iteration 6117, loss = 0.04851417\n",
      "Iteration 6118, loss = 0.04849866\n",
      "Iteration 6119, loss = 0.04848315\n",
      "Iteration 6120, loss = 0.04846766\n",
      "Iteration 6121, loss = 0.04845217\n",
      "Iteration 6122, loss = 0.04843669\n",
      "Iteration 6123, loss = 0.04842122\n",
      "Iteration 6124, loss = 0.04840576\n",
      "Iteration 6125, loss = 0.04839031\n",
      "Iteration 6126, loss = 0.04837487\n",
      "Iteration 6127, loss = 0.04835944\n",
      "Iteration 6128, loss = 0.04834401\n",
      "Iteration 6129, loss = 0.04832859\n",
      "Iteration 6130, loss = 0.04831319\n",
      "Iteration 6131, loss = 0.04829779\n",
      "Iteration 6132, loss = 0.04828240\n",
      "Iteration 6133, loss = 0.04826701\n",
      "Iteration 6134, loss = 0.04825164\n",
      "Iteration 6135, loss = 0.04823628\n",
      "Iteration 6136, loss = 0.04822092\n",
      "Iteration 6137, loss = 0.04820557\n",
      "Iteration 6138, loss = 0.04819024\n",
      "Iteration 6139, loss = 0.04817491\n",
      "Iteration 6140, loss = 0.04815959\n",
      "Iteration 6141, loss = 0.04814427\n",
      "Iteration 6142, loss = 0.04812897\n",
      "Iteration 6143, loss = 0.04811368\n",
      "Iteration 6144, loss = 0.04809839\n",
      "Iteration 6145, loss = 0.04808311\n",
      "Iteration 6146, loss = 0.04806784\n",
      "Iteration 6147, loss = 0.04805258\n",
      "Iteration 6148, loss = 0.04803733\n",
      "Iteration 6149, loss = 0.04802209\n",
      "Iteration 6150, loss = 0.04800685\n",
      "Iteration 6151, loss = 0.04799163\n",
      "Iteration 6152, loss = 0.04797641\n",
      "Iteration 6153, loss = 0.04796120\n",
      "Iteration 6154, loss = 0.04794600\n",
      "Iteration 6155, loss = 0.04793081\n",
      "Iteration 6156, loss = 0.04791563\n",
      "Iteration 6157, loss = 0.04790045\n",
      "Iteration 6158, loss = 0.04788529\n",
      "Iteration 6159, loss = 0.04787013\n",
      "Iteration 6160, loss = 0.04785498\n",
      "Iteration 6161, loss = 0.04783984\n",
      "Iteration 6162, loss = 0.04782471\n",
      "Iteration 6163, loss = 0.04780958\n",
      "Iteration 6164, loss = 0.04779447\n",
      "Iteration 6165, loss = 0.04777936\n",
      "Iteration 6166, loss = 0.04776427\n",
      "Iteration 6167, loss = 0.04774918\n",
      "Iteration 6168, loss = 0.04773410\n",
      "Iteration 6169, loss = 0.04771902\n",
      "Iteration 6170, loss = 0.04770396\n",
      "Iteration 6171, loss = 0.04768891\n",
      "Iteration 6172, loss = 0.04767386\n",
      "Iteration 6173, loss = 0.04765882\n",
      "Iteration 6174, loss = 0.04764379\n",
      "Iteration 6175, loss = 0.04762877\n",
      "Iteration 6176, loss = 0.04761376\n",
      "Iteration 6177, loss = 0.04759875\n",
      "Iteration 6178, loss = 0.04758375\n",
      "Iteration 6179, loss = 0.04756877\n",
      "Iteration 6180, loss = 0.04755379\n",
      "Iteration 6181, loss = 0.04753882\n",
      "Iteration 6182, loss = 0.04752385\n",
      "Iteration 6183, loss = 0.04750890\n",
      "Iteration 6184, loss = 0.04749395\n",
      "Iteration 6185, loss = 0.04747902\n",
      "Iteration 6186, loss = 0.04746409\n",
      "Iteration 6187, loss = 0.04744917\n",
      "Iteration 6188, loss = 0.04743425\n",
      "Iteration 6189, loss = 0.04741935\n",
      "Iteration 6190, loss = 0.04740445\n",
      "Iteration 6191, loss = 0.04738957\n",
      "Iteration 6192, loss = 0.04737469\n",
      "Iteration 6193, loss = 0.04735982\n",
      "Iteration 6194, loss = 0.04734495\n",
      "Iteration 6195, loss = 0.04733010\n",
      "Iteration 6196, loss = 0.04731525\n",
      "Iteration 6197, loss = 0.04730042\n",
      "Iteration 6198, loss = 0.04728559\n",
      "Iteration 6199, loss = 0.04727077\n",
      "Iteration 6200, loss = 0.04725595\n",
      "Iteration 6201, loss = 0.04724115\n",
      "Iteration 6202, loss = 0.04722635\n",
      "Iteration 6203, loss = 0.04721157\n",
      "Iteration 6204, loss = 0.04719679\n",
      "Iteration 6205, loss = 0.04718202\n",
      "Iteration 6206, loss = 0.04716725\n",
      "Iteration 6207, loss = 0.04715250\n",
      "Iteration 6208, loss = 0.04713775\n",
      "Iteration 6209, loss = 0.04712301\n",
      "Iteration 6210, loss = 0.04710828\n",
      "Iteration 6211, loss = 0.04709356\n",
      "Iteration 6212, loss = 0.04707885\n",
      "Iteration 6213, loss = 0.04706414\n",
      "Iteration 6214, loss = 0.04704944\n",
      "Iteration 6215, loss = 0.04703475\n",
      "Iteration 6216, loss = 0.04702007\n",
      "Iteration 6217, loss = 0.04700540\n",
      "Iteration 6218, loss = 0.04699074\n",
      "Iteration 6219, loss = 0.04697608\n",
      "Iteration 6220, loss = 0.04696143\n",
      "Iteration 6221, loss = 0.04694679\n",
      "Iteration 6222, loss = 0.04693216\n",
      "Iteration 6223, loss = 0.04691754\n",
      "Iteration 6224, loss = 0.04690292\n",
      "Iteration 6225, loss = 0.04688831\n",
      "Iteration 6226, loss = 0.04687371\n",
      "Iteration 6227, loss = 0.04685912\n",
      "Iteration 6228, loss = 0.04684454\n",
      "Iteration 6229, loss = 0.04682996\n",
      "Iteration 6230, loss = 0.04681539\n",
      "Iteration 6231, loss = 0.04680084\n",
      "Iteration 6232, loss = 0.04678628\n",
      "Iteration 6233, loss = 0.04677174\n",
      "Iteration 6234, loss = 0.04675721\n",
      "Iteration 6235, loss = 0.04674268\n",
      "Iteration 6236, loss = 0.04672816\n",
      "Iteration 6237, loss = 0.04671365\n",
      "Iteration 6238, loss = 0.04669915\n",
      "Iteration 6239, loss = 0.04668465\n",
      "Iteration 6240, loss = 0.04667017\n",
      "Iteration 6241, loss = 0.04665569\n",
      "Iteration 6242, loss = 0.04664122\n",
      "Iteration 6243, loss = 0.04662676\n",
      "Iteration 6244, loss = 0.04661230\n",
      "Iteration 6245, loss = 0.04659785\n",
      "Iteration 6246, loss = 0.04658342\n",
      "Iteration 6247, loss = 0.04656898\n",
      "Iteration 6248, loss = 0.04655456\n",
      "Iteration 6249, loss = 0.04654015\n",
      "Iteration 6250, loss = 0.04652574\n",
      "Iteration 6251, loss = 0.04651134\n",
      "Iteration 6252, loss = 0.04649695\n",
      "Iteration 6253, loss = 0.04648257\n",
      "Iteration 6254, loss = 0.04646819\n",
      "Iteration 6255, loss = 0.04645383\n",
      "Iteration 6256, loss = 0.04643947\n",
      "Iteration 6257, loss = 0.04642512\n",
      "Iteration 6258, loss = 0.04641077\n",
      "Iteration 6259, loss = 0.04639644\n",
      "Iteration 6260, loss = 0.04638211\n",
      "Iteration 6261, loss = 0.04636779\n",
      "Iteration 6262, loss = 0.04635348\n",
      "Iteration 6263, loss = 0.04633918\n",
      "Iteration 6264, loss = 0.04632488\n",
      "Iteration 6265, loss = 0.04631059\n",
      "Iteration 6266, loss = 0.04629631\n",
      "Iteration 6267, loss = 0.04628204\n",
      "Iteration 6268, loss = 0.04626778\n",
      "Iteration 6269, loss = 0.04625352\n",
      "Iteration 6270, loss = 0.04623927\n",
      "Iteration 6271, loss = 0.04622503\n",
      "Iteration 6272, loss = 0.04621080\n",
      "Iteration 6273, loss = 0.04619657\n",
      "Iteration 6274, loss = 0.04618236\n",
      "Iteration 6275, loss = 0.04616815\n",
      "Iteration 6276, loss = 0.04615395\n",
      "Iteration 6277, loss = 0.04613975\n",
      "Iteration 6278, loss = 0.04612557\n",
      "Iteration 6279, loss = 0.04611139\n",
      "Iteration 6280, loss = 0.04609722\n",
      "Iteration 6281, loss = 0.04608305\n",
      "Iteration 6282, loss = 0.04606890\n",
      "Iteration 6283, loss = 0.04605475\n",
      "Iteration 6284, loss = 0.04604061\n",
      "Iteration 6285, loss = 0.04602648\n",
      "Iteration 6286, loss = 0.04601236\n",
      "Iteration 6287, loss = 0.04599824\n",
      "Iteration 6288, loss = 0.04598413\n",
      "Iteration 6289, loss = 0.04597003\n",
      "Iteration 6290, loss = 0.04595594\n",
      "Iteration 6291, loss = 0.04594186\n",
      "Iteration 6292, loss = 0.04592778\n",
      "Iteration 6293, loss = 0.04591371\n",
      "Iteration 6294, loss = 0.04589965\n",
      "Iteration 6295, loss = 0.04588559\n",
      "Iteration 6296, loss = 0.04587155\n",
      "Iteration 6297, loss = 0.04585751\n",
      "Iteration 6298, loss = 0.04584348\n",
      "Iteration 6299, loss = 0.04582945\n",
      "Iteration 6300, loss = 0.04581544\n",
      "Iteration 6301, loss = 0.04580143\n",
      "Iteration 6302, loss = 0.04578743\n",
      "Iteration 6303, loss = 0.04577343\n",
      "Iteration 6304, loss = 0.04575945\n",
      "Iteration 6305, loss = 0.04574547\n",
      "Iteration 6306, loss = 0.04573150\n",
      "Iteration 6307, loss = 0.04571754\n",
      "Iteration 6308, loss = 0.04570358\n",
      "Iteration 6309, loss = 0.04568964\n",
      "Iteration 6310, loss = 0.04567570\n",
      "Iteration 6311, loss = 0.04566177\n",
      "Iteration 6312, loss = 0.04564784\n",
      "Iteration 6313, loss = 0.04563393\n",
      "Iteration 6314, loss = 0.04562002\n",
      "Iteration 6315, loss = 0.04560611\n",
      "Iteration 6316, loss = 0.04559222\n",
      "Iteration 6317, loss = 0.04557833\n",
      "Iteration 6318, loss = 0.04556446\n",
      "Iteration 6319, loss = 0.04555058\n",
      "Iteration 6320, loss = 0.04553672\n",
      "Iteration 6321, loss = 0.04552287\n",
      "Iteration 6322, loss = 0.04550902\n",
      "Iteration 6323, loss = 0.04549518\n",
      "Iteration 6324, loss = 0.04548134\n",
      "Iteration 6325, loss = 0.04546752\n",
      "Iteration 6326, loss = 0.04545370\n",
      "Iteration 6327, loss = 0.04543989\n",
      "Iteration 6328, loss = 0.04542608\n",
      "Iteration 6329, loss = 0.04541229\n",
      "Iteration 6330, loss = 0.04539850\n",
      "Iteration 6331, loss = 0.04538472\n",
      "Iteration 6332, loss = 0.04537095\n",
      "Iteration 6333, loss = 0.04535718\n",
      "Iteration 6334, loss = 0.04534342\n",
      "Iteration 6335, loss = 0.04532967\n",
      "Iteration 6336, loss = 0.04531593\n",
      "Iteration 6337, loss = 0.04530219\n",
      "Iteration 6338, loss = 0.04528846\n",
      "Iteration 6339, loss = 0.04527474\n",
      "Iteration 6340, loss = 0.04526103\n",
      "Iteration 6341, loss = 0.04524732\n",
      "Iteration 6342, loss = 0.04523362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6343, loss = 0.04521993\n",
      "Iteration 6344, loss = 0.04520625\n",
      "Iteration 6345, loss = 0.04519257\n",
      "Iteration 6346, loss = 0.04517890\n",
      "Iteration 6347, loss = 0.04516524\n",
      "Iteration 6348, loss = 0.04515159\n",
      "Iteration 6349, loss = 0.04513794\n",
      "Iteration 6350, loss = 0.04512430\n",
      "Iteration 6351, loss = 0.04511067\n",
      "Iteration 6352, loss = 0.04509704\n",
      "Iteration 6353, loss = 0.04508343\n",
      "Iteration 6354, loss = 0.04506982\n",
      "Iteration 6355, loss = 0.04505621\n",
      "Iteration 6356, loss = 0.04504262\n",
      "Iteration 6357, loss = 0.04502903\n",
      "Iteration 6358, loss = 0.04501545\n",
      "Iteration 6359, loss = 0.04500188\n",
      "Iteration 6360, loss = 0.04498831\n",
      "Iteration 6361, loss = 0.04497475\n",
      "Iteration 6362, loss = 0.04496120\n",
      "Iteration 6363, loss = 0.04494766\n",
      "Iteration 6364, loss = 0.04493412\n",
      "Iteration 6365, loss = 0.04492059\n",
      "Iteration 6366, loss = 0.04490707\n",
      "Iteration 6367, loss = 0.04489355\n",
      "Iteration 6368, loss = 0.04488005\n",
      "Iteration 6369, loss = 0.04486655\n",
      "Iteration 6370, loss = 0.04485305\n",
      "Iteration 6371, loss = 0.04483957\n",
      "Iteration 6372, loss = 0.04482609\n",
      "Iteration 6373, loss = 0.04481262\n",
      "Iteration 6374, loss = 0.04479915\n",
      "Iteration 6375, loss = 0.04478570\n",
      "Iteration 6376, loss = 0.04477225\n",
      "Iteration 6377, loss = 0.04475881\n",
      "Iteration 6378, loss = 0.04474537\n",
      "Iteration 6379, loss = 0.04473194\n",
      "Iteration 6380, loss = 0.04471852\n",
      "Iteration 6381, loss = 0.04470511\n",
      "Iteration 6382, loss = 0.04469170\n",
      "Iteration 6383, loss = 0.04467831\n",
      "Iteration 6384, loss = 0.04466491\n",
      "Iteration 6385, loss = 0.04465153\n",
      "Iteration 6386, loss = 0.04463815\n",
      "Iteration 6387, loss = 0.04462478\n",
      "Iteration 6388, loss = 0.04461142\n",
      "Iteration 6389, loss = 0.04459807\n",
      "Iteration 6390, loss = 0.04458472\n",
      "Iteration 6391, loss = 0.04457138\n",
      "Iteration 6392, loss = 0.04455804\n",
      "Iteration 6393, loss = 0.04454471\n",
      "Iteration 6394, loss = 0.04453140\n",
      "Iteration 6395, loss = 0.04451808\n",
      "Iteration 6396, loss = 0.04450478\n",
      "Iteration 6397, loss = 0.04449148\n",
      "Iteration 6398, loss = 0.04447819\n",
      "Iteration 6399, loss = 0.04446490\n",
      "Iteration 6400, loss = 0.04445163\n",
      "Iteration 6401, loss = 0.04443836\n",
      "Iteration 6402, loss = 0.04442509\n",
      "Iteration 6403, loss = 0.04441184\n",
      "Iteration 6404, loss = 0.04439859\n",
      "Iteration 6405, loss = 0.04438535\n",
      "Iteration 6406, loss = 0.04437211\n",
      "Iteration 6407, loss = 0.04435889\n",
      "Iteration 6408, loss = 0.04434567\n",
      "Iteration 6409, loss = 0.04433245\n",
      "Iteration 6410, loss = 0.04431925\n",
      "Iteration 6411, loss = 0.04430605\n",
      "Iteration 6412, loss = 0.04429286\n",
      "Iteration 6413, loss = 0.04427967\n",
      "Iteration 6414, loss = 0.04426649\n",
      "Iteration 6415, loss = 0.04425332\n",
      "Iteration 6416, loss = 0.04424016\n",
      "Iteration 6417, loss = 0.04422700\n",
      "Iteration 6418, loss = 0.04421385\n",
      "Iteration 6419, loss = 0.04420071\n",
      "Iteration 6420, loss = 0.04418758\n",
      "Iteration 6421, loss = 0.04417445\n",
      "Iteration 6422, loss = 0.04416133\n",
      "Iteration 6423, loss = 0.04414821\n",
      "Iteration 6424, loss = 0.04413510\n",
      "Iteration 6425, loss = 0.04412200\n",
      "Iteration 6426, loss = 0.04410891\n",
      "Iteration 6427, loss = 0.04409582\n",
      "Iteration 6428, loss = 0.04408274\n",
      "Iteration 6429, loss = 0.04406967\n",
      "Iteration 6430, loss = 0.04405660\n",
      "Iteration 6431, loss = 0.04404355\n",
      "Iteration 6432, loss = 0.04403049\n",
      "Iteration 6433, loss = 0.04401745\n",
      "Iteration 6434, loss = 0.04400441\n",
      "Iteration 6435, loss = 0.04399138\n",
      "Iteration 6436, loss = 0.04397836\n",
      "Iteration 6437, loss = 0.04396534\n",
      "Iteration 6438, loss = 0.04395233\n",
      "Iteration 6439, loss = 0.04393933\n",
      "Iteration 6440, loss = 0.04392633\n",
      "Iteration 6441, loss = 0.04391334\n",
      "Iteration 6442, loss = 0.04390036\n",
      "Iteration 6443, loss = 0.04388738\n",
      "Iteration 6444, loss = 0.04387441\n",
      "Iteration 6445, loss = 0.04386145\n",
      "Iteration 6446, loss = 0.04384850\n",
      "Iteration 6447, loss = 0.04383555\n",
      "Iteration 6448, loss = 0.04382261\n",
      "Iteration 6449, loss = 0.04380967\n",
      "Iteration 6450, loss = 0.04379675\n",
      "Iteration 6451, loss = 0.04378382\n",
      "Iteration 6452, loss = 0.04377091\n",
      "Iteration 6453, loss = 0.04375800\n",
      "Iteration 6454, loss = 0.04374510\n",
      "Iteration 6455, loss = 0.04373221\n",
      "Iteration 6456, loss = 0.04371932\n",
      "Iteration 6457, loss = 0.04370644\n",
      "Iteration 6458, loss = 0.04369357\n",
      "Iteration 6459, loss = 0.04368071\n",
      "Iteration 6460, loss = 0.04366785\n",
      "Iteration 6461, loss = 0.04365499\n",
      "Iteration 6462, loss = 0.04364215\n",
      "Iteration 6463, loss = 0.04362931\n",
      "Iteration 6464, loss = 0.04361648\n",
      "Iteration 6465, loss = 0.04360365\n",
      "Iteration 6466, loss = 0.04359083\n",
      "Iteration 6467, loss = 0.04357802\n",
      "Iteration 6468, loss = 0.04356522\n",
      "Iteration 6469, loss = 0.04355242\n",
      "Iteration 6470, loss = 0.04353963\n",
      "Iteration 6471, loss = 0.04352684\n",
      "Iteration 6472, loss = 0.04351407\n",
      "Iteration 6473, loss = 0.04350130\n",
      "Iteration 6474, loss = 0.04348853\n",
      "Iteration 6475, loss = 0.04347577\n",
      "Iteration 6476, loss = 0.04346302\n",
      "Iteration 6477, loss = 0.04345028\n",
      "Iteration 6478, loss = 0.04343754\n",
      "Iteration 6479, loss = 0.04342481\n",
      "Iteration 6480, loss = 0.04341209\n",
      "Iteration 6481, loss = 0.04339937\n",
      "Iteration 6482, loss = 0.04338666\n",
      "Iteration 6483, loss = 0.04337396\n",
      "Iteration 6484, loss = 0.04336126\n",
      "Iteration 6485, loss = 0.04334857\n",
      "Iteration 6486, loss = 0.04333589\n",
      "Iteration 6487, loss = 0.04332321\n",
      "Iteration 6488, loss = 0.04331054\n",
      "Iteration 6489, loss = 0.04329787\n",
      "Iteration 6490, loss = 0.04328522\n",
      "Iteration 6491, loss = 0.04327257\n",
      "Iteration 6492, loss = 0.04325992\n",
      "Iteration 6493, loss = 0.04324729\n",
      "Iteration 6494, loss = 0.04323466\n",
      "Iteration 6495, loss = 0.04322203\n",
      "Iteration 6496, loss = 0.04320942\n",
      "Iteration 6497, loss = 0.04319680\n",
      "Iteration 6498, loss = 0.04318420\n",
      "Iteration 6499, loss = 0.04317160\n",
      "Iteration 6500, loss = 0.04315901\n",
      "Iteration 6501, loss = 0.04314643\n",
      "Iteration 6502, loss = 0.04313385\n",
      "Iteration 6503, loss = 0.04312128\n",
      "Iteration 6504, loss = 0.04310872\n",
      "Iteration 6505, loss = 0.04309616\n",
      "Iteration 6506, loss = 0.04308361\n",
      "Iteration 6507, loss = 0.04307106\n",
      "Iteration 6508, loss = 0.04305853\n",
      "Iteration 6509, loss = 0.04304599\n",
      "Iteration 6510, loss = 0.04303347\n",
      "Iteration 6511, loss = 0.04302095\n",
      "Iteration 6512, loss = 0.04300844\n",
      "Iteration 6513, loss = 0.04299594\n",
      "Iteration 6514, loss = 0.04298344\n",
      "Iteration 6515, loss = 0.04297095\n",
      "Iteration 6516, loss = 0.04295846\n",
      "Iteration 6517, loss = 0.04294598\n",
      "Iteration 6518, loss = 0.04293351\n",
      "Iteration 6519, loss = 0.04292104\n",
      "Iteration 6520, loss = 0.04290858\n",
      "Iteration 6521, loss = 0.04289613\n",
      "Iteration 6522, loss = 0.04288368\n",
      "Iteration 6523, loss = 0.04287125\n",
      "Iteration 6524, loss = 0.04285881\n",
      "Iteration 6525, loss = 0.04284639\n",
      "Iteration 6526, loss = 0.04283397\n",
      "Iteration 6527, loss = 0.04282155\n",
      "Iteration 6528, loss = 0.04280914\n",
      "Iteration 6529, loss = 0.04279674\n",
      "Iteration 6530, loss = 0.04278435\n",
      "Iteration 6531, loss = 0.04277196\n",
      "Iteration 6532, loss = 0.04275958\n",
      "Iteration 6533, loss = 0.04274721\n",
      "Iteration 6534, loss = 0.04273484\n",
      "Iteration 6535, loss = 0.04272247\n",
      "Iteration 6536, loss = 0.04271012\n",
      "Iteration 6537, loss = 0.04269777\n",
      "Iteration 6538, loss = 0.04268543\n",
      "Iteration 6539, loss = 0.04267309\n",
      "Iteration 6540, loss = 0.04266076\n",
      "Iteration 6541, loss = 0.04264844\n",
      "Iteration 6542, loss = 0.04263612\n",
      "Iteration 6543, loss = 0.04262381\n",
      "Iteration 6544, loss = 0.04261151\n",
      "Iteration 6545, loss = 0.04259921\n",
      "Iteration 6546, loss = 0.04258692\n",
      "Iteration 6547, loss = 0.04257463\n",
      "Iteration 6548, loss = 0.04256235\n",
      "Iteration 6549, loss = 0.04255008\n",
      "Iteration 6550, loss = 0.04253782\n",
      "Iteration 6551, loss = 0.04252556\n",
      "Iteration 6552, loss = 0.04251330\n",
      "Iteration 6553, loss = 0.04250106\n",
      "Iteration 6554, loss = 0.04248882\n",
      "Iteration 6555, loss = 0.04247658\n",
      "Iteration 6556, loss = 0.04246436\n",
      "Iteration 6557, loss = 0.04245214\n",
      "Iteration 6558, loss = 0.04243992\n",
      "Iteration 6559, loss = 0.04242771\n",
      "Iteration 6560, loss = 0.04241551\n",
      "Iteration 6561, loss = 0.04240331\n",
      "Iteration 6562, loss = 0.04239113\n",
      "Iteration 6563, loss = 0.04237894\n",
      "Iteration 6564, loss = 0.04236677\n",
      "Iteration 6565, loss = 0.04235459\n",
      "Iteration 6566, loss = 0.04234243\n",
      "Iteration 6567, loss = 0.04233027\n",
      "Iteration 6568, loss = 0.04231812\n",
      "Iteration 6569, loss = 0.04230598\n",
      "Iteration 6570, loss = 0.04229384\n",
      "Iteration 6571, loss = 0.04228170\n",
      "Iteration 6572, loss = 0.04226958\n",
      "Iteration 6573, loss = 0.04225746\n",
      "Iteration 6574, loss = 0.04224534\n",
      "Iteration 6575, loss = 0.04223324\n",
      "Iteration 6576, loss = 0.04222114\n",
      "Iteration 6577, loss = 0.04220904\n",
      "Iteration 6578, loss = 0.04219695\n",
      "Iteration 6579, loss = 0.04218487\n",
      "Iteration 6580, loss = 0.04217279\n",
      "Iteration 6581, loss = 0.04216072\n",
      "Iteration 6582, loss = 0.04214866\n",
      "Iteration 6583, loss = 0.04213660\n",
      "Iteration 6584, loss = 0.04212455\n",
      "Iteration 6585, loss = 0.04211250\n",
      "Iteration 6586, loss = 0.04210047\n",
      "Iteration 6587, loss = 0.04208843\n",
      "Iteration 6588, loss = 0.04207641\n",
      "Iteration 6589, loss = 0.04206439\n",
      "Iteration 6590, loss = 0.04205237\n",
      "Iteration 6591, loss = 0.04204036\n",
      "Iteration 6592, loss = 0.04202836\n",
      "Iteration 6593, loss = 0.04201637\n",
      "Iteration 6594, loss = 0.04200438\n",
      "Iteration 6595, loss = 0.04199240\n",
      "Iteration 6596, loss = 0.04198042\n",
      "Iteration 6597, loss = 0.04196845\n",
      "Iteration 6598, loss = 0.04195648\n",
      "Iteration 6599, loss = 0.04194452\n",
      "Iteration 6600, loss = 0.04193257\n",
      "Iteration 6601, loss = 0.04192063\n",
      "Iteration 6602, loss = 0.04190869\n",
      "Iteration 6603, loss = 0.04189675\n",
      "Iteration 6604, loss = 0.04188483\n",
      "Iteration 6605, loss = 0.04187290\n",
      "Iteration 6606, loss = 0.04186099\n",
      "Iteration 6607, loss = 0.04184908\n",
      "Iteration 6608, loss = 0.04183718\n",
      "Iteration 6609, loss = 0.04182528\n",
      "Iteration 6610, loss = 0.04181339\n",
      "Iteration 6611, loss = 0.04180150\n",
      "Iteration 6612, loss = 0.04178962\n",
      "Iteration 6613, loss = 0.04177775\n",
      "Iteration 6614, loss = 0.04176589\n",
      "Iteration 6615, loss = 0.04175403\n",
      "Iteration 6616, loss = 0.04174217\n",
      "Iteration 6617, loss = 0.04173032\n",
      "Iteration 6618, loss = 0.04171848\n",
      "Iteration 6619, loss = 0.04170664\n",
      "Iteration 6620, loss = 0.04169482\n",
      "Iteration 6621, loss = 0.04168299\n",
      "Iteration 6622, loss = 0.04167117\n",
      "Iteration 6623, loss = 0.04165936\n",
      "Iteration 6624, loss = 0.04164756\n",
      "Iteration 6625, loss = 0.04163576\n",
      "Iteration 6626, loss = 0.04162396\n",
      "Iteration 6627, loss = 0.04161218\n",
      "Iteration 6628, loss = 0.04160039\n",
      "Iteration 6629, loss = 0.04158862\n",
      "Iteration 6630, loss = 0.04157685\n",
      "Iteration 6631, loss = 0.04156509\n",
      "Iteration 6632, loss = 0.04155333\n",
      "Iteration 6633, loss = 0.04154158\n",
      "Iteration 6634, loss = 0.04152983\n",
      "Iteration 6635, loss = 0.04151809\n",
      "Iteration 6636, loss = 0.04150636\n",
      "Iteration 6637, loss = 0.04149463\n",
      "Iteration 6638, loss = 0.04148291\n",
      "Iteration 6639, loss = 0.04147119\n",
      "Iteration 6640, loss = 0.04145949\n",
      "Iteration 6641, loss = 0.04144778\n",
      "Iteration 6642, loss = 0.04143608\n",
      "Iteration 6643, loss = 0.04142439\n",
      "Iteration 6644, loss = 0.04141271\n",
      "Iteration 6645, loss = 0.04140103\n",
      "Iteration 6646, loss = 0.04138935\n",
      "Iteration 6647, loss = 0.04137769\n",
      "Iteration 6648, loss = 0.04136603\n",
      "Iteration 6649, loss = 0.04135437\n",
      "Iteration 6650, loss = 0.04134272\n",
      "Iteration 6651, loss = 0.04133108\n",
      "Iteration 6652, loss = 0.04131944\n",
      "Iteration 6653, loss = 0.04130781\n",
      "Iteration 6654, loss = 0.04129618\n",
      "Iteration 6655, loss = 0.04128456\n",
      "Iteration 6656, loss = 0.04127295\n",
      "Iteration 6657, loss = 0.04126134\n",
      "Iteration 6658, loss = 0.04124974\n",
      "Iteration 6659, loss = 0.04123814\n",
      "Iteration 6660, loss = 0.04122655\n",
      "Iteration 6661, loss = 0.04121496\n",
      "Iteration 6662, loss = 0.04120339\n",
      "Iteration 6663, loss = 0.04119181\n",
      "Iteration 6664, loss = 0.04118025\n",
      "Iteration 6665, loss = 0.04116869\n",
      "Iteration 6666, loss = 0.04115713\n",
      "Iteration 6667, loss = 0.04114558\n",
      "Iteration 6668, loss = 0.04113404\n",
      "Iteration 6669, loss = 0.04112250\n",
      "Iteration 6670, loss = 0.04111097\n",
      "Iteration 6671, loss = 0.04109944\n",
      "Iteration 6672, loss = 0.04108792\n",
      "Iteration 6673, loss = 0.04107641\n",
      "Iteration 6674, loss = 0.04106490\n",
      "Iteration 6675, loss = 0.04105340\n",
      "Iteration 6676, loss = 0.04104190\n",
      "Iteration 6677, loss = 0.04103041\n",
      "Iteration 6678, loss = 0.04101893\n",
      "Iteration 6679, loss = 0.04100745\n",
      "Iteration 6680, loss = 0.04099597\n",
      "Iteration 6681, loss = 0.04098451\n",
      "Iteration 6682, loss = 0.04097304\n",
      "Iteration 6683, loss = 0.04096159\n",
      "Iteration 6684, loss = 0.04095014\n",
      "Iteration 6685, loss = 0.04093869\n",
      "Iteration 6686, loss = 0.04092726\n",
      "Iteration 6687, loss = 0.04091582\n",
      "Iteration 6688, loss = 0.04090440\n",
      "Iteration 6689, loss = 0.04089298\n",
      "Iteration 6690, loss = 0.04088156\n",
      "Iteration 6691, loss = 0.04087015\n",
      "Iteration 6692, loss = 0.04085875\n",
      "Iteration 6693, loss = 0.04084735\n",
      "Iteration 6694, loss = 0.04083596\n",
      "Iteration 6695, loss = 0.04082457\n",
      "Iteration 6696, loss = 0.04081319\n",
      "Iteration 6697, loss = 0.04080181\n",
      "Iteration 6698, loss = 0.04079045\n",
      "Iteration 6699, loss = 0.04077908\n",
      "Iteration 6700, loss = 0.04076772\n",
      "Iteration 6701, loss = 0.04075637\n",
      "Iteration 6702, loss = 0.04074503\n",
      "Iteration 6703, loss = 0.04073369\n",
      "Iteration 6704, loss = 0.04072235\n",
      "Iteration 6705, loss = 0.04071102\n",
      "Iteration 6706, loss = 0.04069970\n",
      "Iteration 6707, loss = 0.04068838\n",
      "Iteration 6708, loss = 0.04067707\n",
      "Iteration 6709, loss = 0.04066576\n",
      "Iteration 6710, loss = 0.04065446\n",
      "Iteration 6711, loss = 0.04064317\n",
      "Iteration 6712, loss = 0.04063188\n",
      "Iteration 6713, loss = 0.04062060\n",
      "Iteration 6714, loss = 0.04060932\n",
      "Iteration 6715, loss = 0.04059805\n",
      "Iteration 6716, loss = 0.04058678\n",
      "Iteration 6717, loss = 0.04057552\n",
      "Iteration 6718, loss = 0.04056426\n",
      "Iteration 6719, loss = 0.04055301\n",
      "Iteration 6720, loss = 0.04054177\n",
      "Iteration 6721, loss = 0.04053053\n",
      "Iteration 6722, loss = 0.04051930\n",
      "Iteration 6723, loss = 0.04050807\n",
      "Iteration 6724, loss = 0.04049685\n",
      "Iteration 6725, loss = 0.04048564\n",
      "Iteration 6726, loss = 0.04047443\n",
      "Iteration 6727, loss = 0.04046322\n",
      "Iteration 6728, loss = 0.04045203\n",
      "Iteration 6729, loss = 0.04044083\n",
      "Iteration 6730, loss = 0.04042965\n",
      "Iteration 6731, loss = 0.04041846\n",
      "Iteration 6732, loss = 0.04040729\n",
      "Iteration 6733, loss = 0.04039612\n",
      "Iteration 6734, loss = 0.04038495\n",
      "Iteration 6735, loss = 0.04037379\n",
      "Iteration 6736, loss = 0.04036264\n",
      "Iteration 6737, loss = 0.04035149\n",
      "Iteration 6738, loss = 0.04034035\n",
      "Iteration 6739, loss = 0.04032921\n",
      "Iteration 6740, loss = 0.04031808\n",
      "Iteration 6741, loss = 0.04030696\n",
      "Iteration 6742, loss = 0.04029584\n",
      "Iteration 6743, loss = 0.04028472\n",
      "Iteration 6744, loss = 0.04027361\n",
      "Iteration 6745, loss = 0.04026251\n",
      "Iteration 6746, loss = 0.04025141\n",
      "Iteration 6747, loss = 0.04024032\n",
      "Iteration 6748, loss = 0.04022923\n",
      "Iteration 6749, loss = 0.04021815\n",
      "Iteration 6750, loss = 0.04020708\n",
      "Iteration 6751, loss = 0.04019601\n",
      "Iteration 6752, loss = 0.04018494\n",
      "Iteration 6753, loss = 0.04017388\n",
      "Iteration 6754, loss = 0.04016283\n",
      "Iteration 6755, loss = 0.04015178\n",
      "Iteration 6756, loss = 0.04014074\n",
      "Iteration 6757, loss = 0.04012970\n",
      "Iteration 6758, loss = 0.04011867\n",
      "Iteration 6759, loss = 0.04010764\n",
      "Iteration 6760, loss = 0.04009662\n",
      "Iteration 6761, loss = 0.04008561\n",
      "Iteration 6762, loss = 0.04007460\n",
      "Iteration 6763, loss = 0.04006360\n",
      "Iteration 6764, loss = 0.04005260\n",
      "Iteration 6765, loss = 0.04004160\n",
      "Iteration 6766, loss = 0.04003062\n",
      "Iteration 6767, loss = 0.04001964\n",
      "Iteration 6768, loss = 0.04000866\n",
      "Iteration 6769, loss = 0.03999769\n",
      "Iteration 6770, loss = 0.03998672\n",
      "Iteration 6771, loss = 0.03997576\n",
      "Iteration 6772, loss = 0.03996481\n",
      "Iteration 6773, loss = 0.03995386\n",
      "Iteration 6774, loss = 0.03994291\n",
      "Iteration 6775, loss = 0.03993198\n",
      "Iteration 6776, loss = 0.03992104\n",
      "Iteration 6777, loss = 0.03991012\n",
      "Iteration 6778, loss = 0.03989919\n",
      "Iteration 6779, loss = 0.03988828\n",
      "Iteration 6780, loss = 0.03987737\n",
      "Iteration 6781, loss = 0.03986646\n",
      "Iteration 6782, loss = 0.03985556\n",
      "Iteration 6783, loss = 0.03984467\n",
      "Iteration 6784, loss = 0.03983378\n",
      "Iteration 6785, loss = 0.03982289\n",
      "Iteration 6786, loss = 0.03981202\n",
      "Iteration 6787, loss = 0.03980114\n",
      "Iteration 6788, loss = 0.03979027\n",
      "Iteration 6789, loss = 0.03977941\n",
      "Iteration 6790, loss = 0.03976856\n",
      "Iteration 6791, loss = 0.03975770\n",
      "Iteration 6792, loss = 0.03974686\n",
      "Iteration 6793, loss = 0.03973602\n",
      "Iteration 6794, loss = 0.03972518\n",
      "Iteration 6795, loss = 0.03971435\n",
      "Iteration 6796, loss = 0.03970353\n",
      "Iteration 6797, loss = 0.03969271\n",
      "Iteration 6798, loss = 0.03968189\n",
      "Iteration 6799, loss = 0.03967109\n",
      "Iteration 6800, loss = 0.03966028\n",
      "Iteration 6801, loss = 0.03964949\n",
      "Iteration 6802, loss = 0.03963869\n",
      "Iteration 6803, loss = 0.03962791\n",
      "Iteration 6804, loss = 0.03961712\n",
      "Iteration 6805, loss = 0.03960635\n",
      "Iteration 6806, loss = 0.03959558\n",
      "Iteration 6807, loss = 0.03958481\n",
      "Iteration 6808, loss = 0.03957405\n",
      "Iteration 6809, loss = 0.03956330\n",
      "Iteration 6810, loss = 0.03955255\n",
      "Iteration 6811, loss = 0.03954180\n",
      "Iteration 6812, loss = 0.03953106\n",
      "Iteration 6813, loss = 0.03952033\n",
      "Iteration 6814, loss = 0.03950960\n",
      "Iteration 6815, loss = 0.03949888\n",
      "Iteration 6816, loss = 0.03948816\n",
      "Iteration 6817, loss = 0.03947745\n",
      "Iteration 6818, loss = 0.03946674\n",
      "Iteration 6819, loss = 0.03945604\n",
      "Iteration 6820, loss = 0.03944534\n",
      "Iteration 6821, loss = 0.03943465\n",
      "Iteration 6822, loss = 0.03942396\n",
      "Iteration 6823, loss = 0.03941328\n",
      "Iteration 6824, loss = 0.03940261\n",
      "Iteration 6825, loss = 0.03939194\n",
      "Iteration 6826, loss = 0.03938127\n",
      "Iteration 6827, loss = 0.03937061\n",
      "Iteration 6828, loss = 0.03935996\n",
      "Iteration 6829, loss = 0.03934931\n",
      "Iteration 6830, loss = 0.03933866\n",
      "Iteration 6831, loss = 0.03932802\n",
      "Iteration 6832, loss = 0.03931739\n",
      "Iteration 6833, loss = 0.03930676\n",
      "Iteration 6834, loss = 0.03929614\n",
      "Iteration 6835, loss = 0.03928552\n",
      "Iteration 6836, loss = 0.03927491\n",
      "Iteration 6837, loss = 0.03926430\n",
      "Iteration 6838, loss = 0.03925370\n",
      "Iteration 6839, loss = 0.03924310\n",
      "Iteration 6840, loss = 0.03923251\n",
      "Iteration 6841, loss = 0.03922192\n",
      "Iteration 6842, loss = 0.03921134\n",
      "Iteration 6843, loss = 0.03920077\n",
      "Iteration 6844, loss = 0.03919020\n",
      "Iteration 6845, loss = 0.03917963\n",
      "Iteration 6846, loss = 0.03916907\n",
      "Iteration 6847, loss = 0.03915851\n",
      "Iteration 6848, loss = 0.03914796\n",
      "Iteration 6849, loss = 0.03913742\n",
      "Iteration 6850, loss = 0.03912688\n",
      "Iteration 6851, loss = 0.03911634\n",
      "Iteration 6852, loss = 0.03910582\n",
      "Iteration 6853, loss = 0.03909529\n",
      "Iteration 6854, loss = 0.03908477\n",
      "Iteration 6855, loss = 0.03907426\n",
      "Iteration 6856, loss = 0.03906375\n",
      "Iteration 6857, loss = 0.03905325\n",
      "Iteration 6858, loss = 0.03904275\n",
      "Iteration 6859, loss = 0.03903225\n",
      "Iteration 6860, loss = 0.03902177\n",
      "Iteration 6861, loss = 0.03901128\n",
      "Iteration 6862, loss = 0.03900080\n",
      "Iteration 6863, loss = 0.03899033\n",
      "Iteration 6864, loss = 0.03897986\n",
      "Iteration 6865, loss = 0.03896940\n",
      "Iteration 6866, loss = 0.03895894\n",
      "Iteration 6867, loss = 0.03894849\n",
      "Iteration 6868, loss = 0.03893804\n",
      "Iteration 6869, loss = 0.03892760\n",
      "Iteration 6870, loss = 0.03891717\n",
      "Iteration 6871, loss = 0.03890673\n",
      "Iteration 6872, loss = 0.03889631\n",
      "Iteration 6873, loss = 0.03888589\n",
      "Iteration 6874, loss = 0.03887547\n",
      "Iteration 6875, loss = 0.03886506\n",
      "Iteration 6876, loss = 0.03885465\n",
      "Iteration 6877, loss = 0.03884425\n",
      "Iteration 6878, loss = 0.03883385\n",
      "Iteration 6879, loss = 0.03882346\n",
      "Iteration 6880, loss = 0.03881308\n",
      "Iteration 6881, loss = 0.03880270\n",
      "Iteration 6882, loss = 0.03879232\n",
      "Iteration 6883, loss = 0.03878195\n",
      "Iteration 6884, loss = 0.03877158\n",
      "Iteration 6885, loss = 0.03876122\n",
      "Iteration 6886, loss = 0.03875087\n",
      "Iteration 6887, loss = 0.03874052\n",
      "Iteration 6888, loss = 0.03873017\n",
      "Iteration 6889, loss = 0.03871983\n",
      "Iteration 6890, loss = 0.03870949\n",
      "Iteration 6891, loss = 0.03869916\n",
      "Iteration 6892, loss = 0.03868884\n",
      "Iteration 6893, loss = 0.03867852\n",
      "Iteration 6894, loss = 0.03866820\n",
      "Iteration 6895, loss = 0.03865789\n",
      "Iteration 6896, loss = 0.03864759\n",
      "Iteration 6897, loss = 0.03863729\n",
      "Iteration 6898, loss = 0.03862699\n",
      "Iteration 6899, loss = 0.03861670\n",
      "Iteration 6900, loss = 0.03860642\n",
      "Iteration 6901, loss = 0.03859614\n",
      "Iteration 6902, loss = 0.03858586\n",
      "Iteration 6903, loss = 0.03857559\n",
      "Iteration 6904, loss = 0.03856532\n",
      "Iteration 6905, loss = 0.03855506\n",
      "Iteration 6906, loss = 0.03854481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6907, loss = 0.03853456\n",
      "Iteration 6908, loss = 0.03852431\n",
      "Iteration 6909, loss = 0.03851407\n",
      "Iteration 6910, loss = 0.03850384\n",
      "Iteration 6911, loss = 0.03849361\n",
      "Iteration 6912, loss = 0.03848338\n",
      "Iteration 6913, loss = 0.03847316\n",
      "Iteration 6914, loss = 0.03846295\n",
      "Iteration 6915, loss = 0.03845274\n",
      "Iteration 6916, loss = 0.03844253\n",
      "Iteration 6917, loss = 0.03843233\n",
      "Iteration 6918, loss = 0.03842213\n",
      "Iteration 6919, loss = 0.03841194\n",
      "Iteration 6920, loss = 0.03840176\n",
      "Iteration 6921, loss = 0.03839158\n",
      "Iteration 6922, loss = 0.03838140\n",
      "Iteration 6923, loss = 0.03837123\n",
      "Iteration 6924, loss = 0.03836106\n",
      "Iteration 6925, loss = 0.03835090\n",
      "Iteration 6926, loss = 0.03834075\n",
      "Iteration 6927, loss = 0.03833059\n",
      "Iteration 6928, loss = 0.03832045\n",
      "Iteration 6929, loss = 0.03831031\n",
      "Iteration 6930, loss = 0.03830017\n",
      "Iteration 6931, loss = 0.03829004\n",
      "Iteration 6932, loss = 0.03827991\n",
      "Iteration 6933, loss = 0.03826979\n",
      "Iteration 6934, loss = 0.03825967\n",
      "Iteration 6935, loss = 0.03824956\n",
      "Iteration 6936, loss = 0.03823945\n",
      "Iteration 6937, loss = 0.03822935\n",
      "Iteration 6938, loss = 0.03821925\n",
      "Iteration 6939, loss = 0.03820916\n",
      "Iteration 6940, loss = 0.03819907\n",
      "Iteration 6941, loss = 0.03818899\n",
      "Iteration 6942, loss = 0.03817891\n",
      "Iteration 6943, loss = 0.03816884\n",
      "Iteration 6944, loss = 0.03815877\n",
      "Iteration 6945, loss = 0.03814871\n",
      "Iteration 6946, loss = 0.03813865\n",
      "Iteration 6947, loss = 0.03812860\n",
      "Iteration 6948, loss = 0.03811855\n",
      "Iteration 6949, loss = 0.03810850\n",
      "Iteration 6950, loss = 0.03809846\n",
      "Iteration 6951, loss = 0.03808843\n",
      "Iteration 6952, loss = 0.03807840\n",
      "Iteration 6953, loss = 0.03806837\n",
      "Iteration 6954, loss = 0.03805835\n",
      "Iteration 6955, loss = 0.03804834\n",
      "Iteration 6956, loss = 0.03803833\n",
      "Iteration 6957, loss = 0.03802832\n",
      "Iteration 6958, loss = 0.03801832\n",
      "Iteration 6959, loss = 0.03800833\n",
      "Iteration 6960, loss = 0.03799834\n",
      "Iteration 6961, loss = 0.03798835\n",
      "Iteration 6962, loss = 0.03797837\n",
      "Iteration 6963, loss = 0.03796839\n",
      "Iteration 6964, loss = 0.03795842\n",
      "Iteration 6965, loss = 0.03794845\n",
      "Iteration 6966, loss = 0.03793849\n",
      "Iteration 6967, loss = 0.03792854\n",
      "Iteration 6968, loss = 0.03791858\n",
      "Iteration 6969, loss = 0.03790864\n",
      "Iteration 6970, loss = 0.03789869\n",
      "Iteration 6971, loss = 0.03788875\n",
      "Iteration 6972, loss = 0.03787882\n",
      "Iteration 6973, loss = 0.03786889\n",
      "Iteration 6974, loss = 0.03785897\n",
      "Iteration 6975, loss = 0.03784905\n",
      "Iteration 6976, loss = 0.03783913\n",
      "Iteration 6977, loss = 0.03782923\n",
      "Iteration 6978, loss = 0.03781932\n",
      "Iteration 6979, loss = 0.03780942\n",
      "Iteration 6980, loss = 0.03779952\n",
      "Iteration 6981, loss = 0.03778963\n",
      "Iteration 6982, loss = 0.03777975\n",
      "Iteration 6983, loss = 0.03776987\n",
      "Iteration 6984, loss = 0.03775999\n",
      "Iteration 6985, loss = 0.03775012\n",
      "Iteration 6986, loss = 0.03774025\n",
      "Iteration 6987, loss = 0.03773039\n",
      "Iteration 6988, loss = 0.03772053\n",
      "Iteration 6989, loss = 0.03771068\n",
      "Iteration 6990, loss = 0.03770083\n",
      "Iteration 6991, loss = 0.03769099\n",
      "Iteration 6992, loss = 0.03768115\n",
      "Iteration 6993, loss = 0.03767132\n",
      "Iteration 6994, loss = 0.03766149\n",
      "Iteration 6995, loss = 0.03765166\n",
      "Iteration 6996, loss = 0.03764184\n",
      "Iteration 6997, loss = 0.03763203\n",
      "Iteration 6998, loss = 0.03762222\n",
      "Iteration 6999, loss = 0.03761241\n",
      "Iteration 7000, loss = 0.03760261\n",
      "Iteration 7001, loss = 0.03759281\n",
      "Iteration 7002, loss = 0.03758302\n",
      "Iteration 7003, loss = 0.03757323\n",
      "Iteration 7004, loss = 0.03756345\n",
      "Iteration 7005, loss = 0.03755367\n",
      "Iteration 7006, loss = 0.03754390\n",
      "Iteration 7007, loss = 0.03753413\n",
      "Iteration 7008, loss = 0.03752437\n",
      "Iteration 7009, loss = 0.03751461\n",
      "Iteration 7010, loss = 0.03750486\n",
      "Iteration 7011, loss = 0.03749511\n",
      "Iteration 7012, loss = 0.03748536\n",
      "Iteration 7013, loss = 0.03747562\n",
      "Iteration 7014, loss = 0.03746589\n",
      "Iteration 7015, loss = 0.03745615\n",
      "Iteration 7016, loss = 0.03744643\n",
      "Iteration 7017, loss = 0.03743671\n",
      "Iteration 7018, loss = 0.03742699\n",
      "Iteration 7019, loss = 0.03741728\n",
      "Iteration 7020, loss = 0.03740757\n",
      "Iteration 7021, loss = 0.03739786\n",
      "Iteration 7022, loss = 0.03738817\n",
      "Iteration 7023, loss = 0.03737847\n",
      "Iteration 7024, loss = 0.03736878\n",
      "Iteration 7025, loss = 0.03735910\n",
      "Iteration 7026, loss = 0.03734942\n",
      "Iteration 7027, loss = 0.03733974\n",
      "Iteration 7028, loss = 0.03733007\n",
      "Iteration 7029, loss = 0.03732040\n",
      "Iteration 7030, loss = 0.03731074\n",
      "Iteration 7031, loss = 0.03730108\n",
      "Iteration 7032, loss = 0.03729143\n",
      "Iteration 7033, loss = 0.03728178\n",
      "Iteration 7034, loss = 0.03727214\n",
      "Iteration 7035, loss = 0.03726250\n",
      "Iteration 7036, loss = 0.03725287\n",
      "Iteration 7037, loss = 0.03724324\n",
      "Iteration 7038, loss = 0.03723361\n",
      "Iteration 7039, loss = 0.03722399\n",
      "Iteration 7040, loss = 0.03721437\n",
      "Iteration 7041, loss = 0.03720476\n",
      "Iteration 7042, loss = 0.03719515\n",
      "Iteration 7043, loss = 0.03718555\n",
      "Iteration 7044, loss = 0.03717595\n",
      "Iteration 7045, loss = 0.03716636\n",
      "Iteration 7046, loss = 0.03715677\n",
      "Iteration 7047, loss = 0.03714719\n",
      "Iteration 7048, loss = 0.03713761\n",
      "Iteration 7049, loss = 0.03712803\n",
      "Iteration 7050, loss = 0.03711846\n",
      "Iteration 7051, loss = 0.03710890\n",
      "Iteration 7052, loss = 0.03709933\n",
      "Iteration 7053, loss = 0.03708978\n",
      "Iteration 7054, loss = 0.03708022\n",
      "Iteration 7055, loss = 0.03707068\n",
      "Iteration 7056, loss = 0.03706113\n",
      "Iteration 7057, loss = 0.03705159\n",
      "Iteration 7058, loss = 0.03704206\n",
      "Iteration 7059, loss = 0.03703253\n",
      "Iteration 7060, loss = 0.03702300\n",
      "Iteration 7061, loss = 0.03701348\n",
      "Iteration 7062, loss = 0.03700397\n",
      "Iteration 7063, loss = 0.03699445\n",
      "Iteration 7064, loss = 0.03698495\n",
      "Iteration 7065, loss = 0.03697544\n",
      "Iteration 7066, loss = 0.03696594\n",
      "Iteration 7067, loss = 0.03695645\n",
      "Iteration 7068, loss = 0.03694696\n",
      "Iteration 7069, loss = 0.03693748\n",
      "Iteration 7070, loss = 0.03692799\n",
      "Iteration 7071, loss = 0.03691852\n",
      "Iteration 7072, loss = 0.03690905\n",
      "Iteration 7073, loss = 0.03689958\n",
      "Iteration 7074, loss = 0.03689012\n",
      "Iteration 7075, loss = 0.03688066\n",
      "Iteration 7076, loss = 0.03687120\n",
      "Iteration 7077, loss = 0.03686175\n",
      "Iteration 7078, loss = 0.03685231\n",
      "Iteration 7079, loss = 0.03684287\n",
      "Iteration 7080, loss = 0.03683343\n",
      "Iteration 7081, loss = 0.03682400\n",
      "Iteration 7082, loss = 0.03681457\n",
      "Iteration 7083, loss = 0.03680515\n",
      "Iteration 7084, loss = 0.03679573\n",
      "Iteration 7085, loss = 0.03678632\n",
      "Iteration 7086, loss = 0.03677691\n",
      "Iteration 7087, loss = 0.03676750\n",
      "Iteration 7088, loss = 0.03675810\n",
      "Iteration 7089, loss = 0.03674871\n",
      "Iteration 7090, loss = 0.03673932\n",
      "Iteration 7091, loss = 0.03672993\n",
      "Iteration 7092, loss = 0.03672055\n",
      "Iteration 7093, loss = 0.03671117\n",
      "Iteration 7094, loss = 0.03670179\n",
      "Iteration 7095, loss = 0.03669242\n",
      "Iteration 7096, loss = 0.03668306\n",
      "Iteration 7097, loss = 0.03667370\n",
      "Iteration 7098, loss = 0.03666434\n",
      "Iteration 7099, loss = 0.03665499\n",
      "Iteration 7100, loss = 0.03664564\n",
      "Iteration 7101, loss = 0.03663630\n",
      "Iteration 7102, loss = 0.03662696\n",
      "Iteration 7103, loss = 0.03661762\n",
      "Iteration 7104, loss = 0.03660829\n",
      "Iteration 7105, loss = 0.03659897\n",
      "Iteration 7106, loss = 0.03658965\n",
      "Iteration 7107, loss = 0.03658033\n",
      "Iteration 7108, loss = 0.03657102\n",
      "Iteration 7109, loss = 0.03656171\n",
      "Iteration 7110, loss = 0.03655240\n",
      "Iteration 7111, loss = 0.03654310\n",
      "Iteration 7112, loss = 0.03653381\n",
      "Iteration 7113, loss = 0.03652452\n",
      "Iteration 7114, loss = 0.03651523\n",
      "Iteration 7115, loss = 0.03650595\n",
      "Iteration 7116, loss = 0.03649667\n",
      "Iteration 7117, loss = 0.03648740\n",
      "Iteration 7118, loss = 0.03647813\n",
      "Iteration 7119, loss = 0.03646886\n",
      "Iteration 7120, loss = 0.03645960\n",
      "Iteration 7121, loss = 0.03645034\n",
      "Iteration 7122, loss = 0.03644109\n",
      "Iteration 7123, loss = 0.03643185\n",
      "Iteration 7124, loss = 0.03642260\n",
      "Iteration 7125, loss = 0.03641336\n",
      "Iteration 7126, loss = 0.03640413\n",
      "Iteration 7127, loss = 0.03639490\n",
      "Iteration 7128, loss = 0.03638567\n",
      "Iteration 7129, loss = 0.03637645\n",
      "Iteration 7130, loss = 0.03636723\n",
      "Iteration 7131, loss = 0.03635802\n",
      "Iteration 7132, loss = 0.03634881\n",
      "Iteration 7133, loss = 0.03633961\n",
      "Iteration 7134, loss = 0.03633041\n",
      "Iteration 7135, loss = 0.03632121\n",
      "Iteration 7136, loss = 0.03631202\n",
      "Iteration 7137, loss = 0.03630283\n",
      "Iteration 7138, loss = 0.03629365\n",
      "Iteration 7139, loss = 0.03628447\n",
      "Iteration 7140, loss = 0.03627529\n",
      "Iteration 7141, loss = 0.03626612\n",
      "Iteration 7142, loss = 0.03625696\n",
      "Iteration 7143, loss = 0.03624780\n",
      "Iteration 7144, loss = 0.03623864\n",
      "Iteration 7145, loss = 0.03622948\n",
      "Iteration 7146, loss = 0.03622034\n",
      "Iteration 7147, loss = 0.03621119\n",
      "Iteration 7148, loss = 0.03620205\n",
      "Iteration 7149, loss = 0.03619291\n",
      "Iteration 7150, loss = 0.03618378\n",
      "Iteration 7151, loss = 0.03617465\n",
      "Iteration 7152, loss = 0.03616553\n",
      "Iteration 7153, loss = 0.03615641\n",
      "Iteration 7154, loss = 0.03614730\n",
      "Iteration 7155, loss = 0.03613819\n",
      "Iteration 7156, loss = 0.03612908\n",
      "Iteration 7157, loss = 0.03611998\n",
      "Iteration 7158, loss = 0.03611088\n",
      "Iteration 7159, loss = 0.03610179\n",
      "Iteration 7160, loss = 0.03609270\n",
      "Iteration 7161, loss = 0.03608361\n",
      "Iteration 7162, loss = 0.03607453\n",
      "Iteration 7163, loss = 0.03606545\n",
      "Iteration 7164, loss = 0.03605638\n",
      "Iteration 7165, loss = 0.03604731\n",
      "Iteration 7166, loss = 0.03603825\n",
      "Iteration 7167, loss = 0.03602919\n",
      "Iteration 7168, loss = 0.03602013\n",
      "Iteration 7169, loss = 0.03601108\n",
      "Iteration 7170, loss = 0.03600203\n",
      "Iteration 7171, loss = 0.03599299\n",
      "Iteration 7172, loss = 0.03598395\n",
      "Iteration 7173, loss = 0.03597491\n",
      "Iteration 7174, loss = 0.03596588\n",
      "Iteration 7175, loss = 0.03595686\n",
      "Iteration 7176, loss = 0.03594783\n",
      "Iteration 7177, loss = 0.03593881\n",
      "Iteration 7178, loss = 0.03592980\n",
      "Iteration 7179, loss = 0.03592079\n",
      "Iteration 7180, loss = 0.03591179\n",
      "Iteration 7181, loss = 0.03590278\n",
      "Iteration 7182, loss = 0.03589379\n",
      "Iteration 7183, loss = 0.03588479\n",
      "Iteration 7184, loss = 0.03587580\n",
      "Iteration 7185, loss = 0.03586682\n",
      "Iteration 7186, loss = 0.03585784\n",
      "Iteration 7187, loss = 0.03584886\n",
      "Iteration 7188, loss = 0.03583989\n",
      "Iteration 7189, loss = 0.03583092\n",
      "Iteration 7190, loss = 0.03582196\n",
      "Iteration 7191, loss = 0.03581300\n",
      "Iteration 7192, loss = 0.03580404\n",
      "Iteration 7193, loss = 0.03579509\n",
      "Iteration 7194, loss = 0.03578614\n",
      "Iteration 7195, loss = 0.03577720\n",
      "Iteration 7196, loss = 0.03576826\n",
      "Iteration 7197, loss = 0.03575932\n",
      "Iteration 7198, loss = 0.03575039\n",
      "Iteration 7199, loss = 0.03574146\n",
      "Iteration 7200, loss = 0.03573254\n",
      "Iteration 7201, loss = 0.03572362\n",
      "Iteration 7202, loss = 0.03571471\n",
      "Iteration 7203, loss = 0.03570580\n",
      "Iteration 7204, loss = 0.03569689\n",
      "Iteration 7205, loss = 0.03568799\n",
      "Iteration 7206, loss = 0.03567909\n",
      "Iteration 7207, loss = 0.03567019\n",
      "Iteration 7208, loss = 0.03566130\n",
      "Iteration 7209, loss = 0.03565242\n",
      "Iteration 7210, loss = 0.03564354\n",
      "Iteration 7211, loss = 0.03563466\n",
      "Iteration 7212, loss = 0.03562578\n",
      "Iteration 7213, loss = 0.03561691\n",
      "Iteration 7214, loss = 0.03560805\n",
      "Iteration 7215, loss = 0.03559919\n",
      "Iteration 7216, loss = 0.03559033\n",
      "Iteration 7217, loss = 0.03558148\n",
      "Iteration 7218, loss = 0.03557263\n",
      "Iteration 7219, loss = 0.03556378\n",
      "Iteration 7220, loss = 0.03555494\n",
      "Iteration 7221, loss = 0.03554610\n",
      "Iteration 7222, loss = 0.03553727\n",
      "Iteration 7223, loss = 0.03552844\n",
      "Iteration 7224, loss = 0.03551962\n",
      "Iteration 7225, loss = 0.03551080\n",
      "Iteration 7226, loss = 0.03550198\n",
      "Iteration 7227, loss = 0.03549317\n",
      "Iteration 7228, loss = 0.03548436\n",
      "Iteration 7229, loss = 0.03547555\n",
      "Iteration 7230, loss = 0.03546675\n",
      "Iteration 7231, loss = 0.03545795\n",
      "Iteration 7232, loss = 0.03544916\n",
      "Iteration 7233, loss = 0.03544037\n",
      "Iteration 7234, loss = 0.03543159\n",
      "Iteration 7235, loss = 0.03542281\n",
      "Iteration 7236, loss = 0.03541403\n",
      "Iteration 7237, loss = 0.03540526\n",
      "Iteration 7238, loss = 0.03539649\n",
      "Iteration 7239, loss = 0.03538773\n",
      "Iteration 7240, loss = 0.03537896\n",
      "Iteration 7241, loss = 0.03537021\n",
      "Iteration 7242, loss = 0.03536146\n",
      "Iteration 7243, loss = 0.03535271\n",
      "Iteration 7244, loss = 0.03534396\n",
      "Iteration 7245, loss = 0.03533522\n",
      "Iteration 7246, loss = 0.03532649\n",
      "Iteration 7247, loss = 0.03531775\n",
      "Iteration 7248, loss = 0.03530902\n",
      "Iteration 7249, loss = 0.03530030\n",
      "Iteration 7250, loss = 0.03529158\n",
      "Iteration 7251, loss = 0.03528286\n",
      "Iteration 7252, loss = 0.03527415\n",
      "Iteration 7253, loss = 0.03526544\n",
      "Iteration 7254, loss = 0.03525674\n",
      "Iteration 7255, loss = 0.03524804\n",
      "Iteration 7256, loss = 0.03523934\n",
      "Iteration 7257, loss = 0.03523065\n",
      "Iteration 7258, loss = 0.03522196\n",
      "Iteration 7259, loss = 0.03521327\n",
      "Iteration 7260, loss = 0.03520459\n",
      "Iteration 7261, loss = 0.03519592\n",
      "Iteration 7262, loss = 0.03518724\n",
      "Iteration 7263, loss = 0.03517857\n",
      "Iteration 7264, loss = 0.03516991\n",
      "Iteration 7265, loss = 0.03516125\n",
      "Iteration 7266, loss = 0.03515259\n",
      "Iteration 7267, loss = 0.03514394\n",
      "Iteration 7268, loss = 0.03513529\n",
      "Iteration 7269, loss = 0.03512664\n",
      "Iteration 7270, loss = 0.03511800\n",
      "Iteration 7271, loss = 0.03510936\n",
      "Iteration 7272, loss = 0.03510073\n",
      "Iteration 7273, loss = 0.03509210\n",
      "Iteration 7274, loss = 0.03508348\n",
      "Iteration 7275, loss = 0.03507485\n",
      "Iteration 7276, loss = 0.03506624\n",
      "Iteration 7277, loss = 0.03505762\n",
      "Iteration 7278, loss = 0.03504901\n",
      "Iteration 7279, loss = 0.03504041\n",
      "Iteration 7280, loss = 0.03503180\n",
      "Iteration 7281, loss = 0.03502321\n",
      "Iteration 7282, loss = 0.03501461\n",
      "Iteration 7283, loss = 0.03500602\n",
      "Iteration 7284, loss = 0.03499743\n",
      "Iteration 7285, loss = 0.03498885\n",
      "Iteration 7286, loss = 0.03498027\n",
      "Iteration 7287, loss = 0.03497170\n",
      "Iteration 7288, loss = 0.03496313\n",
      "Iteration 7289, loss = 0.03495456\n",
      "Iteration 7290, loss = 0.03494600\n",
      "Iteration 7291, loss = 0.03493744\n",
      "Iteration 7292, loss = 0.03492888\n",
      "Iteration 7293, loss = 0.03492033\n",
      "Iteration 7294, loss = 0.03491178\n",
      "Iteration 7295, loss = 0.03490324\n",
      "Iteration 7296, loss = 0.03489470\n",
      "Iteration 7297, loss = 0.03488616\n",
      "Iteration 7298, loss = 0.03487763\n",
      "Iteration 7299, loss = 0.03486910\n",
      "Iteration 7300, loss = 0.03486058\n",
      "Iteration 7301, loss = 0.03485206\n",
      "Iteration 7302, loss = 0.03484354\n",
      "Iteration 7303, loss = 0.03483503\n",
      "Iteration 7304, loss = 0.03482652\n",
      "Iteration 7305, loss = 0.03481801\n",
      "Iteration 7306, loss = 0.03480951\n",
      "Iteration 7307, loss = 0.03480102\n",
      "Iteration 7308, loss = 0.03479252\n",
      "Iteration 7309, loss = 0.03478403\n",
      "Iteration 7310, loss = 0.03477555\n",
      "Iteration 7311, loss = 0.03476706\n",
      "Iteration 7312, loss = 0.03475859\n",
      "Iteration 7313, loss = 0.03475011\n",
      "Iteration 7314, loss = 0.03474164\n",
      "Iteration 7315, loss = 0.03473317\n",
      "Iteration 7316, loss = 0.03472471\n",
      "Iteration 7317, loss = 0.03471625\n",
      "Iteration 7318, loss = 0.03470780\n",
      "Iteration 7319, loss = 0.03469935\n",
      "Iteration 7320, loss = 0.03469090\n",
      "Iteration 7321, loss = 0.03468245\n",
      "Iteration 7322, loss = 0.03467401\n",
      "Iteration 7323, loss = 0.03466558\n",
      "Iteration 7324, loss = 0.03465715\n",
      "Iteration 7325, loss = 0.03464872\n",
      "Iteration 7326, loss = 0.03464029\n",
      "Iteration 7327, loss = 0.03463187\n",
      "Iteration 7328, loss = 0.03462345\n",
      "Iteration 7329, loss = 0.03461504\n",
      "Iteration 7330, loss = 0.03460663\n",
      "Iteration 7331, loss = 0.03459822\n",
      "Iteration 7332, loss = 0.03458982\n",
      "Iteration 7333, loss = 0.03458142\n",
      "Iteration 7334, loss = 0.03457303\n",
      "Iteration 7335, loss = 0.03456464\n",
      "Iteration 7336, loss = 0.03455625\n",
      "Iteration 7337, loss = 0.03454787\n",
      "Iteration 7338, loss = 0.03453949\n",
      "Iteration 7339, loss = 0.03453111\n",
      "Iteration 7340, loss = 0.03452274\n",
      "Iteration 7341, loss = 0.03451437\n",
      "Iteration 7342, loss = 0.03450601\n",
      "Iteration 7343, loss = 0.03449765\n",
      "Iteration 7344, loss = 0.03448929\n",
      "Iteration 7345, loss = 0.03448094\n",
      "Iteration 7346, loss = 0.03447259\n",
      "Iteration 7347, loss = 0.03446424\n",
      "Iteration 7348, loss = 0.03445590\n",
      "Iteration 7349, loss = 0.03444756\n",
      "Iteration 7350, loss = 0.03443923\n",
      "Iteration 7351, loss = 0.03443090\n",
      "Iteration 7352, loss = 0.03442257\n",
      "Iteration 7353, loss = 0.03441425\n",
      "Iteration 7354, loss = 0.03440593\n",
      "Iteration 7355, loss = 0.03439761\n",
      "Iteration 7356, loss = 0.03438930\n",
      "Iteration 7357, loss = 0.03438099\n",
      "Iteration 7358, loss = 0.03437269\n",
      "Iteration 7359, loss = 0.03436439\n",
      "Iteration 7360, loss = 0.03435609\n",
      "Iteration 7361, loss = 0.03434780\n",
      "Iteration 7362, loss = 0.03433951\n",
      "Iteration 7363, loss = 0.03433122\n",
      "Iteration 7364, loss = 0.03432294\n",
      "Iteration 7365, loss = 0.03431466\n",
      "Iteration 7366, loss = 0.03430639\n",
      "Iteration 7367, loss = 0.03429812\n",
      "Iteration 7368, loss = 0.03428985\n",
      "Iteration 7369, loss = 0.03428159\n",
      "Iteration 7370, loss = 0.03427333\n",
      "Iteration 7371, loss = 0.03426507\n",
      "Iteration 7372, loss = 0.03425682\n",
      "Iteration 7373, loss = 0.03424857\n",
      "Iteration 7374, loss = 0.03424032\n",
      "Iteration 7375, loss = 0.03423208\n",
      "Iteration 7376, loss = 0.03422384\n",
      "Iteration 7377, loss = 0.03421561\n",
      "Iteration 7378, loss = 0.03420738\n",
      "Iteration 7379, loss = 0.03419915\n",
      "Iteration 7380, loss = 0.03419093\n",
      "Iteration 7381, loss = 0.03418271\n",
      "Iteration 7382, loss = 0.03417449\n",
      "Iteration 7383, loss = 0.03416628\n",
      "Iteration 7384, loss = 0.03415807\n",
      "Iteration 7385, loss = 0.03414987\n",
      "Iteration 7386, loss = 0.03414167\n",
      "Iteration 7387, loss = 0.03413347\n",
      "Iteration 7388, loss = 0.03412528\n",
      "Iteration 7389, loss = 0.03411709\n",
      "Iteration 7390, loss = 0.03410890\n",
      "Iteration 7391, loss = 0.03410072\n",
      "Iteration 7392, loss = 0.03409254\n",
      "Iteration 7393, loss = 0.03408436\n",
      "Iteration 7394, loss = 0.03407619\n",
      "Iteration 7395, loss = 0.03406802\n",
      "Iteration 7396, loss = 0.03405986\n",
      "Iteration 7397, loss = 0.03405170\n",
      "Iteration 7398, loss = 0.03404354\n",
      "Iteration 7399, loss = 0.03403539\n",
      "Iteration 7400, loss = 0.03402724\n",
      "Iteration 7401, loss = 0.03401909\n",
      "Iteration 7402, loss = 0.03401095\n",
      "Iteration 7403, loss = 0.03400281\n",
      "Iteration 7404, loss = 0.03399467\n",
      "Iteration 7405, loss = 0.03398654\n",
      "Iteration 7406, loss = 0.03397841\n",
      "Iteration 7407, loss = 0.03397029\n",
      "Iteration 7408, loss = 0.03396217\n",
      "Iteration 7409, loss = 0.03395405\n",
      "Iteration 7410, loss = 0.03394594\n",
      "Iteration 7411, loss = 0.03393782\n",
      "Iteration 7412, loss = 0.03392972\n",
      "Iteration 7413, loss = 0.03392162\n",
      "Iteration 7414, loss = 0.03391352\n",
      "Iteration 7415, loss = 0.03390542\n",
      "Iteration 7416, loss = 0.03389733\n",
      "Iteration 7417, loss = 0.03388924\n",
      "Iteration 7418, loss = 0.03388115\n",
      "Iteration 7419, loss = 0.03387307\n",
      "Iteration 7420, loss = 0.03386499\n",
      "Iteration 7421, loss = 0.03385692\n",
      "Iteration 7422, loss = 0.03384885\n",
      "Iteration 7423, loss = 0.03384078\n",
      "Iteration 7424, loss = 0.03383272\n",
      "Iteration 7425, loss = 0.03382466\n",
      "Iteration 7426, loss = 0.03381660\n",
      "Iteration 7427, loss = 0.03380855\n",
      "Iteration 7428, loss = 0.03380050\n",
      "Iteration 7429, loss = 0.03379245\n",
      "Iteration 7430, loss = 0.03378441\n",
      "Iteration 7431, loss = 0.03377637\n",
      "Iteration 7432, loss = 0.03376834\n",
      "Iteration 7433, loss = 0.03376031\n",
      "Iteration 7434, loss = 0.03375228\n",
      "Iteration 7435, loss = 0.03374425\n",
      "Iteration 7436, loss = 0.03373623\n",
      "Iteration 7437, loss = 0.03372822\n",
      "Iteration 7438, loss = 0.03372020\n",
      "Iteration 7439, loss = 0.03371219\n",
      "Iteration 7440, loss = 0.03370419\n",
      "Iteration 7441, loss = 0.03369618\n",
      "Iteration 7442, loss = 0.03368818\n",
      "Iteration 7443, loss = 0.03368019\n",
      "Iteration 7444, loss = 0.03367219\n",
      "Iteration 7445, loss = 0.03366421\n",
      "Iteration 7446, loss = 0.03365622\n",
      "Iteration 7447, loss = 0.03364824\n",
      "Iteration 7448, loss = 0.03364026\n",
      "Iteration 7449, loss = 0.03363228\n",
      "Iteration 7450, loss = 0.03362431\n",
      "Iteration 7451, loss = 0.03361635\n",
      "Iteration 7452, loss = 0.03360838\n",
      "Iteration 7453, loss = 0.03360042\n",
      "Iteration 7454, loss = 0.03359246\n",
      "Iteration 7455, loss = 0.03358451\n",
      "Iteration 7456, loss = 0.03357656\n",
      "Iteration 7457, loss = 0.03356861\n",
      "Iteration 7458, loss = 0.03356067\n",
      "Iteration 7459, loss = 0.03355273\n",
      "Iteration 7460, loss = 0.03354479\n",
      "Iteration 7461, loss = 0.03353686\n",
      "Iteration 7462, loss = 0.03352893\n",
      "Iteration 7463, loss = 0.03352101\n",
      "Iteration 7464, loss = 0.03351308\n",
      "Iteration 7465, loss = 0.03350516\n",
      "Iteration 7466, loss = 0.03349725\n",
      "Iteration 7467, loss = 0.03348934\n",
      "Iteration 7468, loss = 0.03348143\n",
      "Iteration 7469, loss = 0.03347352\n",
      "Iteration 7470, loss = 0.03346562\n",
      "Iteration 7471, loss = 0.03345772\n",
      "Iteration 7472, loss = 0.03344983\n",
      "Iteration 7473, loss = 0.03344194\n",
      "Iteration 7474, loss = 0.03343405\n",
      "Iteration 7475, loss = 0.03342617\n",
      "Iteration 7476, loss = 0.03341829\n",
      "Iteration 7477, loss = 0.03341041\n",
      "Iteration 7478, loss = 0.03340254\n",
      "Iteration 7479, loss = 0.03339467\n",
      "Iteration 7480, loss = 0.03338680\n",
      "Iteration 7481, loss = 0.03337894\n",
      "Iteration 7482, loss = 0.03337108\n",
      "Iteration 7483, loss = 0.03336322\n",
      "Iteration 7484, loss = 0.03335537\n",
      "Iteration 7485, loss = 0.03334752\n",
      "Iteration 7486, loss = 0.03333967\n",
      "Iteration 7487, loss = 0.03333183\n",
      "Iteration 7488, loss = 0.03332399\n",
      "Iteration 7489, loss = 0.03331615\n",
      "Iteration 7490, loss = 0.03330832\n",
      "Iteration 7491, loss = 0.03330049\n",
      "Iteration 7492, loss = 0.03329267\n",
      "Iteration 7493, loss = 0.03328485\n",
      "Iteration 7494, loss = 0.03327703\n",
      "Iteration 7495, loss = 0.03326921\n",
      "Iteration 7496, loss = 0.03326140\n",
      "Iteration 7497, loss = 0.03325359\n",
      "Iteration 7498, loss = 0.03324579\n",
      "Iteration 7499, loss = 0.03323798\n",
      "Iteration 7500, loss = 0.03323019\n",
      "Iteration 7501, loss = 0.03322239\n",
      "Iteration 7502, loss = 0.03321460\n",
      "Iteration 7503, loss = 0.03320681\n",
      "Iteration 7504, loss = 0.03319903\n",
      "Iteration 7505, loss = 0.03319125\n",
      "Iteration 7506, loss = 0.03318347\n",
      "Iteration 7507, loss = 0.03317570\n",
      "Iteration 7508, loss = 0.03316793\n",
      "Iteration 7509, loss = 0.03316016\n",
      "Iteration 7510, loss = 0.03315239\n",
      "Iteration 7511, loss = 0.03314463\n",
      "Iteration 7512, loss = 0.03313688\n",
      "Iteration 7513, loss = 0.03312912\n",
      "Iteration 7514, loss = 0.03312137\n",
      "Iteration 7515, loss = 0.03311362\n",
      "Iteration 7516, loss = 0.03310588\n",
      "Iteration 7517, loss = 0.03309814\n",
      "Iteration 7518, loss = 0.03309040\n",
      "Iteration 7519, loss = 0.03308267\n",
      "Iteration 7520, loss = 0.03307494\n",
      "Iteration 7521, loss = 0.03306721\n",
      "Iteration 7522, loss = 0.03305949\n",
      "Iteration 7523, loss = 0.03305177\n",
      "Iteration 7524, loss = 0.03304405\n",
      "Iteration 7525, loss = 0.03303634\n",
      "Iteration 7526, loss = 0.03302863\n",
      "Iteration 7527, loss = 0.03302092\n",
      "Iteration 7528, loss = 0.03301322\n",
      "Iteration 7529, loss = 0.03300552\n",
      "Iteration 7530, loss = 0.03299782\n",
      "Iteration 7531, loss = 0.03299013\n",
      "Iteration 7532, loss = 0.03298244\n",
      "Iteration 7533, loss = 0.03297475\n",
      "Iteration 7534, loss = 0.03296707\n",
      "Iteration 7535, loss = 0.03295939\n",
      "Iteration 7536, loss = 0.03295171\n",
      "Iteration 7537, loss = 0.03294404\n",
      "Iteration 7538, loss = 0.03293637\n",
      "Iteration 7539, loss = 0.03292871\n",
      "Iteration 7540, loss = 0.03292104\n",
      "Iteration 7541, loss = 0.03291338\n",
      "Iteration 7542, loss = 0.03290573\n",
      "Iteration 7543, loss = 0.03289807\n",
      "Iteration 7544, loss = 0.03289042\n",
      "Iteration 7545, loss = 0.03288278\n",
      "Iteration 7546, loss = 0.03287513\n",
      "Iteration 7547, loss = 0.03286749\n",
      "Iteration 7548, loss = 0.03285986\n",
      "Iteration 7549, loss = 0.03285222\n",
      "Iteration 7550, loss = 0.03284459\n",
      "Iteration 7551, loss = 0.03283697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7552, loss = 0.03282934\n",
      "Iteration 7553, loss = 0.03282172\n",
      "Iteration 7554, loss = 0.03281411\n",
      "Iteration 7555, loss = 0.03280649\n",
      "Iteration 7556, loss = 0.03279888\n",
      "Iteration 7557, loss = 0.03279128\n",
      "Iteration 7558, loss = 0.03278367\n",
      "Iteration 7559, loss = 0.03277607\n",
      "Iteration 7560, loss = 0.03276848\n",
      "Iteration 7561, loss = 0.03276088\n",
      "Iteration 7562, loss = 0.03275329\n",
      "Iteration 7563, loss = 0.03274571\n",
      "Iteration 7564, loss = 0.03273812\n",
      "Iteration 7565, loss = 0.03273054\n",
      "Iteration 7566, loss = 0.03272296\n",
      "Iteration 7567, loss = 0.03271539\n",
      "Iteration 7568, loss = 0.03270782\n",
      "Iteration 7569, loss = 0.03270025\n",
      "Iteration 7570, loss = 0.03269269\n",
      "Iteration 7571, loss = 0.03268513\n",
      "Iteration 7572, loss = 0.03267757\n",
      "Iteration 7573, loss = 0.03267002\n",
      "Iteration 7574, loss = 0.03266246\n",
      "Iteration 7575, loss = 0.03265492\n",
      "Iteration 7576, loss = 0.03264737\n",
      "Iteration 7577, loss = 0.03263983\n",
      "Iteration 7578, loss = 0.03263229\n",
      "Iteration 7579, loss = 0.03262476\n",
      "Iteration 7580, loss = 0.03261723\n",
      "Iteration 7581, loss = 0.03260970\n",
      "Iteration 7582, loss = 0.03260217\n",
      "Iteration 7583, loss = 0.03259465\n",
      "Iteration 7584, loss = 0.03258713\n",
      "Iteration 7585, loss = 0.03257962\n",
      "Iteration 7586, loss = 0.03257211\n",
      "Iteration 7587, loss = 0.03256460\n",
      "Iteration 7588, loss = 0.03255709\n",
      "Iteration 7589, loss = 0.03254959\n",
      "Iteration 7590, loss = 0.03254209\n",
      "Iteration 7591, loss = 0.03253460\n",
      "Iteration 7592, loss = 0.03252710\n",
      "Iteration 7593, loss = 0.03251961\n",
      "Iteration 7594, loss = 0.03251213\n",
      "Iteration 7595, loss = 0.03250465\n",
      "Iteration 7596, loss = 0.03249717\n",
      "Iteration 7597, loss = 0.03248969\n",
      "Iteration 7598, loss = 0.03248222\n",
      "Iteration 7599, loss = 0.03247475\n",
      "Iteration 7600, loss = 0.03246728\n",
      "Iteration 7601, loss = 0.03245982\n",
      "Iteration 7602, loss = 0.03245236\n",
      "Iteration 7603, loss = 0.03244490\n",
      "Iteration 7604, loss = 0.03243744\n",
      "Iteration 7605, loss = 0.03242999\n",
      "Iteration 7606, loss = 0.03242255\n",
      "Iteration 7607, loss = 0.03241510\n",
      "Iteration 7608, loss = 0.03240766\n",
      "Iteration 7609, loss = 0.03240022\n",
      "Iteration 7610, loss = 0.03239279\n",
      "Iteration 7611, loss = 0.03238536\n",
      "Iteration 7612, loss = 0.03237793\n",
      "Iteration 7613, loss = 0.03237050\n",
      "Iteration 7614, loss = 0.03236308\n",
      "Iteration 7615, loss = 0.03235566\n",
      "Iteration 7616, loss = 0.03234825\n",
      "Iteration 7617, loss = 0.03234084\n",
      "Iteration 7618, loss = 0.03233343\n",
      "Iteration 7619, loss = 0.03232602\n",
      "Iteration 7620, loss = 0.03231862\n",
      "Iteration 7621, loss = 0.03231122\n",
      "Iteration 7622, loss = 0.03230382\n",
      "Iteration 7623, loss = 0.03229643\n",
      "Iteration 7624, loss = 0.03228904\n",
      "Iteration 7625, loss = 0.03228165\n",
      "Iteration 7626, loss = 0.03227427\n",
      "Iteration 7627, loss = 0.03226689\n",
      "Iteration 7628, loss = 0.03225951\n",
      "Iteration 7629, loss = 0.03225213\n",
      "Iteration 7630, loss = 0.03224476\n",
      "Iteration 7631, loss = 0.03223740\n",
      "Iteration 7632, loss = 0.03223003\n",
      "Iteration 7633, loss = 0.03222267\n",
      "Iteration 7634, loss = 0.03221531\n",
      "Iteration 7635, loss = 0.03220795\n",
      "Iteration 7636, loss = 0.03220060\n",
      "Iteration 7637, loss = 0.03219325\n",
      "Iteration 7638, loss = 0.03218591\n",
      "Iteration 7639, loss = 0.03217856\n",
      "Iteration 7640, loss = 0.03217123\n",
      "Iteration 7641, loss = 0.03216389\n",
      "Iteration 7642, loss = 0.03215656\n",
      "Iteration 7643, loss = 0.03214923\n",
      "Iteration 7644, loss = 0.03214190\n",
      "Iteration 7645, loss = 0.03213457\n",
      "Iteration 7646, loss = 0.03212725\n",
      "Iteration 7647, loss = 0.03211994\n",
      "Iteration 7648, loss = 0.03211262\n",
      "Iteration 7649, loss = 0.03210531\n",
      "Iteration 7650, loss = 0.03209800\n",
      "Iteration 7651, loss = 0.03209070\n",
      "Iteration 7652, loss = 0.03208339\n",
      "Iteration 7653, loss = 0.03207609\n",
      "Iteration 7654, loss = 0.03206880\n",
      "Iteration 7655, loss = 0.03206151\n",
      "Iteration 7656, loss = 0.03205422\n",
      "Iteration 7657, loss = 0.03204693\n",
      "Iteration 7658, loss = 0.03203965\n",
      "Iteration 7659, loss = 0.03203237\n",
      "Iteration 7660, loss = 0.03202509\n",
      "Iteration 7661, loss = 0.03201781\n",
      "Iteration 7662, loss = 0.03201054\n",
      "Iteration 7663, loss = 0.03200328\n",
      "Iteration 7664, loss = 0.03199601\n",
      "Iteration 7665, loss = 0.03198875\n",
      "Iteration 7666, loss = 0.03198149\n",
      "Iteration 7667, loss = 0.03197423\n",
      "Iteration 7668, loss = 0.03196698\n",
      "Iteration 7669, loss = 0.03195973\n",
      "Iteration 7670, loss = 0.03195249\n",
      "Iteration 7671, loss = 0.03194524\n",
      "Iteration 7672, loss = 0.03193800\n",
      "Iteration 7673, loss = 0.03193077\n",
      "Iteration 7674, loss = 0.03192353\n",
      "Iteration 7675, loss = 0.03191630\n",
      "Iteration 7676, loss = 0.03190907\n",
      "Iteration 7677, loss = 0.03190185\n",
      "Iteration 7678, loss = 0.03189463\n",
      "Iteration 7679, loss = 0.03188741\n",
      "Iteration 7680, loss = 0.03188019\n",
      "Iteration 7681, loss = 0.03187298\n",
      "Iteration 7682, loss = 0.03186577\n",
      "Iteration 7683, loss = 0.03185856\n",
      "Iteration 7684, loss = 0.03185136\n",
      "Iteration 7685, loss = 0.03184416\n",
      "Iteration 7686, loss = 0.03183696\n",
      "Iteration 7687, loss = 0.03182977\n",
      "Iteration 7688, loss = 0.03182258\n",
      "Iteration 7689, loss = 0.03181539\n",
      "Iteration 7690, loss = 0.03180821\n",
      "Iteration 7691, loss = 0.03180102\n",
      "Iteration 7692, loss = 0.03179384\n",
      "Iteration 7693, loss = 0.03178667\n",
      "Iteration 7694, loss = 0.03177950\n",
      "Iteration 7695, loss = 0.03177233\n",
      "Iteration 7696, loss = 0.03176516\n",
      "Iteration 7697, loss = 0.03175800\n",
      "Iteration 7698, loss = 0.03175084\n",
      "Iteration 7699, loss = 0.03174368\n",
      "Iteration 7700, loss = 0.03173652\n",
      "Iteration 7701, loss = 0.03172937\n",
      "Iteration 7702, loss = 0.03172222\n",
      "Iteration 7703, loss = 0.03171508\n",
      "Iteration 7704, loss = 0.03170793\n",
      "Iteration 7705, loss = 0.03170080\n",
      "Iteration 7706, loss = 0.03169366\n",
      "Iteration 7707, loss = 0.03168653\n",
      "Iteration 7708, loss = 0.03167939\n",
      "Iteration 7709, loss = 0.03167227\n",
      "Iteration 7710, loss = 0.03166514\n",
      "Iteration 7711, loss = 0.03165802\n",
      "Iteration 7712, loss = 0.03165090\n",
      "Iteration 7713, loss = 0.03164379\n",
      "Iteration 7714, loss = 0.03163668\n",
      "Iteration 7715, loss = 0.03162957\n",
      "Iteration 7716, loss = 0.03162246\n",
      "Iteration 7717, loss = 0.03161536\n",
      "Iteration 7718, loss = 0.03160826\n",
      "Iteration 7719, loss = 0.03160116\n",
      "Iteration 7720, loss = 0.03159406\n",
      "Iteration 7721, loss = 0.03158697\n",
      "Iteration 7722, loss = 0.03157988\n",
      "Iteration 7723, loss = 0.03157280\n",
      "Iteration 7724, loss = 0.03156572\n",
      "Iteration 7725, loss = 0.03155864\n",
      "Iteration 7726, loss = 0.03155156\n",
      "Iteration 7727, loss = 0.03154449\n",
      "Iteration 7728, loss = 0.03153742\n",
      "Iteration 7729, loss = 0.03153035\n",
      "Iteration 7730, loss = 0.03152329\n",
      "Iteration 7731, loss = 0.03151622\n",
      "Iteration 7732, loss = 0.03150917\n",
      "Iteration 7733, loss = 0.03150211\n",
      "Iteration 7734, loss = 0.03149506\n",
      "Iteration 7735, loss = 0.03148801\n",
      "Iteration 7736, loss = 0.03148096\n",
      "Iteration 7737, loss = 0.03147392\n",
      "Iteration 7738, loss = 0.03146688\n",
      "Iteration 7739, loss = 0.03145984\n",
      "Iteration 7740, loss = 0.03145280\n",
      "Iteration 7741, loss = 0.03144577\n",
      "Iteration 7742, loss = 0.03143874\n",
      "Iteration 7743, loss = 0.03143172\n",
      "Iteration 7744, loss = 0.03142469\n",
      "Iteration 7745, loss = 0.03141767\n",
      "Iteration 7746, loss = 0.03141066\n",
      "Iteration 7747, loss = 0.03140364\n",
      "Iteration 7748, loss = 0.03139663\n",
      "Iteration 7749, loss = 0.03138962\n",
      "Iteration 7750, loss = 0.03138262\n",
      "Iteration 7751, loss = 0.03137562\n",
      "Iteration 7752, loss = 0.03136862\n",
      "Iteration 7753, loss = 0.03136162\n",
      "Iteration 7754, loss = 0.03135463\n",
      "Iteration 7755, loss = 0.03134764\n",
      "Iteration 7756, loss = 0.03134065\n",
      "Iteration 7757, loss = 0.03133366\n",
      "Iteration 7758, loss = 0.03132668\n",
      "Iteration 7759, loss = 0.03131970\n",
      "Iteration 7760, loss = 0.03131273\n",
      "Iteration 7761, loss = 0.03130575\n",
      "Iteration 7762, loss = 0.03129878\n",
      "Iteration 7763, loss = 0.03129182\n",
      "Iteration 7764, loss = 0.03128485\n",
      "Iteration 7765, loss = 0.03127789\n",
      "Iteration 7766, loss = 0.03127093\n",
      "Iteration 7767, loss = 0.03126398\n",
      "Iteration 7768, loss = 0.03125703\n",
      "Iteration 7769, loss = 0.03125008\n",
      "Iteration 7770, loss = 0.03124313\n",
      "Iteration 7771, loss = 0.03123618\n",
      "Iteration 7772, loss = 0.03122924\n",
      "Iteration 7773, loss = 0.03122231\n",
      "Iteration 7774, loss = 0.03121537\n",
      "Iteration 7775, loss = 0.03120844\n",
      "Iteration 7776, loss = 0.03120151\n",
      "Iteration 7777, loss = 0.03119458\n",
      "Iteration 7778, loss = 0.03118766\n",
      "Iteration 7779, loss = 0.03118074\n",
      "Iteration 7780, loss = 0.03117382\n",
      "Iteration 7781, loss = 0.03116691\n",
      "Iteration 7782, loss = 0.03115999\n",
      "Iteration 7783, loss = 0.03115308\n",
      "Iteration 7784, loss = 0.03114618\n",
      "Iteration 7785, loss = 0.03113928\n",
      "Iteration 7786, loss = 0.03113237\n",
      "Iteration 7787, loss = 0.03112548\n",
      "Iteration 7788, loss = 0.03111858\n",
      "Iteration 7789, loss = 0.03111169\n",
      "Iteration 7790, loss = 0.03110480\n",
      "Iteration 7791, loss = 0.03109792\n",
      "Iteration 7792, loss = 0.03109103\n",
      "Iteration 7793, loss = 0.03108415\n",
      "Iteration 7794, loss = 0.03107728\n",
      "Iteration 7795, loss = 0.03107040\n",
      "Iteration 7796, loss = 0.03106353\n",
      "Iteration 7797, loss = 0.03105666\n",
      "Iteration 7798, loss = 0.03104979\n",
      "Iteration 7799, loss = 0.03104293\n",
      "Iteration 7800, loss = 0.03103607\n",
      "Iteration 7801, loss = 0.03102921\n",
      "Iteration 7802, loss = 0.03102236\n",
      "Iteration 7803, loss = 0.03101551\n",
      "Iteration 7804, loss = 0.03100866\n",
      "Iteration 7805, loss = 0.03100181\n",
      "Iteration 7806, loss = 0.03099497\n",
      "Iteration 7807, loss = 0.03098813\n",
      "Iteration 7808, loss = 0.03098129\n",
      "Iteration 7809, loss = 0.03097446\n",
      "Iteration 7810, loss = 0.03096763\n",
      "Iteration 7811, loss = 0.03096080\n",
      "Iteration 7812, loss = 0.03095397\n",
      "Iteration 7813, loss = 0.03094715\n",
      "Iteration 7814, loss = 0.03094033\n",
      "Iteration 7815, loss = 0.03093351\n",
      "Iteration 7816, loss = 0.03092669\n",
      "Iteration 7817, loss = 0.03091988\n",
      "Iteration 7818, loss = 0.03091307\n",
      "Iteration 7819, loss = 0.03090627\n",
      "Iteration 7820, loss = 0.03089946\n",
      "Iteration 7821, loss = 0.03089266\n",
      "Iteration 7822, loss = 0.03088587\n",
      "Iteration 7823, loss = 0.03087907\n",
      "Iteration 7824, loss = 0.03087228\n",
      "Iteration 7825, loss = 0.03086549\n",
      "Iteration 7826, loss = 0.03085870\n",
      "Iteration 7827, loss = 0.03085192\n",
      "Iteration 7828, loss = 0.03084514\n",
      "Iteration 7829, loss = 0.03083836\n",
      "Iteration 7830, loss = 0.03083158\n",
      "Iteration 7831, loss = 0.03082481\n",
      "Iteration 7832, loss = 0.03081804\n",
      "Iteration 7833, loss = 0.03081128\n",
      "Iteration 7834, loss = 0.03080451\n",
      "Iteration 7835, loss = 0.03079775\n",
      "Iteration 7836, loss = 0.03079099\n",
      "Iteration 7837, loss = 0.03078424\n",
      "Iteration 7838, loss = 0.03077748\n",
      "Iteration 7839, loss = 0.03077073\n",
      "Iteration 7840, loss = 0.03076399\n",
      "Iteration 7841, loss = 0.03075724\n",
      "Iteration 7842, loss = 0.03075050\n",
      "Iteration 7843, loss = 0.03074376\n",
      "Iteration 7844, loss = 0.03073703\n",
      "Iteration 7845, loss = 0.03073029\n",
      "Iteration 7846, loss = 0.03072356\n",
      "Iteration 7847, loss = 0.03071683\n",
      "Iteration 7848, loss = 0.03071011\n",
      "Iteration 7849, loss = 0.03070339\n",
      "Iteration 7850, loss = 0.03069667\n",
      "Iteration 7851, loss = 0.03068995\n",
      "Iteration 7852, loss = 0.03068324\n",
      "Iteration 7853, loss = 0.03067653\n",
      "Iteration 7854, loss = 0.03066982\n",
      "Iteration 7855, loss = 0.03066311\n",
      "Iteration 7856, loss = 0.03065641\n",
      "Iteration 7857, loss = 0.03064971\n",
      "Iteration 7858, loss = 0.03064301\n",
      "Iteration 7859, loss = 0.03063632\n",
      "Iteration 7860, loss = 0.03062963\n",
      "Iteration 7861, loss = 0.03062294\n",
      "Iteration 7862, loss = 0.03061625\n",
      "Iteration 7863, loss = 0.03060957\n",
      "Iteration 7864, loss = 0.03060289\n",
      "Iteration 7865, loss = 0.03059621\n",
      "Iteration 7866, loss = 0.03058954\n",
      "Iteration 7867, loss = 0.03058286\n",
      "Iteration 7868, loss = 0.03057619\n",
      "Iteration 7869, loss = 0.03056953\n",
      "Iteration 7870, loss = 0.03056286\n",
      "Iteration 7871, loss = 0.03055620\n",
      "Iteration 7872, loss = 0.03054954\n",
      "Iteration 7873, loss = 0.03054289\n",
      "Iteration 7874, loss = 0.03053623\n",
      "Iteration 7875, loss = 0.03052958\n",
      "Iteration 7876, loss = 0.03052294\n",
      "Iteration 7877, loss = 0.03051629\n",
      "Iteration 7878, loss = 0.03050965\n",
      "Iteration 7879, loss = 0.03050301\n",
      "Iteration 7880, loss = 0.03049637\n",
      "Iteration 7881, loss = 0.03048974\n",
      "Iteration 7882, loss = 0.03048311\n",
      "Iteration 7883, loss = 0.03047648\n",
      "Iteration 7884, loss = 0.03046985\n",
      "Iteration 7885, loss = 0.03046323\n",
      "Iteration 7886, loss = 0.03045661\n",
      "Iteration 7887, loss = 0.03044999\n",
      "Iteration 7888, loss = 0.03044338\n",
      "Iteration 7889, loss = 0.03043677\n",
      "Iteration 7890, loss = 0.03043016\n",
      "Iteration 7891, loss = 0.03042355\n",
      "Iteration 7892, loss = 0.03041694\n",
      "Iteration 7893, loss = 0.03041034\n",
      "Iteration 7894, loss = 0.03040374\n",
      "Iteration 7895, loss = 0.03039715\n",
      "Iteration 7896, loss = 0.03039056\n",
      "Iteration 7897, loss = 0.03038397\n",
      "Iteration 7898, loss = 0.03037738\n",
      "Iteration 7899, loss = 0.03037079\n",
      "Iteration 7900, loss = 0.03036421\n",
      "Iteration 7901, loss = 0.03035763\n",
      "Iteration 7902, loss = 0.03035105\n",
      "Iteration 7903, loss = 0.03034448\n",
      "Iteration 7904, loss = 0.03033791\n",
      "Iteration 7905, loss = 0.03033134\n",
      "Iteration 7906, loss = 0.03032477\n",
      "Iteration 7907, loss = 0.03031821\n",
      "Iteration 7908, loss = 0.03031165\n",
      "Iteration 7909, loss = 0.03030509\n",
      "Iteration 7910, loss = 0.03029853\n",
      "Iteration 7911, loss = 0.03029198\n",
      "Iteration 7912, loss = 0.03028543\n",
      "Iteration 7913, loss = 0.03027888\n",
      "Iteration 7914, loss = 0.03027234\n",
      "Iteration 7915, loss = 0.03026580\n",
      "Iteration 7916, loss = 0.03025926\n",
      "Iteration 7917, loss = 0.03025272\n",
      "Iteration 7918, loss = 0.03024619\n",
      "Iteration 7919, loss = 0.03023966\n",
      "Iteration 7920, loss = 0.03023313\n",
      "Iteration 7921, loss = 0.03022660\n",
      "Iteration 7922, loss = 0.03022008\n",
      "Iteration 7923, loss = 0.03021356\n",
      "Iteration 7924, loss = 0.03020704\n",
      "Iteration 7925, loss = 0.03020052\n",
      "Iteration 7926, loss = 0.03019401\n",
      "Iteration 7927, loss = 0.03018750\n",
      "Iteration 7928, loss = 0.03018099\n",
      "Iteration 7929, loss = 0.03017449\n",
      "Iteration 7930, loss = 0.03016799\n",
      "Iteration 7931, loss = 0.03016149\n",
      "Iteration 7932, loss = 0.03015499\n",
      "Iteration 7933, loss = 0.03014850\n",
      "Iteration 7934, loss = 0.03014200\n",
      "Iteration 7935, loss = 0.03013552\n",
      "Iteration 7936, loss = 0.03012903\n",
      "Iteration 7937, loss = 0.03012255\n",
      "Iteration 7938, loss = 0.03011606\n",
      "Iteration 7939, loss = 0.03010959\n",
      "Iteration 7940, loss = 0.03010311\n",
      "Iteration 7941, loss = 0.03009664\n",
      "Iteration 7942, loss = 0.03009017\n",
      "Iteration 7943, loss = 0.03008370\n",
      "Iteration 7944, loss = 0.03007723\n",
      "Iteration 7945, loss = 0.03007077\n",
      "Iteration 7946, loss = 0.03006431\n",
      "Iteration 7947, loss = 0.03005786\n",
      "Iteration 7948, loss = 0.03005140\n",
      "Iteration 7949, loss = 0.03004495\n",
      "Iteration 7950, loss = 0.03003850\n",
      "Iteration 7951, loss = 0.03003205\n",
      "Iteration 7952, loss = 0.03002561\n",
      "Iteration 7953, loss = 0.03001917\n",
      "Iteration 7954, loss = 0.03001273\n",
      "Iteration 7955, loss = 0.03000629\n",
      "Iteration 7956, loss = 0.02999986\n",
      "Iteration 7957, loss = 0.02999343\n",
      "Iteration 7958, loss = 0.02998700\n",
      "Iteration 7959, loss = 0.02998057\n",
      "Iteration 7960, loss = 0.02997415\n",
      "Iteration 7961, loss = 0.02996773\n",
      "Iteration 7962, loss = 0.02996131\n",
      "Iteration 7963, loss = 0.02995490\n",
      "Iteration 7964, loss = 0.02994848\n",
      "Iteration 7965, loss = 0.02994207\n",
      "Iteration 7966, loss = 0.02993567\n",
      "Iteration 7967, loss = 0.02992926\n",
      "Iteration 7968, loss = 0.02992286\n",
      "Iteration 7969, loss = 0.02991646\n",
      "Iteration 7970, loss = 0.02991006\n",
      "Iteration 7971, loss = 0.02990367\n",
      "Iteration 7972, loss = 0.02989728\n",
      "Iteration 7973, loss = 0.02989089\n",
      "Iteration 7974, loss = 0.02988450\n",
      "Iteration 7975, loss = 0.02987812\n",
      "Iteration 7976, loss = 0.02987173\n",
      "Iteration 7977, loss = 0.02986536\n",
      "Iteration 7978, loss = 0.02985898\n",
      "Iteration 7979, loss = 0.02985260\n",
      "Iteration 7980, loss = 0.02984623\n",
      "Iteration 7981, loss = 0.02983986\n",
      "Iteration 7982, loss = 0.02983350\n",
      "Iteration 7983, loss = 0.02982714\n",
      "Iteration 7984, loss = 0.02982077\n",
      "Iteration 7985, loss = 0.02981442\n",
      "Iteration 7986, loss = 0.02980806\n",
      "Iteration 7987, loss = 0.02980171\n",
      "Iteration 7988, loss = 0.02979536\n",
      "Iteration 7989, loss = 0.02978901\n",
      "Iteration 7990, loss = 0.02978266\n",
      "Iteration 7991, loss = 0.02977632\n",
      "Iteration 7992, loss = 0.02976998\n",
      "Iteration 7993, loss = 0.02976364\n",
      "Iteration 7994, loss = 0.02975731\n",
      "Iteration 7995, loss = 0.02975097\n",
      "Iteration 7996, loss = 0.02974464\n",
      "Iteration 7997, loss = 0.02973832\n",
      "Iteration 7998, loss = 0.02973199\n",
      "Iteration 7999, loss = 0.02972567\n",
      "Iteration 8000, loss = 0.02971935\n",
      "Iteration 8001, loss = 0.02971303\n",
      "Iteration 8002, loss = 0.02970672\n",
      "Iteration 8003, loss = 0.02970040\n",
      "Iteration 8004, loss = 0.02969409\n",
      "Iteration 8005, loss = 0.02968779\n",
      "Iteration 8006, loss = 0.02968148\n",
      "Iteration 8007, loss = 0.02967518\n",
      "Iteration 8008, loss = 0.02966888\n",
      "Iteration 8009, loss = 0.02966258\n",
      "Iteration 8010, loss = 0.02965629\n",
      "Iteration 8011, loss = 0.02965000\n",
      "Iteration 8012, loss = 0.02964371\n",
      "Iteration 8013, loss = 0.02963742\n",
      "Iteration 8014, loss = 0.02963114\n",
      "Iteration 8015, loss = 0.02962486\n",
      "Iteration 8016, loss = 0.02961858\n",
      "Iteration 8017, loss = 0.02961230\n",
      "Iteration 8018, loss = 0.02960603\n",
      "Iteration 8019, loss = 0.02959975\n",
      "Iteration 8020, loss = 0.02959348\n",
      "Iteration 8021, loss = 0.02958722\n",
      "Iteration 8022, loss = 0.02958095\n",
      "Iteration 8023, loss = 0.02957469\n",
      "Iteration 8024, loss = 0.02956843\n",
      "Iteration 8025, loss = 0.02956218\n",
      "Iteration 8026, loss = 0.02955592\n",
      "Iteration 8027, loss = 0.02954967\n",
      "Iteration 8028, loss = 0.02954342\n",
      "Iteration 8029, loss = 0.02953718\n",
      "Iteration 8030, loss = 0.02953093\n",
      "Iteration 8031, loss = 0.02952469\n",
      "Iteration 8032, loss = 0.02951845\n",
      "Iteration 8033, loss = 0.02951221\n",
      "Iteration 8034, loss = 0.02950598\n",
      "Iteration 8035, loss = 0.02949975\n",
      "Iteration 8036, loss = 0.02949352\n",
      "Iteration 8037, loss = 0.02948729\n",
      "Iteration 8038, loss = 0.02948107\n",
      "Iteration 8039, loss = 0.02947485\n",
      "Iteration 8040, loss = 0.02946863\n",
      "Iteration 8041, loss = 0.02946241\n",
      "Iteration 8042, loss = 0.02945620\n",
      "Iteration 8043, loss = 0.02944999\n",
      "Iteration 8044, loss = 0.02944378\n",
      "Iteration 8045, loss = 0.02943757\n",
      "Iteration 8046, loss = 0.02943137\n",
      "Iteration 8047, loss = 0.02942517\n",
      "Iteration 8048, loss = 0.02941897\n",
      "Iteration 8049, loss = 0.02941277\n",
      "Iteration 8050, loss = 0.02940658\n",
      "Iteration 8051, loss = 0.02940038\n",
      "Iteration 8052, loss = 0.02939420\n",
      "Iteration 8053, loss = 0.02938801\n",
      "Iteration 8054, loss = 0.02938182\n",
      "Iteration 8055, loss = 0.02937564\n",
      "Iteration 8056, loss = 0.02936946\n",
      "Iteration 8057, loss = 0.02936329\n",
      "Iteration 8058, loss = 0.02935711\n",
      "Iteration 8059, loss = 0.02935094\n",
      "Iteration 8060, loss = 0.02934477\n",
      "Iteration 8061, loss = 0.02933860\n",
      "Iteration 8062, loss = 0.02933244\n",
      "Iteration 8063, loss = 0.02932628\n",
      "Iteration 8064, loss = 0.02932012\n",
      "Iteration 8065, loss = 0.02931396\n",
      "Iteration 8066, loss = 0.02930781\n",
      "Iteration 8067, loss = 0.02930166\n",
      "Iteration 8068, loss = 0.02929551\n",
      "Iteration 8069, loss = 0.02928936\n",
      "Iteration 8070, loss = 0.02928321\n",
      "Iteration 8071, loss = 0.02927707\n",
      "Iteration 8072, loss = 0.02927093\n",
      "Iteration 8073, loss = 0.02926479\n",
      "Iteration 8074, loss = 0.02925866\n",
      "Iteration 8075, loss = 0.02925253\n",
      "Iteration 8076, loss = 0.02924640\n",
      "Iteration 8077, loss = 0.02924027\n",
      "Iteration 8078, loss = 0.02923414\n",
      "Iteration 8079, loss = 0.02922802\n",
      "Iteration 8080, loss = 0.02922190\n",
      "Iteration 8081, loss = 0.02921578\n",
      "Iteration 8082, loss = 0.02920967\n",
      "Iteration 8083, loss = 0.02920355\n",
      "Iteration 8084, loss = 0.02919744\n",
      "Iteration 8085, loss = 0.02919134\n",
      "Iteration 8086, loss = 0.02918523\n",
      "Iteration 8087, loss = 0.02917913\n",
      "Iteration 8088, loss = 0.02917303\n",
      "Iteration 8089, loss = 0.02916693\n",
      "Iteration 8090, loss = 0.02916083\n",
      "Iteration 8091, loss = 0.02915474\n",
      "Iteration 8092, loss = 0.02914865\n",
      "Iteration 8093, loss = 0.02914256\n",
      "Iteration 8094, loss = 0.02913647\n",
      "Iteration 8095, loss = 0.02913039\n",
      "Iteration 8096, loss = 0.02912431\n",
      "Iteration 8097, loss = 0.02911823\n",
      "Iteration 8098, loss = 0.02911215\n",
      "Iteration 8099, loss = 0.02910608\n",
      "Iteration 8100, loss = 0.02910001\n",
      "Iteration 8101, loss = 0.02909394\n",
      "Iteration 8102, loss = 0.02908787\n",
      "Iteration 8103, loss = 0.02908181\n",
      "Iteration 8104, loss = 0.02907574\n",
      "Iteration 8105, loss = 0.02906968\n",
      "Iteration 8106, loss = 0.02906363\n",
      "Iteration 8107, loss = 0.02905757\n",
      "Iteration 8108, loss = 0.02905152\n",
      "Iteration 8109, loss = 0.02904547\n",
      "Iteration 8110, loss = 0.02903942\n",
      "Iteration 8111, loss = 0.02903338\n",
      "Iteration 8112, loss = 0.02902733\n",
      "Iteration 8113, loss = 0.02902129\n",
      "Iteration 8114, loss = 0.02901526\n",
      "Iteration 8115, loss = 0.02900922\n",
      "Iteration 8116, loss = 0.02900319\n",
      "Iteration 8117, loss = 0.02899716\n",
      "Iteration 8118, loss = 0.02899113\n",
      "Iteration 8119, loss = 0.02898510\n",
      "Iteration 8120, loss = 0.02897908\n",
      "Iteration 8121, loss = 0.02897306\n",
      "Iteration 8122, loss = 0.02896704\n",
      "Iteration 8123, loss = 0.02896102\n",
      "Iteration 8124, loss = 0.02895501\n",
      "Iteration 8125, loss = 0.02894899\n",
      "Iteration 8126, loss = 0.02894299\n",
      "Iteration 8127, loss = 0.02893698\n",
      "Iteration 8128, loss = 0.02893097\n",
      "Iteration 8129, loss = 0.02892497\n",
      "Iteration 8130, loss = 0.02891897\n",
      "Iteration 8131, loss = 0.02891297\n",
      "Iteration 8132, loss = 0.02890698\n",
      "Iteration 8133, loss = 0.02890099\n",
      "Iteration 8134, loss = 0.02889500\n",
      "Iteration 8135, loss = 0.02888901\n",
      "Iteration 8136, loss = 0.02888302\n",
      "Iteration 8137, loss = 0.02887704\n",
      "Iteration 8138, loss = 0.02887106\n",
      "Iteration 8139, loss = 0.02886508\n",
      "Iteration 8140, loss = 0.02885910\n",
      "Iteration 8141, loss = 0.02885313\n",
      "Iteration 8142, loss = 0.02884716\n",
      "Iteration 8143, loss = 0.02884119\n",
      "Iteration 8144, loss = 0.02883522\n",
      "Iteration 8145, loss = 0.02882926\n",
      "Iteration 8146, loss = 0.02882329\n",
      "Iteration 8147, loss = 0.02881734\n",
      "Iteration 8148, loss = 0.02881138\n",
      "Iteration 8149, loss = 0.02880542\n",
      "Iteration 8150, loss = 0.02879947\n",
      "Iteration 8151, loss = 0.02879352\n",
      "Iteration 8152, loss = 0.02878757\n",
      "Iteration 8153, loss = 0.02878163\n",
      "Iteration 8154, loss = 0.02877568\n",
      "Iteration 8155, loss = 0.02876974\n",
      "Iteration 8156, loss = 0.02876380\n",
      "Iteration 8157, loss = 0.02875787\n",
      "Iteration 8158, loss = 0.02875193\n",
      "Iteration 8159, loss = 0.02874600\n",
      "Iteration 8160, loss = 0.02874007\n",
      "Iteration 8161, loss = 0.02873415\n",
      "Iteration 8162, loss = 0.02872822\n",
      "Iteration 8163, loss = 0.02872230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8164, loss = 0.02871638\n",
      "Iteration 8165, loss = 0.02871046\n",
      "Iteration 8166, loss = 0.02870455\n",
      "Iteration 8167, loss = 0.02869863\n",
      "Iteration 8168, loss = 0.02869272\n",
      "Iteration 8169, loss = 0.02868681\n",
      "Iteration 8170, loss = 0.02868091\n",
      "Iteration 8171, loss = 0.02867500\n",
      "Iteration 8172, loss = 0.02866910\n",
      "Iteration 8173, loss = 0.02866320\n",
      "Iteration 8174, loss = 0.02865731\n",
      "Iteration 8175, loss = 0.02865141\n",
      "Iteration 8176, loss = 0.02864552\n",
      "Iteration 8177, loss = 0.02863963\n",
      "Iteration 8178, loss = 0.02863374\n",
      "Iteration 8179, loss = 0.02862786\n",
      "Iteration 8180, loss = 0.02862198\n",
      "Iteration 8181, loss = 0.02861610\n",
      "Iteration 8182, loss = 0.02861022\n",
      "Iteration 8183, loss = 0.02860434\n",
      "Iteration 8184, loss = 0.02859847\n",
      "Iteration 8185, loss = 0.02859260\n",
      "Iteration 8186, loss = 0.02858673\n",
      "Iteration 8187, loss = 0.02858086\n",
      "Iteration 8188, loss = 0.02857500\n",
      "Iteration 8189, loss = 0.02856914\n",
      "Iteration 8190, loss = 0.02856328\n",
      "Iteration 8191, loss = 0.02855742\n",
      "Iteration 8192, loss = 0.02855156\n",
      "Iteration 8193, loss = 0.02854571\n",
      "Iteration 8194, loss = 0.02853986\n",
      "Iteration 8195, loss = 0.02853401\n",
      "Iteration 8196, loss = 0.02852817\n",
      "Iteration 8197, loss = 0.02852232\n",
      "Iteration 8198, loss = 0.02851648\n",
      "Iteration 8199, loss = 0.02851064\n",
      "Iteration 8200, loss = 0.02850480\n",
      "Iteration 8201, loss = 0.02849897\n",
      "Iteration 8202, loss = 0.02849314\n",
      "Iteration 8203, loss = 0.02848731\n",
      "Iteration 8204, loss = 0.02848148\n",
      "Iteration 8205, loss = 0.02847565\n",
      "Iteration 8206, loss = 0.02846983\n",
      "Iteration 8207, loss = 0.02846401\n",
      "Iteration 8208, loss = 0.02845819\n",
      "Iteration 8209, loss = 0.02845238\n",
      "Iteration 8210, loss = 0.02844656\n",
      "Iteration 8211, loss = 0.02844075\n",
      "Iteration 8212, loss = 0.02843494\n",
      "Iteration 8213, loss = 0.02842913\n",
      "Iteration 8214, loss = 0.02842333\n",
      "Iteration 8215, loss = 0.02841753\n",
      "Iteration 8216, loss = 0.02841172\n",
      "Iteration 8217, loss = 0.02840593\n",
      "Iteration 8218, loss = 0.02840013\n",
      "Iteration 8219, loss = 0.02839434\n",
      "Iteration 8220, loss = 0.02838855\n",
      "Iteration 8221, loss = 0.02838276\n",
      "Iteration 8222, loss = 0.02837697\n",
      "Iteration 8223, loss = 0.02837119\n",
      "Iteration 8224, loss = 0.02836540\n",
      "Iteration 8225, loss = 0.02835962\n",
      "Iteration 8226, loss = 0.02835385\n",
      "Iteration 8227, loss = 0.02834807\n",
      "Iteration 8228, loss = 0.02834230\n",
      "Iteration 8229, loss = 0.02833653\n",
      "Iteration 8230, loss = 0.02833076\n",
      "Iteration 8231, loss = 0.02832499\n",
      "Iteration 8232, loss = 0.02831923\n",
      "Iteration 8233, loss = 0.02831346\n",
      "Iteration 8234, loss = 0.02830770\n",
      "Iteration 8235, loss = 0.02830195\n",
      "Iteration 8236, loss = 0.02829619\n",
      "Iteration 8237, loss = 0.02829044\n",
      "Iteration 8238, loss = 0.02828469\n",
      "Iteration 8239, loss = 0.02827894\n",
      "Iteration 8240, loss = 0.02827319\n",
      "Iteration 8241, loss = 0.02826745\n",
      "Iteration 8242, loss = 0.02826171\n",
      "Iteration 8243, loss = 0.02825597\n",
      "Iteration 8244, loss = 0.02825023\n",
      "Iteration 8245, loss = 0.02824449\n",
      "Iteration 8246, loss = 0.02823876\n",
      "Iteration 8247, loss = 0.02823303\n",
      "Iteration 8248, loss = 0.02822730\n",
      "Iteration 8249, loss = 0.02822157\n",
      "Iteration 8250, loss = 0.02821585\n",
      "Iteration 8251, loss = 0.02821013\n",
      "Iteration 8252, loss = 0.02820441\n",
      "Iteration 8253, loss = 0.02819869\n",
      "Iteration 8254, loss = 0.02819298\n",
      "Iteration 8255, loss = 0.02818726\n",
      "Iteration 8256, loss = 0.02818155\n",
      "Iteration 8257, loss = 0.02817584\n",
      "Iteration 8258, loss = 0.02817014\n",
      "Iteration 8259, loss = 0.02816443\n",
      "Iteration 8260, loss = 0.02815873\n",
      "Iteration 8261, loss = 0.02815303\n",
      "Iteration 8262, loss = 0.02814733\n",
      "Iteration 8263, loss = 0.02814164\n",
      "Iteration 8264, loss = 0.02813595\n",
      "Iteration 8265, loss = 0.02813026\n",
      "Iteration 8266, loss = 0.02812457\n",
      "Iteration 8267, loss = 0.02811888\n",
      "Iteration 8268, loss = 0.02811320\n",
      "Iteration 8269, loss = 0.02810751\n",
      "Iteration 8270, loss = 0.02810183\n",
      "Iteration 8271, loss = 0.02809616\n",
      "Iteration 8272, loss = 0.02809048\n",
      "Iteration 8273, loss = 0.02808481\n",
      "Iteration 8274, loss = 0.02807914\n",
      "Iteration 8275, loss = 0.02807347\n",
      "Iteration 8276, loss = 0.02806780\n",
      "Iteration 8277, loss = 0.02806214\n",
      "Iteration 8278, loss = 0.02805647\n",
      "Iteration 8279, loss = 0.02805081\n",
      "Iteration 8280, loss = 0.02804516\n",
      "Iteration 8281, loss = 0.02803950\n",
      "Iteration 8282, loss = 0.02803385\n",
      "Iteration 8283, loss = 0.02802820\n",
      "Iteration 8284, loss = 0.02802255\n",
      "Iteration 8285, loss = 0.02801690\n",
      "Iteration 8286, loss = 0.02801126\n",
      "Iteration 8287, loss = 0.02800561\n",
      "Iteration 8288, loss = 0.02799997\n",
      "Iteration 8289, loss = 0.02799433\n",
      "Iteration 8290, loss = 0.02798870\n",
      "Iteration 8291, loss = 0.02798306\n",
      "Iteration 8292, loss = 0.02797743\n",
      "Iteration 8293, loss = 0.02797180\n",
      "Iteration 8294, loss = 0.02796618\n",
      "Iteration 8295, loss = 0.02796055\n",
      "Iteration 8296, loss = 0.02795493\n",
      "Iteration 8297, loss = 0.02794931\n",
      "Iteration 8298, loss = 0.02794369\n",
      "Iteration 8299, loss = 0.02793807\n",
      "Iteration 8300, loss = 0.02793246\n",
      "Iteration 8301, loss = 0.02792684\n",
      "Iteration 8302, loss = 0.02792123\n",
      "Iteration 8303, loss = 0.02791563\n",
      "Iteration 8304, loss = 0.02791002\n",
      "Iteration 8305, loss = 0.02790442\n",
      "Iteration 8306, loss = 0.02789882\n",
      "Iteration 8307, loss = 0.02789322\n",
      "Iteration 8308, loss = 0.02788762\n",
      "Iteration 8309, loss = 0.02788202\n",
      "Iteration 8310, loss = 0.02787643\n",
      "Iteration 8311, loss = 0.02787084\n",
      "Iteration 8312, loss = 0.02786525\n",
      "Iteration 8313, loss = 0.02785967\n",
      "Iteration 8314, loss = 0.02785408\n",
      "Iteration 8315, loss = 0.02784850\n",
      "Iteration 8316, loss = 0.02784292\n",
      "Iteration 8317, loss = 0.02783734\n",
      "Iteration 8318, loss = 0.02783177\n",
      "Iteration 8319, loss = 0.02782619\n",
      "Iteration 8320, loss = 0.02782062\n",
      "Iteration 8321, loss = 0.02781505\n",
      "Iteration 8322, loss = 0.02780949\n",
      "Iteration 8323, loss = 0.02780392\n",
      "Iteration 8324, loss = 0.02779836\n",
      "Iteration 8325, loss = 0.02779280\n",
      "Iteration 8326, loss = 0.02778724\n",
      "Iteration 8327, loss = 0.02778168\n",
      "Iteration 8328, loss = 0.02777613\n",
      "Iteration 8329, loss = 0.02777058\n",
      "Iteration 8330, loss = 0.02776503\n",
      "Iteration 8331, loss = 0.02775948\n",
      "Iteration 8332, loss = 0.02775393\n",
      "Iteration 8333, loss = 0.02774839\n",
      "Iteration 8334, loss = 0.02774285\n",
      "Iteration 8335, loss = 0.02773731\n",
      "Iteration 8336, loss = 0.02773177\n",
      "Iteration 8337, loss = 0.02772624\n",
      "Iteration 8338, loss = 0.02772070\n",
      "Iteration 8339, loss = 0.02771517\n",
      "Iteration 8340, loss = 0.02770965\n",
      "Iteration 8341, loss = 0.02770412\n",
      "Iteration 8342, loss = 0.02769859\n",
      "Iteration 8343, loss = 0.02769307\n",
      "Iteration 8344, loss = 0.02768755\n",
      "Iteration 8345, loss = 0.02768203\n",
      "Iteration 8346, loss = 0.02767652\n",
      "Iteration 8347, loss = 0.02767100\n",
      "Iteration 8348, loss = 0.02766549\n",
      "Iteration 8349, loss = 0.02765998\n",
      "Iteration 8350, loss = 0.02765448\n",
      "Iteration 8351, loss = 0.02764897\n",
      "Iteration 8352, loss = 0.02764347\n",
      "Iteration 8353, loss = 0.02763797\n",
      "Iteration 8354, loss = 0.02763247\n",
      "Iteration 8355, loss = 0.02762697\n",
      "Iteration 8356, loss = 0.02762147\n",
      "Iteration 8357, loss = 0.02761598\n",
      "Iteration 8358, loss = 0.02761049\n",
      "Iteration 8359, loss = 0.02760500\n",
      "Iteration 8360, loss = 0.02759952\n",
      "Iteration 8361, loss = 0.02759403\n",
      "Iteration 8362, loss = 0.02758855\n",
      "Iteration 8363, loss = 0.02758307\n",
      "Iteration 8364, loss = 0.02757759\n",
      "Iteration 8365, loss = 0.02757212\n",
      "Iteration 8366, loss = 0.02756664\n",
      "Iteration 8367, loss = 0.02756117\n",
      "Iteration 8368, loss = 0.02755570\n",
      "Iteration 8369, loss = 0.02755023\n",
      "Iteration 8370, loss = 0.02754477\n",
      "Iteration 8371, loss = 0.02753930\n",
      "Iteration 8372, loss = 0.02753384\n",
      "Iteration 8373, loss = 0.02752838\n",
      "Iteration 8374, loss = 0.02752292\n",
      "Iteration 8375, loss = 0.02751747\n",
      "Iteration 8376, loss = 0.02751202\n",
      "Iteration 8377, loss = 0.02750657\n",
      "Iteration 8378, loss = 0.02750112\n",
      "Iteration 8379, loss = 0.02749567\n",
      "Iteration 8380, loss = 0.02749023\n",
      "Iteration 8381, loss = 0.02748478\n",
      "Iteration 8382, loss = 0.02747934\n",
      "Iteration 8383, loss = 0.02747390\n",
      "Iteration 8384, loss = 0.02746847\n",
      "Iteration 8385, loss = 0.02746303\n",
      "Iteration 8386, loss = 0.02745760\n",
      "Iteration 8387, loss = 0.02745217\n",
      "Iteration 8388, loss = 0.02744674\n",
      "Iteration 8389, loss = 0.02744132\n",
      "Iteration 8390, loss = 0.02743589\n",
      "Iteration 8391, loss = 0.02743047\n",
      "Iteration 8392, loss = 0.02742505\n",
      "Iteration 8393, loss = 0.02741963\n",
      "Iteration 8394, loss = 0.02741422\n",
      "Iteration 8395, loss = 0.02740880\n",
      "Iteration 8396, loss = 0.02740339\n",
      "Iteration 8397, loss = 0.02739798\n",
      "Iteration 8398, loss = 0.02739257\n",
      "Iteration 8399, loss = 0.02738717\n",
      "Iteration 8400, loss = 0.02738176\n",
      "Iteration 8401, loss = 0.02737636\n",
      "Iteration 8402, loss = 0.02737096\n",
      "Iteration 8403, loss = 0.02736557\n",
      "Iteration 8404, loss = 0.02736017\n",
      "Iteration 8405, loss = 0.02735478\n",
      "Iteration 8406, loss = 0.02734939\n",
      "Iteration 8407, loss = 0.02734400\n",
      "Iteration 8408, loss = 0.02733861\n",
      "Iteration 8409, loss = 0.02733323\n",
      "Iteration 8410, loss = 0.02732784\n",
      "Iteration 8411, loss = 0.02732246\n",
      "Iteration 8412, loss = 0.02731708\n",
      "Iteration 8413, loss = 0.02731171\n",
      "Iteration 8414, loss = 0.02730633\n",
      "Iteration 8415, loss = 0.02730096\n",
      "Iteration 8416, loss = 0.02729559\n",
      "Iteration 8417, loss = 0.02729022\n",
      "Iteration 8418, loss = 0.02728485\n",
      "Iteration 8419, loss = 0.02727949\n",
      "Iteration 8420, loss = 0.02727412\n",
      "Iteration 8421, loss = 0.02726876\n",
      "Iteration 8422, loss = 0.02726341\n",
      "Iteration 8423, loss = 0.02725805\n",
      "Iteration 8424, loss = 0.02725269\n",
      "Iteration 8425, loss = 0.02724734\n",
      "Iteration 8426, loss = 0.02724199\n",
      "Iteration 8427, loss = 0.02723664\n",
      "Iteration 8428, loss = 0.02723130\n",
      "Iteration 8429, loss = 0.02722595\n",
      "Iteration 8430, loss = 0.02722061\n",
      "Iteration 8431, loss = 0.02721527\n",
      "Iteration 8432, loss = 0.02720993\n",
      "Iteration 8433, loss = 0.02720460\n",
      "Iteration 8434, loss = 0.02719926\n",
      "Iteration 8435, loss = 0.02719393\n",
      "Iteration 8436, loss = 0.02718860\n",
      "Iteration 8437, loss = 0.02718327\n",
      "Iteration 8438, loss = 0.02717794\n",
      "Iteration 8439, loss = 0.02717262\n",
      "Iteration 8440, loss = 0.02716730\n",
      "Iteration 8441, loss = 0.02716198\n",
      "Iteration 8442, loss = 0.02715666\n",
      "Iteration 8443, loss = 0.02715134\n",
      "Iteration 8444, loss = 0.02714603\n",
      "Iteration 8445, loss = 0.02714072\n",
      "Iteration 8446, loss = 0.02713541\n",
      "Iteration 8447, loss = 0.02713010\n",
      "Iteration 8448, loss = 0.02712479\n",
      "Iteration 8449, loss = 0.02711949\n",
      "Iteration 8450, loss = 0.02711419\n",
      "Iteration 8451, loss = 0.02710889\n",
      "Iteration 8452, loss = 0.02710359\n",
      "Iteration 8453, loss = 0.02709829\n",
      "Iteration 8454, loss = 0.02709300\n",
      "Iteration 8455, loss = 0.02708771\n",
      "Iteration 8456, loss = 0.02708242\n",
      "Iteration 8457, loss = 0.02707713\n",
      "Iteration 8458, loss = 0.02707184\n",
      "Iteration 8459, loss = 0.02706656\n",
      "Iteration 8460, loss = 0.02706127\n",
      "Iteration 8461, loss = 0.02705599\n",
      "Iteration 8462, loss = 0.02705072\n",
      "Iteration 8463, loss = 0.02704544\n",
      "Iteration 8464, loss = 0.02704017\n",
      "Iteration 8465, loss = 0.02703489\n",
      "Iteration 8466, loss = 0.02702962\n",
      "Iteration 8467, loss = 0.02702435\n",
      "Iteration 8468, loss = 0.02701909\n",
      "Iteration 8469, loss = 0.02701382\n",
      "Iteration 8470, loss = 0.02700856\n",
      "Iteration 8471, loss = 0.02700330\n",
      "Iteration 8472, loss = 0.02699804\n",
      "Iteration 8473, loss = 0.02699279\n",
      "Iteration 8474, loss = 0.02698753\n",
      "Iteration 8475, loss = 0.02698228\n",
      "Iteration 8476, loss = 0.02697703\n",
      "Iteration 8477, loss = 0.02697178\n",
      "Iteration 8478, loss = 0.02696653\n",
      "Iteration 8479, loss = 0.02696129\n",
      "Iteration 8480, loss = 0.02695605\n",
      "Iteration 8481, loss = 0.02695081\n",
      "Iteration 8482, loss = 0.02694557\n",
      "Iteration 8483, loss = 0.02694033\n",
      "Iteration 8484, loss = 0.02693510\n",
      "Iteration 8485, loss = 0.02692986\n",
      "Iteration 8486, loss = 0.02692463\n",
      "Iteration 8487, loss = 0.02691940\n",
      "Iteration 8488, loss = 0.02691418\n",
      "Iteration 8489, loss = 0.02690895\n",
      "Iteration 8490, loss = 0.02690373\n",
      "Iteration 8491, loss = 0.02689851\n",
      "Iteration 8492, loss = 0.02689329\n",
      "Iteration 8493, loss = 0.02688807\n",
      "Iteration 8494, loss = 0.02688286\n",
      "Iteration 8495, loss = 0.02687764\n",
      "Iteration 8496, loss = 0.02687243\n",
      "Iteration 8497, loss = 0.02686722\n",
      "Iteration 8498, loss = 0.02686202\n",
      "Iteration 8499, loss = 0.02685681\n",
      "Iteration 8500, loss = 0.02685161\n",
      "Iteration 8501, loss = 0.02684641\n",
      "Iteration 8502, loss = 0.02684121\n",
      "Iteration 8503, loss = 0.02683601\n",
      "Iteration 8504, loss = 0.02683081\n",
      "Iteration 8505, loss = 0.02682562\n",
      "Iteration 8506, loss = 0.02682043\n",
      "Iteration 8507, loss = 0.02681524\n",
      "Iteration 8508, loss = 0.02681005\n",
      "Iteration 8509, loss = 0.02680486\n",
      "Iteration 8510, loss = 0.02679968\n",
      "Iteration 8511, loss = 0.02679450\n",
      "Iteration 8512, loss = 0.02678932\n",
      "Iteration 8513, loss = 0.02678414\n",
      "Iteration 8514, loss = 0.02677896\n",
      "Iteration 8515, loss = 0.02677379\n",
      "Iteration 8516, loss = 0.02676862\n",
      "Iteration 8517, loss = 0.02676345\n",
      "Iteration 8518, loss = 0.02675828\n",
      "Iteration 8519, loss = 0.02675311\n",
      "Iteration 8520, loss = 0.02674795\n",
      "Iteration 8521, loss = 0.02674278\n",
      "Iteration 8522, loss = 0.02673762\n",
      "Iteration 8523, loss = 0.02673246\n",
      "Iteration 8524, loss = 0.02672731\n",
      "Iteration 8525, loss = 0.02672215\n",
      "Iteration 8526, loss = 0.02671700\n",
      "Iteration 8527, loss = 0.02671185\n",
      "Iteration 8528, loss = 0.02670670\n",
      "Iteration 8529, loss = 0.02670155\n",
      "Iteration 8530, loss = 0.02669641\n",
      "Iteration 8531, loss = 0.02669126\n",
      "Iteration 8532, loss = 0.02668612\n",
      "Iteration 8533, loss = 0.02668098\n",
      "Iteration 8534, loss = 0.02667584\n",
      "Iteration 8535, loss = 0.02667071\n",
      "Iteration 8536, loss = 0.02666557\n",
      "Iteration 8537, loss = 0.02666044\n",
      "Iteration 8538, loss = 0.02665531\n",
      "Iteration 8539, loss = 0.02665018\n",
      "Iteration 8540, loss = 0.02664506\n",
      "Iteration 8541, loss = 0.02663993\n",
      "Iteration 8542, loss = 0.02663481\n",
      "Iteration 8543, loss = 0.02662969\n",
      "Iteration 8544, loss = 0.02662457\n",
      "Iteration 8545, loss = 0.02661945\n",
      "Iteration 8546, loss = 0.02661434\n",
      "Iteration 8547, loss = 0.02660923\n",
      "Iteration 8548, loss = 0.02660412\n",
      "Iteration 8549, loss = 0.02659901\n",
      "Iteration 8550, loss = 0.02659390\n",
      "Iteration 8551, loss = 0.02658879\n",
      "Iteration 8552, loss = 0.02658369\n",
      "Iteration 8553, loss = 0.02657859\n",
      "Iteration 8554, loss = 0.02657349\n",
      "Iteration 8555, loss = 0.02656839\n",
      "Iteration 8556, loss = 0.02656330\n",
      "Iteration 8557, loss = 0.02655820\n",
      "Iteration 8558, loss = 0.02655311\n",
      "Iteration 8559, loss = 0.02654802\n",
      "Iteration 8560, loss = 0.02654293\n",
      "Iteration 8561, loss = 0.02653784\n",
      "Iteration 8562, loss = 0.02653276\n",
      "Iteration 8563, loss = 0.02652768\n",
      "Iteration 8564, loss = 0.02652260\n",
      "Iteration 8565, loss = 0.02651752\n",
      "Iteration 8566, loss = 0.02651244\n",
      "Iteration 8567, loss = 0.02650737\n",
      "Iteration 8568, loss = 0.02650229\n",
      "Iteration 8569, loss = 0.02649722\n",
      "Iteration 8570, loss = 0.02649215\n",
      "Iteration 8571, loss = 0.02648708\n",
      "Iteration 8572, loss = 0.02648202\n",
      "Iteration 8573, loss = 0.02647695\n",
      "Iteration 8574, loss = 0.02647189\n",
      "Iteration 8575, loss = 0.02646683\n",
      "Iteration 8576, loss = 0.02646177\n",
      "Iteration 8577, loss = 0.02645672\n",
      "Iteration 8578, loss = 0.02645166\n",
      "Iteration 8579, loss = 0.02644661\n",
      "Iteration 8580, loss = 0.02644156\n",
      "Iteration 8581, loss = 0.02643651\n",
      "Iteration 8582, loss = 0.02643146\n",
      "Iteration 8583, loss = 0.02642642\n",
      "Iteration 8584, loss = 0.02642138\n",
      "Iteration 8585, loss = 0.02641633\n",
      "Iteration 8586, loss = 0.02641129\n",
      "Iteration 8587, loss = 0.02640626\n",
      "Iteration 8588, loss = 0.02640122\n",
      "Iteration 8589, loss = 0.02639619\n",
      "Iteration 8590, loss = 0.02639116\n",
      "Iteration 8591, loss = 0.02638613\n",
      "Iteration 8592, loss = 0.02638110\n",
      "Iteration 8593, loss = 0.02637607\n",
      "Iteration 8594, loss = 0.02637105\n",
      "Iteration 8595, loss = 0.02636602\n",
      "Iteration 8596, loss = 0.02636100\n",
      "Iteration 8597, loss = 0.02635598\n",
      "Iteration 8598, loss = 0.02635097\n",
      "Iteration 8599, loss = 0.02634595\n",
      "Iteration 8600, loss = 0.02634094\n",
      "Iteration 8601, loss = 0.02633593\n",
      "Iteration 8602, loss = 0.02633092\n",
      "Iteration 8603, loss = 0.02632591\n",
      "Iteration 8604, loss = 0.02632090\n",
      "Iteration 8605, loss = 0.02631590\n",
      "Iteration 8606, loss = 0.02631089\n",
      "Iteration 8607, loss = 0.02630589\n",
      "Iteration 8608, loss = 0.02630090\n",
      "Iteration 8609, loss = 0.02629590\n",
      "Iteration 8610, loss = 0.02629090\n",
      "Iteration 8611, loss = 0.02628591\n",
      "Iteration 8612, loss = 0.02628092\n",
      "Iteration 8613, loss = 0.02627593\n",
      "Iteration 8614, loss = 0.02627094\n",
      "Iteration 8615, loss = 0.02626596\n",
      "Iteration 8616, loss = 0.02626097\n",
      "Iteration 8617, loss = 0.02625599\n",
      "Iteration 8618, loss = 0.02625101\n",
      "Iteration 8619, loss = 0.02624603\n",
      "Iteration 8620, loss = 0.02624105\n",
      "Iteration 8621, loss = 0.02623608\n",
      "Iteration 8622, loss = 0.02623111\n",
      "Iteration 8623, loss = 0.02622613\n",
      "Iteration 8624, loss = 0.02622117\n",
      "Iteration 8625, loss = 0.02621620\n",
      "Iteration 8626, loss = 0.02621123\n",
      "Iteration 8627, loss = 0.02620627\n",
      "Iteration 8628, loss = 0.02620131\n",
      "Iteration 8629, loss = 0.02619635\n",
      "Iteration 8630, loss = 0.02619139\n",
      "Iteration 8631, loss = 0.02618643\n",
      "Iteration 8632, loss = 0.02618148\n",
      "Iteration 8633, loss = 0.02617652\n",
      "Iteration 8634, loss = 0.02617157\n",
      "Iteration 8635, loss = 0.02616662\n",
      "Iteration 8636, loss = 0.02616168\n",
      "Iteration 8637, loss = 0.02615673\n",
      "Iteration 8638, loss = 0.02615179\n",
      "Iteration 8639, loss = 0.02614684\n",
      "Iteration 8640, loss = 0.02614190\n",
      "Iteration 8641, loss = 0.02613697\n",
      "Iteration 8642, loss = 0.02613203\n",
      "Iteration 8643, loss = 0.02612709\n",
      "Iteration 8644, loss = 0.02612216\n",
      "Iteration 8645, loss = 0.02611723\n",
      "Iteration 8646, loss = 0.02611230\n",
      "Iteration 8647, loss = 0.02610737\n",
      "Iteration 8648, loss = 0.02610245\n",
      "Iteration 8649, loss = 0.02609752\n",
      "Iteration 8650, loss = 0.02609260\n",
      "Iteration 8651, loss = 0.02608768\n",
      "Iteration 8652, loss = 0.02608276\n",
      "Iteration 8653, loss = 0.02607785\n",
      "Iteration 8654, loss = 0.02607293\n",
      "Iteration 8655, loss = 0.02606802\n",
      "Iteration 8656, loss = 0.02606311\n",
      "Iteration 8657, loss = 0.02605820\n",
      "Iteration 8658, loss = 0.02605329\n",
      "Iteration 8659, loss = 0.02604838\n",
      "Iteration 8660, loss = 0.02604348\n",
      "Iteration 8661, loss = 0.02603858\n",
      "Iteration 8662, loss = 0.02603368\n",
      "Iteration 8663, loss = 0.02602878\n",
      "Iteration 8664, loss = 0.02602388\n",
      "Iteration 8665, loss = 0.02601898\n",
      "Iteration 8666, loss = 0.02601409\n",
      "Iteration 8667, loss = 0.02600920\n",
      "Iteration 8668, loss = 0.02600431\n",
      "Iteration 8669, loss = 0.02599942\n",
      "Iteration 8670, loss = 0.02599454\n",
      "Iteration 8671, loss = 0.02598965\n",
      "Iteration 8672, loss = 0.02598477\n",
      "Iteration 8673, loss = 0.02597989\n",
      "Iteration 8674, loss = 0.02597501\n",
      "Iteration 8675, loss = 0.02597013\n",
      "Iteration 8676, loss = 0.02596526\n",
      "Iteration 8677, loss = 0.02596038\n",
      "Iteration 8678, loss = 0.02595551\n",
      "Iteration 8679, loss = 0.02595064\n",
      "Iteration 8680, loss = 0.02594577\n",
      "Iteration 8681, loss = 0.02594090\n",
      "Iteration 8682, loss = 0.02593604\n",
      "Iteration 8683, loss = 0.02593118\n",
      "Iteration 8684, loss = 0.02592631\n",
      "Iteration 8685, loss = 0.02592146\n",
      "Iteration 8686, loss = 0.02591660\n",
      "Iteration 8687, loss = 0.02591174\n",
      "Iteration 8688, loss = 0.02590689\n",
      "Iteration 8689, loss = 0.02590203\n",
      "Iteration 8690, loss = 0.02589718\n",
      "Iteration 8691, loss = 0.02589234\n",
      "Iteration 8692, loss = 0.02588749\n",
      "Iteration 8693, loss = 0.02588264\n",
      "Iteration 8694, loss = 0.02587780\n",
      "Iteration 8695, loss = 0.02587296\n",
      "Iteration 8696, loss = 0.02586812\n",
      "Iteration 8697, loss = 0.02586328\n",
      "Iteration 8698, loss = 0.02585844\n",
      "Iteration 8699, loss = 0.02585361\n",
      "Iteration 8700, loss = 0.02584877\n",
      "Iteration 8701, loss = 0.02584394\n",
      "Iteration 8702, loss = 0.02583911\n",
      "Iteration 8703, loss = 0.02583429\n",
      "Iteration 8704, loss = 0.02582946\n",
      "Iteration 8705, loss = 0.02582463\n",
      "Iteration 8706, loss = 0.02581981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8707, loss = 0.02581499\n",
      "Iteration 8708, loss = 0.02581017\n",
      "Iteration 8709, loss = 0.02580536\n",
      "Iteration 8710, loss = 0.02580054\n",
      "Iteration 8711, loss = 0.02579573\n",
      "Iteration 8712, loss = 0.02579091\n",
      "Iteration 8713, loss = 0.02578610\n",
      "Iteration 8714, loss = 0.02578130\n",
      "Iteration 8715, loss = 0.02577649\n",
      "Iteration 8716, loss = 0.02577168\n",
      "Iteration 8717, loss = 0.02576688\n",
      "Iteration 8718, loss = 0.02576208\n",
      "Iteration 8719, loss = 0.02575728\n",
      "Iteration 8720, loss = 0.02575248\n",
      "Iteration 8721, loss = 0.02574768\n",
      "Iteration 8722, loss = 0.02574289\n",
      "Iteration 8723, loss = 0.02573810\n",
      "Iteration 8724, loss = 0.02573331\n",
      "Iteration 8725, loss = 0.02572852\n",
      "Iteration 8726, loss = 0.02572373\n",
      "Iteration 8727, loss = 0.02571894\n",
      "Iteration 8728, loss = 0.02571416\n",
      "Iteration 8729, loss = 0.02570938\n",
      "Iteration 8730, loss = 0.02570460\n",
      "Iteration 8731, loss = 0.02569982\n",
      "Iteration 8732, loss = 0.02569504\n",
      "Iteration 8733, loss = 0.02569026\n",
      "Iteration 8734, loss = 0.02568549\n",
      "Iteration 8735, loss = 0.02568072\n",
      "Iteration 8736, loss = 0.02567595\n",
      "Iteration 8737, loss = 0.02567118\n",
      "Iteration 8738, loss = 0.02566641\n",
      "Iteration 8739, loss = 0.02566165\n",
      "Iteration 8740, loss = 0.02565688\n",
      "Iteration 8741, loss = 0.02565212\n",
      "Iteration 8742, loss = 0.02564736\n",
      "Iteration 8743, loss = 0.02564260\n",
      "Iteration 8744, loss = 0.02563785\n",
      "Iteration 8745, loss = 0.02563309\n",
      "Iteration 8746, loss = 0.02562834\n",
      "Iteration 8747, loss = 0.02562359\n",
      "Iteration 8748, loss = 0.02561884\n",
      "Iteration 8749, loss = 0.02561409\n",
      "Iteration 8750, loss = 0.02560934\n",
      "Iteration 8751, loss = 0.02560460\n",
      "Iteration 8752, loss = 0.02559986\n",
      "Iteration 8753, loss = 0.02559511\n",
      "Iteration 8754, loss = 0.02559038\n",
      "Iteration 8755, loss = 0.02558564\n",
      "Iteration 8756, loss = 0.02558090\n",
      "Iteration 8757, loss = 0.02557617\n",
      "Iteration 8758, loss = 0.02557144\n",
      "Iteration 8759, loss = 0.02556670\n",
      "Iteration 8760, loss = 0.02556198\n",
      "Iteration 8761, loss = 0.02555725\n",
      "Iteration 8762, loss = 0.02555252\n",
      "Iteration 8763, loss = 0.02554780\n",
      "Iteration 8764, loss = 0.02554308\n",
      "Iteration 8765, loss = 0.02553836\n",
      "Iteration 8766, loss = 0.02553364\n",
      "Iteration 8767, loss = 0.02552892\n",
      "Iteration 8768, loss = 0.02552420\n",
      "Iteration 8769, loss = 0.02551949\n",
      "Iteration 8770, loss = 0.02551478\n",
      "Iteration 8771, loss = 0.02551007\n",
      "Iteration 8772, loss = 0.02550536\n",
      "Iteration 8773, loss = 0.02550065\n",
      "Iteration 8774, loss = 0.02549595\n",
      "Iteration 8775, loss = 0.02549124\n",
      "Iteration 8776, loss = 0.02548654\n",
      "Iteration 8777, loss = 0.02548184\n",
      "Iteration 8778, loss = 0.02547714\n",
      "Iteration 8779, loss = 0.02547244\n",
      "Iteration 8780, loss = 0.02546775\n",
      "Iteration 8781, loss = 0.02546306\n",
      "Iteration 8782, loss = 0.02545836\n",
      "Iteration 8783, loss = 0.02545367\n",
      "Iteration 8784, loss = 0.02544899\n",
      "Iteration 8785, loss = 0.02544430\n",
      "Iteration 8786, loss = 0.02543961\n",
      "Iteration 8787, loss = 0.02543493\n",
      "Iteration 8788, loss = 0.02543025\n",
      "Iteration 8789, loss = 0.02542557\n",
      "Iteration 8790, loss = 0.02542089\n",
      "Iteration 8791, loss = 0.02541621\n",
      "Iteration 8792, loss = 0.02541154\n",
      "Iteration 8793, loss = 0.02540687\n",
      "Iteration 8794, loss = 0.02540219\n",
      "Iteration 8795, loss = 0.02539752\n",
      "Iteration 8796, loss = 0.02539286\n",
      "Iteration 8797, loss = 0.02538819\n",
      "Iteration 8798, loss = 0.02538352\n",
      "Iteration 8799, loss = 0.02537886\n",
      "Iteration 8800, loss = 0.02537420\n",
      "Iteration 8801, loss = 0.02536954\n",
      "Iteration 8802, loss = 0.02536488\n",
      "Iteration 8803, loss = 0.02536023\n",
      "Iteration 8804, loss = 0.02535557\n",
      "Iteration 8805, loss = 0.02535092\n",
      "Iteration 8806, loss = 0.02534627\n",
      "Iteration 8807, loss = 0.02534162\n",
      "Iteration 8808, loss = 0.02533697\n",
      "Iteration 8809, loss = 0.02533232\n",
      "Iteration 8810, loss = 0.02532768\n",
      "Iteration 8811, loss = 0.02532303\n",
      "Iteration 8812, loss = 0.02531839\n",
      "Iteration 8813, loss = 0.02531375\n",
      "Iteration 8814, loss = 0.02530911\n",
      "Iteration 8815, loss = 0.02530448\n",
      "Iteration 8816, loss = 0.02529984\n",
      "Iteration 8817, loss = 0.02529521\n",
      "Iteration 8818, loss = 0.02529058\n",
      "Iteration 8819, loss = 0.02528595\n",
      "Iteration 8820, loss = 0.02528132\n",
      "Iteration 8821, loss = 0.02527669\n",
      "Iteration 8822, loss = 0.02527207\n",
      "Iteration 8823, loss = 0.02526744\n",
      "Iteration 8824, loss = 0.02526282\n",
      "Iteration 8825, loss = 0.02525820\n",
      "Iteration 8826, loss = 0.02525358\n",
      "Iteration 8827, loss = 0.02524897\n",
      "Iteration 8828, loss = 0.02524435\n",
      "Iteration 8829, loss = 0.02523974\n",
      "Iteration 8830, loss = 0.02523513\n",
      "Iteration 8831, loss = 0.02523052\n",
      "Iteration 8832, loss = 0.02522591\n",
      "Iteration 8833, loss = 0.02522130\n",
      "Iteration 8834, loss = 0.02521670\n",
      "Iteration 8835, loss = 0.02521209\n",
      "Iteration 8836, loss = 0.02520749\n",
      "Iteration 8837, loss = 0.02520289\n",
      "Iteration 8838, loss = 0.02519829\n",
      "Iteration 8839, loss = 0.02519370\n",
      "Iteration 8840, loss = 0.02518910\n",
      "Iteration 8841, loss = 0.02518451\n",
      "Iteration 8842, loss = 0.02517991\n",
      "Iteration 8843, loss = 0.02517532\n",
      "Iteration 8844, loss = 0.02517074\n",
      "Iteration 8845, loss = 0.02516615\n",
      "Iteration 8846, loss = 0.02516156\n",
      "Iteration 8847, loss = 0.02515698\n",
      "Iteration 8848, loss = 0.02515240\n",
      "Iteration 8849, loss = 0.02514782\n",
      "Iteration 8850, loss = 0.02514324\n",
      "Iteration 8851, loss = 0.02513866\n",
      "Iteration 8852, loss = 0.02513408\n",
      "Iteration 8853, loss = 0.02512951\n",
      "Iteration 8854, loss = 0.02512494\n",
      "Iteration 8855, loss = 0.02512037\n",
      "Iteration 8856, loss = 0.02511580\n",
      "Iteration 8857, loss = 0.02511123\n",
      "Iteration 8858, loss = 0.02510666\n",
      "Iteration 8859, loss = 0.02510210\n",
      "Iteration 8860, loss = 0.02509754\n",
      "Iteration 8861, loss = 0.02509298\n",
      "Iteration 8862, loss = 0.02508842\n",
      "Iteration 8863, loss = 0.02508386\n",
      "Iteration 8864, loss = 0.02507930\n",
      "Iteration 8865, loss = 0.02507475\n",
      "Iteration 8866, loss = 0.02507020\n",
      "Iteration 8867, loss = 0.02506564\n",
      "Iteration 8868, loss = 0.02506109\n",
      "Iteration 8869, loss = 0.02505655\n",
      "Iteration 8870, loss = 0.02505200\n",
      "Iteration 8871, loss = 0.02504745\n",
      "Iteration 8872, loss = 0.02504291\n",
      "Iteration 8873, loss = 0.02503837\n",
      "Iteration 8874, loss = 0.02503383\n",
      "Iteration 8875, loss = 0.02502929\n",
      "Iteration 8876, loss = 0.02502475\n",
      "Iteration 8877, loss = 0.02502022\n",
      "Iteration 8878, loss = 0.02501569\n",
      "Iteration 8879, loss = 0.02501115\n",
      "Iteration 8880, loss = 0.02500662\n",
      "Iteration 8881, loss = 0.02500210\n",
      "Iteration 8882, loss = 0.02499757\n",
      "Iteration 8883, loss = 0.02499304\n",
      "Iteration 8884, loss = 0.02498852\n",
      "Iteration 8885, loss = 0.02498400\n",
      "Iteration 8886, loss = 0.02497948\n",
      "Iteration 8887, loss = 0.02497496\n",
      "Iteration 8888, loss = 0.02497044\n",
      "Iteration 8889, loss = 0.02496592\n",
      "Iteration 8890, loss = 0.02496141\n",
      "Iteration 8891, loss = 0.02495690\n",
      "Iteration 8892, loss = 0.02495238\n",
      "Iteration 8893, loss = 0.02494788\n",
      "Iteration 8894, loss = 0.02494337\n",
      "Iteration 8895, loss = 0.02493886\n",
      "Iteration 8896, loss = 0.02493436\n",
      "Iteration 8897, loss = 0.02492985\n",
      "Iteration 8898, loss = 0.02492535\n",
      "Iteration 8899, loss = 0.02492085\n",
      "Iteration 8900, loss = 0.02491635\n",
      "Iteration 8901, loss = 0.02491186\n",
      "Iteration 8902, loss = 0.02490736\n",
      "Iteration 8903, loss = 0.02490287\n",
      "Iteration 8904, loss = 0.02489838\n",
      "Iteration 8905, loss = 0.02489389\n",
      "Iteration 8906, loss = 0.02488940\n",
      "Iteration 8907, loss = 0.02488491\n",
      "Iteration 8908, loss = 0.02488042\n",
      "Iteration 8909, loss = 0.02487594\n",
      "Iteration 8910, loss = 0.02487146\n",
      "Iteration 8911, loss = 0.02486698\n",
      "Iteration 8912, loss = 0.02486250\n",
      "Iteration 8913, loss = 0.02485802\n",
      "Iteration 8914, loss = 0.02485354\n",
      "Iteration 8915, loss = 0.02484907\n",
      "Iteration 8916, loss = 0.02484460\n",
      "Iteration 8917, loss = 0.02484012\n",
      "Iteration 8918, loss = 0.02483565\n",
      "Iteration 8919, loss = 0.02483119\n",
      "Iteration 8920, loss = 0.02482672\n",
      "Iteration 8921, loss = 0.02482225\n",
      "Iteration 8922, loss = 0.02481779\n",
      "Iteration 8923, loss = 0.02481333\n",
      "Iteration 8924, loss = 0.02480887\n",
      "Iteration 8925, loss = 0.02480441\n",
      "Iteration 8926, loss = 0.02479995\n",
      "Iteration 8927, loss = 0.02479550\n",
      "Iteration 8928, loss = 0.02479104\n",
      "Iteration 8929, loss = 0.02478659\n",
      "Iteration 8930, loss = 0.02478214\n",
      "Iteration 8931, loss = 0.02477769\n",
      "Iteration 8932, loss = 0.02477324\n",
      "Iteration 8933, loss = 0.02476880\n",
      "Iteration 8934, loss = 0.02476435\n",
      "Iteration 8935, loss = 0.02475991\n",
      "Iteration 8936, loss = 0.02475547\n",
      "Iteration 8937, loss = 0.02475103\n",
      "Iteration 8938, loss = 0.02474659\n",
      "Iteration 8939, loss = 0.02474215\n",
      "Iteration 8940, loss = 0.02473772\n",
      "Iteration 8941, loss = 0.02473328\n",
      "Iteration 8942, loss = 0.02472885\n",
      "Iteration 8943, loss = 0.02472442\n",
      "Iteration 8944, loss = 0.02471999\n",
      "Iteration 8945, loss = 0.02471556\n",
      "Iteration 8946, loss = 0.02471114\n",
      "Iteration 8947, loss = 0.02470671\n",
      "Iteration 8948, loss = 0.02470229\n",
      "Iteration 8949, loss = 0.02469787\n",
      "Iteration 8950, loss = 0.02469345\n",
      "Iteration 8951, loss = 0.02468903\n",
      "Iteration 8952, loss = 0.02468461\n",
      "Iteration 8953, loss = 0.02468020\n",
      "Iteration 8954, loss = 0.02467579\n",
      "Iteration 8955, loss = 0.02467137\n",
      "Iteration 8956, loss = 0.02466696\n",
      "Iteration 8957, loss = 0.02466255\n",
      "Iteration 8958, loss = 0.02465815\n",
      "Iteration 8959, loss = 0.02465374\n",
      "Iteration 8960, loss = 0.02464934\n",
      "Iteration 8961, loss = 0.02464493\n",
      "Iteration 8962, loss = 0.02464053\n",
      "Iteration 8963, loss = 0.02463613\n",
      "Iteration 8964, loss = 0.02463173\n",
      "Iteration 8965, loss = 0.02462734\n",
      "Iteration 8966, loss = 0.02462294\n",
      "Iteration 8967, loss = 0.02461855\n",
      "Iteration 8968, loss = 0.02461416\n",
      "Iteration 8969, loss = 0.02460977\n",
      "Iteration 8970, loss = 0.02460538\n",
      "Iteration 8971, loss = 0.02460099\n",
      "Iteration 8972, loss = 0.02459661\n",
      "Iteration 8973, loss = 0.02459222\n",
      "Iteration 8974, loss = 0.02458784\n",
      "Iteration 8975, loss = 0.02458346\n",
      "Iteration 8976, loss = 0.02457908\n",
      "Iteration 8977, loss = 0.02457470\n",
      "Iteration 8978, loss = 0.02457032\n",
      "Iteration 8979, loss = 0.02456595\n",
      "Iteration 8980, loss = 0.02456158\n",
      "Iteration 8981, loss = 0.02455720\n",
      "Iteration 8982, loss = 0.02455283\n",
      "Iteration 8983, loss = 0.02454846\n",
      "Iteration 8984, loss = 0.02454410\n",
      "Iteration 8985, loss = 0.02453973\n",
      "Iteration 8986, loss = 0.02453537\n",
      "Iteration 8987, loss = 0.02453100\n",
      "Iteration 8988, loss = 0.02452664\n",
      "Iteration 8989, loss = 0.02452228\n",
      "Iteration 8990, loss = 0.02451792\n",
      "Iteration 8991, loss = 0.02451357\n",
      "Iteration 8992, loss = 0.02450921\n",
      "Iteration 8993, loss = 0.02450486\n",
      "Iteration 8994, loss = 0.02450051\n",
      "Iteration 8995, loss = 0.02449616\n",
      "Iteration 8996, loss = 0.02449181\n",
      "Iteration 8997, loss = 0.02448746\n",
      "Iteration 8998, loss = 0.02448311\n",
      "Iteration 8999, loss = 0.02447877\n",
      "Iteration 9000, loss = 0.02447443\n",
      "Iteration 9001, loss = 0.02447008\n",
      "Iteration 9002, loss = 0.02446575\n",
      "Iteration 9003, loss = 0.02446141\n",
      "Iteration 9004, loss = 0.02445707\n",
      "Iteration 9005, loss = 0.02445273\n",
      "Iteration 9006, loss = 0.02444840\n",
      "Iteration 9007, loss = 0.02444407\n",
      "Iteration 9008, loss = 0.02443974\n",
      "Iteration 9009, loss = 0.02443541\n",
      "Iteration 9010, loss = 0.02443108\n",
      "Iteration 9011, loss = 0.02442675\n",
      "Iteration 9012, loss = 0.02442243\n",
      "Iteration 9013, loss = 0.02441811\n",
      "Iteration 9014, loss = 0.02441378\n",
      "Iteration 9015, loss = 0.02440946\n",
      "Iteration 9016, loss = 0.02440514\n",
      "Iteration 9017, loss = 0.02440083\n",
      "Iteration 9018, loss = 0.02439651\n",
      "Iteration 9019, loss = 0.02439220\n",
      "Iteration 9020, loss = 0.02438788\n",
      "Iteration 9021, loss = 0.02438357\n",
      "Iteration 9022, loss = 0.02437926\n",
      "Iteration 9023, loss = 0.02437495\n",
      "Iteration 9024, loss = 0.02437065\n",
      "Iteration 9025, loss = 0.02436634\n",
      "Iteration 9026, loss = 0.02436204\n",
      "Iteration 9027, loss = 0.02435774\n",
      "Iteration 9028, loss = 0.02435344\n",
      "Iteration 9029, loss = 0.02434914\n",
      "Iteration 9030, loss = 0.02434484\n",
      "Iteration 9031, loss = 0.02434054\n",
      "Iteration 9032, loss = 0.02433625\n",
      "Iteration 9033, loss = 0.02433195\n",
      "Iteration 9034, loss = 0.02432766\n",
      "Iteration 9035, loss = 0.02432337\n",
      "Iteration 9036, loss = 0.02431908\n",
      "Iteration 9037, loss = 0.02431479\n",
      "Iteration 9038, loss = 0.02431051\n",
      "Iteration 9039, loss = 0.02430622\n",
      "Iteration 9040, loss = 0.02430194\n",
      "Iteration 9041, loss = 0.02429766\n",
      "Iteration 9042, loss = 0.02429338\n",
      "Iteration 9043, loss = 0.02428910\n",
      "Iteration 9044, loss = 0.02428482\n",
      "Iteration 9045, loss = 0.02428055\n",
      "Iteration 9046, loss = 0.02427628\n",
      "Iteration 9047, loss = 0.02427200\n",
      "Iteration 9048, loss = 0.02426773\n",
      "Iteration 9049, loss = 0.02426346\n",
      "Iteration 9050, loss = 0.02425919\n",
      "Iteration 9051, loss = 0.02425493\n",
      "Iteration 9052, loss = 0.02425066\n",
      "Iteration 9053, loss = 0.02424640\n",
      "Iteration 9054, loss = 0.02424214\n",
      "Iteration 9055, loss = 0.02423788\n",
      "Iteration 9056, loss = 0.02423362\n",
      "Iteration 9057, loss = 0.02422936\n",
      "Iteration 9058, loss = 0.02422510\n",
      "Iteration 9059, loss = 0.02422085\n",
      "Iteration 9060, loss = 0.02421659\n",
      "Iteration 9061, loss = 0.02421234\n",
      "Iteration 9062, loss = 0.02420809\n",
      "Iteration 9063, loss = 0.02420384\n",
      "Iteration 9064, loss = 0.02419960\n",
      "Iteration 9065, loss = 0.02419535\n",
      "Iteration 9066, loss = 0.02419111\n",
      "Iteration 9067, loss = 0.02418686\n",
      "Iteration 9068, loss = 0.02418262\n",
      "Iteration 9069, loss = 0.02417838\n",
      "Iteration 9070, loss = 0.02417414\n",
      "Iteration 9071, loss = 0.02416990\n",
      "Iteration 9072, loss = 0.02416567\n",
      "Iteration 9073, loss = 0.02416143\n",
      "Iteration 9074, loss = 0.02415720\n",
      "Iteration 9075, loss = 0.02415297\n",
      "Iteration 9076, loss = 0.02414874\n",
      "Iteration 9077, loss = 0.02414451\n",
      "Iteration 9078, loss = 0.02414029\n",
      "Iteration 9079, loss = 0.02413606\n",
      "Iteration 9080, loss = 0.02413184\n",
      "Iteration 9081, loss = 0.02412761\n",
      "Iteration 9082, loss = 0.02412339\n",
      "Iteration 9083, loss = 0.02411917\n",
      "Iteration 9084, loss = 0.02411495\n",
      "Iteration 9085, loss = 0.02411074\n",
      "Iteration 9086, loss = 0.02410652\n",
      "Iteration 9087, loss = 0.02410231\n",
      "Iteration 9088, loss = 0.02409810\n",
      "Iteration 9089, loss = 0.02409389\n",
      "Iteration 9090, loss = 0.02408968\n",
      "Iteration 9091, loss = 0.02408547\n",
      "Iteration 9092, loss = 0.02408126\n",
      "Iteration 9093, loss = 0.02407706\n",
      "Iteration 9094, loss = 0.02407285\n",
      "Iteration 9095, loss = 0.02406865\n",
      "Iteration 9096, loss = 0.02406445\n",
      "Iteration 9097, loss = 0.02406025\n",
      "Iteration 9098, loss = 0.02405605\n",
      "Iteration 9099, loss = 0.02405186\n",
      "Iteration 9100, loss = 0.02404766\n",
      "Iteration 9101, loss = 0.02404347\n",
      "Iteration 9102, loss = 0.02403927\n",
      "Iteration 9103, loss = 0.02403508\n",
      "Iteration 9104, loss = 0.02403090\n",
      "Iteration 9105, loss = 0.02402671\n",
      "Iteration 9106, loss = 0.02402252\n",
      "Iteration 9107, loss = 0.02401834\n",
      "Iteration 9108, loss = 0.02401415\n",
      "Iteration 9109, loss = 0.02400997\n",
      "Iteration 9110, loss = 0.02400579\n",
      "Iteration 9111, loss = 0.02400161\n",
      "Iteration 9112, loss = 0.02399743\n",
      "Iteration 9113, loss = 0.02399326\n",
      "Iteration 9114, loss = 0.02398908\n",
      "Iteration 9115, loss = 0.02398491\n",
      "Iteration 9116, loss = 0.02398074\n",
      "Iteration 9117, loss = 0.02397657\n",
      "Iteration 9118, loss = 0.02397240\n",
      "Iteration 9119, loss = 0.02396823\n",
      "Iteration 9120, loss = 0.02396406\n",
      "Iteration 9121, loss = 0.02395990\n",
      "Iteration 9122, loss = 0.02395574\n",
      "Iteration 9123, loss = 0.02395157\n",
      "Iteration 9124, loss = 0.02394741\n",
      "Iteration 9125, loss = 0.02394325\n",
      "Iteration 9126, loss = 0.02393910\n",
      "Iteration 9127, loss = 0.02393494\n",
      "Iteration 9128, loss = 0.02393078\n",
      "Iteration 9129, loss = 0.02392663\n",
      "Iteration 9130, loss = 0.02392248\n",
      "Iteration 9131, loss = 0.02391833\n",
      "Iteration 9132, loss = 0.02391418\n",
      "Iteration 9133, loss = 0.02391003\n",
      "Iteration 9134, loss = 0.02390589\n",
      "Iteration 9135, loss = 0.02390174\n",
      "Iteration 9136, loss = 0.02389760\n",
      "Iteration 9137, loss = 0.02389346\n",
      "Iteration 9138, loss = 0.02388932\n",
      "Iteration 9139, loss = 0.02388518\n",
      "Iteration 9140, loss = 0.02388104\n",
      "Iteration 9141, loss = 0.02387690\n",
      "Iteration 9142, loss = 0.02387277\n",
      "Iteration 9143, loss = 0.02386863\n",
      "Iteration 9144, loss = 0.02386450\n",
      "Iteration 9145, loss = 0.02386037\n",
      "Iteration 9146, loss = 0.02385624\n",
      "Iteration 9147, loss = 0.02385211\n",
      "Iteration 9148, loss = 0.02384799\n",
      "Iteration 9149, loss = 0.02384386\n",
      "Iteration 9150, loss = 0.02383974\n",
      "Iteration 9151, loss = 0.02383562\n",
      "Iteration 9152, loss = 0.02383150\n",
      "Iteration 9153, loss = 0.02382738\n",
      "Iteration 9154, loss = 0.02382326\n",
      "Iteration 9155, loss = 0.02381914\n",
      "Iteration 9156, loss = 0.02381503\n",
      "Iteration 9157, loss = 0.02381091\n",
      "Iteration 9158, loss = 0.02380680\n",
      "Iteration 9159, loss = 0.02380269\n",
      "Iteration 9160, loss = 0.02379858\n",
      "Iteration 9161, loss = 0.02379447\n",
      "Iteration 9162, loss = 0.02379036\n",
      "Iteration 9163, loss = 0.02378626\n",
      "Iteration 9164, loss = 0.02378216\n",
      "Iteration 9165, loss = 0.02377805\n",
      "Iteration 9166, loss = 0.02377395\n",
      "Iteration 9167, loss = 0.02376985\n",
      "Iteration 9168, loss = 0.02376575\n",
      "Iteration 9169, loss = 0.02376166\n",
      "Iteration 9170, loss = 0.02375756\n",
      "Iteration 9171, loss = 0.02375347\n",
      "Iteration 9172, loss = 0.02374937\n",
      "Iteration 9173, loss = 0.02374528\n",
      "Iteration 9174, loss = 0.02374119\n",
      "Iteration 9175, loss = 0.02373710\n",
      "Iteration 9176, loss = 0.02373302\n",
      "Iteration 9177, loss = 0.02372893\n",
      "Iteration 9178, loss = 0.02372485\n",
      "Iteration 9179, loss = 0.02372076\n",
      "Iteration 9180, loss = 0.02371668\n",
      "Iteration 9181, loss = 0.02371260\n",
      "Iteration 9182, loss = 0.02370852\n",
      "Iteration 9183, loss = 0.02370445\n",
      "Iteration 9184, loss = 0.02370037\n",
      "Iteration 9185, loss = 0.02369630\n",
      "Iteration 9186, loss = 0.02369222\n",
      "Iteration 9187, loss = 0.02368815\n",
      "Iteration 9188, loss = 0.02368408\n",
      "Iteration 9189, loss = 0.02368001\n",
      "Iteration 9190, loss = 0.02367594\n",
      "Iteration 9191, loss = 0.02367188\n",
      "Iteration 9192, loss = 0.02366781\n",
      "Iteration 9193, loss = 0.02366375\n",
      "Iteration 9194, loss = 0.02365969\n",
      "Iteration 9195, loss = 0.02365563\n",
      "Iteration 9196, loss = 0.02365157\n",
      "Iteration 9197, loss = 0.02364751\n",
      "Iteration 9198, loss = 0.02364345\n",
      "Iteration 9199, loss = 0.02363940\n",
      "Iteration 9200, loss = 0.02363534\n",
      "Iteration 9201, loss = 0.02363129\n",
      "Iteration 9202, loss = 0.02362724\n",
      "Iteration 9203, loss = 0.02362319\n",
      "Iteration 9204, loss = 0.02361914\n",
      "Iteration 9205, loss = 0.02361509\n",
      "Iteration 9206, loss = 0.02361105\n",
      "Iteration 9207, loss = 0.02360700\n",
      "Iteration 9208, loss = 0.02360296\n",
      "Iteration 9209, loss = 0.02359892\n",
      "Iteration 9210, loss = 0.02359488\n",
      "Iteration 9211, loss = 0.02359084\n",
      "Iteration 9212, loss = 0.02358680\n",
      "Iteration 9213, loss = 0.02358277\n",
      "Iteration 9214, loss = 0.02357873\n",
      "Iteration 9215, loss = 0.02357470\n",
      "Iteration 9216, loss = 0.02357067\n",
      "Iteration 9217, loss = 0.02356664\n",
      "Iteration 9218, loss = 0.02356261\n",
      "Iteration 9219, loss = 0.02355858\n",
      "Iteration 9220, loss = 0.02355455\n",
      "Iteration 9221, loss = 0.02355053\n",
      "Iteration 9222, loss = 0.02354650\n",
      "Iteration 9223, loss = 0.02354248\n",
      "Iteration 9224, loss = 0.02353846\n",
      "Iteration 9225, loss = 0.02353444\n",
      "Iteration 9226, loss = 0.02353042\n",
      "Iteration 9227, loss = 0.02352641\n",
      "Iteration 9228, loss = 0.02352239\n",
      "Iteration 9229, loss = 0.02351838\n",
      "Iteration 9230, loss = 0.02351436\n",
      "Iteration 9231, loss = 0.02351035\n",
      "Iteration 9232, loss = 0.02350634\n",
      "Iteration 9233, loss = 0.02350233\n",
      "Iteration 9234, loss = 0.02349832\n",
      "Iteration 9235, loss = 0.02349432\n",
      "Iteration 9236, loss = 0.02349031\n",
      "Iteration 9237, loss = 0.02348631\n",
      "Iteration 9238, loss = 0.02348231\n",
      "Iteration 9239, loss = 0.02347831\n",
      "Iteration 9240, loss = 0.02347431\n",
      "Iteration 9241, loss = 0.02347031\n",
      "Iteration 9242, loss = 0.02346631\n",
      "Iteration 9243, loss = 0.02346232\n",
      "Iteration 9244, loss = 0.02345833\n",
      "Iteration 9245, loss = 0.02345433\n",
      "Iteration 9246, loss = 0.02345034\n",
      "Iteration 9247, loss = 0.02344635\n",
      "Iteration 9248, loss = 0.02344236\n",
      "Iteration 9249, loss = 0.02343838\n",
      "Iteration 9250, loss = 0.02343439\n",
      "Iteration 9251, loss = 0.02343041\n",
      "Iteration 9252, loss = 0.02342642\n",
      "Iteration 9253, loss = 0.02342244\n",
      "Iteration 9254, loss = 0.02341846\n",
      "Iteration 9255, loss = 0.02341448\n",
      "Iteration 9256, loss = 0.02341050\n",
      "Iteration 9257, loss = 0.02340653\n",
      "Iteration 9258, loss = 0.02340255\n",
      "Iteration 9259, loss = 0.02339858\n",
      "Iteration 9260, loss = 0.02339461\n",
      "Iteration 9261, loss = 0.02339063\n",
      "Iteration 9262, loss = 0.02338666\n",
      "Iteration 9263, loss = 0.02338270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9264, loss = 0.02337873\n",
      "Iteration 9265, loss = 0.02337476\n",
      "Iteration 9266, loss = 0.02337080\n",
      "Iteration 9267, loss = 0.02336684\n",
      "Iteration 9268, loss = 0.02336287\n",
      "Iteration 9269, loss = 0.02335891\n",
      "Iteration 9270, loss = 0.02335496\n",
      "Iteration 9271, loss = 0.02335100\n",
      "Iteration 9272, loss = 0.02334704\n",
      "Iteration 9273, loss = 0.02334309\n",
      "Iteration 9274, loss = 0.02333913\n",
      "Iteration 9275, loss = 0.02333518\n",
      "Iteration 9276, loss = 0.02333123\n",
      "Iteration 9277, loss = 0.02332728\n",
      "Iteration 9278, loss = 0.02332333\n",
      "Iteration 9279, loss = 0.02331938\n",
      "Iteration 9280, loss = 0.02331544\n",
      "Iteration 9281, loss = 0.02331149\n",
      "Iteration 9282, loss = 0.02330755\n",
      "Iteration 9283, loss = 0.02330361\n",
      "Iteration 9284, loss = 0.02329967\n",
      "Iteration 9285, loss = 0.02329573\n",
      "Iteration 9286, loss = 0.02329179\n",
      "Iteration 9287, loss = 0.02328785\n",
      "Iteration 9288, loss = 0.02328392\n",
      "Iteration 9289, loss = 0.02327999\n",
      "Iteration 9290, loss = 0.02327605\n",
      "Iteration 9291, loss = 0.02327212\n",
      "Iteration 9292, loss = 0.02326819\n",
      "Iteration 9293, loss = 0.02326426\n",
      "Iteration 9294, loss = 0.02326034\n",
      "Iteration 9295, loss = 0.02325641\n",
      "Iteration 9296, loss = 0.02325249\n",
      "Iteration 9297, loss = 0.02324856\n",
      "Iteration 9298, loss = 0.02324464\n",
      "Iteration 9299, loss = 0.02324072\n",
      "Iteration 9300, loss = 0.02323680\n",
      "Iteration 9301, loss = 0.02323288\n",
      "Iteration 9302, loss = 0.02322897\n",
      "Iteration 9303, loss = 0.02322505\n",
      "Iteration 9304, loss = 0.02322114\n",
      "Iteration 9305, loss = 0.02321722\n",
      "Iteration 9306, loss = 0.02321331\n",
      "Iteration 9307, loss = 0.02320940\n",
      "Iteration 9308, loss = 0.02320549\n",
      "Iteration 9309, loss = 0.02320159\n",
      "Iteration 9310, loss = 0.02319768\n",
      "Iteration 9311, loss = 0.02319377\n",
      "Iteration 9312, loss = 0.02318987\n",
      "Iteration 9313, loss = 0.02318597\n",
      "Iteration 9314, loss = 0.02318207\n",
      "Iteration 9315, loss = 0.02317817\n",
      "Iteration 9316, loss = 0.02317427\n",
      "Iteration 9317, loss = 0.02317037\n",
      "Iteration 9318, loss = 0.02316648\n",
      "Iteration 9319, loss = 0.02316258\n",
      "Iteration 9320, loss = 0.02315869\n",
      "Iteration 9321, loss = 0.02315480\n",
      "Iteration 9322, loss = 0.02315091\n",
      "Iteration 9323, loss = 0.02314702\n",
      "Iteration 9324, loss = 0.02314313\n",
      "Iteration 9325, loss = 0.02313924\n",
      "Iteration 9326, loss = 0.02313536\n",
      "Iteration 9327, loss = 0.02313147\n",
      "Iteration 9328, loss = 0.02312759\n",
      "Iteration 9329, loss = 0.02312371\n",
      "Iteration 9330, loss = 0.02311983\n",
      "Iteration 9331, loss = 0.02311595\n",
      "Iteration 9332, loss = 0.02311207\n",
      "Iteration 9333, loss = 0.02310820\n",
      "Iteration 9334, loss = 0.02310432\n",
      "Iteration 9335, loss = 0.02310045\n",
      "Iteration 9336, loss = 0.02309658\n",
      "Iteration 9337, loss = 0.02309270\n",
      "Iteration 9338, loss = 0.02308883\n",
      "Iteration 9339, loss = 0.02308497\n",
      "Iteration 9340, loss = 0.02308110\n",
      "Iteration 9341, loss = 0.02307723\n",
      "Iteration 9342, loss = 0.02307337\n",
      "Iteration 9343, loss = 0.02306951\n",
      "Iteration 9344, loss = 0.02306564\n",
      "Iteration 9345, loss = 0.02306178\n",
      "Iteration 9346, loss = 0.02305792\n",
      "Iteration 9347, loss = 0.02305406\n",
      "Iteration 9348, loss = 0.02305021\n",
      "Iteration 9349, loss = 0.02304635\n",
      "Iteration 9350, loss = 0.02304250\n",
      "Iteration 9351, loss = 0.02303864\n",
      "Iteration 9352, loss = 0.02303479\n",
      "Iteration 9353, loss = 0.02303094\n",
      "Iteration 9354, loss = 0.02302709\n",
      "Iteration 9355, loss = 0.02302324\n",
      "Iteration 9356, loss = 0.02301940\n",
      "Iteration 9357, loss = 0.02301555\n",
      "Iteration 9358, loss = 0.02301171\n",
      "Iteration 9359, loss = 0.02300787\n",
      "Iteration 9360, loss = 0.02300402\n",
      "Iteration 9361, loss = 0.02300018\n",
      "Iteration 9362, loss = 0.02299635\n",
      "Iteration 9363, loss = 0.02299251\n",
      "Iteration 9364, loss = 0.02298867\n",
      "Iteration 9365, loss = 0.02298484\n",
      "Iteration 9366, loss = 0.02298100\n",
      "Iteration 9367, loss = 0.02297717\n",
      "Iteration 9368, loss = 0.02297334\n",
      "Iteration 9369, loss = 0.02296951\n",
      "Iteration 9370, loss = 0.02296568\n",
      "Iteration 9371, loss = 0.02296185\n",
      "Iteration 9372, loss = 0.02295803\n",
      "Iteration 9373, loss = 0.02295420\n",
      "Iteration 9374, loss = 0.02295038\n",
      "Iteration 9375, loss = 0.02294655\n",
      "Iteration 9376, loss = 0.02294273\n",
      "Iteration 9377, loss = 0.02293891\n",
      "Iteration 9378, loss = 0.02293509\n",
      "Iteration 9379, loss = 0.02293128\n",
      "Iteration 9380, loss = 0.02292746\n",
      "Iteration 9381, loss = 0.02292365\n",
      "Iteration 9382, loss = 0.02291983\n",
      "Iteration 9383, loss = 0.02291602\n",
      "Iteration 9384, loss = 0.02291221\n",
      "Iteration 9385, loss = 0.02290840\n",
      "Iteration 9386, loss = 0.02290459\n",
      "Iteration 9387, loss = 0.02290078\n",
      "Iteration 9388, loss = 0.02289698\n",
      "Iteration 9389, loss = 0.02289317\n",
      "Iteration 9390, loss = 0.02288937\n",
      "Iteration 9391, loss = 0.02288557\n",
      "Iteration 9392, loss = 0.02288177\n",
      "Iteration 9393, loss = 0.02287797\n",
      "Iteration 9394, loss = 0.02287417\n",
      "Iteration 9395, loss = 0.02287037\n",
      "Iteration 9396, loss = 0.02286658\n",
      "Iteration 9397, loss = 0.02286278\n",
      "Iteration 9398, loss = 0.02285899\n",
      "Iteration 9399, loss = 0.02285520\n",
      "Iteration 9400, loss = 0.02285140\n",
      "Iteration 9401, loss = 0.02284761\n",
      "Iteration 9402, loss = 0.02284383\n",
      "Iteration 9403, loss = 0.02284004\n",
      "Iteration 9404, loss = 0.02283625\n",
      "Iteration 9405, loss = 0.02283247\n",
      "Iteration 9406, loss = 0.02282869\n",
      "Iteration 9407, loss = 0.02282490\n",
      "Iteration 9408, loss = 0.02282112\n",
      "Iteration 9409, loss = 0.02281734\n",
      "Iteration 9410, loss = 0.02281356\n",
      "Iteration 9411, loss = 0.02280979\n",
      "Iteration 9412, loss = 0.02280601\n",
      "Iteration 9413, loss = 0.02280224\n",
      "Iteration 9414, loss = 0.02279846\n",
      "Iteration 9415, loss = 0.02279469\n",
      "Iteration 9416, loss = 0.02279092\n",
      "Iteration 9417, loss = 0.02278715\n",
      "Iteration 9418, loss = 0.02278338\n",
      "Iteration 9419, loss = 0.02277962\n",
      "Iteration 9420, loss = 0.02277585\n",
      "Iteration 9421, loss = 0.02277208\n",
      "Iteration 9422, loss = 0.02276832\n",
      "Iteration 9423, loss = 0.02276456\n",
      "Iteration 9424, loss = 0.02276080\n",
      "Iteration 9425, loss = 0.02275704\n",
      "Iteration 9426, loss = 0.02275328\n",
      "Iteration 9427, loss = 0.02274952\n",
      "Iteration 9428, loss = 0.02274577\n",
      "Iteration 9429, loss = 0.02274201\n",
      "Iteration 9430, loss = 0.02273826\n",
      "Iteration 9431, loss = 0.02273451\n",
      "Iteration 9432, loss = 0.02273076\n",
      "Iteration 9433, loss = 0.02272701\n",
      "Iteration 9434, loss = 0.02272326\n",
      "Iteration 9435, loss = 0.02271951\n",
      "Iteration 9436, loss = 0.02271576\n",
      "Iteration 9437, loss = 0.02271202\n",
      "Iteration 9438, loss = 0.02270828\n",
      "Iteration 9439, loss = 0.02270453\n",
      "Iteration 9440, loss = 0.02270079\n",
      "Iteration 9441, loss = 0.02269705\n",
      "Iteration 9442, loss = 0.02269331\n",
      "Iteration 9443, loss = 0.02268958\n",
      "Iteration 9444, loss = 0.02268584\n",
      "Iteration 9445, loss = 0.02268210\n",
      "Iteration 9446, loss = 0.02267837\n",
      "Iteration 9447, loss = 0.02267464\n",
      "Iteration 9448, loss = 0.02267091\n",
      "Iteration 9449, loss = 0.02266718\n",
      "Iteration 9450, loss = 0.02266345\n",
      "Iteration 9451, loss = 0.02265972\n",
      "Iteration 9452, loss = 0.02265599\n",
      "Iteration 9453, loss = 0.02265227\n",
      "Iteration 9454, loss = 0.02264854\n",
      "Iteration 9455, loss = 0.02264482\n",
      "Iteration 9456, loss = 0.02264110\n",
      "Iteration 9457, loss = 0.02263738\n",
      "Iteration 9458, loss = 0.02263366\n",
      "Iteration 9459, loss = 0.02262994\n",
      "Iteration 9460, loss = 0.02262623\n",
      "Iteration 9461, loss = 0.02262251\n",
      "Iteration 9462, loss = 0.02261880\n",
      "Iteration 9463, loss = 0.02261508\n",
      "Iteration 9464, loss = 0.02261137\n",
      "Iteration 9465, loss = 0.02260766\n",
      "Iteration 9466, loss = 0.02260395\n",
      "Iteration 9467, loss = 0.02260024\n",
      "Iteration 9468, loss = 0.02259654\n",
      "Iteration 9469, loss = 0.02259283\n",
      "Iteration 9470, loss = 0.02258913\n",
      "Iteration 9471, loss = 0.02258542\n",
      "Iteration 9472, loss = 0.02258172\n",
      "Iteration 9473, loss = 0.02257802\n",
      "Iteration 9474, loss = 0.02257432\n",
      "Iteration 9475, loss = 0.02257062\n",
      "Iteration 9476, loss = 0.02256693\n",
      "Iteration 9477, loss = 0.02256323\n",
      "Iteration 9478, loss = 0.02255954\n",
      "Iteration 9479, loss = 0.02255584\n",
      "Iteration 9480, loss = 0.02255215\n",
      "Iteration 9481, loss = 0.02254846\n",
      "Iteration 9482, loss = 0.02254477\n",
      "Iteration 9483, loss = 0.02254108\n",
      "Iteration 9484, loss = 0.02253739\n",
      "Iteration 9485, loss = 0.02253371\n",
      "Iteration 9486, loss = 0.02253002\n",
      "Iteration 9487, loss = 0.02252634\n",
      "Iteration 9488, loss = 0.02252266\n",
      "Iteration 9489, loss = 0.02251897\n",
      "Iteration 9490, loss = 0.02251529\n",
      "Iteration 9491, loss = 0.02251161\n",
      "Iteration 9492, loss = 0.02250794\n",
      "Iteration 9493, loss = 0.02250426\n",
      "Iteration 9494, loss = 0.02250058\n",
      "Iteration 9495, loss = 0.02249691\n",
      "Iteration 9496, loss = 0.02249324\n",
      "Iteration 9497, loss = 0.02248957\n",
      "Iteration 9498, loss = 0.02248590\n",
      "Iteration 9499, loss = 0.02248223\n",
      "Iteration 9500, loss = 0.02247856\n",
      "Iteration 9501, loss = 0.02247489\n",
      "Iteration 9502, loss = 0.02247122\n",
      "Iteration 9503, loss = 0.02246756\n",
      "Iteration 9504, loss = 0.02246390\n",
      "Iteration 9505, loss = 0.02246023\n",
      "Iteration 9506, loss = 0.02245657\n",
      "Iteration 9507, loss = 0.02245291\n",
      "Iteration 9508, loss = 0.02244925\n",
      "Iteration 9509, loss = 0.02244560\n",
      "Iteration 9510, loss = 0.02244194\n",
      "Iteration 9511, loss = 0.02243829\n",
      "Iteration 9512, loss = 0.02243463\n",
      "Iteration 9513, loss = 0.02243098\n",
      "Iteration 9514, loss = 0.02242733\n",
      "Iteration 9515, loss = 0.02242368\n",
      "Iteration 9516, loss = 0.02242003\n",
      "Iteration 9517, loss = 0.02241638\n",
      "Iteration 9518, loss = 0.02241273\n",
      "Iteration 9519, loss = 0.02240909\n",
      "Iteration 9520, loss = 0.02240544\n",
      "Iteration 9521, loss = 0.02240180\n",
      "Iteration 9522, loss = 0.02239816\n",
      "Iteration 9523, loss = 0.02239452\n",
      "Iteration 9524, loss = 0.02239088\n",
      "Iteration 9525, loss = 0.02238724\n",
      "Iteration 9526, loss = 0.02238360\n",
      "Iteration 9527, loss = 0.02237997\n",
      "Iteration 9528, loss = 0.02237633\n",
      "Iteration 9529, loss = 0.02237270\n",
      "Iteration 9530, loss = 0.02236907\n",
      "Iteration 9531, loss = 0.02236543\n",
      "Iteration 9532, loss = 0.02236180\n",
      "Iteration 9533, loss = 0.02235818\n",
      "Iteration 9534, loss = 0.02235455\n",
      "Iteration 9535, loss = 0.02235092\n",
      "Iteration 9536, loss = 0.02234730\n",
      "Iteration 9537, loss = 0.02234367\n",
      "Iteration 9538, loss = 0.02234005\n",
      "Iteration 9539, loss = 0.02233643\n",
      "Iteration 9540, loss = 0.02233281\n",
      "Iteration 9541, loss = 0.02232919\n",
      "Iteration 9542, loss = 0.02232557\n",
      "Iteration 9543, loss = 0.02232195\n",
      "Iteration 9544, loss = 0.02231833\n",
      "Iteration 9545, loss = 0.02231472\n",
      "Iteration 9546, loss = 0.02231111\n",
      "Iteration 9547, loss = 0.02230749\n",
      "Iteration 9548, loss = 0.02230388\n",
      "Iteration 9549, loss = 0.02230027\n",
      "Iteration 9550, loss = 0.02229666\n",
      "Iteration 9551, loss = 0.02229306\n",
      "Iteration 9552, loss = 0.02228945\n",
      "Iteration 9553, loss = 0.02228584\n",
      "Iteration 9554, loss = 0.02228224\n",
      "Iteration 9555, loss = 0.02227864\n",
      "Iteration 9556, loss = 0.02227503\n",
      "Iteration 9557, loss = 0.02227143\n",
      "Iteration 9558, loss = 0.02226783\n",
      "Iteration 9559, loss = 0.02226424\n",
      "Iteration 9560, loss = 0.02226064\n",
      "Iteration 9561, loss = 0.02225704\n",
      "Iteration 9562, loss = 0.02225345\n",
      "Iteration 9563, loss = 0.02224985\n",
      "Iteration 9564, loss = 0.02224626\n",
      "Iteration 9565, loss = 0.02224267\n",
      "Iteration 9566, loss = 0.02223908\n",
      "Iteration 9567, loss = 0.02223549\n",
      "Iteration 9568, loss = 0.02223190\n",
      "Iteration 9569, loss = 0.02222832\n",
      "Iteration 9570, loss = 0.02222473\n",
      "Iteration 9571, loss = 0.02222115\n",
      "Iteration 9572, loss = 0.02221756\n",
      "Iteration 9573, loss = 0.02221398\n",
      "Iteration 9574, loss = 0.02221040\n",
      "Iteration 9575, loss = 0.02220682\n",
      "Iteration 9576, loss = 0.02220324\n",
      "Iteration 9577, loss = 0.02219966\n",
      "Iteration 9578, loss = 0.02219609\n",
      "Iteration 9579, loss = 0.02219251\n",
      "Iteration 9580, loss = 0.02218894\n",
      "Iteration 9581, loss = 0.02218536\n",
      "Iteration 9582, loss = 0.02218179\n",
      "Iteration 9583, loss = 0.02217822\n",
      "Iteration 9584, loss = 0.02217465\n",
      "Iteration 9585, loss = 0.02217108\n",
      "Iteration 9586, loss = 0.02216752\n",
      "Iteration 9587, loss = 0.02216395\n",
      "Iteration 9588, loss = 0.02216039\n",
      "Iteration 9589, loss = 0.02215682\n",
      "Iteration 9590, loss = 0.02215326\n",
      "Iteration 9591, loss = 0.02214970\n",
      "Iteration 9592, loss = 0.02214614\n",
      "Iteration 9593, loss = 0.02214258\n",
      "Iteration 9594, loss = 0.02213902\n",
      "Iteration 9595, loss = 0.02213547\n",
      "Iteration 9596, loss = 0.02213191\n",
      "Iteration 9597, loss = 0.02212836\n",
      "Iteration 9598, loss = 0.02212480\n",
      "Iteration 9599, loss = 0.02212125\n",
      "Iteration 9600, loss = 0.02211770\n",
      "Iteration 9601, loss = 0.02211415\n",
      "Iteration 9602, loss = 0.02211060\n",
      "Iteration 9603, loss = 0.02210705\n",
      "Iteration 9604, loss = 0.02210351\n",
      "Iteration 9605, loss = 0.02209996\n",
      "Iteration 9606, loss = 0.02209642\n",
      "Iteration 9607, loss = 0.02209287\n",
      "Iteration 9608, loss = 0.02208933\n",
      "Iteration 9609, loss = 0.02208579\n",
      "Iteration 9610, loss = 0.02208225\n",
      "Iteration 9611, loss = 0.02207871\n",
      "Iteration 9612, loss = 0.02207518\n",
      "Iteration 9613, loss = 0.02207164\n",
      "Iteration 9614, loss = 0.02206811\n",
      "Iteration 9615, loss = 0.02206457\n",
      "Iteration 9616, loss = 0.02206104\n",
      "Iteration 9617, loss = 0.02205751\n",
      "Iteration 9618, loss = 0.02205398\n",
      "Iteration 9619, loss = 0.02205045\n",
      "Iteration 9620, loss = 0.02204692\n",
      "Iteration 9621, loss = 0.02204339\n",
      "Iteration 9622, loss = 0.02203987\n",
      "Iteration 9623, loss = 0.02203634\n",
      "Iteration 9624, loss = 0.02203282\n",
      "Iteration 9625, loss = 0.02202930\n",
      "Iteration 9626, loss = 0.02202577\n",
      "Iteration 9627, loss = 0.02202225\n",
      "Iteration 9628, loss = 0.02201873\n",
      "Iteration 9629, loss = 0.02201522\n",
      "Iteration 9630, loss = 0.02201170\n",
      "Iteration 9631, loss = 0.02200818\n",
      "Iteration 9632, loss = 0.02200467\n",
      "Iteration 9633, loss = 0.02200116\n",
      "Iteration 9634, loss = 0.02199764\n",
      "Iteration 9635, loss = 0.02199413\n",
      "Iteration 9636, loss = 0.02199062\n",
      "Iteration 9637, loss = 0.02198711\n",
      "Iteration 9638, loss = 0.02198361\n",
      "Iteration 9639, loss = 0.02198010\n",
      "Iteration 9640, loss = 0.02197659\n",
      "Iteration 9641, loss = 0.02197309\n",
      "Iteration 9642, loss = 0.02196959\n",
      "Iteration 9643, loss = 0.02196608\n",
      "Iteration 9644, loss = 0.02196258\n",
      "Iteration 9645, loss = 0.02195908\n",
      "Iteration 9646, loss = 0.02195558\n",
      "Iteration 9647, loss = 0.02195209\n",
      "Iteration 9648, loss = 0.02194859\n",
      "Iteration 9649, loss = 0.02194509\n",
      "Iteration 9650, loss = 0.02194160\n",
      "Iteration 9651, loss = 0.02193811\n",
      "Iteration 9652, loss = 0.02193461\n",
      "Iteration 9653, loss = 0.02193112\n",
      "Iteration 9654, loss = 0.02192763\n",
      "Iteration 9655, loss = 0.02192414\n",
      "Iteration 9656, loss = 0.02192066\n",
      "Iteration 9657, loss = 0.02191717\n",
      "Iteration 9658, loss = 0.02191368\n",
      "Iteration 9659, loss = 0.02191020\n",
      "Iteration 9660, loss = 0.02190672\n",
      "Iteration 9661, loss = 0.02190324\n",
      "Iteration 9662, loss = 0.02189975\n",
      "Iteration 9663, loss = 0.02189627\n",
      "Iteration 9664, loss = 0.02189280\n",
      "Iteration 9665, loss = 0.02188932\n",
      "Iteration 9666, loss = 0.02188584\n",
      "Iteration 9667, loss = 0.02188237\n",
      "Iteration 9668, loss = 0.02187889\n",
      "Iteration 9669, loss = 0.02187542\n",
      "Iteration 9670, loss = 0.02187195\n",
      "Iteration 9671, loss = 0.02186848\n",
      "Iteration 9672, loss = 0.02186501\n",
      "Iteration 9673, loss = 0.02186154\n",
      "Iteration 9674, loss = 0.02185807\n",
      "Iteration 9675, loss = 0.02185460\n",
      "Iteration 9676, loss = 0.02185114\n",
      "Iteration 9677, loss = 0.02184767\n",
      "Iteration 9678, loss = 0.02184421\n",
      "Iteration 9679, loss = 0.02184075\n",
      "Iteration 9680, loss = 0.02183729\n",
      "Iteration 9681, loss = 0.02183383\n",
      "Iteration 9682, loss = 0.02183037\n",
      "Iteration 9683, loss = 0.02182691\n",
      "Iteration 9684, loss = 0.02182345\n",
      "Iteration 9685, loss = 0.02182000\n",
      "Iteration 9686, loss = 0.02181654\n",
      "Iteration 9687, loss = 0.02181309\n",
      "Iteration 9688, loss = 0.02180964\n",
      "Iteration 9689, loss = 0.02180619\n",
      "Iteration 9690, loss = 0.02180274\n",
      "Iteration 9691, loss = 0.02179929\n",
      "Iteration 9692, loss = 0.02179584\n",
      "Iteration 9693, loss = 0.02179240\n",
      "Iteration 9694, loss = 0.02178895\n",
      "Iteration 9695, loss = 0.02178551\n",
      "Iteration 9696, loss = 0.02178206\n",
      "Iteration 9697, loss = 0.02177862\n",
      "Iteration 9698, loss = 0.02177518\n",
      "Iteration 9699, loss = 0.02177174\n",
      "Iteration 9700, loss = 0.02176830\n",
      "Iteration 9701, loss = 0.02176486\n",
      "Iteration 9702, loss = 0.02176143\n",
      "Iteration 9703, loss = 0.02175799\n",
      "Iteration 9704, loss = 0.02175456\n",
      "Iteration 9705, loss = 0.02175112\n",
      "Iteration 9706, loss = 0.02174769\n",
      "Iteration 9707, loss = 0.02174426\n",
      "Iteration 9708, loss = 0.02174083\n",
      "Iteration 9709, loss = 0.02173740\n",
      "Iteration 9710, loss = 0.02173397\n",
      "Iteration 9711, loss = 0.02173054\n",
      "Iteration 9712, loss = 0.02172712\n",
      "Iteration 9713, loss = 0.02172369\n",
      "Iteration 9714, loss = 0.02172027\n",
      "Iteration 9715, loss = 0.02171685\n",
      "Iteration 9716, loss = 0.02171343\n",
      "Iteration 9717, loss = 0.02171001\n",
      "Iteration 9718, loss = 0.02170659\n",
      "Iteration 9719, loss = 0.02170317\n",
      "Iteration 9720, loss = 0.02169975\n",
      "Iteration 9721, loss = 0.02169633\n",
      "Iteration 9722, loss = 0.02169292\n",
      "Iteration 9723, loss = 0.02168951\n",
      "Iteration 9724, loss = 0.02168609\n",
      "Iteration 9725, loss = 0.02168268\n",
      "Iteration 9726, loss = 0.02167927\n",
      "Iteration 9727, loss = 0.02167586\n",
      "Iteration 9728, loss = 0.02167245\n",
      "Iteration 9729, loss = 0.02166905\n",
      "Iteration 9730, loss = 0.02166564\n",
      "Iteration 9731, loss = 0.02166223\n",
      "Iteration 9732, loss = 0.02165883\n",
      "Iteration 9733, loss = 0.02165543\n",
      "Iteration 9734, loss = 0.02165202\n",
      "Iteration 9735, loss = 0.02164862\n",
      "Iteration 9736, loss = 0.02164522\n",
      "Iteration 9737, loss = 0.02164183\n",
      "Iteration 9738, loss = 0.02163843\n",
      "Iteration 9739, loss = 0.02163503\n",
      "Iteration 9740, loss = 0.02163164\n",
      "Iteration 9741, loss = 0.02162824\n",
      "Iteration 9742, loss = 0.02162485\n",
      "Iteration 9743, loss = 0.02162146\n",
      "Iteration 9744, loss = 0.02161806\n",
      "Iteration 9745, loss = 0.02161467\n",
      "Iteration 9746, loss = 0.02161129\n",
      "Iteration 9747, loss = 0.02160790\n",
      "Iteration 9748, loss = 0.02160451\n",
      "Iteration 9749, loss = 0.02160112\n",
      "Iteration 9750, loss = 0.02159774\n",
      "Iteration 9751, loss = 0.02159436\n",
      "Iteration 9752, loss = 0.02159097\n",
      "Iteration 9753, loss = 0.02158759\n",
      "Iteration 9754, loss = 0.02158421\n",
      "Iteration 9755, loss = 0.02158083\n",
      "Iteration 9756, loss = 0.02157745\n",
      "Iteration 9757, loss = 0.02157408\n",
      "Iteration 9758, loss = 0.02157070\n",
      "Iteration 9759, loss = 0.02156733\n",
      "Iteration 9760, loss = 0.02156395\n",
      "Iteration 9761, loss = 0.02156058\n",
      "Iteration 9762, loss = 0.02155721\n",
      "Iteration 9763, loss = 0.02155384\n",
      "Iteration 9764, loss = 0.02155047\n",
      "Iteration 9765, loss = 0.02154710\n",
      "Iteration 9766, loss = 0.02154373\n",
      "Iteration 9767, loss = 0.02154036\n",
      "Iteration 9768, loss = 0.02153700\n",
      "Iteration 9769, loss = 0.02153363\n",
      "Iteration 9770, loss = 0.02153027\n",
      "Iteration 9771, loss = 0.02152691\n",
      "Iteration 9772, loss = 0.02152355\n",
      "Iteration 9773, loss = 0.02152019\n",
      "Iteration 9774, loss = 0.02151683\n",
      "Iteration 9775, loss = 0.02151347\n",
      "Iteration 9776, loss = 0.02151011\n",
      "Iteration 9777, loss = 0.02150676\n",
      "Iteration 9778, loss = 0.02150340\n",
      "Iteration 9779, loss = 0.02150005\n",
      "Iteration 9780, loss = 0.02149669\n",
      "Iteration 9781, loss = 0.02149334\n",
      "Iteration 9782, loss = 0.02148999\n",
      "Iteration 9783, loss = 0.02148664\n",
      "Iteration 9784, loss = 0.02148329\n",
      "Iteration 9785, loss = 0.02147995\n",
      "Iteration 9786, loss = 0.02147660\n",
      "Iteration 9787, loss = 0.02147325\n",
      "Iteration 9788, loss = 0.02146991\n",
      "Iteration 9789, loss = 0.02146657\n",
      "Iteration 9790, loss = 0.02146322\n",
      "Iteration 9791, loss = 0.02145988\n",
      "Iteration 9792, loss = 0.02145654\n",
      "Iteration 9793, loss = 0.02145320\n",
      "Iteration 9794, loss = 0.02144987\n",
      "Iteration 9795, loss = 0.02144653\n",
      "Iteration 9796, loss = 0.02144319\n",
      "Iteration 9797, loss = 0.02143986\n",
      "Iteration 9798, loss = 0.02143652\n",
      "Iteration 9799, loss = 0.02143319\n",
      "Iteration 9800, loss = 0.02142986\n",
      "Iteration 9801, loss = 0.02142653\n",
      "Iteration 9802, loss = 0.02142320\n",
      "Iteration 9803, loss = 0.02141987\n",
      "Iteration 9804, loss = 0.02141654\n",
      "Iteration 9805, loss = 0.02141322\n",
      "Iteration 9806, loss = 0.02140989\n",
      "Iteration 9807, loss = 0.02140657\n",
      "Iteration 9808, loss = 0.02140324\n",
      "Iteration 9809, loss = 0.02139992\n",
      "Iteration 9810, loss = 0.02139660\n",
      "Iteration 9811, loss = 0.02139328\n",
      "Iteration 9812, loss = 0.02138996\n",
      "Iteration 9813, loss = 0.02138664\n",
      "Iteration 9814, loss = 0.02138333\n",
      "Iteration 9815, loss = 0.02138001\n",
      "Iteration 9816, loss = 0.02137669\n",
      "Iteration 9817, loss = 0.02137338\n",
      "Iteration 9818, loss = 0.02137007\n",
      "Iteration 9819, loss = 0.02136676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9820, loss = 0.02136344\n",
      "Iteration 9821, loss = 0.02136013\n",
      "Iteration 9822, loss = 0.02135683\n",
      "Iteration 9823, loss = 0.02135352\n",
      "Iteration 9824, loss = 0.02135021\n",
      "Iteration 9825, loss = 0.02134691\n",
      "Iteration 9826, loss = 0.02134360\n",
      "Iteration 9827, loss = 0.02134030\n",
      "Iteration 9828, loss = 0.02133700\n",
      "Iteration 9829, loss = 0.02133369\n",
      "Iteration 9830, loss = 0.02133039\n",
      "Iteration 9831, loss = 0.02132709\n",
      "Iteration 9832, loss = 0.02132380\n",
      "Iteration 9833, loss = 0.02132050\n",
      "Iteration 9834, loss = 0.02131720\n",
      "Iteration 9835, loss = 0.02131391\n",
      "Iteration 9836, loss = 0.02131061\n",
      "Iteration 9837, loss = 0.02130732\n",
      "Iteration 9838, loss = 0.02130403\n",
      "Iteration 9839, loss = 0.02130074\n",
      "Iteration 9840, loss = 0.02129745\n",
      "Iteration 9841, loss = 0.02129416\n",
      "Iteration 9842, loss = 0.02129087\n",
      "Iteration 9843, loss = 0.02128758\n",
      "Iteration 9844, loss = 0.02128430\n",
      "Iteration 9845, loss = 0.02128101\n",
      "Iteration 9846, loss = 0.02127773\n",
      "Iteration 9847, loss = 0.02127444\n",
      "Iteration 9848, loss = 0.02127116\n",
      "Iteration 9849, loss = 0.02126788\n",
      "Iteration 9850, loss = 0.02126460\n",
      "Iteration 9851, loss = 0.02126132\n",
      "Iteration 9852, loss = 0.02125804\n",
      "Iteration 9853, loss = 0.02125477\n",
      "Iteration 9854, loss = 0.02125149\n",
      "Iteration 9855, loss = 0.02124822\n",
      "Iteration 9856, loss = 0.02124494\n",
      "Iteration 9857, loss = 0.02124167\n",
      "Iteration 9858, loss = 0.02123840\n",
      "Iteration 9859, loss = 0.02123513\n",
      "Iteration 9860, loss = 0.02123186\n",
      "Iteration 9861, loss = 0.02122859\n",
      "Iteration 9862, loss = 0.02122532\n",
      "Iteration 9863, loss = 0.02122206\n",
      "Iteration 9864, loss = 0.02121879\n",
      "Iteration 9865, loss = 0.02121553\n",
      "Iteration 9866, loss = 0.02121226\n",
      "Iteration 9867, loss = 0.02120900\n",
      "Iteration 9868, loss = 0.02120574\n",
      "Iteration 9869, loss = 0.02120248\n",
      "Iteration 9870, loss = 0.02119922\n",
      "Iteration 9871, loss = 0.02119596\n",
      "Iteration 9872, loss = 0.02119270\n",
      "Iteration 9873, loss = 0.02118945\n",
      "Iteration 9874, loss = 0.02118619\n",
      "Iteration 9875, loss = 0.02118294\n",
      "Iteration 9876, loss = 0.02117968\n",
      "Iteration 9877, loss = 0.02117643\n",
      "Iteration 9878, loss = 0.02117318\n",
      "Iteration 9879, loss = 0.02116993\n",
      "Iteration 9880, loss = 0.02116668\n",
      "Iteration 9881, loss = 0.02116343\n",
      "Iteration 9882, loss = 0.02116018\n",
      "Iteration 9883, loss = 0.02115694\n",
      "Iteration 9884, loss = 0.02115369\n",
      "Iteration 9885, loss = 0.02115045\n",
      "Iteration 9886, loss = 0.02114721\n",
      "Iteration 9887, loss = 0.02114396\n",
      "Iteration 9888, loss = 0.02114072\n",
      "Iteration 9889, loss = 0.02113748\n",
      "Iteration 9890, loss = 0.02113424\n",
      "Iteration 9891, loss = 0.02113100\n",
      "Iteration 9892, loss = 0.02112777\n",
      "Iteration 9893, loss = 0.02112453\n",
      "Iteration 9894, loss = 0.02112130\n",
      "Iteration 9895, loss = 0.02111806\n",
      "Iteration 9896, loss = 0.02111483\n",
      "Iteration 9897, loss = 0.02111160\n",
      "Iteration 9898, loss = 0.02110837\n",
      "Iteration 9899, loss = 0.02110513\n",
      "Iteration 9900, loss = 0.02110191\n",
      "Iteration 9901, loss = 0.02109868\n",
      "Iteration 9902, loss = 0.02109545\n",
      "Iteration 9903, loss = 0.02109222\n",
      "Iteration 9904, loss = 0.02108900\n",
      "Iteration 9905, loss = 0.02108578\n",
      "Iteration 9906, loss = 0.02108255\n",
      "Iteration 9907, loss = 0.02107933\n",
      "Iteration 9908, loss = 0.02107611\n",
      "Iteration 9909, loss = 0.02107289\n",
      "Iteration 9910, loss = 0.02106967\n",
      "Iteration 9911, loss = 0.02106645\n",
      "Iteration 9912, loss = 0.02106323\n",
      "Iteration 9913, loss = 0.02106002\n",
      "Iteration 9914, loss = 0.02105680\n",
      "Iteration 9915, loss = 0.02105359\n",
      "Iteration 9916, loss = 0.02105038\n",
      "Iteration 9917, loss = 0.02104716\n",
      "Iteration 9918, loss = 0.02104395\n",
      "Iteration 9919, loss = 0.02104074\n",
      "Iteration 9920, loss = 0.02103753\n",
      "Iteration 9921, loss = 0.02103433\n",
      "Iteration 9922, loss = 0.02103112\n",
      "Iteration 9923, loss = 0.02102791\n",
      "Iteration 9924, loss = 0.02102471\n",
      "Iteration 9925, loss = 0.02102150\n",
      "Iteration 9926, loss = 0.02101830\n",
      "Iteration 9927, loss = 0.02101510\n",
      "Iteration 9928, loss = 0.02101190\n",
      "Iteration 9929, loss = 0.02100870\n",
      "Iteration 9930, loss = 0.02100550\n",
      "Iteration 9931, loss = 0.02100230\n",
      "Iteration 9932, loss = 0.02099910\n",
      "Iteration 9933, loss = 0.02099591\n",
      "Iteration 9934, loss = 0.02099271\n",
      "Iteration 9935, loss = 0.02098952\n",
      "Iteration 9936, loss = 0.02098632\n",
      "Iteration 9937, loss = 0.02098313\n",
      "Iteration 9938, loss = 0.02097994\n",
      "Iteration 9939, loss = 0.02097675\n",
      "Iteration 9940, loss = 0.02097356\n",
      "Iteration 9941, loss = 0.02097037\n",
      "Iteration 9942, loss = 0.02096718\n",
      "Iteration 9943, loss = 0.02096400\n",
      "Iteration 9944, loss = 0.02096081\n",
      "Iteration 9945, loss = 0.02095763\n",
      "Iteration 9946, loss = 0.02095444\n",
      "Iteration 9947, loss = 0.02095126\n",
      "Iteration 9948, loss = 0.02094808\n",
      "Iteration 9949, loss = 0.02094490\n",
      "Iteration 9950, loss = 0.02094172\n",
      "Iteration 9951, loss = 0.02093854\n",
      "Iteration 9952, loss = 0.02093536\n",
      "Iteration 9953, loss = 0.02093219\n",
      "Iteration 9954, loss = 0.02092901\n",
      "Iteration 9955, loss = 0.02092584\n",
      "Iteration 9956, loss = 0.02092266\n",
      "Iteration 9957, loss = 0.02091949\n",
      "Iteration 9958, loss = 0.02091632\n",
      "Iteration 9959, loss = 0.02091315\n",
      "Iteration 9960, loss = 0.02090998\n",
      "Iteration 9961, loss = 0.02090681\n",
      "Iteration 9962, loss = 0.02090364\n",
      "Iteration 9963, loss = 0.02090048\n",
      "Iteration 9964, loss = 0.02089731\n",
      "Iteration 9965, loss = 0.02089415\n",
      "Iteration 9966, loss = 0.02089098\n",
      "Iteration 9967, loss = 0.02088782\n",
      "Iteration 9968, loss = 0.02088466\n",
      "Iteration 9969, loss = 0.02088150\n",
      "Iteration 9970, loss = 0.02087834\n",
      "Iteration 9971, loss = 0.02087518\n",
      "Iteration 9972, loss = 0.02087202\n",
      "Iteration 9973, loss = 0.02086886\n",
      "Iteration 9974, loss = 0.02086571\n",
      "Iteration 9975, loss = 0.02086255\n",
      "Iteration 9976, loss = 0.02085940\n",
      "Iteration 9977, loss = 0.02085625\n",
      "Iteration 9978, loss = 0.02085309\n",
      "Iteration 9979, loss = 0.02084994\n",
      "Iteration 9980, loss = 0.02084679\n",
      "Iteration 9981, loss = 0.02084364\n",
      "Iteration 9982, loss = 0.02084049\n",
      "Iteration 9983, loss = 0.02083735\n",
      "Iteration 9984, loss = 0.02083420\n",
      "Iteration 9985, loss = 0.02083106\n",
      "Iteration 9986, loss = 0.02082791\n",
      "Iteration 9987, loss = 0.02082477\n",
      "Iteration 9988, loss = 0.02082163\n",
      "Iteration 9989, loss = 0.02081848\n",
      "Iteration 9990, loss = 0.02081534\n",
      "Iteration 9991, loss = 0.02081220\n",
      "Iteration 9992, loss = 0.02080907\n",
      "Iteration 9993, loss = 0.02080593\n",
      "Iteration 9994, loss = 0.02080279\n",
      "Iteration 9995, loss = 0.02079966\n",
      "Iteration 9996, loss = 0.02079652\n",
      "Iteration 9997, loss = 0.02079339\n",
      "Iteration 9998, loss = 0.02079025\n",
      "Iteration 9999, loss = 0.02078712\n",
      "Iteration 10000, loss = 0.02078399\n",
      "Iteration 10001, loss = 0.02078086\n",
      "Iteration 10002, loss = 0.02077773\n",
      "Iteration 10003, loss = 0.02077460\n",
      "Iteration 10004, loss = 0.02077148\n",
      "Iteration 10005, loss = 0.02076835\n",
      "Iteration 10006, loss = 0.02076523\n",
      "Iteration 10007, loss = 0.02076210\n",
      "Iteration 10008, loss = 0.02075898\n",
      "Iteration 10009, loss = 0.02075586\n",
      "Iteration 10010, loss = 0.02075274\n",
      "Iteration 10011, loss = 0.02074962\n",
      "Iteration 10012, loss = 0.02074650\n",
      "Iteration 10013, loss = 0.02074338\n",
      "Iteration 10014, loss = 0.02074026\n",
      "Iteration 10015, loss = 0.02073714\n",
      "Iteration 10016, loss = 0.02073403\n",
      "Iteration 10017, loss = 0.02073091\n",
      "Iteration 10018, loss = 0.02072780\n",
      "Iteration 10019, loss = 0.02072469\n",
      "Iteration 10020, loss = 0.02072158\n",
      "Iteration 10021, loss = 0.02071847\n",
      "Iteration 10022, loss = 0.02071536\n",
      "Iteration 10023, loss = 0.02071225\n",
      "Iteration 10024, loss = 0.02070914\n",
      "Iteration 10025, loss = 0.02070603\n",
      "Iteration 10026, loss = 0.02070293\n",
      "Iteration 10027, loss = 0.02069982\n",
      "Iteration 10028, loss = 0.02069672\n",
      "Iteration 10029, loss = 0.02069361\n",
      "Iteration 10030, loss = 0.02069051\n",
      "Iteration 10031, loss = 0.02068741\n",
      "Iteration 10032, loss = 0.02068431\n",
      "Iteration 10033, loss = 0.02068121\n",
      "Iteration 10034, loss = 0.02067811\n",
      "Iteration 10035, loss = 0.02067501\n",
      "Iteration 10036, loss = 0.02067192\n",
      "Iteration 10037, loss = 0.02066882\n",
      "Iteration 10038, loss = 0.02066573\n",
      "Iteration 10039, loss = 0.02066263\n",
      "Iteration 10040, loss = 0.02065954\n",
      "Iteration 10041, loss = 0.02065645\n",
      "Iteration 10042, loss = 0.02065336\n",
      "Iteration 10043, loss = 0.02065027\n",
      "Iteration 10044, loss = 0.02064718\n",
      "Iteration 10045, loss = 0.02064409\n",
      "Iteration 10046, loss = 0.02064101\n",
      "Iteration 10047, loss = 0.02063792\n",
      "Iteration 10048, loss = 0.02063483\n",
      "Iteration 10049, loss = 0.02063175\n",
      "Iteration 10050, loss = 0.02062867\n",
      "Iteration 10051, loss = 0.02062558\n",
      "Iteration 10052, loss = 0.02062250\n",
      "Iteration 10053, loss = 0.02061942\n",
      "Iteration 10054, loss = 0.02061634\n",
      "Iteration 10055, loss = 0.02061326\n",
      "Iteration 10056, loss = 0.02061019\n",
      "Iteration 10057, loss = 0.02060711\n",
      "Iteration 10058, loss = 0.02060403\n",
      "Iteration 10059, loss = 0.02060096\n",
      "Iteration 10060, loss = 0.02059789\n",
      "Iteration 10061, loss = 0.02059481\n",
      "Iteration 10062, loss = 0.02059174\n",
      "Iteration 10063, loss = 0.02058867\n",
      "Iteration 10064, loss = 0.02058560\n",
      "Iteration 10065, loss = 0.02058253\n",
      "Iteration 10066, loss = 0.02057946\n",
      "Iteration 10067, loss = 0.02057639\n",
      "Iteration 10068, loss = 0.02057333\n",
      "Iteration 10069, loss = 0.02057026\n",
      "Iteration 10070, loss = 0.02056720\n",
      "Iteration 10071, loss = 0.02056413\n",
      "Iteration 10072, loss = 0.02056107\n",
      "Iteration 10073, loss = 0.02055801\n",
      "Iteration 10074, loss = 0.02055495\n",
      "Iteration 10075, loss = 0.02055189\n",
      "Iteration 10076, loss = 0.02054883\n",
      "Iteration 10077, loss = 0.02054577\n",
      "Iteration 10078, loss = 0.02054272\n",
      "Iteration 10079, loss = 0.02053966\n",
      "Iteration 10080, loss = 0.02053660\n",
      "Iteration 10081, loss = 0.02053355\n",
      "Iteration 10082, loss = 0.02053050\n",
      "Iteration 10083, loss = 0.02052744\n",
      "Iteration 10084, loss = 0.02052439\n",
      "Iteration 10085, loss = 0.02052134\n",
      "Iteration 10086, loss = 0.02051829\n",
      "Iteration 10087, loss = 0.02051524\n",
      "Iteration 10088, loss = 0.02051220\n",
      "Iteration 10089, loss = 0.02050915\n",
      "Iteration 10090, loss = 0.02050610\n",
      "Iteration 10091, loss = 0.02050306\n",
      "Iteration 10092, loss = 0.02050001\n",
      "Iteration 10093, loss = 0.02049697\n",
      "Iteration 10094, loss = 0.02049393\n",
      "Iteration 10095, loss = 0.02049089\n",
      "Iteration 10096, loss = 0.02048785\n",
      "Iteration 10097, loss = 0.02048481\n",
      "Iteration 10098, loss = 0.02048177\n",
      "Iteration 10099, loss = 0.02047873\n",
      "Iteration 10100, loss = 0.02047569\n",
      "Iteration 10101, loss = 0.02047266\n",
      "Iteration 10102, loss = 0.02046962\n",
      "Iteration 10103, loss = 0.02046659\n",
      "Iteration 10104, loss = 0.02046356\n",
      "Iteration 10105, loss = 0.02046053\n",
      "Iteration 10106, loss = 0.02045749\n",
      "Iteration 10107, loss = 0.02045446\n",
      "Iteration 10108, loss = 0.02045143\n",
      "Iteration 10109, loss = 0.02044841\n",
      "Iteration 10110, loss = 0.02044538\n",
      "Iteration 10111, loss = 0.02044235\n",
      "Iteration 10112, loss = 0.02043933\n",
      "Iteration 10113, loss = 0.02043630\n",
      "Iteration 10114, loss = 0.02043328\n",
      "Iteration 10115, loss = 0.02043025\n",
      "Iteration 10116, loss = 0.02042723\n",
      "Iteration 10117, loss = 0.02042421\n",
      "Iteration 10118, loss = 0.02042119\n",
      "Iteration 10119, loss = 0.02041817\n",
      "Iteration 10120, loss = 0.02041515\n",
      "Iteration 10121, loss = 0.02041214\n",
      "Iteration 10122, loss = 0.02040912\n",
      "Iteration 10123, loss = 0.02040610\n",
      "Iteration 10124, loss = 0.02040309\n",
      "Iteration 10125, loss = 0.02040008\n",
      "Iteration 10126, loss = 0.02039706\n",
      "Iteration 10127, loss = 0.02039405\n",
      "Iteration 10128, loss = 0.02039104\n",
      "Iteration 10129, loss = 0.02038803\n",
      "Iteration 10130, loss = 0.02038502\n",
      "Iteration 10131, loss = 0.02038201\n",
      "Iteration 10132, loss = 0.02037900\n",
      "Iteration 10133, loss = 0.02037600\n",
      "Iteration 10134, loss = 0.02037299\n",
      "Iteration 10135, loss = 0.02036999\n",
      "Iteration 10136, loss = 0.02036698\n",
      "Iteration 10137, loss = 0.02036398\n",
      "Iteration 10138, loss = 0.02036098\n",
      "Iteration 10139, loss = 0.02035798\n",
      "Iteration 10140, loss = 0.02035498\n",
      "Iteration 10141, loss = 0.02035198\n",
      "Iteration 10142, loss = 0.02034898\n",
      "Iteration 10143, loss = 0.02034598\n",
      "Iteration 10144, loss = 0.02034299\n",
      "Iteration 10145, loss = 0.02033999\n",
      "Iteration 10146, loss = 0.02033700\n",
      "Iteration 10147, loss = 0.02033400\n",
      "Iteration 10148, loss = 0.02033101\n",
      "Iteration 10149, loss = 0.02032802\n",
      "Iteration 10150, loss = 0.02032503\n",
      "Iteration 10151, loss = 0.02032204\n",
      "Iteration 10152, loss = 0.02031905\n",
      "Iteration 10153, loss = 0.02031606\n",
      "Iteration 10154, loss = 0.02031307\n",
      "Iteration 10155, loss = 0.02031008\n",
      "Iteration 10156, loss = 0.02030710\n",
      "Iteration 10157, loss = 0.02030411\n",
      "Iteration 10158, loss = 0.02030113\n",
      "Iteration 10159, loss = 0.02029815\n",
      "Iteration 10160, loss = 0.02029517\n",
      "Iteration 10161, loss = 0.02029218\n",
      "Iteration 10162, loss = 0.02028920\n",
      "Iteration 10163, loss = 0.02028622\n",
      "Iteration 10164, loss = 0.02028325\n",
      "Iteration 10165, loss = 0.02028027\n",
      "Iteration 10166, loss = 0.02027729\n",
      "Iteration 10167, loss = 0.02027432\n",
      "Iteration 10168, loss = 0.02027134\n",
      "Iteration 10169, loss = 0.02026837\n",
      "Iteration 10170, loss = 0.02026539\n",
      "Iteration 10171, loss = 0.02026242\n",
      "Iteration 10172, loss = 0.02025945\n",
      "Iteration 10173, loss = 0.02025648\n",
      "Iteration 10174, loss = 0.02025351\n",
      "Iteration 10175, loss = 0.02025054\n",
      "Iteration 10176, loss = 0.02024757\n",
      "Iteration 10177, loss = 0.02024461\n",
      "Iteration 10178, loss = 0.02024164\n",
      "Iteration 10179, loss = 0.02023868\n",
      "Iteration 10180, loss = 0.02023571\n",
      "Iteration 10181, loss = 0.02023275\n",
      "Iteration 10182, loss = 0.02022979\n",
      "Iteration 10183, loss = 0.02022682\n",
      "Iteration 10184, loss = 0.02022386\n",
      "Iteration 10185, loss = 0.02022090\n",
      "Iteration 10186, loss = 0.02021795\n",
      "Iteration 10187, loss = 0.02021499\n",
      "Iteration 10188, loss = 0.02021203\n",
      "Iteration 10189, loss = 0.02020907\n",
      "Iteration 10190, loss = 0.02020612\n",
      "Iteration 10191, loss = 0.02020316\n",
      "Iteration 10192, loss = 0.02020021\n",
      "Iteration 10193, loss = 0.02019726\n",
      "Iteration 10194, loss = 0.02019431\n",
      "Iteration 10195, loss = 0.02019136\n",
      "Iteration 10196, loss = 0.02018841\n",
      "Iteration 10197, loss = 0.02018546\n",
      "Iteration 10198, loss = 0.02018251\n",
      "Iteration 10199, loss = 0.02017956\n",
      "Iteration 10200, loss = 0.02017661\n",
      "Iteration 10201, loss = 0.02017367\n",
      "Iteration 10202, loss = 0.02017072\n",
      "Iteration 10203, loss = 0.02016778\n",
      "Iteration 10204, loss = 0.02016484\n",
      "Iteration 10205, loss = 0.02016190\n",
      "Iteration 10206, loss = 0.02015895\n",
      "Iteration 10207, loss = 0.02015601\n",
      "Iteration 10208, loss = 0.02015307\n",
      "Iteration 10209, loss = 0.02015014\n",
      "Iteration 10210, loss = 0.02014720\n",
      "Iteration 10211, loss = 0.02014426\n",
      "Iteration 10212, loss = 0.02014133\n",
      "Iteration 10213, loss = 0.02013839\n",
      "Iteration 10214, loss = 0.02013546\n",
      "Iteration 10215, loss = 0.02013252\n",
      "Iteration 10216, loss = 0.02012959\n",
      "Iteration 10217, loss = 0.02012666\n",
      "Iteration 10218, loss = 0.02012373\n",
      "Iteration 10219, loss = 0.02012080\n",
      "Iteration 10220, loss = 0.02011787\n",
      "Iteration 10221, loss = 0.02011494\n",
      "Iteration 10222, loss = 0.02011202\n",
      "Iteration 10223, loss = 0.02010909\n",
      "Iteration 10224, loss = 0.02010616\n",
      "Iteration 10225, loss = 0.02010324\n",
      "Iteration 10226, loss = 0.02010032\n",
      "Iteration 10227, loss = 0.02009739\n",
      "Iteration 10228, loss = 0.02009447\n",
      "Iteration 10229, loss = 0.02009155\n",
      "Iteration 10230, loss = 0.02008863\n",
      "Iteration 10231, loss = 0.02008571\n",
      "Iteration 10232, loss = 0.02008279\n",
      "Iteration 10233, loss = 0.02007988\n",
      "Iteration 10234, loss = 0.02007696\n",
      "Iteration 10235, loss = 0.02007404\n",
      "Iteration 10236, loss = 0.02007113\n",
      "Iteration 10237, loss = 0.02006821\n",
      "Iteration 10238, loss = 0.02006530\n",
      "Iteration 10239, loss = 0.02006239\n",
      "Iteration 10240, loss = 0.02005948\n",
      "Iteration 10241, loss = 0.02005657\n",
      "Iteration 10242, loss = 0.02005366\n",
      "Iteration 10243, loss = 0.02005075\n",
      "Iteration 10244, loss = 0.02004784\n",
      "Iteration 10245, loss = 0.02004493\n",
      "Iteration 10246, loss = 0.02004203\n",
      "Iteration 10247, loss = 0.02003912\n",
      "Iteration 10248, loss = 0.02003622\n",
      "Iteration 10249, loss = 0.02003331\n",
      "Iteration 10250, loss = 0.02003041\n",
      "Iteration 10251, loss = 0.02002751\n",
      "Iteration 10252, loss = 0.02002461\n",
      "Iteration 10253, loss = 0.02002171\n",
      "Iteration 10254, loss = 0.02001881\n",
      "Iteration 10255, loss = 0.02001591\n",
      "Iteration 10256, loss = 0.02001301\n",
      "Iteration 10257, loss = 0.02001012\n",
      "Iteration 10258, loss = 0.02000722\n",
      "Iteration 10259, loss = 0.02000433\n",
      "Iteration 10260, loss = 0.02000143\n",
      "Iteration 10261, loss = 0.01999854\n",
      "Iteration 10262, loss = 0.01999565\n",
      "Iteration 10263, loss = 0.01999275\n",
      "Iteration 10264, loss = 0.01998986\n",
      "Iteration 10265, loss = 0.01998697\n",
      "Iteration 10266, loss = 0.01998409\n",
      "Iteration 10267, loss = 0.01998120\n",
      "Iteration 10268, loss = 0.01997831\n",
      "Iteration 10269, loss = 0.01997542\n",
      "Iteration 10270, loss = 0.01997254\n",
      "Iteration 10271, loss = 0.01996965\n",
      "Iteration 10272, loss = 0.01996677\n",
      "Iteration 10273, loss = 0.01996389\n",
      "Iteration 10274, loss = 0.01996101\n",
      "Iteration 10275, loss = 0.01995812\n",
      "Iteration 10276, loss = 0.01995524\n",
      "Iteration 10277, loss = 0.01995236\n",
      "Iteration 10278, loss = 0.01994949\n",
      "Iteration 10279, loss = 0.01994661\n",
      "Iteration 10280, loss = 0.01994373\n",
      "Iteration 10281, loss = 0.01994085\n",
      "Iteration 10282, loss = 0.01993798\n",
      "Iteration 10283, loss = 0.01993511\n",
      "Iteration 10284, loss = 0.01993223\n",
      "Iteration 10285, loss = 0.01992936\n",
      "Iteration 10286, loss = 0.01992649\n",
      "Iteration 10287, loss = 0.01992362\n",
      "Iteration 10288, loss = 0.01992075\n",
      "Iteration 10289, loss = 0.01991788\n",
      "Iteration 10290, loss = 0.01991501\n",
      "Iteration 10291, loss = 0.01991214\n",
      "Iteration 10292, loss = 0.01990927\n",
      "Iteration 10293, loss = 0.01990641\n",
      "Iteration 10294, loss = 0.01990354\n",
      "Iteration 10295, loss = 0.01990068\n",
      "Iteration 10296, loss = 0.01989782\n",
      "Iteration 10297, loss = 0.01989495\n",
      "Iteration 10298, loss = 0.01989209\n",
      "Iteration 10299, loss = 0.01988923\n",
      "Iteration 10300, loss = 0.01988637\n",
      "Iteration 10301, loss = 0.01988351\n",
      "Iteration 10302, loss = 0.01988065\n",
      "Iteration 10303, loss = 0.01987780\n",
      "Iteration 10304, loss = 0.01987494\n",
      "Iteration 10305, loss = 0.01987209\n",
      "Iteration 10306, loss = 0.01986923\n",
      "Iteration 10307, loss = 0.01986638\n",
      "Iteration 10308, loss = 0.01986352\n",
      "Iteration 10309, loss = 0.01986067\n",
      "Iteration 10310, loss = 0.01985782\n",
      "Iteration 10311, loss = 0.01985497\n",
      "Iteration 10312, loss = 0.01985212\n",
      "Iteration 10313, loss = 0.01984927\n",
      "Iteration 10314, loss = 0.01984642\n",
      "Iteration 10315, loss = 0.01984358\n",
      "Iteration 10316, loss = 0.01984073\n",
      "Iteration 10317, loss = 0.01983788\n",
      "Iteration 10318, loss = 0.01983504\n",
      "Iteration 10319, loss = 0.01983219\n",
      "Iteration 10320, loss = 0.01982935\n",
      "Iteration 10321, loss = 0.01982651\n",
      "Iteration 10322, loss = 0.01982367\n",
      "Iteration 10323, loss = 0.01982083\n",
      "Iteration 10324, loss = 0.01981799\n",
      "Iteration 10325, loss = 0.01981515\n",
      "Iteration 10326, loss = 0.01981231\n",
      "Iteration 10327, loss = 0.01980947\n",
      "Iteration 10328, loss = 0.01980664\n",
      "Iteration 10329, loss = 0.01980380\n",
      "Iteration 10330, loss = 0.01980097\n",
      "Iteration 10331, loss = 0.01979813\n",
      "Iteration 10332, loss = 0.01979530\n",
      "Iteration 10333, loss = 0.01979247\n",
      "Iteration 10334, loss = 0.01978964\n",
      "Iteration 10335, loss = 0.01978681\n",
      "Iteration 10336, loss = 0.01978398\n",
      "Iteration 10337, loss = 0.01978115\n",
      "Iteration 10338, loss = 0.01977832\n",
      "Iteration 10339, loss = 0.01977549\n",
      "Iteration 10340, loss = 0.01977267\n",
      "Iteration 10341, loss = 0.01976984\n",
      "Iteration 10342, loss = 0.01976702\n",
      "Iteration 10343, loss = 0.01976419\n",
      "Iteration 10344, loss = 0.01976137\n",
      "Iteration 10345, loss = 0.01975855\n",
      "Iteration 10346, loss = 0.01975573\n",
      "Iteration 10347, loss = 0.01975291\n",
      "Iteration 10348, loss = 0.01975009\n",
      "Iteration 10349, loss = 0.01974727\n",
      "Iteration 10350, loss = 0.01974445\n",
      "Iteration 10351, loss = 0.01974163\n",
      "Iteration 10352, loss = 0.01973882\n",
      "Iteration 10353, loss = 0.01973600\n",
      "Iteration 10354, loss = 0.01973319\n",
      "Iteration 10355, loss = 0.01973037\n",
      "Iteration 10356, loss = 0.01972756\n",
      "Iteration 10357, loss = 0.01972475\n",
      "Iteration 10358, loss = 0.01972194\n",
      "Iteration 10359, loss = 0.01971913\n",
      "Iteration 10360, loss = 0.01971632\n",
      "Iteration 10361, loss = 0.01971351\n",
      "Iteration 10362, loss = 0.01971070\n",
      "Iteration 10363, loss = 0.01970789\n",
      "Iteration 10364, loss = 0.01970509\n",
      "Iteration 10365, loss = 0.01970228\n",
      "Iteration 10366, loss = 0.01969948\n",
      "Iteration 10367, loss = 0.01969667\n",
      "Iteration 10368, loss = 0.01969387\n",
      "Iteration 10369, loss = 0.01969107\n",
      "Iteration 10370, loss = 0.01968827\n",
      "Iteration 10371, loss = 0.01968547\n",
      "Iteration 10372, loss = 0.01968267\n",
      "Iteration 10373, loss = 0.01967987\n",
      "Iteration 10374, loss = 0.01967707\n",
      "Iteration 10375, loss = 0.01967427\n",
      "Iteration 10376, loss = 0.01967148\n",
      "Iteration 10377, loss = 0.01966868\n",
      "Iteration 10378, loss = 0.01966589\n",
      "Iteration 10379, loss = 0.01966309\n",
      "Iteration 10380, loss = 0.01966030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10381, loss = 0.01965751\n",
      "Iteration 10382, loss = 0.01965471\n",
      "Iteration 10383, loss = 0.01965192\n",
      "Iteration 10384, loss = 0.01964913\n",
      "Iteration 10385, loss = 0.01964635\n",
      "Iteration 10386, loss = 0.01964356\n",
      "Iteration 10387, loss = 0.01964077\n",
      "Iteration 10388, loss = 0.01963798\n",
      "Iteration 10389, loss = 0.01963520\n",
      "Iteration 10390, loss = 0.01963241\n",
      "Iteration 10391, loss = 0.01962963\n",
      "Iteration 10392, loss = 0.01962684\n",
      "Iteration 10393, loss = 0.01962406\n",
      "Iteration 10394, loss = 0.01962128\n",
      "Iteration 10395, loss = 0.01961850\n",
      "Iteration 10396, loss = 0.01961572\n",
      "Iteration 10397, loss = 0.01961294\n",
      "Iteration 10398, loss = 0.01961016\n",
      "Iteration 10399, loss = 0.01960738\n",
      "Iteration 10400, loss = 0.01960461\n",
      "Iteration 10401, loss = 0.01960183\n",
      "Iteration 10402, loss = 0.01959906\n",
      "Iteration 10403, loss = 0.01959628\n",
      "Iteration 10404, loss = 0.01959351\n",
      "Iteration 10405, loss = 0.01959074\n",
      "Iteration 10406, loss = 0.01958796\n",
      "Iteration 10407, loss = 0.01958519\n",
      "Iteration 10408, loss = 0.01958242\n",
      "Iteration 10409, loss = 0.01957965\n",
      "Iteration 10410, loss = 0.01957688\n",
      "Iteration 10411, loss = 0.01957412\n",
      "Iteration 10412, loss = 0.01957135\n",
      "Iteration 10413, loss = 0.01956858\n",
      "Iteration 10414, loss = 0.01956582\n",
      "Iteration 10415, loss = 0.01956305\n",
      "Iteration 10416, loss = 0.01956029\n",
      "Iteration 10417, loss = 0.01955753\n",
      "Iteration 10418, loss = 0.01955476\n",
      "Iteration 10419, loss = 0.01955200\n",
      "Iteration 10420, loss = 0.01954924\n",
      "Iteration 10421, loss = 0.01954648\n",
      "Iteration 10422, loss = 0.01954372\n",
      "Iteration 10423, loss = 0.01954096\n",
      "Iteration 10424, loss = 0.01953821\n",
      "Iteration 10425, loss = 0.01953545\n",
      "Iteration 10426, loss = 0.01953269\n",
      "Iteration 10427, loss = 0.01952994\n",
      "Iteration 10428, loss = 0.01952719\n",
      "Iteration 10429, loss = 0.01952443\n",
      "Iteration 10430, loss = 0.01952168\n",
      "Iteration 10431, loss = 0.01951893\n",
      "Iteration 10432, loss = 0.01951618\n",
      "Iteration 10433, loss = 0.01951343\n",
      "Iteration 10434, loss = 0.01951068\n",
      "Iteration 10435, loss = 0.01950793\n",
      "Iteration 10436, loss = 0.01950518\n",
      "Iteration 10437, loss = 0.01950243\n",
      "Iteration 10438, loss = 0.01949969\n",
      "Iteration 10439, loss = 0.01949694\n",
      "Iteration 10440, loss = 0.01949420\n",
      "Iteration 10441, loss = 0.01949145\n",
      "Iteration 10442, loss = 0.01948871\n",
      "Iteration 10443, loss = 0.01948597\n",
      "Iteration 10444, loss = 0.01948323\n",
      "Iteration 10445, loss = 0.01948049\n",
      "Iteration 10446, loss = 0.01947775\n",
      "Iteration 10447, loss = 0.01947501\n",
      "Iteration 10448, loss = 0.01947227\n",
      "Iteration 10449, loss = 0.01946953\n",
      "Iteration 10450, loss = 0.01946680\n",
      "Iteration 10451, loss = 0.01946406\n",
      "Iteration 10452, loss = 0.01946133\n",
      "Iteration 10453, loss = 0.01945859\n",
      "Iteration 10454, loss = 0.01945586\n",
      "Iteration 10455, loss = 0.01945313\n",
      "Iteration 10456, loss = 0.01945039\n",
      "Iteration 10457, loss = 0.01944766\n",
      "Iteration 10458, loss = 0.01944493\n",
      "Iteration 10459, loss = 0.01944220\n",
      "Iteration 10460, loss = 0.01943948\n",
      "Iteration 10461, loss = 0.01943675\n",
      "Iteration 10462, loss = 0.01943402\n",
      "Iteration 10463, loss = 0.01943129\n",
      "Iteration 10464, loss = 0.01942857\n",
      "Iteration 10465, loss = 0.01942584\n",
      "Iteration 10466, loss = 0.01942312\n",
      "Iteration 10467, loss = 0.01942040\n",
      "Iteration 10468, loss = 0.01941768\n",
      "Iteration 10469, loss = 0.01941495\n",
      "Iteration 10470, loss = 0.01941223\n",
      "Iteration 10471, loss = 0.01940951\n",
      "Iteration 10472, loss = 0.01940679\n",
      "Iteration 10473, loss = 0.01940408\n",
      "Iteration 10474, loss = 0.01940136\n",
      "Iteration 10475, loss = 0.01939864\n",
      "Iteration 10476, loss = 0.01939593\n",
      "Iteration 10477, loss = 0.01939321\n",
      "Iteration 10478, loss = 0.01939050\n",
      "Iteration 10479, loss = 0.01938778\n",
      "Iteration 10480, loss = 0.01938507\n",
      "Iteration 10481, loss = 0.01938236\n",
      "Iteration 10482, loss = 0.01937965\n",
      "Iteration 10483, loss = 0.01937694\n",
      "Iteration 10484, loss = 0.01937423\n",
      "Iteration 10485, loss = 0.01937152\n",
      "Iteration 10486, loss = 0.01936881\n",
      "Iteration 10487, loss = 0.01936610\n",
      "Iteration 10488, loss = 0.01936340\n",
      "Iteration 10489, loss = 0.01936069\n",
      "Iteration 10490, loss = 0.01935799\n",
      "Iteration 10491, loss = 0.01935528\n",
      "Iteration 10492, loss = 0.01935258\n",
      "Iteration 10493, loss = 0.01934988\n",
      "Iteration 10494, loss = 0.01934717\n",
      "Iteration 10495, loss = 0.01934447\n",
      "Iteration 10496, loss = 0.01934177\n",
      "Iteration 10497, loss = 0.01933907\n",
      "Iteration 10498, loss = 0.01933638\n",
      "Iteration 10499, loss = 0.01933368\n",
      "Iteration 10500, loss = 0.01933098\n",
      "Iteration 10501, loss = 0.01932828\n",
      "Iteration 10502, loss = 0.01932559\n",
      "Iteration 10503, loss = 0.01932289\n",
      "Iteration 10504, loss = 0.01932020\n",
      "Iteration 10505, loss = 0.01931751\n",
      "Iteration 10506, loss = 0.01931481\n",
      "Iteration 10507, loss = 0.01931212\n",
      "Iteration 10508, loss = 0.01930943\n",
      "Iteration 10509, loss = 0.01930674\n",
      "Iteration 10510, loss = 0.01930405\n",
      "Iteration 10511, loss = 0.01930136\n",
      "Iteration 10512, loss = 0.01929868\n",
      "Iteration 10513, loss = 0.01929599\n",
      "Iteration 10514, loss = 0.01929330\n",
      "Iteration 10515, loss = 0.01929062\n",
      "Iteration 10516, loss = 0.01928793\n",
      "Iteration 10517, loss = 0.01928525\n",
      "Iteration 10518, loss = 0.01928257\n",
      "Iteration 10519, loss = 0.01927988\n",
      "Iteration 10520, loss = 0.01927720\n",
      "Iteration 10521, loss = 0.01927452\n",
      "Iteration 10522, loss = 0.01927184\n",
      "Iteration 10523, loss = 0.01926916\n",
      "Iteration 10524, loss = 0.01926648\n",
      "Iteration 10525, loss = 0.01926381\n",
      "Iteration 10526, loss = 0.01926113\n",
      "Iteration 10527, loss = 0.01925845\n",
      "Iteration 10528, loss = 0.01925578\n",
      "Iteration 10529, loss = 0.01925310\n",
      "Iteration 10530, loss = 0.01925043\n",
      "Iteration 10531, loss = 0.01924776\n",
      "Iteration 10532, loss = 0.01924508\n",
      "Iteration 10533, loss = 0.01924241\n",
      "Iteration 10534, loss = 0.01923974\n",
      "Iteration 10535, loss = 0.01923707\n",
      "Iteration 10536, loss = 0.01923440\n",
      "Iteration 10537, loss = 0.01923174\n",
      "Iteration 10538, loss = 0.01922907\n",
      "Iteration 10539, loss = 0.01922640\n",
      "Iteration 10540, loss = 0.01922373\n",
      "Iteration 10541, loss = 0.01922107\n",
      "Iteration 10542, loss = 0.01921840\n",
      "Iteration 10543, loss = 0.01921574\n",
      "Iteration 10544, loss = 0.01921308\n",
      "Iteration 10545, loss = 0.01921042\n",
      "Iteration 10546, loss = 0.01920775\n",
      "Iteration 10547, loss = 0.01920509\n",
      "Iteration 10548, loss = 0.01920243\n",
      "Iteration 10549, loss = 0.01919977\n",
      "Iteration 10550, loss = 0.01919712\n",
      "Iteration 10551, loss = 0.01919446\n",
      "Iteration 10552, loss = 0.01919180\n",
      "Iteration 10553, loss = 0.01918915\n",
      "Iteration 10554, loss = 0.01918649\n",
      "Iteration 10555, loss = 0.01918384\n",
      "Iteration 10556, loss = 0.01918118\n",
      "Iteration 10557, loss = 0.01917853\n",
      "Iteration 10558, loss = 0.01917588\n",
      "Iteration 10559, loss = 0.01917323\n",
      "Iteration 10560, loss = 0.01917057\n",
      "Iteration 10561, loss = 0.01916792\n",
      "Iteration 10562, loss = 0.01916528\n",
      "Iteration 10563, loss = 0.01916263\n",
      "Iteration 10564, loss = 0.01915998\n",
      "Iteration 10565, loss = 0.01915733\n",
      "Iteration 10566, loss = 0.01915469\n",
      "Iteration 10567, loss = 0.01915204\n",
      "Iteration 10568, loss = 0.01914940\n",
      "Iteration 10569, loss = 0.01914675\n",
      "Iteration 10570, loss = 0.01914411\n",
      "Iteration 10571, loss = 0.01914147\n",
      "Iteration 10572, loss = 0.01913882\n",
      "Iteration 10573, loss = 0.01913618\n",
      "Iteration 10574, loss = 0.01913354\n",
      "Iteration 10575, loss = 0.01913090\n",
      "Iteration 10576, loss = 0.01912827\n",
      "Iteration 10577, loss = 0.01912563\n",
      "Iteration 10578, loss = 0.01912299\n",
      "Iteration 10579, loss = 0.01912035\n",
      "Iteration 10580, loss = 0.01911772\n",
      "Iteration 10581, loss = 0.01911508\n",
      "Iteration 10582, loss = 0.01911245\n",
      "Iteration 10583, loss = 0.01910982\n",
      "Iteration 10584, loss = 0.01910718\n",
      "Iteration 10585, loss = 0.01910455\n",
      "Iteration 10586, loss = 0.01910192\n",
      "Iteration 10587, loss = 0.01909929\n",
      "Iteration 10588, loss = 0.01909666\n",
      "Iteration 10589, loss = 0.01909403\n",
      "Iteration 10590, loss = 0.01909141\n",
      "Iteration 10591, loss = 0.01908878\n",
      "Iteration 10592, loss = 0.01908615\n",
      "Iteration 10593, loss = 0.01908353\n",
      "Iteration 10594, loss = 0.01908090\n",
      "Iteration 10595, loss = 0.01907828\n",
      "Iteration 10596, loss = 0.01907565\n",
      "Iteration 10597, loss = 0.01907303\n",
      "Iteration 10598, loss = 0.01907041\n",
      "Iteration 10599, loss = 0.01906779\n",
      "Iteration 10600, loss = 0.01906517\n",
      "Iteration 10601, loss = 0.01906255\n",
      "Iteration 10602, loss = 0.01905993\n",
      "Iteration 10603, loss = 0.01905731\n",
      "Iteration 10604, loss = 0.01905469\n",
      "Iteration 10605, loss = 0.01905208\n",
      "Iteration 10606, loss = 0.01904946\n",
      "Iteration 10607, loss = 0.01904684\n",
      "Iteration 10608, loss = 0.01904423\n",
      "Iteration 10609, loss = 0.01904162\n",
      "Iteration 10610, loss = 0.01903900\n",
      "Iteration 10611, loss = 0.01903639\n",
      "Iteration 10612, loss = 0.01903378\n",
      "Iteration 10613, loss = 0.01903117\n",
      "Iteration 10614, loss = 0.01902856\n",
      "Iteration 10615, loss = 0.01902595\n",
      "Iteration 10616, loss = 0.01902334\n",
      "Iteration 10617, loss = 0.01902073\n",
      "Iteration 10618, loss = 0.01901813\n",
      "Iteration 10619, loss = 0.01901552\n",
      "Iteration 10620, loss = 0.01901291\n",
      "Iteration 10621, loss = 0.01901031\n",
      "Iteration 10622, loss = 0.01900771\n",
      "Iteration 10623, loss = 0.01900510\n",
      "Iteration 10624, loss = 0.01900250\n",
      "Iteration 10625, loss = 0.01899990\n",
      "Iteration 10626, loss = 0.01899730\n",
      "Iteration 10627, loss = 0.01899470\n",
      "Iteration 10628, loss = 0.01899210\n",
      "Iteration 10629, loss = 0.01898950\n",
      "Iteration 10630, loss = 0.01898690\n",
      "Iteration 10631, loss = 0.01898430\n",
      "Iteration 10632, loss = 0.01898171\n",
      "Iteration 10633, loss = 0.01897911\n",
      "Iteration 10634, loss = 0.01897651\n",
      "Iteration 10635, loss = 0.01897392\n",
      "Iteration 10636, loss = 0.01897133\n",
      "Iteration 10637, loss = 0.01896873\n",
      "Iteration 10638, loss = 0.01896614\n",
      "Iteration 10639, loss = 0.01896355\n",
      "Iteration 10640, loss = 0.01896096\n",
      "Iteration 10641, loss = 0.01895837\n",
      "Iteration 10642, loss = 0.01895578\n",
      "Iteration 10643, loss = 0.01895319\n",
      "Iteration 10644, loss = 0.01895060\n",
      "Iteration 10645, loss = 0.01894802\n",
      "Iteration 10646, loss = 0.01894543\n",
      "Iteration 10647, loss = 0.01894285\n",
      "Iteration 10648, loss = 0.01894026\n",
      "Iteration 10649, loss = 0.01893768\n",
      "Iteration 10650, loss = 0.01893509\n",
      "Iteration 10651, loss = 0.01893251\n",
      "Iteration 10652, loss = 0.01892993\n",
      "Iteration 10653, loss = 0.01892735\n",
      "Iteration 10654, loss = 0.01892477\n",
      "Iteration 10655, loss = 0.01892219\n",
      "Iteration 10656, loss = 0.01891961\n",
      "Iteration 10657, loss = 0.01891703\n",
      "Iteration 10658, loss = 0.01891445\n",
      "Iteration 10659, loss = 0.01891188\n",
      "Iteration 10660, loss = 0.01890930\n",
      "Iteration 10661, loss = 0.01890673\n",
      "Iteration 10662, loss = 0.01890415\n",
      "Iteration 10663, loss = 0.01890158\n",
      "Iteration 10664, loss = 0.01889900\n",
      "Iteration 10665, loss = 0.01889643\n",
      "Iteration 10666, loss = 0.01889386\n",
      "Iteration 10667, loss = 0.01889129\n",
      "Iteration 10668, loss = 0.01888872\n",
      "Iteration 10669, loss = 0.01888615\n",
      "Iteration 10670, loss = 0.01888358\n",
      "Iteration 10671, loss = 0.01888101\n",
      "Iteration 10672, loss = 0.01887845\n",
      "Iteration 10673, loss = 0.01887588\n",
      "Iteration 10674, loss = 0.01887331\n",
      "Iteration 10675, loss = 0.01887075\n",
      "Iteration 10676, loss = 0.01886818\n",
      "Iteration 10677, loss = 0.01886562\n",
      "Iteration 10678, loss = 0.01886306\n",
      "Iteration 10679, loss = 0.01886050\n",
      "Iteration 10680, loss = 0.01885793\n",
      "Iteration 10681, loss = 0.01885537\n",
      "Iteration 10682, loss = 0.01885281\n",
      "Iteration 10683, loss = 0.01885025\n",
      "Iteration 10684, loss = 0.01884770\n",
      "Iteration 10685, loss = 0.01884514\n",
      "Iteration 10686, loss = 0.01884258\n",
      "Iteration 10687, loss = 0.01884003\n",
      "Iteration 10688, loss = 0.01883747\n",
      "Iteration 10689, loss = 0.01883491\n",
      "Iteration 10690, loss = 0.01883236\n",
      "Iteration 10691, loss = 0.01882981\n",
      "Iteration 10692, loss = 0.01882725\n",
      "Iteration 10693, loss = 0.01882470\n",
      "Iteration 10694, loss = 0.01882215\n",
      "Iteration 10695, loss = 0.01881960\n",
      "Iteration 10696, loss = 0.01881705\n",
      "Iteration 10697, loss = 0.01881450\n",
      "Iteration 10698, loss = 0.01881195\n",
      "Iteration 10699, loss = 0.01880941\n",
      "Iteration 10700, loss = 0.01880686\n",
      "Iteration 10701, loss = 0.01880431\n",
      "Iteration 10702, loss = 0.01880177\n",
      "Iteration 10703, loss = 0.01879922\n",
      "Iteration 10704, loss = 0.01879668\n",
      "Iteration 10705, loss = 0.01879414\n",
      "Iteration 10706, loss = 0.01879159\n",
      "Iteration 10707, loss = 0.01878905\n",
      "Iteration 10708, loss = 0.01878651\n",
      "Iteration 10709, loss = 0.01878397\n",
      "Iteration 10710, loss = 0.01878143\n",
      "Iteration 10711, loss = 0.01877889\n",
      "Iteration 10712, loss = 0.01877635\n",
      "Iteration 10713, loss = 0.01877382\n",
      "Iteration 10714, loss = 0.01877128\n",
      "Iteration 10715, loss = 0.01876874\n",
      "Iteration 10716, loss = 0.01876621\n",
      "Iteration 10717, loss = 0.01876367\n",
      "Iteration 10718, loss = 0.01876114\n",
      "Iteration 10719, loss = 0.01875861\n",
      "Iteration 10720, loss = 0.01875607\n",
      "Iteration 10721, loss = 0.01875354\n",
      "Iteration 10722, loss = 0.01875101\n",
      "Iteration 10723, loss = 0.01874848\n",
      "Iteration 10724, loss = 0.01874595\n",
      "Iteration 10725, loss = 0.01874342\n",
      "Iteration 10726, loss = 0.01874089\n",
      "Iteration 10727, loss = 0.01873836\n",
      "Iteration 10728, loss = 0.01873584\n",
      "Iteration 10729, loss = 0.01873331\n",
      "Iteration 10730, loss = 0.01873079\n",
      "Iteration 10731, loss = 0.01872826\n",
      "Iteration 10732, loss = 0.01872574\n",
      "Iteration 10733, loss = 0.01872321\n",
      "Iteration 10734, loss = 0.01872069\n",
      "Iteration 10735, loss = 0.01871817\n",
      "Iteration 10736, loss = 0.01871565\n",
      "Iteration 10737, loss = 0.01871313\n",
      "Iteration 10738, loss = 0.01871061\n",
      "Iteration 10739, loss = 0.01870809\n",
      "Iteration 10740, loss = 0.01870557\n",
      "Iteration 10741, loss = 0.01870305\n",
      "Iteration 10742, loss = 0.01870054\n",
      "Iteration 10743, loss = 0.01869802\n",
      "Iteration 10744, loss = 0.01869551\n",
      "Iteration 10745, loss = 0.01869299\n",
      "Iteration 10746, loss = 0.01869048\n",
      "Iteration 10747, loss = 0.01868796\n",
      "Iteration 10748, loss = 0.01868545\n",
      "Iteration 10749, loss = 0.01868294\n",
      "Iteration 10750, loss = 0.01868043\n",
      "Iteration 10751, loss = 0.01867792\n",
      "Iteration 10752, loss = 0.01867541\n",
      "Iteration 10753, loss = 0.01867290\n",
      "Iteration 10754, loss = 0.01867039\n",
      "Iteration 10755, loss = 0.01866788\n",
      "Iteration 10756, loss = 0.01866538\n",
      "Iteration 10757, loss = 0.01866287\n",
      "Iteration 10758, loss = 0.01866036\n",
      "Iteration 10759, loss = 0.01865786\n",
      "Iteration 10760, loss = 0.01865536\n",
      "Iteration 10761, loss = 0.01865285\n",
      "Iteration 10762, loss = 0.01865035\n",
      "Iteration 10763, loss = 0.01864785\n",
      "Iteration 10764, loss = 0.01864535\n",
      "Iteration 10765, loss = 0.01864285\n",
      "Iteration 10766, loss = 0.01864035\n",
      "Iteration 10767, loss = 0.01863785\n",
      "Iteration 10768, loss = 0.01863535\n",
      "Iteration 10769, loss = 0.01863285\n",
      "Iteration 10770, loss = 0.01863035\n",
      "Iteration 10771, loss = 0.01862786\n",
      "Iteration 10772, loss = 0.01862536\n",
      "Iteration 10773, loss = 0.01862287\n",
      "Iteration 10774, loss = 0.01862037\n",
      "Iteration 10775, loss = 0.01861788\n",
      "Iteration 10776, loss = 0.01861539\n",
      "Iteration 10777, loss = 0.01861289\n",
      "Iteration 10778, loss = 0.01861040\n",
      "Iteration 10779, loss = 0.01860791\n",
      "Iteration 10780, loss = 0.01860542\n",
      "Iteration 10781, loss = 0.01860293\n",
      "Iteration 10782, loss = 0.01860044\n",
      "Iteration 10783, loss = 0.01859796\n",
      "Iteration 10784, loss = 0.01859547\n",
      "Iteration 10785, loss = 0.01859298\n",
      "Iteration 10786, loss = 0.01859050\n",
      "Iteration 10787, loss = 0.01858801\n",
      "Iteration 10788, loss = 0.01858553\n",
      "Iteration 10789, loss = 0.01858304\n",
      "Iteration 10790, loss = 0.01858056\n",
      "Iteration 10791, loss = 0.01857808\n",
      "Iteration 10792, loss = 0.01857559\n",
      "Iteration 10793, loss = 0.01857311\n",
      "Iteration 10794, loss = 0.01857063\n",
      "Iteration 10795, loss = 0.01856815\n",
      "Iteration 10796, loss = 0.01856567\n",
      "Iteration 10797, loss = 0.01856320\n",
      "Iteration 10798, loss = 0.01856072\n",
      "Iteration 10799, loss = 0.01855824\n",
      "Iteration 10800, loss = 0.01855577\n",
      "Iteration 10801, loss = 0.01855329\n",
      "Iteration 10802, loss = 0.01855082\n",
      "Iteration 10803, loss = 0.01854834\n",
      "Iteration 10804, loss = 0.01854587\n",
      "Iteration 10805, loss = 0.01854340\n",
      "Iteration 10806, loss = 0.01854092\n",
      "Iteration 10807, loss = 0.01853845\n",
      "Iteration 10808, loss = 0.01853598\n",
      "Iteration 10809, loss = 0.01853351\n",
      "Iteration 10810, loss = 0.01853104\n",
      "Iteration 10811, loss = 0.01852857\n",
      "Iteration 10812, loss = 0.01852611\n",
      "Iteration 10813, loss = 0.01852364\n",
      "Iteration 10814, loss = 0.01852117\n",
      "Iteration 10815, loss = 0.01851871\n",
      "Iteration 10816, loss = 0.01851624\n",
      "Iteration 10817, loss = 0.01851378\n",
      "Iteration 10818, loss = 0.01851131\n",
      "Iteration 10819, loss = 0.01850885\n",
      "Iteration 10820, loss = 0.01850639\n",
      "Iteration 10821, loss = 0.01850393\n",
      "Iteration 10822, loss = 0.01850147\n",
      "Iteration 10823, loss = 0.01849901\n",
      "Iteration 10824, loss = 0.01849655\n",
      "Iteration 10825, loss = 0.01849409\n",
      "Iteration 10826, loss = 0.01849163\n",
      "Iteration 10827, loss = 0.01848917\n",
      "Iteration 10828, loss = 0.01848671\n",
      "Iteration 10829, loss = 0.01848426\n",
      "Iteration 10830, loss = 0.01848180\n",
      "Iteration 10831, loss = 0.01847935\n",
      "Iteration 10832, loss = 0.01847689\n",
      "Iteration 10833, loss = 0.01847444\n",
      "Iteration 10834, loss = 0.01847199\n",
      "Iteration 10835, loss = 0.01846954\n",
      "Iteration 10836, loss = 0.01846708\n",
      "Iteration 10837, loss = 0.01846463\n",
      "Iteration 10838, loss = 0.01846218\n",
      "Iteration 10839, loss = 0.01845973\n",
      "Iteration 10840, loss = 0.01845729\n",
      "Iteration 10841, loss = 0.01845484\n",
      "Iteration 10842, loss = 0.01845239\n",
      "Iteration 10843, loss = 0.01844994\n",
      "Iteration 10844, loss = 0.01844750\n",
      "Iteration 10845, loss = 0.01844505\n",
      "Iteration 10846, loss = 0.01844261\n",
      "Iteration 10847, loss = 0.01844017\n",
      "Iteration 10848, loss = 0.01843772\n",
      "Iteration 10849, loss = 0.01843528\n",
      "Iteration 10850, loss = 0.01843284\n",
      "Iteration 10851, loss = 0.01843040\n",
      "Iteration 10852, loss = 0.01842796\n",
      "Iteration 10853, loss = 0.01842552\n",
      "Iteration 10854, loss = 0.01842308\n",
      "Iteration 10855, loss = 0.01842064\n",
      "Iteration 10856, loss = 0.01841820\n",
      "Iteration 10857, loss = 0.01841576\n",
      "Iteration 10858, loss = 0.01841333\n",
      "Iteration 10859, loss = 0.01841089\n",
      "Iteration 10860, loss = 0.01840846\n",
      "Iteration 10861, loss = 0.01840602\n",
      "Iteration 10862, loss = 0.01840359\n",
      "Iteration 10863, loss = 0.01840116\n",
      "Iteration 10864, loss = 0.01839872\n",
      "Iteration 10865, loss = 0.01839629\n",
      "Iteration 10866, loss = 0.01839386\n",
      "Iteration 10867, loss = 0.01839143\n",
      "Iteration 10868, loss = 0.01838900\n",
      "Iteration 10869, loss = 0.01838657\n",
      "Iteration 10870, loss = 0.01838414\n",
      "Iteration 10871, loss = 0.01838172\n",
      "Iteration 10872, loss = 0.01837929\n",
      "Iteration 10873, loss = 0.01837686\n",
      "Iteration 10874, loss = 0.01837444\n",
      "Iteration 10875, loss = 0.01837201\n",
      "Iteration 10876, loss = 0.01836959\n",
      "Iteration 10877, loss = 0.01836717\n",
      "Iteration 10878, loss = 0.01836474\n",
      "Iteration 10879, loss = 0.01836232\n",
      "Iteration 10880, loss = 0.01835990\n",
      "Iteration 10881, loss = 0.01835748\n",
      "Iteration 10882, loss = 0.01835506\n",
      "Iteration 10883, loss = 0.01835264\n",
      "Iteration 10884, loss = 0.01835022\n",
      "Iteration 10885, loss = 0.01834780\n",
      "Iteration 10886, loss = 0.01834538\n",
      "Iteration 10887, loss = 0.01834297\n",
      "Iteration 10888, loss = 0.01834055\n",
      "Iteration 10889, loss = 0.01833814\n",
      "Iteration 10890, loss = 0.01833572\n",
      "Iteration 10891, loss = 0.01833331\n",
      "Iteration 10892, loss = 0.01833089\n",
      "Iteration 10893, loss = 0.01832848\n",
      "Iteration 10894, loss = 0.01832607\n",
      "Iteration 10895, loss = 0.01832366\n",
      "Iteration 10896, loss = 0.01832125\n",
      "Iteration 10897, loss = 0.01831884\n",
      "Iteration 10898, loss = 0.01831643\n",
      "Iteration 10899, loss = 0.01831402\n",
      "Iteration 10900, loss = 0.01831161\n",
      "Iteration 10901, loss = 0.01830920\n",
      "Iteration 10902, loss = 0.01830680\n",
      "Iteration 10903, loss = 0.01830439\n",
      "Iteration 10904, loss = 0.01830198\n",
      "Iteration 10905, loss = 0.01829958\n",
      "Iteration 10906, loss = 0.01829717\n",
      "Iteration 10907, loss = 0.01829477\n",
      "Iteration 10908, loss = 0.01829237\n",
      "Iteration 10909, loss = 0.01828997\n",
      "Iteration 10910, loss = 0.01828757\n",
      "Iteration 10911, loss = 0.01828516\n",
      "Iteration 10912, loss = 0.01828276\n",
      "Iteration 10913, loss = 0.01828036\n",
      "Iteration 10914, loss = 0.01827797\n",
      "Iteration 10915, loss = 0.01827557\n",
      "Iteration 10916, loss = 0.01827317\n",
      "Iteration 10917, loss = 0.01827077\n",
      "Iteration 10918, loss = 0.01826838\n",
      "Iteration 10919, loss = 0.01826598\n",
      "Iteration 10920, loss = 0.01826359\n",
      "Iteration 10921, loss = 0.01826119\n",
      "Iteration 10922, loss = 0.01825880\n",
      "Iteration 10923, loss = 0.01825641\n",
      "Iteration 10924, loss = 0.01825401\n",
      "Iteration 10925, loss = 0.01825162\n",
      "Iteration 10926, loss = 0.01824923\n",
      "Iteration 10927, loss = 0.01824684\n",
      "Iteration 10928, loss = 0.01824445\n",
      "Iteration 10929, loss = 0.01824206\n",
      "Iteration 10930, loss = 0.01823968\n",
      "Iteration 10931, loss = 0.01823729\n",
      "Iteration 10932, loss = 0.01823490\n",
      "Iteration 10933, loss = 0.01823251\n",
      "Iteration 10934, loss = 0.01823013\n",
      "Iteration 10935, loss = 0.01822774\n",
      "Iteration 10936, loss = 0.01822536\n",
      "Iteration 10937, loss = 0.01822298\n",
      "Iteration 10938, loss = 0.01822059\n",
      "Iteration 10939, loss = 0.01821821\n",
      "Iteration 10940, loss = 0.01821583\n",
      "Iteration 10941, loss = 0.01821345\n",
      "Iteration 10942, loss = 0.01821107\n",
      "Iteration 10943, loss = 0.01820869\n",
      "Iteration 10944, loss = 0.01820631\n",
      "Iteration 10945, loss = 0.01820393\n",
      "Iteration 10946, loss = 0.01820155\n",
      "Iteration 10947, loss = 0.01819918\n",
      "Iteration 10948, loss = 0.01819680\n",
      "Iteration 10949, loss = 0.01819442\n",
      "Iteration 10950, loss = 0.01819205\n",
      "Iteration 10951, loss = 0.01818968\n",
      "Iteration 10952, loss = 0.01818730\n",
      "Iteration 10953, loss = 0.01818493\n",
      "Iteration 10954, loss = 0.01818256\n",
      "Iteration 10955, loss = 0.01818018\n",
      "Iteration 10956, loss = 0.01817781\n",
      "Iteration 10957, loss = 0.01817544\n",
      "Iteration 10958, loss = 0.01817307\n",
      "Iteration 10959, loss = 0.01817070\n",
      "Iteration 10960, loss = 0.01816833\n",
      "Iteration 10961, loss = 0.01816597\n",
      "Iteration 10962, loss = 0.01816360\n",
      "Iteration 10963, loss = 0.01816123\n",
      "Iteration 10964, loss = 0.01815887\n",
      "Iteration 10965, loss = 0.01815650\n",
      "Iteration 10966, loss = 0.01815414\n",
      "Iteration 10967, loss = 0.01815177\n",
      "Iteration 10968, loss = 0.01814941\n",
      "Iteration 10969, loss = 0.01814705\n",
      "Iteration 10970, loss = 0.01814469\n",
      "Iteration 10971, loss = 0.01814232\n",
      "Iteration 10972, loss = 0.01813996\n",
      "Iteration 10973, loss = 0.01813760\n",
      "Iteration 10974, loss = 0.01813524\n",
      "Iteration 10975, loss = 0.01813288\n",
      "Iteration 10976, loss = 0.01813053\n",
      "Iteration 10977, loss = 0.01812817\n",
      "Iteration 10978, loss = 0.01812581\n",
      "Iteration 10979, loss = 0.01812346\n",
      "Iteration 10980, loss = 0.01812110\n",
      "Iteration 10981, loss = 0.01811875\n",
      "Iteration 10982, loss = 0.01811639\n",
      "Iteration 10983, loss = 0.01811404\n",
      "Iteration 10984, loss = 0.01811169\n",
      "Iteration 10985, loss = 0.01810933\n",
      "Iteration 10986, loss = 0.01810698\n",
      "Iteration 10987, loss = 0.01810463\n",
      "Iteration 10988, loss = 0.01810228\n",
      "Iteration 10989, loss = 0.01809993\n",
      "Iteration 10990, loss = 0.01809758\n",
      "Iteration 10991, loss = 0.01809523\n",
      "Iteration 10992, loss = 0.01809288\n",
      "Iteration 10993, loss = 0.01809054\n",
      "Iteration 10994, loss = 0.01808819\n",
      "Iteration 10995, loss = 0.01808585\n",
      "Iteration 10996, loss = 0.01808350\n",
      "Iteration 10997, loss = 0.01808116\n",
      "Iteration 10998, loss = 0.01807881\n",
      "Iteration 10999, loss = 0.01807647\n",
      "Iteration 11000, loss = 0.01807413\n",
      "Iteration 11001, loss = 0.01807178\n",
      "Iteration 11002, loss = 0.01806944\n",
      "Iteration 11003, loss = 0.01806710\n",
      "Iteration 11004, loss = 0.01806476\n",
      "Iteration 11005, loss = 0.01806242\n",
      "Iteration 11006, loss = 0.01806008\n",
      "Iteration 11007, loss = 0.01805774\n",
      "Iteration 11008, loss = 0.01805541\n",
      "Iteration 11009, loss = 0.01805307\n",
      "Iteration 11010, loss = 0.01805073\n",
      "Iteration 11011, loss = 0.01804840\n",
      "Iteration 11012, loss = 0.01804606\n",
      "Iteration 11013, loss = 0.01804373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11014, loss = 0.01804140\n",
      "Iteration 11015, loss = 0.01803906\n",
      "Iteration 11016, loss = 0.01803673\n",
      "Iteration 11017, loss = 0.01803440\n",
      "Iteration 11018, loss = 0.01803207\n",
      "Iteration 11019, loss = 0.01802974\n",
      "Iteration 11020, loss = 0.01802741\n",
      "Iteration 11021, loss = 0.01802508\n",
      "Iteration 11022, loss = 0.01802275\n",
      "Iteration 11023, loss = 0.01802042\n",
      "Iteration 11024, loss = 0.01801809\n",
      "Iteration 11025, loss = 0.01801577\n",
      "Iteration 11026, loss = 0.01801344\n",
      "Iteration 11027, loss = 0.01801112\n",
      "Iteration 11028, loss = 0.01800879\n",
      "Iteration 11029, loss = 0.01800647\n",
      "Iteration 11030, loss = 0.01800414\n",
      "Iteration 11031, loss = 0.01800182\n",
      "Iteration 11032, loss = 0.01799950\n",
      "Iteration 11033, loss = 0.01799718\n",
      "Iteration 11034, loss = 0.01799486\n",
      "Iteration 11035, loss = 0.01799254\n",
      "Iteration 11036, loss = 0.01799022\n",
      "Iteration 11037, loss = 0.01798790\n",
      "Iteration 11038, loss = 0.01798558\n",
      "Iteration 11039, loss = 0.01798326\n",
      "Iteration 11040, loss = 0.01798094\n",
      "Iteration 11041, loss = 0.01797863\n",
      "Iteration 11042, loss = 0.01797631\n",
      "Iteration 11043, loss = 0.01797400\n",
      "Iteration 11044, loss = 0.01797168\n",
      "Iteration 11045, loss = 0.01796937\n",
      "Iteration 11046, loss = 0.01796705\n",
      "Iteration 11047, loss = 0.01796474\n",
      "Iteration 11048, loss = 0.01796243\n",
      "Iteration 11049, loss = 0.01796012\n",
      "Iteration 11050, loss = 0.01795781\n",
      "Iteration 11051, loss = 0.01795550\n",
      "Iteration 11052, loss = 0.01795319\n",
      "Iteration 11053, loss = 0.01795088\n",
      "Iteration 11054, loss = 0.01794857\n",
      "Iteration 11055, loss = 0.01794626\n",
      "Iteration 11056, loss = 0.01794395\n",
      "Iteration 11057, loss = 0.01794165\n",
      "Iteration 11058, loss = 0.01793934\n",
      "Iteration 11059, loss = 0.01793704\n",
      "Iteration 11060, loss = 0.01793473\n",
      "Iteration 11061, loss = 0.01793243\n",
      "Iteration 11062, loss = 0.01793013\n",
      "Iteration 11063, loss = 0.01792782\n",
      "Iteration 11064, loss = 0.01792552\n",
      "Iteration 11065, loss = 0.01792322\n",
      "Iteration 11066, loss = 0.01792092\n",
      "Iteration 11067, loss = 0.01791862\n",
      "Iteration 11068, loss = 0.01791632\n",
      "Iteration 11069, loss = 0.01791402\n",
      "Iteration 11070, loss = 0.01791172\n",
      "Iteration 11071, loss = 0.01790942\n",
      "Iteration 11072, loss = 0.01790713\n",
      "Iteration 11073, loss = 0.01790483\n",
      "Iteration 11074, loss = 0.01790253\n",
      "Iteration 11075, loss = 0.01790024\n",
      "Iteration 11076, loss = 0.01789794\n",
      "Iteration 11077, loss = 0.01789565\n",
      "Iteration 11078, loss = 0.01789336\n",
      "Iteration 11079, loss = 0.01789107\n",
      "Iteration 11080, loss = 0.01788877\n",
      "Iteration 11081, loss = 0.01788648\n",
      "Iteration 11082, loss = 0.01788419\n",
      "Iteration 11083, loss = 0.01788190\n",
      "Iteration 11084, loss = 0.01787961\n",
      "Iteration 11085, loss = 0.01787732\n",
      "Iteration 11086, loss = 0.01787503\n",
      "Iteration 11087, loss = 0.01787275\n",
      "Iteration 11088, loss = 0.01787046\n",
      "Iteration 11089, loss = 0.01786817\n",
      "Iteration 11090, loss = 0.01786589\n",
      "Iteration 11091, loss = 0.01786360\n",
      "Iteration 11092, loss = 0.01786132\n",
      "Iteration 11093, loss = 0.01785903\n",
      "Iteration 11094, loss = 0.01785675\n",
      "Iteration 11095, loss = 0.01785447\n",
      "Iteration 11096, loss = 0.01785218\n",
      "Iteration 11097, loss = 0.01784990\n",
      "Iteration 11098, loss = 0.01784762\n",
      "Iteration 11099, loss = 0.01784534\n",
      "Iteration 11100, loss = 0.01784306\n",
      "Iteration 11101, loss = 0.01784078\n",
      "Iteration 11102, loss = 0.01783850\n",
      "Iteration 11103, loss = 0.01783623\n",
      "Iteration 11104, loss = 0.01783395\n",
      "Iteration 11105, loss = 0.01783167\n",
      "Iteration 11106, loss = 0.01782940\n",
      "Iteration 11107, loss = 0.01782712\n",
      "Iteration 11108, loss = 0.01782485\n",
      "Iteration 11109, loss = 0.01782257\n",
      "Iteration 11110, loss = 0.01782030\n",
      "Iteration 11111, loss = 0.01781803\n",
      "Iteration 11112, loss = 0.01781575\n",
      "Iteration 11113, loss = 0.01781348\n",
      "Iteration 11114, loss = 0.01781121\n",
      "Iteration 11115, loss = 0.01780894\n",
      "Iteration 11116, loss = 0.01780667\n",
      "Iteration 11117, loss = 0.01780440\n",
      "Iteration 11118, loss = 0.01780213\n",
      "Iteration 11119, loss = 0.01779987\n",
      "Iteration 11120, loss = 0.01779760\n",
      "Iteration 11121, loss = 0.01779533\n",
      "Iteration 11122, loss = 0.01779307\n",
      "Iteration 11123, loss = 0.01779080\n",
      "Iteration 11124, loss = 0.01778854\n",
      "Iteration 11125, loss = 0.01778627\n",
      "Iteration 11126, loss = 0.01778401\n",
      "Iteration 11127, loss = 0.01778174\n",
      "Iteration 11128, loss = 0.01777948\n",
      "Iteration 11129, loss = 0.01777722\n",
      "Iteration 11130, loss = 0.01777496\n",
      "Iteration 11131, loss = 0.01777270\n",
      "Iteration 11132, loss = 0.01777044\n",
      "Iteration 11133, loss = 0.01776818\n",
      "Iteration 11134, loss = 0.01776592\n",
      "Iteration 11135, loss = 0.01776366\n",
      "Iteration 11136, loss = 0.01776141\n",
      "Iteration 11137, loss = 0.01775915\n",
      "Iteration 11138, loss = 0.01775689\n",
      "Iteration 11139, loss = 0.01775464\n",
      "Iteration 11140, loss = 0.01775238\n",
      "Iteration 11141, loss = 0.01775013\n",
      "Iteration 11142, loss = 0.01774787\n",
      "Iteration 11143, loss = 0.01774562\n",
      "Iteration 11144, loss = 0.01774337\n",
      "Iteration 11145, loss = 0.01774111\n",
      "Iteration 11146, loss = 0.01773886\n",
      "Iteration 11147, loss = 0.01773661\n",
      "Iteration 11148, loss = 0.01773436\n",
      "Iteration 11149, loss = 0.01773211\n",
      "Iteration 11150, loss = 0.01772986\n",
      "Iteration 11151, loss = 0.01772762\n",
      "Iteration 11152, loss = 0.01772537\n",
      "Iteration 11153, loss = 0.01772312\n",
      "Iteration 11154, loss = 0.01772087\n",
      "Iteration 11155, loss = 0.01771863\n",
      "Iteration 11156, loss = 0.01771638\n",
      "Iteration 11157, loss = 0.01771414\n",
      "Iteration 11158, loss = 0.01771189\n",
      "Iteration 11159, loss = 0.01770965\n",
      "Iteration 11160, loss = 0.01770741\n",
      "Iteration 11161, loss = 0.01770516\n",
      "Iteration 11162, loss = 0.01770292\n",
      "Iteration 11163, loss = 0.01770068\n",
      "Iteration 11164, loss = 0.01769844\n",
      "Iteration 11165, loss = 0.01769620\n",
      "Iteration 11166, loss = 0.01769396\n",
      "Iteration 11167, loss = 0.01769172\n",
      "Iteration 11168, loss = 0.01768948\n",
      "Iteration 11169, loss = 0.01768725\n",
      "Iteration 11170, loss = 0.01768501\n",
      "Iteration 11171, loss = 0.01768277\n",
      "Iteration 11172, loss = 0.01768054\n",
      "Iteration 11173, loss = 0.01767830\n",
      "Iteration 11174, loss = 0.01767607\n",
      "Iteration 11175, loss = 0.01767383\n",
      "Iteration 11176, loss = 0.01767160\n",
      "Iteration 11177, loss = 0.01766937\n",
      "Iteration 11178, loss = 0.01766714\n",
      "Iteration 11179, loss = 0.01766490\n",
      "Iteration 11180, loss = 0.01766267\n",
      "Iteration 11181, loss = 0.01766044\n",
      "Iteration 11182, loss = 0.01765821\n",
      "Iteration 11183, loss = 0.01765598\n",
      "Iteration 11184, loss = 0.01765376\n",
      "Iteration 11185, loss = 0.01765153\n",
      "Iteration 11186, loss = 0.01764930\n",
      "Iteration 11187, loss = 0.01764707\n",
      "Iteration 11188, loss = 0.01764485\n",
      "Iteration 11189, loss = 0.01764262\n",
      "Iteration 11190, loss = 0.01764040\n",
      "Iteration 11191, loss = 0.01763817\n",
      "Iteration 11192, loss = 0.01763595\n",
      "Iteration 11193, loss = 0.01763373\n",
      "Iteration 11194, loss = 0.01763150\n",
      "Iteration 11195, loss = 0.01762928\n",
      "Iteration 11196, loss = 0.01762706\n",
      "Iteration 11197, loss = 0.01762484\n",
      "Iteration 11198, loss = 0.01762262\n",
      "Iteration 11199, loss = 0.01762040\n",
      "Iteration 11200, loss = 0.01761818\n",
      "Iteration 11201, loss = 0.01761596\n",
      "Iteration 11202, loss = 0.01761374\n",
      "Iteration 11203, loss = 0.01761153\n",
      "Iteration 11204, loss = 0.01760931\n",
      "Iteration 11205, loss = 0.01760710\n",
      "Iteration 11206, loss = 0.01760488\n",
      "Iteration 11207, loss = 0.01760266\n",
      "Iteration 11208, loss = 0.01760045\n",
      "Iteration 11209, loss = 0.01759824\n",
      "Iteration 11210, loss = 0.01759602\n",
      "Iteration 11211, loss = 0.01759381\n",
      "Iteration 11212, loss = 0.01759160\n",
      "Iteration 11213, loss = 0.01758939\n",
      "Iteration 11214, loss = 0.01758718\n",
      "Iteration 11215, loss = 0.01758497\n",
      "Iteration 11216, loss = 0.01758276\n",
      "Iteration 11217, loss = 0.01758055\n",
      "Iteration 11218, loss = 0.01757834\n",
      "Iteration 11219, loss = 0.01757613\n",
      "Iteration 11220, loss = 0.01757393\n",
      "Iteration 11221, loss = 0.01757172\n",
      "Iteration 11222, loss = 0.01756951\n",
      "Iteration 11223, loss = 0.01756731\n",
      "Iteration 11224, loss = 0.01756510\n",
      "Iteration 11225, loss = 0.01756290\n",
      "Iteration 11226, loss = 0.01756070\n",
      "Iteration 11227, loss = 0.01755849\n",
      "Iteration 11228, loss = 0.01755629\n",
      "Iteration 11229, loss = 0.01755409\n",
      "Iteration 11230, loss = 0.01755189\n",
      "Iteration 11231, loss = 0.01754969\n",
      "Iteration 11232, loss = 0.01754749\n",
      "Iteration 11233, loss = 0.01754529\n",
      "Iteration 11234, loss = 0.01754309\n",
      "Iteration 11235, loss = 0.01754089\n",
      "Iteration 11236, loss = 0.01753869\n",
      "Iteration 11237, loss = 0.01753650\n",
      "Iteration 11238, loss = 0.01753430\n",
      "Iteration 11239, loss = 0.01753210\n",
      "Iteration 11240, loss = 0.01752991\n",
      "Iteration 11241, loss = 0.01752771\n",
      "Iteration 11242, loss = 0.01752552\n",
      "Iteration 11243, loss = 0.01752333\n",
      "Iteration 11244, loss = 0.01752113\n",
      "Iteration 11245, loss = 0.01751894\n",
      "Iteration 11246, loss = 0.01751675\n",
      "Iteration 11247, loss = 0.01751456\n",
      "Iteration 11248, loss = 0.01751237\n",
      "Iteration 11249, loss = 0.01751018\n",
      "Iteration 11250, loss = 0.01750799\n",
      "Iteration 11251, loss = 0.01750580\n",
      "Iteration 11252, loss = 0.01750361\n",
      "Iteration 11253, loss = 0.01750142\n",
      "Iteration 11254, loss = 0.01749923\n",
      "Iteration 11255, loss = 0.01749705\n",
      "Iteration 11256, loss = 0.01749486\n",
      "Iteration 11257, loss = 0.01749268\n",
      "Iteration 11258, loss = 0.01749049\n",
      "Iteration 11259, loss = 0.01748831\n",
      "Iteration 11260, loss = 0.01748612\n",
      "Iteration 11261, loss = 0.01748394\n",
      "Iteration 11262, loss = 0.01748176\n",
      "Iteration 11263, loss = 0.01747958\n",
      "Iteration 11264, loss = 0.01747740\n",
      "Iteration 11265, loss = 0.01747521\n",
      "Iteration 11266, loss = 0.01747303\n",
      "Iteration 11267, loss = 0.01747085\n",
      "Iteration 11268, loss = 0.01746868\n",
      "Iteration 11269, loss = 0.01746650\n",
      "Iteration 11270, loss = 0.01746432\n",
      "Iteration 11271, loss = 0.01746214\n",
      "Iteration 11272, loss = 0.01745997\n",
      "Iteration 11273, loss = 0.01745779\n",
      "Iteration 11274, loss = 0.01745561\n",
      "Iteration 11275, loss = 0.01745344\n",
      "Iteration 11276, loss = 0.01745126\n",
      "Iteration 11277, loss = 0.01744909\n",
      "Iteration 11278, loss = 0.01744692\n",
      "Iteration 11279, loss = 0.01744475\n",
      "Iteration 11280, loss = 0.01744257\n",
      "Iteration 11281, loss = 0.01744040\n",
      "Iteration 11282, loss = 0.01743823\n",
      "Iteration 11283, loss = 0.01743606\n",
      "Iteration 11284, loss = 0.01743389\n",
      "Iteration 11285, loss = 0.01743172\n",
      "Iteration 11286, loss = 0.01742955\n",
      "Iteration 11287, loss = 0.01742739\n",
      "Iteration 11288, loss = 0.01742522\n",
      "Iteration 11289, loss = 0.01742305\n",
      "Iteration 11290, loss = 0.01742088\n",
      "Iteration 11291, loss = 0.01741872\n",
      "Iteration 11292, loss = 0.01741655\n",
      "Iteration 11293, loss = 0.01741439\n",
      "Iteration 11294, loss = 0.01741223\n",
      "Iteration 11295, loss = 0.01741006\n",
      "Iteration 11296, loss = 0.01740790\n",
      "Iteration 11297, loss = 0.01740574\n",
      "Iteration 11298, loss = 0.01740358\n",
      "Iteration 11299, loss = 0.01740141\n",
      "Iteration 11300, loss = 0.01739925\n",
      "Iteration 11301, loss = 0.01739709\n",
      "Iteration 11302, loss = 0.01739493\n",
      "Iteration 11303, loss = 0.01739278\n",
      "Iteration 11304, loss = 0.01739062\n",
      "Iteration 11305, loss = 0.01738846\n",
      "Iteration 11306, loss = 0.01738630\n",
      "Iteration 11307, loss = 0.01738415\n",
      "Iteration 11308, loss = 0.01738199\n",
      "Iteration 11309, loss = 0.01737983\n",
      "Iteration 11310, loss = 0.01737768\n",
      "Iteration 11311, loss = 0.01737553\n",
      "Iteration 11312, loss = 0.01737337\n",
      "Iteration 11313, loss = 0.01737122\n",
      "Iteration 11314, loss = 0.01736907\n",
      "Iteration 11315, loss = 0.01736691\n",
      "Iteration 11316, loss = 0.01736476\n",
      "Iteration 11317, loss = 0.01736261\n",
      "Iteration 11318, loss = 0.01736046\n",
      "Iteration 11319, loss = 0.01735831\n",
      "Iteration 11320, loss = 0.01735616\n",
      "Iteration 11321, loss = 0.01735401\n",
      "Iteration 11322, loss = 0.01735187\n",
      "Iteration 11323, loss = 0.01734972\n",
      "Iteration 11324, loss = 0.01734757\n",
      "Iteration 11325, loss = 0.01734543\n",
      "Iteration 11326, loss = 0.01734328\n",
      "Iteration 11327, loss = 0.01734114\n",
      "Iteration 11328, loss = 0.01733899\n",
      "Iteration 11329, loss = 0.01733685\n",
      "Iteration 11330, loss = 0.01733470\n",
      "Iteration 11331, loss = 0.01733256\n",
      "Iteration 11332, loss = 0.01733042\n",
      "Iteration 11333, loss = 0.01732828\n",
      "Iteration 11334, loss = 0.01732614\n",
      "Iteration 11335, loss = 0.01732399\n",
      "Iteration 11336, loss = 0.01732185\n",
      "Iteration 11337, loss = 0.01731972\n",
      "Iteration 11338, loss = 0.01731758\n",
      "Iteration 11339, loss = 0.01731544\n",
      "Iteration 11340, loss = 0.01731330\n",
      "Iteration 11341, loss = 0.01731116\n",
      "Iteration 11342, loss = 0.01730903\n",
      "Iteration 11343, loss = 0.01730689\n",
      "Iteration 11344, loss = 0.01730475\n",
      "Iteration 11345, loss = 0.01730262\n",
      "Iteration 11346, loss = 0.01730049\n",
      "Iteration 11347, loss = 0.01729835\n",
      "Iteration 11348, loss = 0.01729622\n",
      "Iteration 11349, loss = 0.01729409\n",
      "Iteration 11350, loss = 0.01729195\n",
      "Iteration 11351, loss = 0.01728982\n",
      "Iteration 11352, loss = 0.01728769\n",
      "Iteration 11353, loss = 0.01728556\n",
      "Iteration 11354, loss = 0.01728343\n",
      "Iteration 11355, loss = 0.01728130\n",
      "Iteration 11356, loss = 0.01727917\n",
      "Iteration 11357, loss = 0.01727704\n",
      "Iteration 11358, loss = 0.01727492\n",
      "Iteration 11359, loss = 0.01727279\n",
      "Iteration 11360, loss = 0.01727066\n",
      "Iteration 11361, loss = 0.01726854\n",
      "Iteration 11362, loss = 0.01726641\n",
      "Iteration 11363, loss = 0.01726429\n",
      "Iteration 11364, loss = 0.01726216\n",
      "Iteration 11365, loss = 0.01726004\n",
      "Iteration 11366, loss = 0.01725792\n",
      "Iteration 11367, loss = 0.01725579\n",
      "Iteration 11368, loss = 0.01725367\n",
      "Iteration 11369, loss = 0.01725155\n",
      "Iteration 11370, loss = 0.01724943\n",
      "Iteration 11371, loss = 0.01724731\n",
      "Iteration 11372, loss = 0.01724519\n",
      "Iteration 11373, loss = 0.01724307\n",
      "Iteration 11374, loss = 0.01724095\n",
      "Iteration 11375, loss = 0.01723883\n",
      "Iteration 11376, loss = 0.01723672\n",
      "Iteration 11377, loss = 0.01723460\n",
      "Iteration 11378, loss = 0.01723248\n",
      "Iteration 11379, loss = 0.01723037\n",
      "Iteration 11380, loss = 0.01722825\n",
      "Iteration 11381, loss = 0.01722614\n",
      "Iteration 11382, loss = 0.01722402\n",
      "Iteration 11383, loss = 0.01722191\n",
      "Iteration 11384, loss = 0.01721980\n",
      "Iteration 11385, loss = 0.01721768\n",
      "Iteration 11386, loss = 0.01721557\n",
      "Iteration 11387, loss = 0.01721346\n",
      "Iteration 11388, loss = 0.01721135\n",
      "Iteration 11389, loss = 0.01720924\n",
      "Iteration 11390, loss = 0.01720713\n",
      "Iteration 11391, loss = 0.01720502\n",
      "Iteration 11392, loss = 0.01720291\n",
      "Iteration 11393, loss = 0.01720080\n",
      "Iteration 11394, loss = 0.01719870\n",
      "Iteration 11395, loss = 0.01719659\n",
      "Iteration 11396, loss = 0.01719448\n",
      "Iteration 11397, loss = 0.01719238\n",
      "Iteration 11398, loss = 0.01719027\n",
      "Iteration 11399, loss = 0.01718817\n",
      "Iteration 11400, loss = 0.01718606\n",
      "Iteration 11401, loss = 0.01718396\n",
      "Iteration 11402, loss = 0.01718186\n",
      "Iteration 11403, loss = 0.01717975\n",
      "Iteration 11404, loss = 0.01717765\n",
      "Iteration 11405, loss = 0.01717555\n",
      "Iteration 11406, loss = 0.01717345\n",
      "Iteration 11407, loss = 0.01717135\n",
      "Iteration 11408, loss = 0.01716925\n",
      "Iteration 11409, loss = 0.01716715\n",
      "Iteration 11410, loss = 0.01716505\n",
      "Iteration 11411, loss = 0.01716295\n",
      "Iteration 11412, loss = 0.01716085\n",
      "Iteration 11413, loss = 0.01715876\n",
      "Iteration 11414, loss = 0.01715666\n",
      "Iteration 11415, loss = 0.01715456\n",
      "Iteration 11416, loss = 0.01715247\n",
      "Iteration 11417, loss = 0.01715037\n",
      "Iteration 11418, loss = 0.01714828\n",
      "Iteration 11419, loss = 0.01714619\n",
      "Iteration 11420, loss = 0.01714409\n",
      "Iteration 11421, loss = 0.01714200\n",
      "Iteration 11422, loss = 0.01713991\n",
      "Iteration 11423, loss = 0.01713782\n",
      "Iteration 11424, loss = 0.01713573\n",
      "Iteration 11425, loss = 0.01713364\n",
      "Iteration 11426, loss = 0.01713155\n",
      "Iteration 11427, loss = 0.01712946\n",
      "Iteration 11428, loss = 0.01712737\n",
      "Iteration 11429, loss = 0.01712528\n",
      "Iteration 11430, loss = 0.01712319\n",
      "Iteration 11431, loss = 0.01712110\n",
      "Iteration 11432, loss = 0.01711902\n",
      "Iteration 11433, loss = 0.01711693\n",
      "Iteration 11434, loss = 0.01711485\n",
      "Iteration 11435, loss = 0.01711276\n",
      "Iteration 11436, loss = 0.01711068\n",
      "Iteration 11437, loss = 0.01710859\n",
      "Iteration 11438, loss = 0.01710651\n",
      "Iteration 11439, loss = 0.01710443\n",
      "Iteration 11440, loss = 0.01710234\n",
      "Iteration 11441, loss = 0.01710026\n",
      "Iteration 11442, loss = 0.01709818\n",
      "Iteration 11443, loss = 0.01709610\n",
      "Iteration 11444, loss = 0.01709402\n",
      "Iteration 11445, loss = 0.01709194\n",
      "Iteration 11446, loss = 0.01708986\n",
      "Iteration 11447, loss = 0.01708778\n",
      "Iteration 11448, loss = 0.01708571\n",
      "Iteration 11449, loss = 0.01708363\n",
      "Iteration 11450, loss = 0.01708155\n",
      "Iteration 11451, loss = 0.01707948\n",
      "Iteration 11452, loss = 0.01707740\n",
      "Iteration 11453, loss = 0.01707532\n",
      "Iteration 11454, loss = 0.01707325\n",
      "Iteration 11455, loss = 0.01707118\n",
      "Iteration 11456, loss = 0.01706910\n",
      "Iteration 11457, loss = 0.01706703\n",
      "Iteration 11458, loss = 0.01706496\n",
      "Iteration 11459, loss = 0.01706289\n",
      "Iteration 11460, loss = 0.01706081\n",
      "Iteration 11461, loss = 0.01705874\n",
      "Iteration 11462, loss = 0.01705667\n",
      "Iteration 11463, loss = 0.01705460\n",
      "Iteration 11464, loss = 0.01705253\n",
      "Iteration 11465, loss = 0.01705046\n",
      "Iteration 11466, loss = 0.01704840\n",
      "Iteration 11467, loss = 0.01704633\n",
      "Iteration 11468, loss = 0.01704426\n",
      "Iteration 11469, loss = 0.01704220\n",
      "Iteration 11470, loss = 0.01704013\n",
      "Iteration 11471, loss = 0.01703806\n",
      "Iteration 11472, loss = 0.01703600\n",
      "Iteration 11473, loss = 0.01703393\n",
      "Iteration 11474, loss = 0.01703187\n",
      "Iteration 11475, loss = 0.01702981\n",
      "Iteration 11476, loss = 0.01702774\n",
      "Iteration 11477, loss = 0.01702568\n",
      "Iteration 11478, loss = 0.01702362\n",
      "Iteration 11479, loss = 0.01702156\n",
      "Iteration 11480, loss = 0.01701950\n",
      "Iteration 11481, loss = 0.01701744\n",
      "Iteration 11482, loss = 0.01701538\n",
      "Iteration 11483, loss = 0.01701332\n",
      "Iteration 11484, loss = 0.01701126\n",
      "Iteration 11485, loss = 0.01700920\n",
      "Iteration 11486, loss = 0.01700715\n",
      "Iteration 11487, loss = 0.01700509\n",
      "Iteration 11488, loss = 0.01700303\n",
      "Iteration 11489, loss = 0.01700098\n",
      "Iteration 11490, loss = 0.01699892\n",
      "Iteration 11491, loss = 0.01699687\n",
      "Iteration 11492, loss = 0.01699481\n",
      "Iteration 11493, loss = 0.01699276\n",
      "Iteration 11494, loss = 0.01699071\n",
      "Iteration 11495, loss = 0.01698865\n",
      "Iteration 11496, loss = 0.01698660\n",
      "Iteration 11497, loss = 0.01698455\n",
      "Iteration 11498, loss = 0.01698250\n",
      "Iteration 11499, loss = 0.01698045\n",
      "Iteration 11500, loss = 0.01697840\n",
      "Iteration 11501, loss = 0.01697635\n",
      "Iteration 11502, loss = 0.01697430\n",
      "Iteration 11503, loss = 0.01697225\n",
      "Iteration 11504, loss = 0.01697020\n",
      "Iteration 11505, loss = 0.01696816\n",
      "Iteration 11506, loss = 0.01696611\n",
      "Iteration 11507, loss = 0.01696406\n",
      "Iteration 11508, loss = 0.01696202\n",
      "Iteration 11509, loss = 0.01695997\n",
      "Iteration 11510, loss = 0.01695793\n",
      "Iteration 11511, loss = 0.01695589\n",
      "Iteration 11512, loss = 0.01695384\n",
      "Iteration 11513, loss = 0.01695180\n",
      "Iteration 11514, loss = 0.01694976\n",
      "Iteration 11515, loss = 0.01694771\n",
      "Iteration 11516, loss = 0.01694567\n",
      "Iteration 11517, loss = 0.01694363\n",
      "Iteration 11518, loss = 0.01694159\n",
      "Iteration 11519, loss = 0.01693955\n",
      "Iteration 11520, loss = 0.01693751\n",
      "Iteration 11521, loss = 0.01693547\n",
      "Iteration 11522, loss = 0.01693344\n",
      "Iteration 11523, loss = 0.01693140\n",
      "Iteration 11524, loss = 0.01692936\n",
      "Iteration 11525, loss = 0.01692732\n",
      "Iteration 11526, loss = 0.01692529\n",
      "Iteration 11527, loss = 0.01692325\n",
      "Iteration 11528, loss = 0.01692122\n",
      "Iteration 11529, loss = 0.01691918\n",
      "Iteration 11530, loss = 0.01691715\n",
      "Iteration 11531, loss = 0.01691512\n",
      "Iteration 11532, loss = 0.01691308\n",
      "Iteration 11533, loss = 0.01691105\n",
      "Iteration 11534, loss = 0.01690902\n",
      "Iteration 11535, loss = 0.01690699\n",
      "Iteration 11536, loss = 0.01690496\n",
      "Iteration 11537, loss = 0.01690293\n",
      "Iteration 11538, loss = 0.01690090\n",
      "Iteration 11539, loss = 0.01689887\n",
      "Iteration 11540, loss = 0.01689684\n",
      "Iteration 11541, loss = 0.01689481\n",
      "Iteration 11542, loss = 0.01689278\n",
      "Iteration 11543, loss = 0.01689075\n",
      "Iteration 11544, loss = 0.01688873\n",
      "Iteration 11545, loss = 0.01688670\n",
      "Iteration 11546, loss = 0.01688468\n",
      "Iteration 11547, loss = 0.01688265\n",
      "Iteration 11548, loss = 0.01688063\n",
      "Iteration 11549, loss = 0.01687860\n",
      "Iteration 11550, loss = 0.01687658\n",
      "Iteration 11551, loss = 0.01687456\n",
      "Iteration 11552, loss = 0.01687253\n",
      "Iteration 11553, loss = 0.01687051\n",
      "Iteration 11554, loss = 0.01686849\n",
      "Iteration 11555, loss = 0.01686647\n",
      "Iteration 11556, loss = 0.01686445\n",
      "Iteration 11557, loss = 0.01686243\n",
      "Iteration 11558, loss = 0.01686041\n",
      "Iteration 11559, loss = 0.01685839\n",
      "Iteration 11560, loss = 0.01685637\n",
      "Iteration 11561, loss = 0.01685436\n",
      "Iteration 11562, loss = 0.01685234\n",
      "Iteration 11563, loss = 0.01685032\n",
      "Iteration 11564, loss = 0.01684830\n",
      "Iteration 11565, loss = 0.01684629\n",
      "Iteration 11566, loss = 0.01684427\n",
      "Iteration 11567, loss = 0.01684226\n",
      "Iteration 11568, loss = 0.01684025\n",
      "Iteration 11569, loss = 0.01683823\n",
      "Iteration 11570, loss = 0.01683622\n",
      "Iteration 11571, loss = 0.01683421\n",
      "Iteration 11572, loss = 0.01683219\n",
      "Iteration 11573, loss = 0.01683018\n",
      "Iteration 11574, loss = 0.01682817\n",
      "Iteration 11575, loss = 0.01682616\n",
      "Iteration 11576, loss = 0.01682415\n",
      "Iteration 11577, loss = 0.01682214\n",
      "Iteration 11578, loss = 0.01682013\n",
      "Iteration 11579, loss = 0.01681812\n",
      "Iteration 11580, loss = 0.01681612\n",
      "Iteration 11581, loss = 0.01681411\n",
      "Iteration 11582, loss = 0.01681210\n",
      "Iteration 11583, loss = 0.01681010\n",
      "Iteration 11584, loss = 0.01680809\n",
      "Iteration 11585, loss = 0.01680609\n",
      "Iteration 11586, loss = 0.01680408\n",
      "Iteration 11587, loss = 0.01680208\n",
      "Iteration 11588, loss = 0.01680007\n",
      "Iteration 11589, loss = 0.01679807\n",
      "Iteration 11590, loss = 0.01679607\n",
      "Iteration 11591, loss = 0.01679406\n",
      "Iteration 11592, loss = 0.01679206\n",
      "Iteration 11593, loss = 0.01679006\n",
      "Iteration 11594, loss = 0.01678806\n",
      "Iteration 11595, loss = 0.01678606\n",
      "Iteration 11596, loss = 0.01678406\n",
      "Iteration 11597, loss = 0.01678206\n",
      "Iteration 11598, loss = 0.01678006\n",
      "Iteration 11599, loss = 0.01677807\n",
      "Iteration 11600, loss = 0.01677607\n",
      "Iteration 11601, loss = 0.01677407\n",
      "Iteration 11602, loss = 0.01677207\n",
      "Iteration 11603, loss = 0.01677008\n",
      "Iteration 11604, loss = 0.01676808\n",
      "Iteration 11605, loss = 0.01676609\n",
      "Iteration 11606, loss = 0.01676409\n",
      "Iteration 11607, loss = 0.01676210\n",
      "Iteration 11608, loss = 0.01676011\n",
      "Iteration 11609, loss = 0.01675811\n",
      "Iteration 11610, loss = 0.01675612\n",
      "Iteration 11611, loss = 0.01675413\n",
      "Iteration 11612, loss = 0.01675214\n",
      "Iteration 11613, loss = 0.01675015\n",
      "Iteration 11614, loss = 0.01674816\n",
      "Iteration 11615, loss = 0.01674617\n",
      "Iteration 11616, loss = 0.01674418\n",
      "Iteration 11617, loss = 0.01674219\n",
      "Iteration 11618, loss = 0.01674020\n",
      "Iteration 11619, loss = 0.01673821\n",
      "Iteration 11620, loss = 0.01673622\n",
      "Iteration 11621, loss = 0.01673424\n",
      "Iteration 11622, loss = 0.01673225\n",
      "Iteration 11623, loss = 0.01673027\n",
      "Iteration 11624, loss = 0.01672828\n",
      "Iteration 11625, loss = 0.01672630\n",
      "Iteration 11626, loss = 0.01672431\n",
      "Iteration 11627, loss = 0.01672233\n",
      "Iteration 11628, loss = 0.01672034\n",
      "Iteration 11629, loss = 0.01671836\n",
      "Iteration 11630, loss = 0.01671638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11631, loss = 0.01671440\n",
      "Iteration 11632, loss = 0.01671242\n",
      "Iteration 11633, loss = 0.01671044\n",
      "Iteration 11634, loss = 0.01670846\n",
      "Iteration 11635, loss = 0.01670648\n",
      "Iteration 11636, loss = 0.01670450\n",
      "Iteration 11637, loss = 0.01670252\n",
      "Iteration 11638, loss = 0.01670054\n",
      "Iteration 11639, loss = 0.01669856\n",
      "Iteration 11640, loss = 0.01669659\n",
      "Iteration 11641, loss = 0.01669461\n",
      "Iteration 11642, loss = 0.01669263\n",
      "Iteration 11643, loss = 0.01669066\n",
      "Iteration 11644, loss = 0.01668868\n",
      "Iteration 11645, loss = 0.01668671\n",
      "Iteration 11646, loss = 0.01668473\n",
      "Iteration 11647, loss = 0.01668276\n",
      "Iteration 11648, loss = 0.01668079\n",
      "Iteration 11649, loss = 0.01667881\n",
      "Iteration 11650, loss = 0.01667684\n",
      "Iteration 11651, loss = 0.01667487\n",
      "Iteration 11652, loss = 0.01667290\n",
      "Iteration 11653, loss = 0.01667093\n",
      "Iteration 11654, loss = 0.01666896\n",
      "Iteration 11655, loss = 0.01666699\n",
      "Iteration 11656, loss = 0.01666502\n",
      "Iteration 11657, loss = 0.01666305\n",
      "Iteration 11658, loss = 0.01666108\n",
      "Iteration 11659, loss = 0.01665912\n",
      "Iteration 11660, loss = 0.01665715\n",
      "Iteration 11661, loss = 0.01665518\n",
      "Iteration 11662, loss = 0.01665322\n",
      "Iteration 11663, loss = 0.01665125\n",
      "Iteration 11664, loss = 0.01664929\n",
      "Iteration 11665, loss = 0.01664732\n",
      "Iteration 11666, loss = 0.01664536\n",
      "Iteration 11667, loss = 0.01664339\n",
      "Iteration 11668, loss = 0.01664143\n",
      "Iteration 11669, loss = 0.01663947\n",
      "Iteration 11670, loss = 0.01663751\n",
      "Iteration 11671, loss = 0.01663554\n",
      "Iteration 11672, loss = 0.01663358\n",
      "Iteration 11673, loss = 0.01663162\n",
      "Iteration 11674, loss = 0.01662966\n",
      "Iteration 11675, loss = 0.01662770\n",
      "Iteration 11676, loss = 0.01662574\n",
      "Iteration 11677, loss = 0.01662379\n",
      "Iteration 11678, loss = 0.01662183\n",
      "Iteration 11679, loss = 0.01661987\n",
      "Iteration 11680, loss = 0.01661791\n",
      "Iteration 11681, loss = 0.01661596\n",
      "Iteration 11682, loss = 0.01661400\n",
      "Iteration 11683, loss = 0.01661205\n",
      "Iteration 11684, loss = 0.01661009\n",
      "Iteration 11685, loss = 0.01660814\n",
      "Iteration 11686, loss = 0.01660618\n",
      "Iteration 11687, loss = 0.01660423\n",
      "Iteration 11688, loss = 0.01660228\n",
      "Iteration 11689, loss = 0.01660032\n",
      "Iteration 11690, loss = 0.01659837\n",
      "Iteration 11691, loss = 0.01659642\n",
      "Iteration 11692, loss = 0.01659447\n",
      "Iteration 11693, loss = 0.01659252\n",
      "Iteration 11694, loss = 0.01659057\n",
      "Iteration 11695, loss = 0.01658862\n",
      "Iteration 11696, loss = 0.01658667\n",
      "Iteration 11697, loss = 0.01658472\n",
      "Iteration 11698, loss = 0.01658277\n",
      "Iteration 11699, loss = 0.01658083\n",
      "Iteration 11700, loss = 0.01657888\n",
      "Iteration 11701, loss = 0.01657693\n",
      "Iteration 11702, loss = 0.01657499\n",
      "Iteration 11703, loss = 0.01657304\n",
      "Iteration 11704, loss = 0.01657110\n",
      "Iteration 11705, loss = 0.01656915\n",
      "Iteration 11706, loss = 0.01656721\n",
      "Iteration 11707, loss = 0.01656526\n",
      "Iteration 11708, loss = 0.01656332\n",
      "Iteration 11709, loss = 0.01656138\n",
      "Iteration 11710, loss = 0.01655944\n",
      "Iteration 11711, loss = 0.01655749\n",
      "Iteration 11712, loss = 0.01655555\n",
      "Iteration 11713, loss = 0.01655361\n",
      "Iteration 11714, loss = 0.01655167\n",
      "Iteration 11715, loss = 0.01654973\n",
      "Iteration 11716, loss = 0.01654779\n",
      "Iteration 11717, loss = 0.01654585\n",
      "Iteration 11718, loss = 0.01654392\n",
      "Iteration 11719, loss = 0.01654198\n",
      "Iteration 11720, loss = 0.01654004\n",
      "Iteration 11721, loss = 0.01653810\n",
      "Iteration 11722, loss = 0.01653617\n",
      "Iteration 11723, loss = 0.01653423\n",
      "Iteration 11724, loss = 0.01653230\n",
      "Iteration 11725, loss = 0.01653036\n",
      "Iteration 11726, loss = 0.01652843\n",
      "Iteration 11727, loss = 0.01652650\n",
      "Iteration 11728, loss = 0.01652456\n",
      "Iteration 11729, loss = 0.01652263\n",
      "Iteration 11730, loss = 0.01652070\n",
      "Iteration 11731, loss = 0.01651877\n",
      "Iteration 11732, loss = 0.01651683\n",
      "Iteration 11733, loss = 0.01651490\n",
      "Iteration 11734, loss = 0.01651297\n",
      "Iteration 11735, loss = 0.01651104\n",
      "Iteration 11736, loss = 0.01650911\n",
      "Iteration 11737, loss = 0.01650719\n",
      "Iteration 11738, loss = 0.01650526\n",
      "Iteration 11739, loss = 0.01650333\n",
      "Iteration 11740, loss = 0.01650140\n",
      "Iteration 11741, loss = 0.01649947\n",
      "Iteration 11742, loss = 0.01649755\n",
      "Iteration 11743, loss = 0.01649562\n",
      "Iteration 11744, loss = 0.01649370\n",
      "Iteration 11745, loss = 0.01649177\n",
      "Iteration 11746, loss = 0.01648985\n",
      "Iteration 11747, loss = 0.01648792\n",
      "Iteration 11748, loss = 0.01648600\n",
      "Iteration 11749, loss = 0.01648408\n",
      "Iteration 11750, loss = 0.01648216\n",
      "Iteration 11751, loss = 0.01648023\n",
      "Iteration 11752, loss = 0.01647831\n",
      "Iteration 11753, loss = 0.01647639\n",
      "Iteration 11754, loss = 0.01647447\n",
      "Iteration 11755, loss = 0.01647255\n",
      "Iteration 11756, loss = 0.01647063\n",
      "Iteration 11757, loss = 0.01646871\n",
      "Iteration 11758, loss = 0.01646679\n",
      "Iteration 11759, loss = 0.01646487\n",
      "Iteration 11760, loss = 0.01646296\n",
      "Iteration 11761, loss = 0.01646104\n",
      "Iteration 11762, loss = 0.01645912\n",
      "Iteration 11763, loss = 0.01645721\n",
      "Iteration 11764, loss = 0.01645529\n",
      "Iteration 11765, loss = 0.01645338\n",
      "Iteration 11766, loss = 0.01645146\n",
      "Iteration 11767, loss = 0.01644955\n",
      "Iteration 11768, loss = 0.01644763\n",
      "Iteration 11769, loss = 0.01644572\n",
      "Iteration 11770, loss = 0.01644381\n",
      "Iteration 11771, loss = 0.01644190\n",
      "Iteration 11772, loss = 0.01643998\n",
      "Iteration 11773, loss = 0.01643807\n",
      "Iteration 11774, loss = 0.01643616\n",
      "Iteration 11775, loss = 0.01643425\n",
      "Iteration 11776, loss = 0.01643234\n",
      "Iteration 11777, loss = 0.01643043\n",
      "Iteration 11778, loss = 0.01642852\n",
      "Iteration 11779, loss = 0.01642662\n",
      "Iteration 11780, loss = 0.01642471\n",
      "Iteration 11781, loss = 0.01642280\n",
      "Iteration 11782, loss = 0.01642089\n",
      "Iteration 11783, loss = 0.01641899\n",
      "Iteration 11784, loss = 0.01641708\n",
      "Iteration 11785, loss = 0.01641518\n",
      "Iteration 11786, loss = 0.01641327\n",
      "Iteration 11787, loss = 0.01641137\n",
      "Iteration 11788, loss = 0.01640946\n",
      "Iteration 11789, loss = 0.01640756\n",
      "Iteration 11790, loss = 0.01640566\n",
      "Iteration 11791, loss = 0.01640375\n",
      "Iteration 11792, loss = 0.01640185\n",
      "Iteration 11793, loss = 0.01639995\n",
      "Iteration 11794, loss = 0.01639805\n",
      "Iteration 11795, loss = 0.01639615\n",
      "Iteration 11796, loss = 0.01639425\n",
      "Iteration 11797, loss = 0.01639235\n",
      "Iteration 11798, loss = 0.01639045\n",
      "Iteration 11799, loss = 0.01638855\n",
      "Iteration 11800, loss = 0.01638665\n",
      "Iteration 11801, loss = 0.01638475\n",
      "Iteration 11802, loss = 0.01638286\n",
      "Iteration 11803, loss = 0.01638096\n",
      "Iteration 11804, loss = 0.01637906\n",
      "Iteration 11805, loss = 0.01637717\n",
      "Iteration 11806, loss = 0.01637527\n",
      "Iteration 11807, loss = 0.01637338\n",
      "Iteration 11808, loss = 0.01637148\n",
      "Iteration 11809, loss = 0.01636959\n",
      "Iteration 11810, loss = 0.01636769\n",
      "Iteration 11811, loss = 0.01636580\n",
      "Iteration 11812, loss = 0.01636391\n",
      "Iteration 11813, loss = 0.01636202\n",
      "Iteration 11814, loss = 0.01636012\n",
      "Iteration 11815, loss = 0.01635823\n",
      "Iteration 11816, loss = 0.01635634\n",
      "Iteration 11817, loss = 0.01635445\n",
      "Iteration 11818, loss = 0.01635256\n",
      "Iteration 11819, loss = 0.01635067\n",
      "Iteration 11820, loss = 0.01634879\n",
      "Iteration 11821, loss = 0.01634690\n",
      "Iteration 11822, loss = 0.01634501\n",
      "Iteration 11823, loss = 0.01634312\n",
      "Iteration 11824, loss = 0.01634123\n",
      "Iteration 11825, loss = 0.01633935\n",
      "Iteration 11826, loss = 0.01633746\n",
      "Iteration 11827, loss = 0.01633558\n",
      "Iteration 11828, loss = 0.01633369\n",
      "Iteration 11829, loss = 0.01633181\n",
      "Iteration 11830, loss = 0.01632992\n",
      "Iteration 11831, loss = 0.01632804\n",
      "Iteration 11832, loss = 0.01632616\n",
      "Iteration 11833, loss = 0.01632427\n",
      "Iteration 11834, loss = 0.01632239\n",
      "Iteration 11835, loss = 0.01632051\n",
      "Iteration 11836, loss = 0.01631863\n",
      "Iteration 11837, loss = 0.01631675\n",
      "Iteration 11838, loss = 0.01631487\n",
      "Iteration 11839, loss = 0.01631299\n",
      "Iteration 11840, loss = 0.01631111\n",
      "Iteration 11841, loss = 0.01630923\n",
      "Iteration 11842, loss = 0.01630735\n",
      "Iteration 11843, loss = 0.01630547\n",
      "Iteration 11844, loss = 0.01630360\n",
      "Iteration 11845, loss = 0.01630172\n",
      "Iteration 11846, loss = 0.01629984\n",
      "Iteration 11847, loss = 0.01629797\n",
      "Iteration 11848, loss = 0.01629609\n",
      "Iteration 11849, loss = 0.01629422\n",
      "Iteration 11850, loss = 0.01629234\n",
      "Iteration 11851, loss = 0.01629047\n",
      "Iteration 11852, loss = 0.01628860\n",
      "Iteration 11853, loss = 0.01628672\n",
      "Iteration 11854, loss = 0.01628485\n",
      "Iteration 11855, loss = 0.01628298\n",
      "Iteration 11856, loss = 0.01628111\n",
      "Iteration 11857, loss = 0.01627923\n",
      "Iteration 11858, loss = 0.01627736\n",
      "Iteration 11859, loss = 0.01627549\n",
      "Iteration 11860, loss = 0.01627362\n",
      "Iteration 11861, loss = 0.01627175\n",
      "Iteration 11862, loss = 0.01626989\n",
      "Iteration 11863, loss = 0.01626802\n",
      "Iteration 11864, loss = 0.01626615\n",
      "Iteration 11865, loss = 0.01626428\n",
      "Iteration 11866, loss = 0.01626241\n",
      "Iteration 11867, loss = 0.01626055\n",
      "Iteration 11868, loss = 0.01625868\n",
      "Iteration 11869, loss = 0.01625682\n",
      "Iteration 11870, loss = 0.01625495\n",
      "Iteration 11871, loss = 0.01625309\n",
      "Iteration 11872, loss = 0.01625122\n",
      "Iteration 11873, loss = 0.01624936\n",
      "Iteration 11874, loss = 0.01624749\n",
      "Iteration 11875, loss = 0.01624563\n",
      "Iteration 11876, loss = 0.01624377\n",
      "Iteration 11877, loss = 0.01624191\n",
      "Iteration 11878, loss = 0.01624005\n",
      "Iteration 11879, loss = 0.01623819\n",
      "Iteration 11880, loss = 0.01623632\n",
      "Iteration 11881, loss = 0.01623446\n",
      "Iteration 11882, loss = 0.01623261\n",
      "Iteration 11883, loss = 0.01623075\n",
      "Iteration 11884, loss = 0.01622889\n",
      "Iteration 11885, loss = 0.01622703\n",
      "Iteration 11886, loss = 0.01622517\n",
      "Iteration 11887, loss = 0.01622331\n",
      "Iteration 11888, loss = 0.01622146\n",
      "Iteration 11889, loss = 0.01621960\n",
      "Iteration 11890, loss = 0.01621775\n",
      "Iteration 11891, loss = 0.01621589\n",
      "Iteration 11892, loss = 0.01621404\n",
      "Iteration 11893, loss = 0.01621218\n",
      "Iteration 11894, loss = 0.01621033\n",
      "Iteration 11895, loss = 0.01620847\n",
      "Iteration 11896, loss = 0.01620662\n",
      "Iteration 11897, loss = 0.01620477\n",
      "Iteration 11898, loss = 0.01620292\n",
      "Iteration 11899, loss = 0.01620106\n",
      "Iteration 11900, loss = 0.01619921\n",
      "Iteration 11901, loss = 0.01619736\n",
      "Iteration 11902, loss = 0.01619551\n",
      "Iteration 11903, loss = 0.01619366\n",
      "Iteration 11904, loss = 0.01619181\n",
      "Iteration 11905, loss = 0.01618996\n",
      "Iteration 11906, loss = 0.01618811\n",
      "Iteration 11907, loss = 0.01618627\n",
      "Iteration 11908, loss = 0.01618442\n",
      "Iteration 11909, loss = 0.01618257\n",
      "Iteration 11910, loss = 0.01618073\n",
      "Iteration 11911, loss = 0.01617888\n",
      "Iteration 11912, loss = 0.01617703\n",
      "Iteration 11913, loss = 0.01617519\n",
      "Iteration 11914, loss = 0.01617334\n",
      "Iteration 11915, loss = 0.01617150\n",
      "Iteration 11916, loss = 0.01616966\n",
      "Iteration 11917, loss = 0.01616781\n",
      "Iteration 11918, loss = 0.01616597\n",
      "Iteration 11919, loss = 0.01616413\n",
      "Iteration 11920, loss = 0.01616228\n",
      "Iteration 11921, loss = 0.01616044\n",
      "Iteration 11922, loss = 0.01615860\n",
      "Iteration 11923, loss = 0.01615676\n",
      "Iteration 11924, loss = 0.01615492\n",
      "Iteration 11925, loss = 0.01615308\n",
      "Iteration 11926, loss = 0.01615124\n",
      "Iteration 11927, loss = 0.01614940\n",
      "Iteration 11928, loss = 0.01614757\n",
      "Iteration 11929, loss = 0.01614573\n",
      "Iteration 11930, loss = 0.01614389\n",
      "Iteration 11931, loss = 0.01614205\n",
      "Iteration 11932, loss = 0.01614022\n",
      "Iteration 11933, loss = 0.01613838\n",
      "Iteration 11934, loss = 0.01613654\n",
      "Iteration 11935, loss = 0.01613471\n",
      "Iteration 11936, loss = 0.01613288\n",
      "Iteration 11937, loss = 0.01613104\n",
      "Iteration 11938, loss = 0.01612921\n",
      "Iteration 11939, loss = 0.01612737\n",
      "Iteration 11940, loss = 0.01612554\n",
      "Iteration 11941, loss = 0.01612371\n",
      "Iteration 11942, loss = 0.01612188\n",
      "Iteration 11943, loss = 0.01612005\n",
      "Iteration 11944, loss = 0.01611821\n",
      "Iteration 11945, loss = 0.01611638\n",
      "Iteration 11946, loss = 0.01611455\n",
      "Iteration 11947, loss = 0.01611272\n",
      "Iteration 11948, loss = 0.01611089\n",
      "Iteration 11949, loss = 0.01610907\n",
      "Iteration 11950, loss = 0.01610724\n",
      "Iteration 11951, loss = 0.01610541\n",
      "Iteration 11952, loss = 0.01610358\n",
      "Iteration 11953, loss = 0.01610176\n",
      "Iteration 11954, loss = 0.01609993\n",
      "Iteration 11955, loss = 0.01609810\n",
      "Iteration 11956, loss = 0.01609628\n",
      "Iteration 11957, loss = 0.01609445\n",
      "Iteration 11958, loss = 0.01609263\n",
      "Iteration 11959, loss = 0.01609080\n",
      "Iteration 11960, loss = 0.01608898\n",
      "Iteration 11961, loss = 0.01608716\n",
      "Iteration 11962, loss = 0.01608533\n",
      "Iteration 11963, loss = 0.01608351\n",
      "Iteration 11964, loss = 0.01608169\n",
      "Iteration 11965, loss = 0.01607987\n",
      "Iteration 11966, loss = 0.01607805\n",
      "Iteration 11967, loss = 0.01607623\n",
      "Iteration 11968, loss = 0.01607441\n",
      "Iteration 11969, loss = 0.01607259\n",
      "Iteration 11970, loss = 0.01607077\n",
      "Iteration 11971, loss = 0.01606895\n",
      "Iteration 11972, loss = 0.01606713\n",
      "Iteration 11973, loss = 0.01606531\n",
      "Iteration 11974, loss = 0.01606349\n",
      "Iteration 11975, loss = 0.01606168\n",
      "Iteration 11976, loss = 0.01605986\n",
      "Iteration 11977, loss = 0.01605804\n",
      "Iteration 11978, loss = 0.01605623\n",
      "Iteration 11979, loss = 0.01605441\n",
      "Iteration 11980, loss = 0.01605260\n",
      "Iteration 11981, loss = 0.01605078\n",
      "Iteration 11982, loss = 0.01604897\n",
      "Iteration 11983, loss = 0.01604716\n",
      "Iteration 11984, loss = 0.01604534\n",
      "Iteration 11985, loss = 0.01604353\n",
      "Iteration 11986, loss = 0.01604172\n",
      "Iteration 11987, loss = 0.01603991\n",
      "Iteration 11988, loss = 0.01603810\n",
      "Iteration 11989, loss = 0.01603629\n",
      "Iteration 11990, loss = 0.01603448\n",
      "Iteration 11991, loss = 0.01603267\n",
      "Iteration 11992, loss = 0.01603086\n",
      "Iteration 11993, loss = 0.01602905\n",
      "Iteration 11994, loss = 0.01602724\n",
      "Iteration 11995, loss = 0.01602543\n",
      "Iteration 11996, loss = 0.01602362\n",
      "Iteration 11997, loss = 0.01602182\n",
      "Iteration 11998, loss = 0.01602001\n",
      "Iteration 11999, loss = 0.01601820\n",
      "Iteration 12000, loss = 0.01601640\n",
      "Iteration 12001, loss = 0.01601459\n",
      "Iteration 12002, loss = 0.01601279\n",
      "Iteration 12003, loss = 0.01601098\n",
      "Iteration 12004, loss = 0.01600918\n",
      "Iteration 12005, loss = 0.01600738\n",
      "Iteration 12006, loss = 0.01600557\n",
      "Iteration 12007, loss = 0.01600377\n",
      "Iteration 12008, loss = 0.01600197\n",
      "Iteration 12009, loss = 0.01600017\n",
      "Iteration 12010, loss = 0.01599837\n",
      "Iteration 12011, loss = 0.01599656\n",
      "Iteration 12012, loss = 0.01599476\n",
      "Iteration 12013, loss = 0.01599296\n",
      "Iteration 12014, loss = 0.01599117\n",
      "Iteration 12015, loss = 0.01598937\n",
      "Iteration 12016, loss = 0.01598757\n",
      "Iteration 12017, loss = 0.01598577\n",
      "Iteration 12018, loss = 0.01598397\n",
      "Iteration 12019, loss = 0.01598217\n",
      "Iteration 12020, loss = 0.01598038\n",
      "Iteration 12021, loss = 0.01597858\n",
      "Iteration 12022, loss = 0.01597678\n",
      "Iteration 12023, loss = 0.01597499\n",
      "Iteration 12024, loss = 0.01597319\n",
      "Iteration 12025, loss = 0.01597140\n",
      "Iteration 12026, loss = 0.01596961\n",
      "Iteration 12027, loss = 0.01596781\n",
      "Iteration 12028, loss = 0.01596602\n",
      "Iteration 12029, loss = 0.01596423\n",
      "Iteration 12030, loss = 0.01596243\n",
      "Iteration 12031, loss = 0.01596064\n",
      "Iteration 12032, loss = 0.01595885\n",
      "Iteration 12033, loss = 0.01595706\n",
      "Iteration 12034, loss = 0.01595527\n",
      "Iteration 12035, loss = 0.01595348\n",
      "Iteration 12036, loss = 0.01595169\n",
      "Iteration 12037, loss = 0.01594990\n",
      "Iteration 12038, loss = 0.01594811\n",
      "Iteration 12039, loss = 0.01594632\n",
      "Iteration 12040, loss = 0.01594453\n",
      "Iteration 12041, loss = 0.01594275\n",
      "Iteration 12042, loss = 0.01594096\n",
      "Iteration 12043, loss = 0.01593917\n",
      "Iteration 12044, loss = 0.01593739\n",
      "Iteration 12045, loss = 0.01593560\n",
      "Iteration 12046, loss = 0.01593381\n",
      "Iteration 12047, loss = 0.01593203\n",
      "Iteration 12048, loss = 0.01593024\n",
      "Iteration 12049, loss = 0.01592846\n",
      "Iteration 12050, loss = 0.01592668\n",
      "Iteration 12051, loss = 0.01592489\n",
      "Iteration 12052, loss = 0.01592311\n",
      "Iteration 12053, loss = 0.01592133\n",
      "Iteration 12054, loss = 0.01591955\n",
      "Iteration 12055, loss = 0.01591777\n",
      "Iteration 12056, loss = 0.01591598\n",
      "Iteration 12057, loss = 0.01591420\n",
      "Iteration 12058, loss = 0.01591242\n",
      "Iteration 12059, loss = 0.01591064\n",
      "Iteration 12060, loss = 0.01590886\n",
      "Iteration 12061, loss = 0.01590709\n",
      "Iteration 12062, loss = 0.01590531\n",
      "Iteration 12063, loss = 0.01590353\n",
      "Iteration 12064, loss = 0.01590175\n",
      "Iteration 12065, loss = 0.01589998\n",
      "Iteration 12066, loss = 0.01589820\n",
      "Iteration 12067, loss = 0.01589642\n",
      "Iteration 12068, loss = 0.01589465\n",
      "Iteration 12069, loss = 0.01589287\n",
      "Iteration 12070, loss = 0.01589110\n",
      "Iteration 12071, loss = 0.01588932\n",
      "Iteration 12072, loss = 0.01588755\n",
      "Iteration 12073, loss = 0.01588577\n",
      "Iteration 12074, loss = 0.01588400\n",
      "Iteration 12075, loss = 0.01588223\n",
      "Iteration 12076, loss = 0.01588046\n",
      "Iteration 12077, loss = 0.01587868\n",
      "Iteration 12078, loss = 0.01587691\n",
      "Iteration 12079, loss = 0.01587514\n",
      "Iteration 12080, loss = 0.01587337\n",
      "Iteration 12081, loss = 0.01587160\n",
      "Iteration 12082, loss = 0.01586983\n",
      "Iteration 12083, loss = 0.01586806\n",
      "Iteration 12084, loss = 0.01586629\n",
      "Iteration 12085, loss = 0.01586453\n",
      "Iteration 12086, loss = 0.01586276\n",
      "Iteration 12087, loss = 0.01586099\n",
      "Iteration 12088, loss = 0.01585922\n",
      "Iteration 12089, loss = 0.01585746\n",
      "Iteration 12090, loss = 0.01585569\n",
      "Iteration 12091, loss = 0.01585392\n",
      "Iteration 12092, loss = 0.01585216\n",
      "Iteration 12093, loss = 0.01585039\n",
      "Iteration 12094, loss = 0.01584863\n",
      "Iteration 12095, loss = 0.01584687\n",
      "Iteration 12096, loss = 0.01584510\n",
      "Iteration 12097, loss = 0.01584334\n",
      "Iteration 12098, loss = 0.01584158\n",
      "Iteration 12099, loss = 0.01583981\n",
      "Iteration 12100, loss = 0.01583805\n",
      "Iteration 12101, loss = 0.01583629\n",
      "Iteration 12102, loss = 0.01583453\n",
      "Iteration 12103, loss = 0.01583277\n",
      "Iteration 12104, loss = 0.01583101\n",
      "Iteration 12105, loss = 0.01582925\n",
      "Iteration 12106, loss = 0.01582749\n",
      "Iteration 12107, loss = 0.01582573\n",
      "Iteration 12108, loss = 0.01582397\n",
      "Iteration 12109, loss = 0.01582222\n",
      "Iteration 12110, loss = 0.01582046\n",
      "Iteration 12111, loss = 0.01581870\n",
      "Iteration 12112, loss = 0.01581694\n",
      "Iteration 12113, loss = 0.01581519\n",
      "Iteration 12114, loss = 0.01581343\n",
      "Iteration 12115, loss = 0.01581168\n",
      "Iteration 12116, loss = 0.01580992\n",
      "Iteration 12117, loss = 0.01580817\n",
      "Iteration 12118, loss = 0.01580641\n",
      "Iteration 12119, loss = 0.01580466\n",
      "Iteration 12120, loss = 0.01580291\n",
      "Iteration 12121, loss = 0.01580115\n",
      "Iteration 12122, loss = 0.01579940\n",
      "Iteration 12123, loss = 0.01579765\n",
      "Iteration 12124, loss = 0.01579590\n",
      "Iteration 12125, loss = 0.01579415\n",
      "Iteration 12126, loss = 0.01579240\n",
      "Iteration 12127, loss = 0.01579065\n",
      "Iteration 12128, loss = 0.01578890\n",
      "Iteration 12129, loss = 0.01578715\n",
      "Iteration 12130, loss = 0.01578540\n",
      "Iteration 12131, loss = 0.01578365\n",
      "Iteration 12132, loss = 0.01578190\n",
      "Iteration 12133, loss = 0.01578015\n",
      "Iteration 12134, loss = 0.01577841\n",
      "Iteration 12135, loss = 0.01577666\n",
      "Iteration 12136, loss = 0.01577491\n",
      "Iteration 12137, loss = 0.01577317\n",
      "Iteration 12138, loss = 0.01577142\n",
      "Iteration 12139, loss = 0.01576968\n",
      "Iteration 12140, loss = 0.01576793\n",
      "Iteration 12141, loss = 0.01576619\n",
      "Iteration 12142, loss = 0.01576444\n",
      "Iteration 12143, loss = 0.01576270\n",
      "Iteration 12144, loss = 0.01576096\n",
      "Iteration 12145, loss = 0.01575921\n",
      "Iteration 12146, loss = 0.01575747\n",
      "Iteration 12147, loss = 0.01575573\n",
      "Iteration 12148, loss = 0.01575399\n",
      "Iteration 12149, loss = 0.01575225\n",
      "Iteration 12150, loss = 0.01575051\n",
      "Iteration 12151, loss = 0.01574877\n",
      "Iteration 12152, loss = 0.01574703\n",
      "Iteration 12153, loss = 0.01574529\n",
      "Iteration 12154, loss = 0.01574355\n",
      "Iteration 12155, loss = 0.01574181\n",
      "Iteration 12156, loss = 0.01574007\n",
      "Iteration 12157, loss = 0.01573834\n",
      "Iteration 12158, loss = 0.01573660\n",
      "Iteration 12159, loss = 0.01573486\n",
      "Iteration 12160, loss = 0.01573313\n",
      "Iteration 12161, loss = 0.01573139\n",
      "Iteration 12162, loss = 0.01572965\n",
      "Iteration 12163, loss = 0.01572792\n",
      "Iteration 12164, loss = 0.01572619\n",
      "Iteration 12165, loss = 0.01572445\n",
      "Iteration 12166, loss = 0.01572272\n",
      "Iteration 12167, loss = 0.01572098\n",
      "Iteration 12168, loss = 0.01571925\n",
      "Iteration 12169, loss = 0.01571752\n",
      "Iteration 12170, loss = 0.01571579\n",
      "Iteration 12171, loss = 0.01571406\n",
      "Iteration 12172, loss = 0.01571232\n",
      "Iteration 12173, loss = 0.01571059\n",
      "Iteration 12174, loss = 0.01570886\n",
      "Iteration 12175, loss = 0.01570713\n",
      "Iteration 12176, loss = 0.01570540\n",
      "Iteration 12177, loss = 0.01570367\n",
      "Iteration 12178, loss = 0.01570195\n",
      "Iteration 12179, loss = 0.01570022\n",
      "Iteration 12180, loss = 0.01569849\n",
      "Iteration 12181, loss = 0.01569676\n",
      "Iteration 12182, loss = 0.01569504\n",
      "Iteration 12183, loss = 0.01569331\n",
      "Iteration 12184, loss = 0.01569158\n",
      "Iteration 12185, loss = 0.01568986\n",
      "Iteration 12186, loss = 0.01568813\n",
      "Iteration 12187, loss = 0.01568641\n",
      "Iteration 12188, loss = 0.01568468\n",
      "Iteration 12189, loss = 0.01568296\n",
      "Iteration 12190, loss = 0.01568124\n",
      "Iteration 12191, loss = 0.01567951\n",
      "Iteration 12192, loss = 0.01567779\n",
      "Iteration 12193, loss = 0.01567607\n",
      "Iteration 12194, loss = 0.01567435\n",
      "Iteration 12195, loss = 0.01567262\n",
      "Iteration 12196, loss = 0.01567090\n",
      "Iteration 12197, loss = 0.01566918\n",
      "Iteration 12198, loss = 0.01566746\n",
      "Iteration 12199, loss = 0.01566574\n",
      "Iteration 12200, loss = 0.01566402\n",
      "Iteration 12201, loss = 0.01566230\n",
      "Iteration 12202, loss = 0.01566059\n",
      "Iteration 12203, loss = 0.01565887\n",
      "Iteration 12204, loss = 0.01565715\n",
      "Iteration 12205, loss = 0.01565543\n",
      "Iteration 12206, loss = 0.01565371\n",
      "Iteration 12207, loss = 0.01565200\n",
      "Iteration 12208, loss = 0.01565028\n",
      "Iteration 12209, loss = 0.01564857\n",
      "Iteration 12210, loss = 0.01564685\n",
      "Iteration 12211, loss = 0.01564514\n",
      "Iteration 12212, loss = 0.01564342\n",
      "Iteration 12213, loss = 0.01564171\n",
      "Iteration 12214, loss = 0.01563999\n",
      "Iteration 12215, loss = 0.01563828\n",
      "Iteration 12216, loss = 0.01563657\n",
      "Iteration 12217, loss = 0.01563486\n",
      "Iteration 12218, loss = 0.01563314\n",
      "Iteration 12219, loss = 0.01563143\n",
      "Iteration 12220, loss = 0.01562972\n",
      "Iteration 12221, loss = 0.01562801\n",
      "Iteration 12222, loss = 0.01562630\n",
      "Iteration 12223, loss = 0.01562459\n",
      "Iteration 12224, loss = 0.01562288\n",
      "Iteration 12225, loss = 0.01562117\n",
      "Iteration 12226, loss = 0.01561946\n",
      "Iteration 12227, loss = 0.01561776\n",
      "Iteration 12228, loss = 0.01561605\n",
      "Iteration 12229, loss = 0.01561434\n",
      "Iteration 12230, loss = 0.01561263\n",
      "Iteration 12231, loss = 0.01561093\n",
      "Iteration 12232, loss = 0.01560922\n",
      "Iteration 12233, loss = 0.01560752\n",
      "Iteration 12234, loss = 0.01560581\n",
      "Iteration 12235, loss = 0.01560411\n",
      "Iteration 12236, loss = 0.01560240\n",
      "Iteration 12237, loss = 0.01560070\n",
      "Iteration 12238, loss = 0.01559899\n",
      "Iteration 12239, loss = 0.01559729\n",
      "Iteration 12240, loss = 0.01559559\n",
      "Iteration 12241, loss = 0.01559388\n",
      "Iteration 12242, loss = 0.01559218\n",
      "Iteration 12243, loss = 0.01559048\n",
      "Iteration 12244, loss = 0.01558878\n",
      "Iteration 12245, loss = 0.01558708\n",
      "Iteration 12246, loss = 0.01558538\n",
      "Iteration 12247, loss = 0.01558368\n",
      "Iteration 12248, loss = 0.01558198\n",
      "Iteration 12249, loss = 0.01558028\n",
      "Iteration 12250, loss = 0.01557858\n",
      "Iteration 12251, loss = 0.01557688\n",
      "Iteration 12252, loss = 0.01557518\n",
      "Iteration 12253, loss = 0.01557349\n",
      "Iteration 12254, loss = 0.01557179\n",
      "Iteration 12255, loss = 0.01557009\n",
      "Iteration 12256, loss = 0.01556840\n",
      "Iteration 12257, loss = 0.01556670\n",
      "Iteration 12258, loss = 0.01556501\n",
      "Iteration 12259, loss = 0.01556331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12260, loss = 0.01556162\n",
      "Iteration 12261, loss = 0.01555992\n",
      "Iteration 12262, loss = 0.01555823\n",
      "Iteration 12263, loss = 0.01555653\n",
      "Iteration 12264, loss = 0.01555484\n",
      "Iteration 12265, loss = 0.01555315\n",
      "Iteration 12266, loss = 0.01555146\n",
      "Iteration 12267, loss = 0.01554977\n",
      "Iteration 12268, loss = 0.01554807\n",
      "Iteration 12269, loss = 0.01554638\n",
      "Iteration 12270, loss = 0.01554469\n",
      "Iteration 12271, loss = 0.01554300\n",
      "Iteration 12272, loss = 0.01554131\n",
      "Iteration 12273, loss = 0.01553962\n",
      "Iteration 12274, loss = 0.01553793\n",
      "Iteration 12275, loss = 0.01553625\n",
      "Iteration 12276, loss = 0.01553456\n",
      "Iteration 12277, loss = 0.01553287\n",
      "Iteration 12278, loss = 0.01553118\n",
      "Iteration 12279, loss = 0.01552950\n",
      "Iteration 12280, loss = 0.01552781\n",
      "Iteration 12281, loss = 0.01552612\n",
      "Iteration 12282, loss = 0.01552444\n",
      "Iteration 12283, loss = 0.01552275\n",
      "Iteration 12284, loss = 0.01552107\n",
      "Iteration 12285, loss = 0.01551938\n",
      "Iteration 12286, loss = 0.01551770\n",
      "Iteration 12287, loss = 0.01551602\n",
      "Iteration 12288, loss = 0.01551433\n",
      "Iteration 12289, loss = 0.01551265\n",
      "Iteration 12290, loss = 0.01551097\n",
      "Iteration 12291, loss = 0.01550928\n",
      "Iteration 12292, loss = 0.01550760\n",
      "Iteration 12293, loss = 0.01550592\n",
      "Iteration 12294, loss = 0.01550424\n",
      "Iteration 12295, loss = 0.01550256\n",
      "Iteration 12296, loss = 0.01550088\n",
      "Iteration 12297, loss = 0.01549920\n",
      "Iteration 12298, loss = 0.01549752\n",
      "Iteration 12299, loss = 0.01549584\n",
      "Iteration 12300, loss = 0.01549416\n",
      "Iteration 12301, loss = 0.01549249\n",
      "Iteration 12302, loss = 0.01549081\n",
      "Iteration 12303, loss = 0.01548913\n",
      "Iteration 12304, loss = 0.01548746\n",
      "Iteration 12305, loss = 0.01548578\n",
      "Iteration 12306, loss = 0.01548410\n",
      "Iteration 12307, loss = 0.01548243\n",
      "Iteration 12308, loss = 0.01548075\n",
      "Iteration 12309, loss = 0.01547908\n",
      "Iteration 12310, loss = 0.01547740\n",
      "Iteration 12311, loss = 0.01547573\n",
      "Iteration 12312, loss = 0.01547406\n",
      "Iteration 12313, loss = 0.01547238\n",
      "Iteration 12314, loss = 0.01547071\n",
      "Iteration 12315, loss = 0.01546904\n",
      "Iteration 12316, loss = 0.01546737\n",
      "Iteration 12317, loss = 0.01546569\n",
      "Iteration 12318, loss = 0.01546402\n",
      "Iteration 12319, loss = 0.01546235\n",
      "Iteration 12320, loss = 0.01546068\n",
      "Iteration 12321, loss = 0.01545901\n",
      "Iteration 12322, loss = 0.01545734\n",
      "Iteration 12323, loss = 0.01545567\n",
      "Iteration 12324, loss = 0.01545400\n",
      "Iteration 12325, loss = 0.01545234\n",
      "Iteration 12326, loss = 0.01545067\n",
      "Iteration 12327, loss = 0.01544900\n",
      "Iteration 12328, loss = 0.01544733\n",
      "Iteration 12329, loss = 0.01544567\n",
      "Iteration 12330, loss = 0.01544400\n",
      "Iteration 12331, loss = 0.01544233\n",
      "Iteration 12332, loss = 0.01544067\n",
      "Iteration 12333, loss = 0.01543900\n",
      "Iteration 12334, loss = 0.01543734\n",
      "Iteration 12335, loss = 0.01543567\n",
      "Iteration 12336, loss = 0.01543401\n",
      "Iteration 12337, loss = 0.01543235\n",
      "Iteration 12338, loss = 0.01543068\n",
      "Iteration 12339, loss = 0.01542902\n",
      "Iteration 12340, loss = 0.01542736\n",
      "Iteration 12341, loss = 0.01542570\n",
      "Iteration 12342, loss = 0.01542404\n",
      "Iteration 12343, loss = 0.01542237\n",
      "Iteration 12344, loss = 0.01542071\n",
      "Iteration 12345, loss = 0.01541905\n",
      "Iteration 12346, loss = 0.01541739\n",
      "Iteration 12347, loss = 0.01541573\n",
      "Iteration 12348, loss = 0.01541407\n",
      "Iteration 12349, loss = 0.01541242\n",
      "Iteration 12350, loss = 0.01541076\n",
      "Iteration 12351, loss = 0.01540910\n",
      "Iteration 12352, loss = 0.01540744\n",
      "Iteration 12353, loss = 0.01540578\n",
      "Iteration 12354, loss = 0.01540413\n",
      "Iteration 12355, loss = 0.01540247\n",
      "Iteration 12356, loss = 0.01540082\n",
      "Iteration 12357, loss = 0.01539916\n",
      "Iteration 12358, loss = 0.01539750\n",
      "Iteration 12359, loss = 0.01539585\n",
      "Iteration 12360, loss = 0.01539420\n",
      "Iteration 12361, loss = 0.01539254\n",
      "Iteration 12362, loss = 0.01539089\n",
      "Iteration 12363, loss = 0.01538923\n",
      "Iteration 12364, loss = 0.01538758\n",
      "Iteration 12365, loss = 0.01538593\n",
      "Iteration 12366, loss = 0.01538428\n",
      "Iteration 12367, loss = 0.01538263\n",
      "Iteration 12368, loss = 0.01538097\n",
      "Iteration 12369, loss = 0.01537932\n",
      "Iteration 12370, loss = 0.01537767\n",
      "Iteration 12371, loss = 0.01537602\n",
      "Iteration 12372, loss = 0.01537437\n",
      "Iteration 12373, loss = 0.01537272\n",
      "Iteration 12374, loss = 0.01537107\n",
      "Iteration 12375, loss = 0.01536943\n",
      "Iteration 12376, loss = 0.01536778\n",
      "Iteration 12377, loss = 0.01536613\n",
      "Iteration 12378, loss = 0.01536448\n",
      "Iteration 12379, loss = 0.01536284\n",
      "Iteration 12380, loss = 0.01536119\n",
      "Iteration 12381, loss = 0.01535954\n",
      "Iteration 12382, loss = 0.01535790\n",
      "Iteration 12383, loss = 0.01535625\n",
      "Iteration 12384, loss = 0.01535461\n",
      "Iteration 12385, loss = 0.01535296\n",
      "Iteration 12386, loss = 0.01535132\n",
      "Iteration 12387, loss = 0.01534967\n",
      "Iteration 12388, loss = 0.01534803\n",
      "Iteration 12389, loss = 0.01534639\n",
      "Iteration 12390, loss = 0.01534475\n",
      "Iteration 12391, loss = 0.01534310\n",
      "Iteration 12392, loss = 0.01534146\n",
      "Iteration 12393, loss = 0.01533982\n",
      "Iteration 12394, loss = 0.01533818\n",
      "Iteration 12395, loss = 0.01533654\n",
      "Iteration 12396, loss = 0.01533490\n",
      "Iteration 12397, loss = 0.01533326\n",
      "Iteration 12398, loss = 0.01533162\n",
      "Iteration 12399, loss = 0.01532998\n",
      "Iteration 12400, loss = 0.01532834\n",
      "Iteration 12401, loss = 0.01532670\n",
      "Iteration 12402, loss = 0.01532506\n",
      "Iteration 12403, loss = 0.01532343\n",
      "Iteration 12404, loss = 0.01532179\n",
      "Iteration 12405, loss = 0.01532015\n",
      "Iteration 12406, loss = 0.01531852\n",
      "Iteration 12407, loss = 0.01531688\n",
      "Iteration 12408, loss = 0.01531525\n",
      "Iteration 12409, loss = 0.01531361\n",
      "Iteration 12410, loss = 0.01531198\n",
      "Iteration 12411, loss = 0.01531034\n",
      "Iteration 12412, loss = 0.01530871\n",
      "Iteration 12413, loss = 0.01530707\n",
      "Iteration 12414, loss = 0.01530544\n",
      "Iteration 12415, loss = 0.01530381\n",
      "Iteration 12416, loss = 0.01530217\n",
      "Iteration 12417, loss = 0.01530054\n",
      "Iteration 12418, loss = 0.01529891\n",
      "Iteration 12419, loss = 0.01529728\n",
      "Iteration 12420, loss = 0.01529565\n",
      "Iteration 12421, loss = 0.01529402\n",
      "Iteration 12422, loss = 0.01529239\n",
      "Iteration 12423, loss = 0.01529076\n",
      "Iteration 12424, loss = 0.01528913\n",
      "Iteration 12425, loss = 0.01528750\n",
      "Iteration 12426, loss = 0.01528587\n",
      "Iteration 12427, loss = 0.01528424\n",
      "Iteration 12428, loss = 0.01528262\n",
      "Iteration 12429, loss = 0.01528099\n",
      "Iteration 12430, loss = 0.01527936\n",
      "Iteration 12431, loss = 0.01527773\n",
      "Iteration 12432, loss = 0.01527611\n",
      "Iteration 12433, loss = 0.01527448\n",
      "Iteration 12434, loss = 0.01527286\n",
      "Iteration 12435, loss = 0.01527123\n",
      "Iteration 12436, loss = 0.01526961\n",
      "Iteration 12437, loss = 0.01526798\n",
      "Iteration 12438, loss = 0.01526636\n",
      "Iteration 12439, loss = 0.01526473\n",
      "Iteration 12440, loss = 0.01526311\n",
      "Iteration 12441, loss = 0.01526149\n",
      "Iteration 12442, loss = 0.01525987\n",
      "Iteration 12443, loss = 0.01525824\n",
      "Iteration 12444, loss = 0.01525662\n",
      "Iteration 12445, loss = 0.01525500\n",
      "Iteration 12446, loss = 0.01525338\n",
      "Iteration 12447, loss = 0.01525176\n",
      "Iteration 12448, loss = 0.01525014\n",
      "Iteration 12449, loss = 0.01524852\n",
      "Iteration 12450, loss = 0.01524690\n",
      "Iteration 12451, loss = 0.01524528\n",
      "Iteration 12452, loss = 0.01524366\n",
      "Iteration 12453, loss = 0.01524204\n",
      "Iteration 12454, loss = 0.01524043\n",
      "Iteration 12455, loss = 0.01523881\n",
      "Iteration 12456, loss = 0.01523719\n",
      "Iteration 12457, loss = 0.01523558\n",
      "Iteration 12458, loss = 0.01523396\n",
      "Iteration 12459, loss = 0.01523234\n",
      "Iteration 12460, loss = 0.01523073\n",
      "Iteration 12461, loss = 0.01522911\n",
      "Iteration 12462, loss = 0.01522750\n",
      "Iteration 12463, loss = 0.01522588\n",
      "Iteration 12464, loss = 0.01522427\n",
      "Iteration 12465, loss = 0.01522266\n",
      "Iteration 12466, loss = 0.01522104\n",
      "Iteration 12467, loss = 0.01521943\n",
      "Iteration 12468, loss = 0.01521782\n",
      "Iteration 12469, loss = 0.01521621\n",
      "Iteration 12470, loss = 0.01521459\n",
      "Iteration 12471, loss = 0.01521298\n",
      "Iteration 12472, loss = 0.01521137\n",
      "Iteration 12473, loss = 0.01520976\n",
      "Iteration 12474, loss = 0.01520815\n",
      "Iteration 12475, loss = 0.01520654\n",
      "Iteration 12476, loss = 0.01520493\n",
      "Iteration 12477, loss = 0.01520332\n",
      "Iteration 12478, loss = 0.01520171\n",
      "Iteration 12479, loss = 0.01520011\n",
      "Iteration 12480, loss = 0.01519850\n",
      "Iteration 12481, loss = 0.01519689\n",
      "Iteration 12482, loss = 0.01519528\n",
      "Iteration 12483, loss = 0.01519368\n",
      "Iteration 12484, loss = 0.01519207\n",
      "Iteration 12485, loss = 0.01519046\n",
      "Iteration 12486, loss = 0.01518886\n",
      "Iteration 12487, loss = 0.01518725\n",
      "Iteration 12488, loss = 0.01518565\n",
      "Iteration 12489, loss = 0.01518404\n",
      "Iteration 12490, loss = 0.01518244\n",
      "Iteration 12491, loss = 0.01518084\n",
      "Iteration 12492, loss = 0.01517923\n",
      "Iteration 12493, loss = 0.01517763\n",
      "Iteration 12494, loss = 0.01517603\n",
      "Iteration 12495, loss = 0.01517442\n",
      "Iteration 12496, loss = 0.01517282\n",
      "Iteration 12497, loss = 0.01517122\n",
      "Iteration 12498, loss = 0.01516962\n",
      "Iteration 12499, loss = 0.01516802\n",
      "Iteration 12500, loss = 0.01516642\n",
      "Iteration 12501, loss = 0.01516482\n",
      "Iteration 12502, loss = 0.01516322\n",
      "Iteration 12503, loss = 0.01516162\n",
      "Iteration 12504, loss = 0.01516002\n",
      "Iteration 12505, loss = 0.01515842\n",
      "Iteration 12506, loss = 0.01515682\n",
      "Iteration 12507, loss = 0.01515523\n",
      "Iteration 12508, loss = 0.01515363\n",
      "Iteration 12509, loss = 0.01515203\n",
      "Iteration 12510, loss = 0.01515044\n",
      "Iteration 12511, loss = 0.01514884\n",
      "Iteration 12512, loss = 0.01514724\n",
      "Iteration 12513, loss = 0.01514565\n",
      "Iteration 12514, loss = 0.01514405\n",
      "Iteration 12515, loss = 0.01514246\n",
      "Iteration 12516, loss = 0.01514086\n",
      "Iteration 12517, loss = 0.01513927\n",
      "Iteration 12518, loss = 0.01513768\n",
      "Iteration 12519, loss = 0.01513608\n",
      "Iteration 12520, loss = 0.01513449\n",
      "Iteration 12521, loss = 0.01513290\n",
      "Iteration 12522, loss = 0.01513131\n",
      "Iteration 12523, loss = 0.01512972\n",
      "Iteration 12524, loss = 0.01512812\n",
      "Iteration 12525, loss = 0.01512653\n",
      "Iteration 12526, loss = 0.01512494\n",
      "Iteration 12527, loss = 0.01512335\n",
      "Iteration 12528, loss = 0.01512176\n",
      "Iteration 12529, loss = 0.01512017\n",
      "Iteration 12530, loss = 0.01511858\n",
      "Iteration 12531, loss = 0.01511700\n",
      "Iteration 12532, loss = 0.01511541\n",
      "Iteration 12533, loss = 0.01511382\n",
      "Iteration 12534, loss = 0.01511223\n",
      "Iteration 12535, loss = 0.01511064\n",
      "Iteration 12536, loss = 0.01510906\n",
      "Iteration 12537, loss = 0.01510747\n",
      "Iteration 12538, loss = 0.01510588\n",
      "Iteration 12539, loss = 0.01510430\n",
      "Iteration 12540, loss = 0.01510271\n",
      "Iteration 12541, loss = 0.01510113\n",
      "Iteration 12542, loss = 0.01509954\n",
      "Iteration 12543, loss = 0.01509796\n",
      "Iteration 12544, loss = 0.01509638\n",
      "Iteration 12545, loss = 0.01509479\n",
      "Iteration 12546, loss = 0.01509321\n",
      "Iteration 12547, loss = 0.01509163\n",
      "Iteration 12548, loss = 0.01509004\n",
      "Iteration 12549, loss = 0.01508846\n",
      "Iteration 12550, loss = 0.01508688\n",
      "Iteration 12551, loss = 0.01508530\n",
      "Iteration 12552, loss = 0.01508372\n",
      "Iteration 12553, loss = 0.01508214\n",
      "Iteration 12554, loss = 0.01508056\n",
      "Iteration 12555, loss = 0.01507898\n",
      "Iteration 12556, loss = 0.01507740\n",
      "Iteration 12557, loss = 0.01507582\n",
      "Iteration 12558, loss = 0.01507424\n",
      "Iteration 12559, loss = 0.01507266\n",
      "Iteration 12560, loss = 0.01507109\n",
      "Iteration 12561, loss = 0.01506951\n",
      "Iteration 12562, loss = 0.01506793\n",
      "Iteration 12563, loss = 0.01506635\n",
      "Iteration 12564, loss = 0.01506478\n",
      "Iteration 12565, loss = 0.01506320\n",
      "Iteration 12566, loss = 0.01506163\n",
      "Iteration 12567, loss = 0.01506005\n",
      "Iteration 12568, loss = 0.01505848\n",
      "Iteration 12569, loss = 0.01505690\n",
      "Iteration 12570, loss = 0.01505533\n",
      "Iteration 12571, loss = 0.01505375\n",
      "Iteration 12572, loss = 0.01505218\n",
      "Iteration 12573, loss = 0.01505061\n",
      "Iteration 12574, loss = 0.01504903\n",
      "Iteration 12575, loss = 0.01504746\n",
      "Iteration 12576, loss = 0.01504589\n",
      "Iteration 12577, loss = 0.01504432\n",
      "Iteration 12578, loss = 0.01504275\n",
      "Iteration 12579, loss = 0.01504118\n",
      "Iteration 12580, loss = 0.01503960\n",
      "Iteration 12581, loss = 0.01503803\n",
      "Iteration 12582, loss = 0.01503646\n",
      "Iteration 12583, loss = 0.01503490\n",
      "Iteration 12584, loss = 0.01503333\n",
      "Iteration 12585, loss = 0.01503176\n",
      "Iteration 12586, loss = 0.01503019\n",
      "Iteration 12587, loss = 0.01502862\n",
      "Iteration 12588, loss = 0.01502705\n",
      "Iteration 12589, loss = 0.01502549\n",
      "Iteration 12590, loss = 0.01502392\n",
      "Iteration 12591, loss = 0.01502235\n",
      "Iteration 12592, loss = 0.01502079\n",
      "Iteration 12593, loss = 0.01501922\n",
      "Iteration 12594, loss = 0.01501766\n",
      "Iteration 12595, loss = 0.01501609\n",
      "Iteration 12596, loss = 0.01501453\n",
      "Iteration 12597, loss = 0.01501296\n",
      "Iteration 12598, loss = 0.01501140\n",
      "Iteration 12599, loss = 0.01500983\n",
      "Iteration 12600, loss = 0.01500827\n",
      "Iteration 12601, loss = 0.01500671\n",
      "Iteration 12602, loss = 0.01500515\n",
      "Iteration 12603, loss = 0.01500358\n",
      "Iteration 12604, loss = 0.01500202\n",
      "Iteration 12605, loss = 0.01500046\n",
      "Iteration 12606, loss = 0.01499890\n",
      "Iteration 12607, loss = 0.01499734\n",
      "Iteration 12608, loss = 0.01499578\n",
      "Iteration 12609, loss = 0.01499422\n",
      "Iteration 12610, loss = 0.01499266\n",
      "Iteration 12611, loss = 0.01499110\n",
      "Iteration 12612, loss = 0.01498954\n",
      "Iteration 12613, loss = 0.01498798\n",
      "Iteration 12614, loss = 0.01498642\n",
      "Iteration 12615, loss = 0.01498487\n",
      "Iteration 12616, loss = 0.01498331\n",
      "Iteration 12617, loss = 0.01498175\n",
      "Iteration 12618, loss = 0.01498020\n",
      "Iteration 12619, loss = 0.01497864\n",
      "Iteration 12620, loss = 0.01497708\n",
      "Iteration 12621, loss = 0.01497553\n",
      "Iteration 12622, loss = 0.01497397\n",
      "Iteration 12623, loss = 0.01497242\n",
      "Iteration 12624, loss = 0.01497086\n",
      "Iteration 12625, loss = 0.01496931\n",
      "Iteration 12626, loss = 0.01496776\n",
      "Iteration 12627, loss = 0.01496620\n",
      "Iteration 12628, loss = 0.01496465\n",
      "Iteration 12629, loss = 0.01496310\n",
      "Iteration 12630, loss = 0.01496154\n",
      "Iteration 12631, loss = 0.01495999\n",
      "Iteration 12632, loss = 0.01495844\n",
      "Iteration 12633, loss = 0.01495689\n",
      "Iteration 12634, loss = 0.01495534\n",
      "Iteration 12635, loss = 0.01495379\n",
      "Iteration 12636, loss = 0.01495224\n",
      "Iteration 12637, loss = 0.01495069\n",
      "Iteration 12638, loss = 0.01494914\n",
      "Iteration 12639, loss = 0.01494759\n",
      "Iteration 12640, loss = 0.01494604\n",
      "Iteration 12641, loss = 0.01494449\n",
      "Iteration 12642, loss = 0.01494295\n",
      "Iteration 12643, loss = 0.01494140\n",
      "Iteration 12644, loss = 0.01493985\n",
      "Iteration 12645, loss = 0.01493830\n",
      "Iteration 12646, loss = 0.01493676\n",
      "Iteration 12647, loss = 0.01493521\n",
      "Iteration 12648, loss = 0.01493367\n",
      "Iteration 12649, loss = 0.01493212\n",
      "Iteration 12650, loss = 0.01493058\n",
      "Iteration 12651, loss = 0.01492903\n",
      "Iteration 12652, loss = 0.01492749\n",
      "Iteration 12653, loss = 0.01492594\n",
      "Iteration 12654, loss = 0.01492440\n",
      "Iteration 12655, loss = 0.01492286\n",
      "Iteration 12656, loss = 0.01492131\n",
      "Iteration 12657, loss = 0.01491977\n",
      "Iteration 12658, loss = 0.01491823\n",
      "Iteration 12659, loss = 0.01491669\n",
      "Iteration 12660, loss = 0.01491514\n",
      "Iteration 12661, loss = 0.01491360\n",
      "Iteration 12662, loss = 0.01491206\n",
      "Iteration 12663, loss = 0.01491052\n",
      "Iteration 12664, loss = 0.01490898\n",
      "Iteration 12665, loss = 0.01490744\n",
      "Iteration 12666, loss = 0.01490590\n",
      "Iteration 12667, loss = 0.01490436\n",
      "Iteration 12668, loss = 0.01490283\n",
      "Iteration 12669, loss = 0.01490129\n",
      "Iteration 12670, loss = 0.01489975\n",
      "Iteration 12671, loss = 0.01489821\n",
      "Iteration 12672, loss = 0.01489667\n",
      "Iteration 12673, loss = 0.01489514\n",
      "Iteration 12674, loss = 0.01489360\n",
      "Iteration 12675, loss = 0.01489206\n",
      "Iteration 12676, loss = 0.01489053\n",
      "Iteration 12677, loss = 0.01488899\n",
      "Iteration 12678, loss = 0.01488746\n",
      "Iteration 12679, loss = 0.01488592\n",
      "Iteration 12680, loss = 0.01488439\n",
      "Iteration 12681, loss = 0.01488286\n",
      "Iteration 12682, loss = 0.01488132\n",
      "Iteration 12683, loss = 0.01487979\n",
      "Iteration 12684, loss = 0.01487826\n",
      "Iteration 12685, loss = 0.01487672\n",
      "Iteration 12686, loss = 0.01487519\n",
      "Iteration 12687, loss = 0.01487366\n",
      "Iteration 12688, loss = 0.01487213\n",
      "Iteration 12689, loss = 0.01487060\n",
      "Iteration 12690, loss = 0.01486907\n",
      "Iteration 12691, loss = 0.01486754\n",
      "Iteration 12692, loss = 0.01486601\n",
      "Iteration 12693, loss = 0.01486448\n",
      "Iteration 12694, loss = 0.01486295\n",
      "Iteration 12695, loss = 0.01486142\n",
      "Iteration 12696, loss = 0.01485989\n",
      "Iteration 12697, loss = 0.01485836\n",
      "Iteration 12698, loss = 0.01485683\n",
      "Iteration 12699, loss = 0.01485530\n",
      "Iteration 12700, loss = 0.01485378\n",
      "Iteration 12701, loss = 0.01485225\n",
      "Iteration 12702, loss = 0.01485072\n",
      "Iteration 12703, loss = 0.01484920\n",
      "Iteration 12704, loss = 0.01484767\n",
      "Iteration 12705, loss = 0.01484615\n",
      "Iteration 12706, loss = 0.01484462\n",
      "Iteration 12707, loss = 0.01484310\n",
      "Iteration 12708, loss = 0.01484157\n",
      "Iteration 12709, loss = 0.01484005\n",
      "Iteration 12710, loss = 0.01483852\n",
      "Iteration 12711, loss = 0.01483700\n",
      "Iteration 12712, loss = 0.01483548\n",
      "Iteration 12713, loss = 0.01483396\n",
      "Iteration 12714, loss = 0.01483243\n",
      "Iteration 12715, loss = 0.01483091\n",
      "Iteration 12716, loss = 0.01482939\n",
      "Iteration 12717, loss = 0.01482787\n",
      "Iteration 12718, loss = 0.01482635\n",
      "Iteration 12719, loss = 0.01482483\n",
      "Iteration 12720, loss = 0.01482331\n",
      "Iteration 12721, loss = 0.01482179\n",
      "Iteration 12722, loss = 0.01482027\n",
      "Iteration 12723, loss = 0.01481875\n",
      "Iteration 12724, loss = 0.01481723\n",
      "Iteration 12725, loss = 0.01481571\n",
      "Iteration 12726, loss = 0.01481419\n",
      "Iteration 12727, loss = 0.01481267\n",
      "Iteration 12728, loss = 0.01481116\n",
      "Iteration 12729, loss = 0.01480964\n",
      "Iteration 12730, loss = 0.01480812\n",
      "Iteration 12731, loss = 0.01480661\n",
      "Iteration 12732, loss = 0.01480509\n",
      "Iteration 12733, loss = 0.01480358\n",
      "Iteration 12734, loss = 0.01480206\n",
      "Iteration 12735, loss = 0.01480055\n",
      "Iteration 12736, loss = 0.01479903\n",
      "Iteration 12737, loss = 0.01479752\n",
      "Iteration 12738, loss = 0.01479600\n",
      "Iteration 12739, loss = 0.01479449\n",
      "Iteration 12740, loss = 0.01479298\n",
      "Iteration 12741, loss = 0.01479146\n",
      "Iteration 12742, loss = 0.01478995\n",
      "Iteration 12743, loss = 0.01478844\n",
      "Iteration 12744, loss = 0.01478693\n",
      "Iteration 12745, loss = 0.01478542\n",
      "Iteration 12746, loss = 0.01478390\n",
      "Iteration 12747, loss = 0.01478239\n",
      "Iteration 12748, loss = 0.01478088\n",
      "Iteration 12749, loss = 0.01477937\n",
      "Iteration 12750, loss = 0.01477786\n",
      "Iteration 12751, loss = 0.01477635\n",
      "Iteration 12752, loss = 0.01477485\n",
      "Iteration 12753, loss = 0.01477334\n",
      "Iteration 12754, loss = 0.01477183\n",
      "Iteration 12755, loss = 0.01477032\n",
      "Iteration 12756, loss = 0.01476881\n",
      "Iteration 12757, loss = 0.01476731\n",
      "Iteration 12758, loss = 0.01476580\n",
      "Iteration 12759, loss = 0.01476429\n",
      "Iteration 12760, loss = 0.01476279\n",
      "Iteration 12761, loss = 0.01476128\n",
      "Iteration 12762, loss = 0.01475977\n",
      "Iteration 12763, loss = 0.01475827\n",
      "Iteration 12764, loss = 0.01475676\n",
      "Iteration 12765, loss = 0.01475526\n",
      "Iteration 12766, loss = 0.01475376\n",
      "Iteration 12767, loss = 0.01475225\n",
      "Iteration 12768, loss = 0.01475075\n",
      "Iteration 12769, loss = 0.01474925\n",
      "Iteration 12770, loss = 0.01474774\n",
      "Iteration 12771, loss = 0.01474624\n",
      "Iteration 12772, loss = 0.01474474\n",
      "Iteration 12773, loss = 0.01474324\n",
      "Iteration 12774, loss = 0.01474173\n",
      "Iteration 12775, loss = 0.01474023\n",
      "Iteration 12776, loss = 0.01473873\n",
      "Iteration 12777, loss = 0.01473723\n",
      "Iteration 12778, loss = 0.01473573\n",
      "Iteration 12779, loss = 0.01473423\n",
      "Iteration 12780, loss = 0.01473273\n",
      "Iteration 12781, loss = 0.01473123\n",
      "Iteration 12782, loss = 0.01472974\n",
      "Iteration 12783, loss = 0.01472824\n",
      "Iteration 12784, loss = 0.01472674\n",
      "Iteration 12785, loss = 0.01472524\n",
      "Iteration 12786, loss = 0.01472374\n",
      "Iteration 12787, loss = 0.01472225\n",
      "Iteration 12788, loss = 0.01472075\n",
      "Iteration 12789, loss = 0.01471926\n",
      "Iteration 12790, loss = 0.01471776\n",
      "Iteration 12791, loss = 0.01471626\n",
      "Iteration 12792, loss = 0.01471477\n",
      "Iteration 12793, loss = 0.01471327\n",
      "Iteration 12794, loss = 0.01471178\n",
      "Iteration 12795, loss = 0.01471029\n",
      "Iteration 12796, loss = 0.01470879\n",
      "Iteration 12797, loss = 0.01470730\n",
      "Iteration 12798, loss = 0.01470580\n",
      "Iteration 12799, loss = 0.01470431\n",
      "Iteration 12800, loss = 0.01470282\n",
      "Iteration 12801, loss = 0.01470133\n",
      "Iteration 12802, loss = 0.01469984\n",
      "Iteration 12803, loss = 0.01469834\n",
      "Iteration 12804, loss = 0.01469685\n",
      "Iteration 12805, loss = 0.01469536\n",
      "Iteration 12806, loss = 0.01469387\n",
      "Iteration 12807, loss = 0.01469238\n",
      "Iteration 12808, loss = 0.01469089\n",
      "Iteration 12809, loss = 0.01468940\n",
      "Iteration 12810, loss = 0.01468791\n",
      "Iteration 12811, loss = 0.01468643\n",
      "Iteration 12812, loss = 0.01468494\n",
      "Iteration 12813, loss = 0.01468345\n",
      "Iteration 12814, loss = 0.01468196\n",
      "Iteration 12815, loss = 0.01468047\n",
      "Iteration 12816, loss = 0.01467899\n",
      "Iteration 12817, loss = 0.01467750\n",
      "Iteration 12818, loss = 0.01467601\n",
      "Iteration 12819, loss = 0.01467453\n",
      "Iteration 12820, loss = 0.01467304\n",
      "Iteration 12821, loss = 0.01467156\n",
      "Iteration 12822, loss = 0.01467007\n",
      "Iteration 12823, loss = 0.01466859\n",
      "Iteration 12824, loss = 0.01466710\n",
      "Iteration 12825, loss = 0.01466562\n",
      "Iteration 12826, loss = 0.01466414\n",
      "Iteration 12827, loss = 0.01466265\n",
      "Iteration 12828, loss = 0.01466117\n",
      "Iteration 12829, loss = 0.01465969\n",
      "Iteration 12830, loss = 0.01465821\n",
      "Iteration 12831, loss = 0.01465672\n",
      "Iteration 12832, loss = 0.01465524\n",
      "Iteration 12833, loss = 0.01465376\n",
      "Iteration 12834, loss = 0.01465228\n",
      "Iteration 12835, loss = 0.01465080\n",
      "Iteration 12836, loss = 0.01464932\n",
      "Iteration 12837, loss = 0.01464784\n",
      "Iteration 12838, loss = 0.01464636\n",
      "Iteration 12839, loss = 0.01464488\n",
      "Iteration 12840, loss = 0.01464340\n",
      "Iteration 12841, loss = 0.01464192\n",
      "Iteration 12842, loss = 0.01464044\n",
      "Iteration 12843, loss = 0.01463897\n",
      "Iteration 12844, loss = 0.01463749\n",
      "Iteration 12845, loss = 0.01463601\n",
      "Iteration 12846, loss = 0.01463454\n",
      "Iteration 12847, loss = 0.01463306\n",
      "Iteration 12848, loss = 0.01463158\n",
      "Iteration 12849, loss = 0.01463011\n",
      "Iteration 12850, loss = 0.01462863\n",
      "Iteration 12851, loss = 0.01462716\n",
      "Iteration 12852, loss = 0.01462568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12853, loss = 0.01462421\n",
      "Iteration 12854, loss = 0.01462273\n",
      "Iteration 12855, loss = 0.01462126\n",
      "Iteration 12856, loss = 0.01461979\n",
      "Iteration 12857, loss = 0.01461831\n",
      "Iteration 12858, loss = 0.01461684\n",
      "Iteration 12859, loss = 0.01461537\n",
      "Iteration 12860, loss = 0.01461390\n",
      "Iteration 12861, loss = 0.01461242\n",
      "Iteration 12862, loss = 0.01461095\n",
      "Iteration 12863, loss = 0.01460948\n",
      "Iteration 12864, loss = 0.01460801\n",
      "Iteration 12865, loss = 0.01460654\n",
      "Iteration 12866, loss = 0.01460507\n",
      "Iteration 12867, loss = 0.01460360\n",
      "Iteration 12868, loss = 0.01460213\n",
      "Iteration 12869, loss = 0.01460066\n",
      "Iteration 12870, loss = 0.01459919\n",
      "Iteration 12871, loss = 0.01459773\n",
      "Iteration 12872, loss = 0.01459626\n",
      "Iteration 12873, loss = 0.01459479\n",
      "Iteration 12874, loss = 0.01459332\n",
      "Iteration 12875, loss = 0.01459185\n",
      "Iteration 12876, loss = 0.01459039\n",
      "Iteration 12877, loss = 0.01458892\n",
      "Iteration 12878, loss = 0.01458746\n",
      "Iteration 12879, loss = 0.01458599\n",
      "Iteration 12880, loss = 0.01458452\n",
      "Iteration 12881, loss = 0.01458306\n",
      "Iteration 12882, loss = 0.01458159\n",
      "Iteration 12883, loss = 0.01458013\n",
      "Iteration 12884, loss = 0.01457867\n",
      "Iteration 12885, loss = 0.01457720\n",
      "Iteration 12886, loss = 0.01457574\n",
      "Iteration 12887, loss = 0.01457428\n",
      "Iteration 12888, loss = 0.01457281\n",
      "Iteration 12889, loss = 0.01457135\n",
      "Iteration 12890, loss = 0.01456989\n",
      "Iteration 12891, loss = 0.01456843\n",
      "Iteration 12892, loss = 0.01456697\n",
      "Iteration 12893, loss = 0.01456550\n",
      "Iteration 12894, loss = 0.01456404\n",
      "Iteration 12895, loss = 0.01456258\n",
      "Iteration 12896, loss = 0.01456112\n",
      "Iteration 12897, loss = 0.01455966\n",
      "Iteration 12898, loss = 0.01455820\n",
      "Iteration 12899, loss = 0.01455675\n",
      "Iteration 12900, loss = 0.01455529\n",
      "Iteration 12901, loss = 0.01455383\n",
      "Iteration 12902, loss = 0.01455237\n",
      "Iteration 12903, loss = 0.01455091\n",
      "Iteration 12904, loss = 0.01454946\n",
      "Iteration 12905, loss = 0.01454800\n",
      "Iteration 12906, loss = 0.01454654\n",
      "Iteration 12907, loss = 0.01454508\n",
      "Iteration 12908, loss = 0.01454363\n",
      "Iteration 12909, loss = 0.01454217\n",
      "Iteration 12910, loss = 0.01454072\n",
      "Iteration 12911, loss = 0.01453926\n",
      "Iteration 12912, loss = 0.01453781\n",
      "Iteration 12913, loss = 0.01453635\n",
      "Iteration 12914, loss = 0.01453490\n",
      "Iteration 12915, loss = 0.01453345\n",
      "Iteration 12916, loss = 0.01453199\n",
      "Iteration 12917, loss = 0.01453054\n",
      "Iteration 12918, loss = 0.01452909\n",
      "Iteration 12919, loss = 0.01452763\n",
      "Iteration 12920, loss = 0.01452618\n",
      "Iteration 12921, loss = 0.01452473\n",
      "Iteration 12922, loss = 0.01452328\n",
      "Iteration 12923, loss = 0.01452183\n",
      "Iteration 12924, loss = 0.01452038\n",
      "Iteration 12925, loss = 0.01451893\n",
      "Iteration 12926, loss = 0.01451748\n",
      "Iteration 12927, loss = 0.01451603\n",
      "Iteration 12928, loss = 0.01451458\n",
      "Iteration 12929, loss = 0.01451313\n",
      "Iteration 12930, loss = 0.01451168\n",
      "Iteration 12931, loss = 0.01451023\n",
      "Iteration 12932, loss = 0.01450878\n",
      "Iteration 12933, loss = 0.01450733\n",
      "Iteration 12934, loss = 0.01450589\n",
      "Iteration 12935, loss = 0.01450444\n",
      "Iteration 12936, loss = 0.01450299\n",
      "Iteration 12937, loss = 0.01450155\n",
      "Iteration 12938, loss = 0.01450010\n",
      "Iteration 12939, loss = 0.01449865\n",
      "Iteration 12940, loss = 0.01449721\n",
      "Iteration 12941, loss = 0.01449576\n",
      "Iteration 12942, loss = 0.01449432\n",
      "Iteration 12943, loss = 0.01449287\n",
      "Iteration 12944, loss = 0.01449143\n",
      "Iteration 12945, loss = 0.01448999\n",
      "Iteration 12946, loss = 0.01448854\n",
      "Iteration 12947, loss = 0.01448710\n",
      "Iteration 12948, loss = 0.01448566\n",
      "Iteration 12949, loss = 0.01448421\n",
      "Iteration 12950, loss = 0.01448277\n",
      "Iteration 12951, loss = 0.01448133\n",
      "Iteration 12952, loss = 0.01447989\n",
      "Iteration 12953, loss = 0.01447845\n",
      "Iteration 12954, loss = 0.01447701\n",
      "Iteration 12955, loss = 0.01447556\n",
      "Iteration 12956, loss = 0.01447412\n",
      "Iteration 12957, loss = 0.01447268\n",
      "Iteration 12958, loss = 0.01447124\n",
      "Iteration 12959, loss = 0.01446981\n",
      "Iteration 12960, loss = 0.01446837\n",
      "Iteration 12961, loss = 0.01446693\n",
      "Iteration 12962, loss = 0.01446549\n",
      "Iteration 12963, loss = 0.01446405\n",
      "Iteration 12964, loss = 0.01446261\n",
      "Iteration 12965, loss = 0.01446118\n",
      "Iteration 12966, loss = 0.01445974\n",
      "Iteration 12967, loss = 0.01445830\n",
      "Iteration 12968, loss = 0.01445687\n",
      "Iteration 12969, loss = 0.01445543\n",
      "Iteration 12970, loss = 0.01445399\n",
      "Iteration 12971, loss = 0.01445256\n",
      "Iteration 12972, loss = 0.01445112\n",
      "Iteration 12973, loss = 0.01444969\n",
      "Iteration 12974, loss = 0.01444825\n",
      "Iteration 12975, loss = 0.01444682\n",
      "Iteration 12976, loss = 0.01444539\n",
      "Iteration 12977, loss = 0.01444395\n",
      "Iteration 12978, loss = 0.01444252\n",
      "Iteration 12979, loss = 0.01444109\n",
      "Iteration 12980, loss = 0.01443965\n",
      "Iteration 12981, loss = 0.01443822\n",
      "Iteration 12982, loss = 0.01443679\n",
      "Iteration 12983, loss = 0.01443536\n",
      "Iteration 12984, loss = 0.01443393\n",
      "Iteration 12985, loss = 0.01443250\n",
      "Iteration 12986, loss = 0.01443107\n",
      "Iteration 12987, loss = 0.01442964\n",
      "Iteration 12988, loss = 0.01442821\n",
      "Iteration 12989, loss = 0.01442678\n",
      "Iteration 12990, loss = 0.01442535\n",
      "Iteration 12991, loss = 0.01442392\n",
      "Iteration 12992, loss = 0.01442249\n",
      "Iteration 12993, loss = 0.01442106\n",
      "Iteration 12994, loss = 0.01441963\n",
      "Iteration 12995, loss = 0.01441820\n",
      "Iteration 12996, loss = 0.01441678\n",
      "Iteration 12997, loss = 0.01441535\n",
      "Iteration 12998, loss = 0.01441392\n",
      "Iteration 12999, loss = 0.01441250\n",
      "Iteration 13000, loss = 0.01441107\n",
      "Iteration 13001, loss = 0.01440965\n",
      "Iteration 13002, loss = 0.01440822\n",
      "Iteration 13003, loss = 0.01440679\n",
      "Iteration 13004, loss = 0.01440537\n",
      "Iteration 13005, loss = 0.01440395\n",
      "Iteration 13006, loss = 0.01440252\n",
      "Iteration 13007, loss = 0.01440110\n",
      "Iteration 13008, loss = 0.01439967\n",
      "Iteration 13009, loss = 0.01439825\n",
      "Iteration 13010, loss = 0.01439683\n",
      "Iteration 13011, loss = 0.01439541\n",
      "Iteration 13012, loss = 0.01439398\n",
      "Iteration 13013, loss = 0.01439256\n",
      "Iteration 13014, loss = 0.01439114\n",
      "Iteration 13015, loss = 0.01438972\n",
      "Iteration 13016, loss = 0.01438830\n",
      "Iteration 13017, loss = 0.01438688\n",
      "Iteration 13018, loss = 0.01438546\n",
      "Iteration 13019, loss = 0.01438404\n",
      "Iteration 13020, loss = 0.01438262\n",
      "Iteration 13021, loss = 0.01438120\n",
      "Iteration 13022, loss = 0.01437978\n",
      "Iteration 13023, loss = 0.01437836\n",
      "Iteration 13024, loss = 0.01437694\n",
      "Iteration 13025, loss = 0.01437552\n",
      "Iteration 13026, loss = 0.01437411\n",
      "Iteration 13027, loss = 0.01437269\n",
      "Iteration 13028, loss = 0.01437127\n",
      "Iteration 13029, loss = 0.01436985\n",
      "Iteration 13030, loss = 0.01436844\n",
      "Iteration 13031, loss = 0.01436702\n",
      "Iteration 13032, loss = 0.01436561\n",
      "Iteration 13033, loss = 0.01436419\n",
      "Iteration 13034, loss = 0.01436278\n",
      "Iteration 13035, loss = 0.01436136\n",
      "Iteration 13036, loss = 0.01435995\n",
      "Iteration 13037, loss = 0.01435853\n",
      "Iteration 13038, loss = 0.01435712\n",
      "Iteration 13039, loss = 0.01435570\n",
      "Iteration 13040, loss = 0.01435429\n",
      "Iteration 13041, loss = 0.01435288\n",
      "Iteration 13042, loss = 0.01435147\n",
      "Iteration 13043, loss = 0.01435005\n",
      "Iteration 13044, loss = 0.01434864\n",
      "Iteration 13045, loss = 0.01434723\n",
      "Iteration 13046, loss = 0.01434582\n",
      "Iteration 13047, loss = 0.01434441\n",
      "Iteration 13048, loss = 0.01434300\n",
      "Iteration 13049, loss = 0.01434159\n",
      "Iteration 13050, loss = 0.01434018\n",
      "Iteration 13051, loss = 0.01433877\n",
      "Iteration 13052, loss = 0.01433736\n",
      "Iteration 13053, loss = 0.01433595\n",
      "Iteration 13054, loss = 0.01433454\n",
      "Iteration 13055, loss = 0.01433313\n",
      "Iteration 13056, loss = 0.01433172\n",
      "Iteration 13057, loss = 0.01433031\n",
      "Iteration 13058, loss = 0.01432891\n",
      "Iteration 13059, loss = 0.01432750\n",
      "Iteration 13060, loss = 0.01432609\n",
      "Iteration 13061, loss = 0.01432468\n",
      "Iteration 13062, loss = 0.01432328\n",
      "Iteration 13063, loss = 0.01432187\n",
      "Iteration 13064, loss = 0.01432047\n",
      "Iteration 13065, loss = 0.01431906\n",
      "Iteration 13066, loss = 0.01431766\n",
      "Iteration 13067, loss = 0.01431625\n",
      "Iteration 13068, loss = 0.01431485\n",
      "Iteration 13069, loss = 0.01431344\n",
      "Iteration 13070, loss = 0.01431204\n",
      "Iteration 13071, loss = 0.01431064\n",
      "Iteration 13072, loss = 0.01430923\n",
      "Iteration 13073, loss = 0.01430783\n",
      "Iteration 13074, loss = 0.01430643\n",
      "Iteration 13075, loss = 0.01430502\n",
      "Iteration 13076, loss = 0.01430362\n",
      "Iteration 13077, loss = 0.01430222\n",
      "Iteration 13078, loss = 0.01430082\n",
      "Iteration 13079, loss = 0.01429942\n",
      "Iteration 13080, loss = 0.01429802\n",
      "Iteration 13081, loss = 0.01429662\n",
      "Iteration 13082, loss = 0.01429522\n",
      "Iteration 13083, loss = 0.01429382\n",
      "Iteration 13084, loss = 0.01429242\n",
      "Iteration 13085, loss = 0.01429102\n",
      "Iteration 13086, loss = 0.01428962\n",
      "Iteration 13087, loss = 0.01428822\n",
      "Iteration 13088, loss = 0.01428682\n",
      "Iteration 13089, loss = 0.01428543\n",
      "Iteration 13090, loss = 0.01428403\n",
      "Iteration 13091, loss = 0.01428263\n",
      "Iteration 13092, loss = 0.01428123\n",
      "Iteration 13093, loss = 0.01427984\n",
      "Iteration 13094, loss = 0.01427844\n",
      "Iteration 13095, loss = 0.01427705\n",
      "Iteration 13096, loss = 0.01427565\n",
      "Iteration 13097, loss = 0.01427425\n",
      "Iteration 13098, loss = 0.01427286\n",
      "Iteration 13099, loss = 0.01427146\n",
      "Iteration 13100, loss = 0.01427007\n",
      "Iteration 13101, loss = 0.01426868\n",
      "Iteration 13102, loss = 0.01426728\n",
      "Iteration 13103, loss = 0.01426589\n",
      "Iteration 13104, loss = 0.01426450\n",
      "Iteration 13105, loss = 0.01426310\n",
      "Iteration 13106, loss = 0.01426171\n",
      "Iteration 13107, loss = 0.01426032\n",
      "Iteration 13108, loss = 0.01425893\n",
      "Iteration 13109, loss = 0.01425753\n",
      "Iteration 13110, loss = 0.01425614\n",
      "Iteration 13111, loss = 0.01425475\n",
      "Iteration 13112, loss = 0.01425336\n",
      "Iteration 13113, loss = 0.01425197\n",
      "Iteration 13114, loss = 0.01425058\n",
      "Iteration 13115, loss = 0.01424919\n",
      "Iteration 13116, loss = 0.01424780\n",
      "Iteration 13117, loss = 0.01424641\n",
      "Iteration 13118, loss = 0.01424502\n",
      "Iteration 13119, loss = 0.01424363\n",
      "Iteration 13120, loss = 0.01424225\n",
      "Iteration 13121, loss = 0.01424086\n",
      "Iteration 13122, loss = 0.01423947\n",
      "Iteration 13123, loss = 0.01423808\n",
      "Iteration 13124, loss = 0.01423670\n",
      "Iteration 13125, loss = 0.01423531\n",
      "Iteration 13126, loss = 0.01423392\n",
      "Iteration 13127, loss = 0.01423254\n",
      "Iteration 13128, loss = 0.01423115\n",
      "Iteration 13129, loss = 0.01422977\n",
      "Iteration 13130, loss = 0.01422838\n",
      "Iteration 13131, loss = 0.01422700\n",
      "Iteration 13132, loss = 0.01422561\n",
      "Iteration 13133, loss = 0.01422423\n",
      "Iteration 13134, loss = 0.01422284\n",
      "Iteration 13135, loss = 0.01422146\n",
      "Iteration 13136, loss = 0.01422008\n",
      "Iteration 13137, loss = 0.01421869\n",
      "Iteration 13138, loss = 0.01421731\n",
      "Iteration 13139, loss = 0.01421593\n",
      "Iteration 13140, loss = 0.01421455\n",
      "Iteration 13141, loss = 0.01421317\n",
      "Iteration 13142, loss = 0.01421178\n",
      "Iteration 13143, loss = 0.01421040\n",
      "Iteration 13144, loss = 0.01420902\n",
      "Iteration 13145, loss = 0.01420764\n",
      "Iteration 13146, loss = 0.01420626\n",
      "Iteration 13147, loss = 0.01420488\n",
      "Iteration 13148, loss = 0.01420350\n",
      "Iteration 13149, loss = 0.01420212\n",
      "Iteration 13150, loss = 0.01420074\n",
      "Iteration 13151, loss = 0.01419936\n",
      "Iteration 13152, loss = 0.01419799\n",
      "Iteration 13153, loss = 0.01419661\n",
      "Iteration 13154, loss = 0.01419523\n",
      "Iteration 13155, loss = 0.01419385\n",
      "Iteration 13156, loss = 0.01419248\n",
      "Iteration 13157, loss = 0.01419110\n",
      "Iteration 13158, loss = 0.01418972\n",
      "Iteration 13159, loss = 0.01418835\n",
      "Iteration 13160, loss = 0.01418697\n",
      "Iteration 13161, loss = 0.01418560\n",
      "Iteration 13162, loss = 0.01418422\n",
      "Iteration 13163, loss = 0.01418284\n",
      "Iteration 13164, loss = 0.01418147\n",
      "Iteration 13165, loss = 0.01418010\n",
      "Iteration 13166, loss = 0.01417872\n",
      "Iteration 13167, loss = 0.01417735\n",
      "Iteration 13168, loss = 0.01417597\n",
      "Iteration 13169, loss = 0.01417460\n",
      "Iteration 13170, loss = 0.01417323\n",
      "Iteration 13171, loss = 0.01417186\n",
      "Iteration 13172, loss = 0.01417048\n",
      "Iteration 13173, loss = 0.01416911\n",
      "Iteration 13174, loss = 0.01416774\n",
      "Iteration 13175, loss = 0.01416637\n",
      "Iteration 13176, loss = 0.01416500\n",
      "Iteration 13177, loss = 0.01416363\n",
      "Iteration 13178, loss = 0.01416226\n",
      "Iteration 13179, loss = 0.01416089\n",
      "Iteration 13180, loss = 0.01415952\n",
      "Iteration 13181, loss = 0.01415815\n",
      "Iteration 13182, loss = 0.01415678\n",
      "Iteration 13183, loss = 0.01415541\n",
      "Iteration 13184, loss = 0.01415404\n",
      "Iteration 13185, loss = 0.01415267\n",
      "Iteration 13186, loss = 0.01415130\n",
      "Iteration 13187, loss = 0.01414994\n",
      "Iteration 13188, loss = 0.01414857\n",
      "Iteration 13189, loss = 0.01414720\n",
      "Iteration 13190, loss = 0.01414584\n",
      "Iteration 13191, loss = 0.01414447\n",
      "Iteration 13192, loss = 0.01414310\n",
      "Iteration 13193, loss = 0.01414174\n",
      "Iteration 13194, loss = 0.01414037\n",
      "Iteration 13195, loss = 0.01413901\n",
      "Iteration 13196, loss = 0.01413764\n",
      "Iteration 13197, loss = 0.01413628\n",
      "Iteration 13198, loss = 0.01413491\n",
      "Iteration 13199, loss = 0.01413355\n",
      "Iteration 13200, loss = 0.01413218\n",
      "Iteration 13201, loss = 0.01413082\n",
      "Iteration 13202, loss = 0.01412946\n",
      "Iteration 13203, loss = 0.01412809\n",
      "Iteration 13204, loss = 0.01412673\n",
      "Iteration 13205, loss = 0.01412537\n",
      "Iteration 13206, loss = 0.01412401\n",
      "Iteration 13207, loss = 0.01412265\n",
      "Iteration 13208, loss = 0.01412128\n",
      "Iteration 13209, loss = 0.01411992\n",
      "Iteration 13210, loss = 0.01411856\n",
      "Iteration 13211, loss = 0.01411720\n",
      "Iteration 13212, loss = 0.01411584\n",
      "Iteration 13213, loss = 0.01411448\n",
      "Iteration 13214, loss = 0.01411312\n",
      "Iteration 13215, loss = 0.01411176\n",
      "Iteration 13216, loss = 0.01411040\n",
      "Iteration 13217, loss = 0.01410905\n",
      "Iteration 13218, loss = 0.01410769\n",
      "Iteration 13219, loss = 0.01410633\n",
      "Iteration 13220, loss = 0.01410497\n",
      "Iteration 13221, loss = 0.01410361\n",
      "Iteration 13222, loss = 0.01410226\n",
      "Iteration 13223, loss = 0.01410090\n",
      "Iteration 13224, loss = 0.01409954\n",
      "Iteration 13225, loss = 0.01409819\n",
      "Iteration 13226, loss = 0.01409683\n",
      "Iteration 13227, loss = 0.01409548\n",
      "Iteration 13228, loss = 0.01409412\n",
      "Iteration 13229, loss = 0.01409277\n",
      "Iteration 13230, loss = 0.01409141\n",
      "Iteration 13231, loss = 0.01409006\n",
      "Iteration 13232, loss = 0.01408870\n",
      "Iteration 13233, loss = 0.01408735\n",
      "Iteration 13234, loss = 0.01408600\n",
      "Iteration 13235, loss = 0.01408464\n",
      "Iteration 13236, loss = 0.01408329\n",
      "Iteration 13237, loss = 0.01408194\n",
      "Iteration 13238, loss = 0.01408058\n",
      "Iteration 13239, loss = 0.01407923\n",
      "Iteration 13240, loss = 0.01407788\n",
      "Iteration 13241, loss = 0.01407653\n",
      "Iteration 13242, loss = 0.01407518\n",
      "Iteration 13243, loss = 0.01407383\n",
      "Iteration 13244, loss = 0.01407248\n",
      "Iteration 13245, loss = 0.01407113\n",
      "Iteration 13246, loss = 0.01406978\n",
      "Iteration 13247, loss = 0.01406843\n",
      "Iteration 13248, loss = 0.01406708\n",
      "Iteration 13249, loss = 0.01406573\n",
      "Iteration 13250, loss = 0.01406438\n",
      "Iteration 13251, loss = 0.01406303\n",
      "Iteration 13252, loss = 0.01406168\n",
      "Iteration 13253, loss = 0.01406033\n",
      "Iteration 13254, loss = 0.01405899\n",
      "Iteration 13255, loss = 0.01405764\n",
      "Iteration 13256, loss = 0.01405629\n",
      "Iteration 13257, loss = 0.01405495\n",
      "Iteration 13258, loss = 0.01405360\n",
      "Iteration 13259, loss = 0.01405225\n",
      "Iteration 13260, loss = 0.01405091\n",
      "Iteration 13261, loss = 0.01404956\n",
      "Iteration 13262, loss = 0.01404822\n",
      "Iteration 13263, loss = 0.01404687\n",
      "Iteration 13264, loss = 0.01404553\n",
      "Iteration 13265, loss = 0.01404418\n",
      "Iteration 13266, loss = 0.01404284\n",
      "Iteration 13267, loss = 0.01404150\n",
      "Iteration 13268, loss = 0.01404015\n",
      "Iteration 13269, loss = 0.01403881\n",
      "Iteration 13270, loss = 0.01403747\n",
      "Iteration 13271, loss = 0.01403612\n",
      "Iteration 13272, loss = 0.01403478\n",
      "Iteration 13273, loss = 0.01403344\n",
      "Iteration 13274, loss = 0.01403210\n",
      "Iteration 13275, loss = 0.01403076\n",
      "Iteration 13276, loss = 0.01402942\n",
      "Iteration 13277, loss = 0.01402807\n",
      "Iteration 13278, loss = 0.01402673\n",
      "Iteration 13279, loss = 0.01402539\n",
      "Iteration 13280, loss = 0.01402405\n",
      "Iteration 13281, loss = 0.01402271\n",
      "Iteration 13282, loss = 0.01402138\n",
      "Iteration 13283, loss = 0.01402004\n",
      "Iteration 13284, loss = 0.01401870\n",
      "Iteration 13285, loss = 0.01401736\n",
      "Iteration 13286, loss = 0.01401602\n",
      "Iteration 13287, loss = 0.01401468\n",
      "Iteration 13288, loss = 0.01401335\n",
      "Iteration 13289, loss = 0.01401201\n",
      "Iteration 13290, loss = 0.01401067\n",
      "Iteration 13291, loss = 0.01400933\n",
      "Iteration 13292, loss = 0.01400800\n",
      "Iteration 13293, loss = 0.01400666\n",
      "Iteration 13294, loss = 0.01400533\n",
      "Iteration 13295, loss = 0.01400399\n",
      "Iteration 13296, loss = 0.01400266\n",
      "Iteration 13297, loss = 0.01400132\n",
      "Iteration 13298, loss = 0.01399999\n",
      "Iteration 13299, loss = 0.01399865\n",
      "Iteration 13300, loss = 0.01399732\n",
      "Iteration 13301, loss = 0.01399598\n",
      "Iteration 13302, loss = 0.01399465\n",
      "Iteration 13303, loss = 0.01399332\n",
      "Iteration 13304, loss = 0.01399198\n",
      "Iteration 13305, loss = 0.01399065\n",
      "Iteration 13306, loss = 0.01398932\n",
      "Iteration 13307, loss = 0.01398799\n",
      "Iteration 13308, loss = 0.01398666\n",
      "Iteration 13309, loss = 0.01398533\n",
      "Iteration 13310, loss = 0.01398399\n",
      "Iteration 13311, loss = 0.01398266\n",
      "Iteration 13312, loss = 0.01398133\n",
      "Iteration 13313, loss = 0.01398000\n",
      "Iteration 13314, loss = 0.01397867\n",
      "Iteration 13315, loss = 0.01397734\n",
      "Iteration 13316, loss = 0.01397601\n",
      "Iteration 13317, loss = 0.01397468\n",
      "Iteration 13318, loss = 0.01397336\n",
      "Iteration 13319, loss = 0.01397203\n",
      "Iteration 13320, loss = 0.01397070\n",
      "Iteration 13321, loss = 0.01396937\n",
      "Iteration 13322, loss = 0.01396804\n",
      "Iteration 13323, loss = 0.01396672\n",
      "Iteration 13324, loss = 0.01396539\n",
      "Iteration 13325, loss = 0.01396406\n",
      "Iteration 13326, loss = 0.01396274\n",
      "Iteration 13327, loss = 0.01396141\n",
      "Iteration 13328, loss = 0.01396008\n",
      "Iteration 13329, loss = 0.01395876\n",
      "Iteration 13330, loss = 0.01395743\n",
      "Iteration 13331, loss = 0.01395611\n",
      "Iteration 13332, loss = 0.01395478\n",
      "Iteration 13333, loss = 0.01395346\n",
      "Iteration 13334, loss = 0.01395214\n",
      "Iteration 13335, loss = 0.01395081\n",
      "Iteration 13336, loss = 0.01394949\n",
      "Iteration 13337, loss = 0.01394816\n",
      "Iteration 13338, loss = 0.01394684\n",
      "Iteration 13339, loss = 0.01394552\n",
      "Iteration 13340, loss = 0.01394420\n",
      "Iteration 13341, loss = 0.01394287\n",
      "Iteration 13342, loss = 0.01394155\n",
      "Iteration 13343, loss = 0.01394023\n",
      "Iteration 13344, loss = 0.01393891\n",
      "Iteration 13345, loss = 0.01393759\n",
      "Iteration 13346, loss = 0.01393627\n",
      "Iteration 13347, loss = 0.01393495\n",
      "Iteration 13348, loss = 0.01393363\n",
      "Iteration 13349, loss = 0.01393231\n",
      "Iteration 13350, loss = 0.01393099\n",
      "Iteration 13351, loss = 0.01392967\n",
      "Iteration 13352, loss = 0.01392835\n",
      "Iteration 13353, loss = 0.01392703\n",
      "Iteration 13354, loss = 0.01392571\n",
      "Iteration 13355, loss = 0.01392440\n",
      "Iteration 13356, loss = 0.01392308\n",
      "Iteration 13357, loss = 0.01392176\n",
      "Iteration 13358, loss = 0.01392044\n",
      "Iteration 13359, loss = 0.01391913\n",
      "Iteration 13360, loss = 0.01391781\n",
      "Iteration 13361, loss = 0.01391649\n",
      "Iteration 13362, loss = 0.01391518\n",
      "Iteration 13363, loss = 0.01391386\n",
      "Iteration 13364, loss = 0.01391255\n",
      "Iteration 13365, loss = 0.01391123\n",
      "Iteration 13366, loss = 0.01390992\n",
      "Iteration 13367, loss = 0.01390860\n",
      "Iteration 13368, loss = 0.01390729\n",
      "Iteration 13369, loss = 0.01390597\n",
      "Iteration 13370, loss = 0.01390466\n",
      "Iteration 13371, loss = 0.01390335\n",
      "Iteration 13372, loss = 0.01390203\n",
      "Iteration 13373, loss = 0.01390072\n",
      "Iteration 13374, loss = 0.01389941\n",
      "Iteration 13375, loss = 0.01389810\n",
      "Iteration 13376, loss = 0.01389678\n",
      "Iteration 13377, loss = 0.01389547\n",
      "Iteration 13378, loss = 0.01389416\n",
      "Iteration 13379, loss = 0.01389285\n",
      "Iteration 13380, loss = 0.01389154\n",
      "Iteration 13381, loss = 0.01389023\n",
      "Iteration 13382, loss = 0.01388892\n",
      "Iteration 13383, loss = 0.01388761\n",
      "Iteration 13384, loss = 0.01388630\n",
      "Iteration 13385, loss = 0.01388499\n",
      "Iteration 13386, loss = 0.01388368\n",
      "Iteration 13387, loss = 0.01388237\n",
      "Iteration 13388, loss = 0.01388106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13389, loss = 0.01387975\n",
      "Iteration 13390, loss = 0.01387845\n",
      "Iteration 13391, loss = 0.01387714\n",
      "Iteration 13392, loss = 0.01387583\n",
      "Iteration 13393, loss = 0.01387452\n",
      "Iteration 13394, loss = 0.01387322\n",
      "Iteration 13395, loss = 0.01387191\n",
      "Iteration 13396, loss = 0.01387060\n",
      "Iteration 13397, loss = 0.01386930\n",
      "Iteration 13398, loss = 0.01386799\n",
      "Iteration 13399, loss = 0.01386669\n",
      "Iteration 13400, loss = 0.01386538\n",
      "Iteration 13401, loss = 0.01386408\n",
      "Iteration 13402, loss = 0.01386277\n",
      "Iteration 13403, loss = 0.01386147\n",
      "Iteration 13404, loss = 0.01386016\n",
      "Iteration 13405, loss = 0.01385886\n",
      "Iteration 13406, loss = 0.01385756\n",
      "Iteration 13407, loss = 0.01385625\n",
      "Iteration 13408, loss = 0.01385495\n",
      "Iteration 13409, loss = 0.01385365\n",
      "Iteration 13410, loss = 0.01385234\n",
      "Iteration 13411, loss = 0.01385104\n",
      "Iteration 13412, loss = 0.01384974\n",
      "Iteration 13413, loss = 0.01384844\n",
      "Iteration 13414, loss = 0.01384714\n",
      "Iteration 13415, loss = 0.01384584\n",
      "Iteration 13416, loss = 0.01384454\n",
      "Iteration 13417, loss = 0.01384324\n",
      "Iteration 13418, loss = 0.01384194\n",
      "Iteration 13419, loss = 0.01384064\n",
      "Iteration 13420, loss = 0.01383934\n",
      "Iteration 13421, loss = 0.01383804\n",
      "Iteration 13422, loss = 0.01383674\n",
      "Iteration 13423, loss = 0.01383544\n",
      "Iteration 13424, loss = 0.01383414\n",
      "Iteration 13425, loss = 0.01383284\n",
      "Iteration 13426, loss = 0.01383155\n",
      "Iteration 13427, loss = 0.01383025\n",
      "Iteration 13428, loss = 0.01382895\n",
      "Iteration 13429, loss = 0.01382765\n",
      "Iteration 13430, loss = 0.01382636\n",
      "Iteration 13431, loss = 0.01382506\n",
      "Iteration 13432, loss = 0.01382376\n",
      "Iteration 13433, loss = 0.01382247\n",
      "Iteration 13434, loss = 0.01382117\n",
      "Iteration 13435, loss = 0.01381988\n",
      "Iteration 13436, loss = 0.01381858\n",
      "Iteration 13437, loss = 0.01381729\n",
      "Iteration 13438, loss = 0.01381599\n",
      "Iteration 13439, loss = 0.01381470\n",
      "Iteration 13440, loss = 0.01381341\n",
      "Iteration 13441, loss = 0.01381211\n",
      "Iteration 13442, loss = 0.01381082\n",
      "Iteration 13443, loss = 0.01380953\n",
      "Iteration 13444, loss = 0.01380823\n",
      "Iteration 13445, loss = 0.01380694\n",
      "Iteration 13446, loss = 0.01380565\n",
      "Iteration 13447, loss = 0.01380436\n",
      "Iteration 13448, loss = 0.01380307\n",
      "Iteration 13449, loss = 0.01380177\n",
      "Iteration 13450, loss = 0.01380048\n",
      "Iteration 13451, loss = 0.01379919\n",
      "Iteration 13452, loss = 0.01379790\n",
      "Iteration 13453, loss = 0.01379661\n",
      "Iteration 13454, loss = 0.01379532\n",
      "Iteration 13455, loss = 0.01379403\n",
      "Iteration 13456, loss = 0.01379274\n",
      "Iteration 13457, loss = 0.01379145\n",
      "Iteration 13458, loss = 0.01379016\n",
      "Iteration 13459, loss = 0.01378888\n",
      "Iteration 13460, loss = 0.01378759\n",
      "Iteration 13461, loss = 0.01378630\n",
      "Iteration 13462, loss = 0.01378501\n",
      "Iteration 13463, loss = 0.01378372\n",
      "Iteration 13464, loss = 0.01378244\n",
      "Iteration 13465, loss = 0.01378115\n",
      "Iteration 13466, loss = 0.01377986\n",
      "Iteration 13467, loss = 0.01377858\n",
      "Iteration 13468, loss = 0.01377729\n",
      "Iteration 13469, loss = 0.01377601\n",
      "Iteration 13470, loss = 0.01377472\n",
      "Iteration 13471, loss = 0.01377343\n",
      "Iteration 13472, loss = 0.01377215\n",
      "Iteration 13473, loss = 0.01377087\n",
      "Iteration 13474, loss = 0.01376958\n",
      "Iteration 13475, loss = 0.01376830\n",
      "Iteration 13476, loss = 0.01376701\n",
      "Iteration 13477, loss = 0.01376573\n",
      "Iteration 13478, loss = 0.01376445\n",
      "Iteration 13479, loss = 0.01376316\n",
      "Iteration 13480, loss = 0.01376188\n",
      "Iteration 13481, loss = 0.01376060\n",
      "Iteration 13482, loss = 0.01375932\n",
      "Iteration 13483, loss = 0.01375803\n",
      "Iteration 13484, loss = 0.01375675\n",
      "Iteration 13485, loss = 0.01375547\n",
      "Iteration 13486, loss = 0.01375419\n",
      "Iteration 13487, loss = 0.01375291\n",
      "Iteration 13488, loss = 0.01375163\n",
      "Iteration 13489, loss = 0.01375035\n",
      "Iteration 13490, loss = 0.01374907\n",
      "Iteration 13491, loss = 0.01374779\n",
      "Iteration 13492, loss = 0.01374651\n",
      "Iteration 13493, loss = 0.01374523\n",
      "Iteration 13494, loss = 0.01374395\n",
      "Iteration 13495, loss = 0.01374267\n",
      "Iteration 13496, loss = 0.01374139\n",
      "Iteration 13497, loss = 0.01374012\n",
      "Iteration 13498, loss = 0.01373884\n",
      "Iteration 13499, loss = 0.01373756\n",
      "Iteration 13500, loss = 0.01373628\n",
      "Iteration 13501, loss = 0.01373501\n",
      "Iteration 13502, loss = 0.01373373\n",
      "Iteration 13503, loss = 0.01373245\n",
      "Iteration 13504, loss = 0.01373118\n",
      "Iteration 13505, loss = 0.01372990\n",
      "Iteration 13506, loss = 0.01372863\n",
      "Iteration 13507, loss = 0.01372735\n",
      "Iteration 13508, loss = 0.01372608\n",
      "Iteration 13509, loss = 0.01372480\n",
      "Iteration 13510, loss = 0.01372353\n",
      "Iteration 13511, loss = 0.01372225\n",
      "Iteration 13512, loss = 0.01372098\n",
      "Iteration 13513, loss = 0.01371971\n",
      "Iteration 13514, loss = 0.01371843\n",
      "Iteration 13515, loss = 0.01371716\n",
      "Iteration 13516, loss = 0.01371589\n",
      "Iteration 13517, loss = 0.01371461\n",
      "Iteration 13518, loss = 0.01371334\n",
      "Iteration 13519, loss = 0.01371207\n",
      "Iteration 13520, loss = 0.01371080\n",
      "Iteration 13521, loss = 0.01370953\n",
      "Iteration 13522, loss = 0.01370826\n",
      "Iteration 13523, loss = 0.01370698\n",
      "Iteration 13524, loss = 0.01370571\n",
      "Iteration 13525, loss = 0.01370444\n",
      "Iteration 13526, loss = 0.01370317\n",
      "Iteration 13527, loss = 0.01370190\n",
      "Iteration 13528, loss = 0.01370063\n",
      "Iteration 13529, loss = 0.01369937\n",
      "Iteration 13530, loss = 0.01369810\n",
      "Iteration 13531, loss = 0.01369683\n",
      "Iteration 13532, loss = 0.01369556\n",
      "Iteration 13533, loss = 0.01369429\n",
      "Iteration 13534, loss = 0.01369302\n",
      "Iteration 13535, loss = 0.01369176\n",
      "Iteration 13536, loss = 0.01369049\n",
      "Iteration 13537, loss = 0.01368922\n",
      "Iteration 13538, loss = 0.01368795\n",
      "Iteration 13539, loss = 0.01368669\n",
      "Iteration 13540, loss = 0.01368542\n",
      "Iteration 13541, loss = 0.01368416\n",
      "Iteration 13542, loss = 0.01368289\n",
      "Iteration 13543, loss = 0.01368162\n",
      "Iteration 13544, loss = 0.01368036\n",
      "Iteration 13545, loss = 0.01367909\n",
      "Iteration 13546, loss = 0.01367783\n",
      "Iteration 13547, loss = 0.01367657\n",
      "Iteration 13548, loss = 0.01367530\n",
      "Iteration 13549, loss = 0.01367404\n",
      "Iteration 13550, loss = 0.01367277\n",
      "Iteration 13551, loss = 0.01367151\n",
      "Iteration 13552, loss = 0.01367025\n",
      "Iteration 13553, loss = 0.01366899\n",
      "Iteration 13554, loss = 0.01366772\n",
      "Iteration 13555, loss = 0.01366646\n",
      "Iteration 13556, loss = 0.01366520\n",
      "Iteration 13557, loss = 0.01366394\n",
      "Iteration 13558, loss = 0.01366268\n",
      "Iteration 13559, loss = 0.01366142\n",
      "Iteration 13560, loss = 0.01366015\n",
      "Iteration 13561, loss = 0.01365889\n",
      "Iteration 13562, loss = 0.01365763\n",
      "Iteration 13563, loss = 0.01365637\n",
      "Iteration 13564, loss = 0.01365511\n",
      "Iteration 13565, loss = 0.01365385\n",
      "Iteration 13566, loss = 0.01365260\n",
      "Iteration 13567, loss = 0.01365134\n",
      "Iteration 13568, loss = 0.01365008\n",
      "Iteration 13569, loss = 0.01364882\n",
      "Iteration 13570, loss = 0.01364756\n",
      "Iteration 13571, loss = 0.01364630\n",
      "Iteration 13572, loss = 0.01364505\n",
      "Iteration 13573, loss = 0.01364379\n",
      "Iteration 13574, loss = 0.01364253\n",
      "Iteration 13575, loss = 0.01364127\n",
      "Iteration 13576, loss = 0.01364002\n",
      "Iteration 13577, loss = 0.01363876\n",
      "Iteration 13578, loss = 0.01363751\n",
      "Iteration 13579, loss = 0.01363625\n",
      "Iteration 13580, loss = 0.01363500\n",
      "Iteration 13581, loss = 0.01363374\n",
      "Iteration 13582, loss = 0.01363249\n",
      "Iteration 13583, loss = 0.01363123\n",
      "Iteration 13584, loss = 0.01362998\n",
      "Iteration 13585, loss = 0.01362872\n",
      "Iteration 13586, loss = 0.01362747\n",
      "Iteration 13587, loss = 0.01362621\n",
      "Iteration 13588, loss = 0.01362496\n",
      "Iteration 13589, loss = 0.01362371\n",
      "Iteration 13590, loss = 0.01362246\n",
      "Iteration 13591, loss = 0.01362120\n",
      "Iteration 13592, loss = 0.01361995\n",
      "Iteration 13593, loss = 0.01361870\n",
      "Iteration 13594, loss = 0.01361745\n",
      "Iteration 13595, loss = 0.01361620\n",
      "Iteration 13596, loss = 0.01361495\n",
      "Iteration 13597, loss = 0.01361369\n",
      "Iteration 13598, loss = 0.01361244\n",
      "Iteration 13599, loss = 0.01361119\n",
      "Iteration 13600, loss = 0.01360994\n",
      "Iteration 13601, loss = 0.01360869\n",
      "Iteration 13602, loss = 0.01360744\n",
      "Iteration 13603, loss = 0.01360620\n",
      "Iteration 13604, loss = 0.01360495\n",
      "Iteration 13605, loss = 0.01360370\n",
      "Iteration 13606, loss = 0.01360245\n",
      "Iteration 13607, loss = 0.01360120\n",
      "Iteration 13608, loss = 0.01359995\n",
      "Iteration 13609, loss = 0.01359871\n",
      "Iteration 13610, loss = 0.01359746\n",
      "Iteration 13611, loss = 0.01359621\n",
      "Iteration 13612, loss = 0.01359496\n",
      "Iteration 13613, loss = 0.01359372\n",
      "Iteration 13614, loss = 0.01359247\n",
      "Iteration 13615, loss = 0.01359123\n",
      "Iteration 13616, loss = 0.01358998\n",
      "Iteration 13617, loss = 0.01358873\n",
      "Iteration 13618, loss = 0.01358749\n",
      "Iteration 13619, loss = 0.01358624\n",
      "Iteration 13620, loss = 0.01358500\n",
      "Iteration 13621, loss = 0.01358375\n",
      "Iteration 13622, loss = 0.01358251\n",
      "Iteration 13623, loss = 0.01358127\n",
      "Iteration 13624, loss = 0.01358002\n",
      "Iteration 13625, loss = 0.01357878\n",
      "Iteration 13626, loss = 0.01357754\n",
      "Iteration 13627, loss = 0.01357629\n",
      "Iteration 13628, loss = 0.01357505\n",
      "Iteration 13629, loss = 0.01357381\n",
      "Iteration 13630, loss = 0.01357257\n",
      "Iteration 13631, loss = 0.01357132\n",
      "Iteration 13632, loss = 0.01357008\n",
      "Iteration 13633, loss = 0.01356884\n",
      "Iteration 13634, loss = 0.01356760\n",
      "Iteration 13635, loss = 0.01356636\n",
      "Iteration 13636, loss = 0.01356512\n",
      "Iteration 13637, loss = 0.01356388\n",
      "Iteration 13638, loss = 0.01356264\n",
      "Iteration 13639, loss = 0.01356140\n",
      "Iteration 13640, loss = 0.01356016\n",
      "Iteration 13641, loss = 0.01355892\n",
      "Iteration 13642, loss = 0.01355768\n",
      "Iteration 13643, loss = 0.01355644\n",
      "Iteration 13644, loss = 0.01355521\n",
      "Iteration 13645, loss = 0.01355397\n",
      "Iteration 13646, loss = 0.01355273\n",
      "Iteration 13647, loss = 0.01355149\n",
      "Iteration 13648, loss = 0.01355026\n",
      "Iteration 13649, loss = 0.01354902\n",
      "Iteration 13650, loss = 0.01354778\n",
      "Iteration 13651, loss = 0.01354654\n",
      "Iteration 13652, loss = 0.01354531\n",
      "Iteration 13653, loss = 0.01354407\n",
      "Iteration 13654, loss = 0.01354284\n",
      "Iteration 13655, loss = 0.01354160\n",
      "Iteration 13656, loss = 0.01354037\n",
      "Iteration 13657, loss = 0.01353913\n",
      "Iteration 13658, loss = 0.01353790\n",
      "Iteration 13659, loss = 0.01353666\n",
      "Iteration 13660, loss = 0.01353543\n",
      "Iteration 13661, loss = 0.01353419\n",
      "Iteration 13662, loss = 0.01353296\n",
      "Iteration 13663, loss = 0.01353173\n",
      "Iteration 13664, loss = 0.01353049\n",
      "Iteration 13665, loss = 0.01352926\n",
      "Iteration 13666, loss = 0.01352803\n",
      "Iteration 13667, loss = 0.01352680\n",
      "Iteration 13668, loss = 0.01352556\n",
      "Iteration 13669, loss = 0.01352433\n",
      "Iteration 13670, loss = 0.01352310\n",
      "Iteration 13671, loss = 0.01352187\n",
      "Iteration 13672, loss = 0.01352064\n",
      "Iteration 13673, loss = 0.01351941\n",
      "Iteration 13674, loss = 0.01351818\n",
      "Iteration 13675, loss = 0.01351695\n",
      "Iteration 13676, loss = 0.01351572\n",
      "Iteration 13677, loss = 0.01351449\n",
      "Iteration 13678, loss = 0.01351326\n",
      "Iteration 13679, loss = 0.01351203\n",
      "Iteration 13680, loss = 0.01351080\n",
      "Iteration 13681, loss = 0.01350957\n",
      "Iteration 13682, loss = 0.01350834\n",
      "Iteration 13683, loss = 0.01350712\n",
      "Iteration 13684, loss = 0.01350589\n",
      "Iteration 13685, loss = 0.01350466\n",
      "Iteration 13686, loss = 0.01350343\n",
      "Iteration 13687, loss = 0.01350221\n",
      "Iteration 13688, loss = 0.01350098\n",
      "Iteration 13689, loss = 0.01349975\n",
      "Iteration 13690, loss = 0.01349853\n",
      "Iteration 13691, loss = 0.01349730\n",
      "Iteration 13692, loss = 0.01349607\n",
      "Iteration 13693, loss = 0.01349485\n",
      "Iteration 13694, loss = 0.01349362\n",
      "Iteration 13695, loss = 0.01349240\n",
      "Iteration 13696, loss = 0.01349117\n",
      "Iteration 13697, loss = 0.01348995\n",
      "Iteration 13698, loss = 0.01348873\n",
      "Iteration 13699, loss = 0.01348750\n",
      "Iteration 13700, loss = 0.01348628\n",
      "Iteration 13701, loss = 0.01348505\n",
      "Iteration 13702, loss = 0.01348383\n",
      "Iteration 13703, loss = 0.01348261\n",
      "Iteration 13704, loss = 0.01348139\n",
      "Iteration 13705, loss = 0.01348016\n",
      "Iteration 13706, loss = 0.01347894\n",
      "Iteration 13707, loss = 0.01347772\n",
      "Iteration 13708, loss = 0.01347650\n",
      "Iteration 13709, loss = 0.01347528\n",
      "Iteration 13710, loss = 0.01347406\n",
      "Iteration 13711, loss = 0.01347283\n",
      "Iteration 13712, loss = 0.01347161\n",
      "Iteration 13713, loss = 0.01347039\n",
      "Iteration 13714, loss = 0.01346917\n",
      "Iteration 13715, loss = 0.01346795\n",
      "Iteration 13716, loss = 0.01346673\n",
      "Iteration 13717, loss = 0.01346552\n",
      "Iteration 13718, loss = 0.01346430\n",
      "Iteration 13719, loss = 0.01346308\n",
      "Iteration 13720, loss = 0.01346186\n",
      "Iteration 13721, loss = 0.01346064\n",
      "Iteration 13722, loss = 0.01345942\n",
      "Iteration 13723, loss = 0.01345820\n",
      "Iteration 13724, loss = 0.01345699\n",
      "Iteration 13725, loss = 0.01345577\n",
      "Iteration 13726, loss = 0.01345455\n",
      "Iteration 13727, loss = 0.01345334\n",
      "Iteration 13728, loss = 0.01345212\n",
      "Iteration 13729, loss = 0.01345090\n",
      "Iteration 13730, loss = 0.01344969\n",
      "Iteration 13731, loss = 0.01344847\n",
      "Iteration 13732, loss = 0.01344726\n",
      "Iteration 13733, loss = 0.01344604\n",
      "Iteration 13734, loss = 0.01344483\n",
      "Iteration 13735, loss = 0.01344361\n",
      "Iteration 13736, loss = 0.01344240\n",
      "Iteration 13737, loss = 0.01344118\n",
      "Iteration 13738, loss = 0.01343997\n",
      "Iteration 13739, loss = 0.01343876\n",
      "Iteration 13740, loss = 0.01343754\n",
      "Iteration 13741, loss = 0.01343633\n",
      "Iteration 13742, loss = 0.01343512\n",
      "Iteration 13743, loss = 0.01343390\n",
      "Iteration 13744, loss = 0.01343269\n",
      "Iteration 13745, loss = 0.01343148\n",
      "Iteration 13746, loss = 0.01343027\n",
      "Iteration 13747, loss = 0.01342906\n",
      "Iteration 13748, loss = 0.01342785\n",
      "Iteration 13749, loss = 0.01342663\n",
      "Iteration 13750, loss = 0.01342542\n",
      "Iteration 13751, loss = 0.01342421\n",
      "Iteration 13752, loss = 0.01342300\n",
      "Iteration 13753, loss = 0.01342179\n",
      "Iteration 13754, loss = 0.01342058\n",
      "Iteration 13755, loss = 0.01341937\n",
      "Iteration 13756, loss = 0.01341816\n",
      "Iteration 13757, loss = 0.01341696\n",
      "Iteration 13758, loss = 0.01341575\n",
      "Iteration 13759, loss = 0.01341454\n",
      "Iteration 13760, loss = 0.01341333\n",
      "Iteration 13761, loss = 0.01341212\n",
      "Iteration 13762, loss = 0.01341091\n",
      "Iteration 13763, loss = 0.01340971\n",
      "Iteration 13764, loss = 0.01340850\n",
      "Iteration 13765, loss = 0.01340729\n",
      "Iteration 13766, loss = 0.01340609\n",
      "Iteration 13767, loss = 0.01340488\n",
      "Iteration 13768, loss = 0.01340367\n",
      "Iteration 13769, loss = 0.01340247\n",
      "Iteration 13770, loss = 0.01340126\n",
      "Iteration 13771, loss = 0.01340006\n",
      "Iteration 13772, loss = 0.01339885\n",
      "Iteration 13773, loss = 0.01339765\n",
      "Iteration 13774, loss = 0.01339644\n",
      "Iteration 13775, loss = 0.01339524\n",
      "Iteration 13776, loss = 0.01339403\n",
      "Iteration 13777, loss = 0.01339283\n",
      "Iteration 13778, loss = 0.01339163\n",
      "Iteration 13779, loss = 0.01339042\n",
      "Iteration 13780, loss = 0.01338922\n",
      "Iteration 13781, loss = 0.01338802\n",
      "Iteration 13782, loss = 0.01338681\n",
      "Iteration 13783, loss = 0.01338561\n",
      "Iteration 13784, loss = 0.01338441\n",
      "Iteration 13785, loss = 0.01338321\n",
      "Iteration 13786, loss = 0.01338201\n",
      "Iteration 13787, loss = 0.01338080\n",
      "Iteration 13788, loss = 0.01337960\n",
      "Iteration 13789, loss = 0.01337840\n",
      "Iteration 13790, loss = 0.01337720\n",
      "Iteration 13791, loss = 0.01337600\n",
      "Iteration 13792, loss = 0.01337480\n",
      "Iteration 13793, loss = 0.01337360\n",
      "Iteration 13794, loss = 0.01337240\n",
      "Iteration 13795, loss = 0.01337120\n",
      "Iteration 13796, loss = 0.01337000\n",
      "Iteration 13797, loss = 0.01336880\n",
      "Iteration 13798, loss = 0.01336761\n",
      "Iteration 13799, loss = 0.01336641\n",
      "Iteration 13800, loss = 0.01336521\n",
      "Iteration 13801, loss = 0.01336401\n",
      "Iteration 13802, loss = 0.01336281\n",
      "Iteration 13803, loss = 0.01336162\n",
      "Iteration 13804, loss = 0.01336042\n",
      "Iteration 13805, loss = 0.01335922\n",
      "Iteration 13806, loss = 0.01335803\n",
      "Iteration 13807, loss = 0.01335683\n",
      "Iteration 13808, loss = 0.01335563\n",
      "Iteration 13809, loss = 0.01335444\n",
      "Iteration 13810, loss = 0.01335324\n",
      "Iteration 13811, loss = 0.01335205\n",
      "Iteration 13812, loss = 0.01335085\n",
      "Iteration 13813, loss = 0.01334966\n",
      "Iteration 13814, loss = 0.01334846\n",
      "Iteration 13815, loss = 0.01334727\n",
      "Iteration 13816, loss = 0.01334607\n",
      "Iteration 13817, loss = 0.01334488\n",
      "Iteration 13818, loss = 0.01334369\n",
      "Iteration 13819, loss = 0.01334249\n",
      "Iteration 13820, loss = 0.01334130\n",
      "Iteration 13821, loss = 0.01334011\n",
      "Iteration 13822, loss = 0.01333891\n",
      "Iteration 13823, loss = 0.01333772\n",
      "Iteration 13824, loss = 0.01333653\n",
      "Iteration 13825, loss = 0.01333534\n",
      "Iteration 13826, loss = 0.01333415\n",
      "Iteration 13827, loss = 0.01333296\n",
      "Iteration 13828, loss = 0.01333176\n",
      "Iteration 13829, loss = 0.01333057\n",
      "Iteration 13830, loss = 0.01332938\n",
      "Iteration 13831, loss = 0.01332819\n",
      "Iteration 13832, loss = 0.01332700\n",
      "Iteration 13833, loss = 0.01332581\n",
      "Iteration 13834, loss = 0.01332462\n",
      "Iteration 13835, loss = 0.01332343\n",
      "Iteration 13836, loss = 0.01332224\n",
      "Iteration 13837, loss = 0.01332106\n",
      "Iteration 13838, loss = 0.01331987\n",
      "Iteration 13839, loss = 0.01331868\n",
      "Iteration 13840, loss = 0.01331749\n",
      "Iteration 13841, loss = 0.01331630\n",
      "Iteration 13842, loss = 0.01331512\n",
      "Iteration 13843, loss = 0.01331393\n",
      "Iteration 13844, loss = 0.01331274\n",
      "Iteration 13845, loss = 0.01331155\n",
      "Iteration 13846, loss = 0.01331037\n",
      "Iteration 13847, loss = 0.01330918\n",
      "Iteration 13848, loss = 0.01330800\n",
      "Iteration 13849, loss = 0.01330681\n",
      "Iteration 13850, loss = 0.01330562\n",
      "Iteration 13851, loss = 0.01330444\n",
      "Iteration 13852, loss = 0.01330325\n",
      "Iteration 13853, loss = 0.01330207\n",
      "Iteration 13854, loss = 0.01330088\n",
      "Iteration 13855, loss = 0.01329970\n",
      "Iteration 13856, loss = 0.01329852\n",
      "Iteration 13857, loss = 0.01329733\n",
      "Iteration 13858, loss = 0.01329615\n",
      "Iteration 13859, loss = 0.01329497\n",
      "Iteration 13860, loss = 0.01329378\n",
      "Iteration 13861, loss = 0.01329260\n",
      "Iteration 13862, loss = 0.01329142\n",
      "Iteration 13863, loss = 0.01329023\n",
      "Iteration 13864, loss = 0.01328905\n",
      "Iteration 13865, loss = 0.01328787\n",
      "Iteration 13866, loss = 0.01328669\n",
      "Iteration 13867, loss = 0.01328551\n",
      "Iteration 13868, loss = 0.01328433\n",
      "Iteration 13869, loss = 0.01328315\n",
      "Iteration 13870, loss = 0.01328196\n",
      "Iteration 13871, loss = 0.01328078\n",
      "Iteration 13872, loss = 0.01327960\n",
      "Iteration 13873, loss = 0.01327842\n",
      "Iteration 13874, loss = 0.01327724\n",
      "Iteration 13875, loss = 0.01327607\n",
      "Iteration 13876, loss = 0.01327489\n",
      "Iteration 13877, loss = 0.01327371\n",
      "Iteration 13878, loss = 0.01327253\n",
      "Iteration 13879, loss = 0.01327135\n",
      "Iteration 13880, loss = 0.01327017\n",
      "Iteration 13881, loss = 0.01326899\n",
      "Iteration 13882, loss = 0.01326782\n",
      "Iteration 13883, loss = 0.01326664\n",
      "Iteration 13884, loss = 0.01326546\n",
      "Iteration 13885, loss = 0.01326429\n",
      "Iteration 13886, loss = 0.01326311\n",
      "Iteration 13887, loss = 0.01326193\n",
      "Iteration 13888, loss = 0.01326076\n",
      "Iteration 13889, loss = 0.01325958\n",
      "Iteration 13890, loss = 0.01325840\n",
      "Iteration 13891, loss = 0.01325723\n",
      "Iteration 13892, loss = 0.01325605\n",
      "Iteration 13893, loss = 0.01325488\n",
      "Iteration 13894, loss = 0.01325370\n",
      "Iteration 13895, loss = 0.01325253\n",
      "Iteration 13896, loss = 0.01325136\n",
      "Iteration 13897, loss = 0.01325018\n",
      "Iteration 13898, loss = 0.01324901\n",
      "Iteration 13899, loss = 0.01324783\n",
      "Iteration 13900, loss = 0.01324666\n",
      "Iteration 13901, loss = 0.01324549\n",
      "Iteration 13902, loss = 0.01324432\n",
      "Iteration 13903, loss = 0.01324314\n",
      "Iteration 13904, loss = 0.01324197\n",
      "Iteration 13905, loss = 0.01324080\n",
      "Iteration 13906, loss = 0.01323963\n",
      "Iteration 13907, loss = 0.01323845\n",
      "Iteration 13908, loss = 0.01323728\n",
      "Iteration 13909, loss = 0.01323611\n",
      "Iteration 13910, loss = 0.01323494\n",
      "Iteration 13911, loss = 0.01323377\n",
      "Iteration 13912, loss = 0.01323260\n",
      "Iteration 13913, loss = 0.01323143\n",
      "Iteration 13914, loss = 0.01323026\n",
      "Iteration 13915, loss = 0.01322909\n",
      "Iteration 13916, loss = 0.01322792\n",
      "Iteration 13917, loss = 0.01322675\n",
      "Iteration 13918, loss = 0.01322558\n",
      "Iteration 13919, loss = 0.01322442\n",
      "Iteration 13920, loss = 0.01322325\n",
      "Iteration 13921, loss = 0.01322208\n",
      "Iteration 13922, loss = 0.01322091\n",
      "Iteration 13923, loss = 0.01321974\n",
      "Iteration 13924, loss = 0.01321858\n",
      "Iteration 13925, loss = 0.01321741\n",
      "Iteration 13926, loss = 0.01321624\n",
      "Iteration 13927, loss = 0.01321508\n",
      "Iteration 13928, loss = 0.01321391\n",
      "Iteration 13929, loss = 0.01321274\n",
      "Iteration 13930, loss = 0.01321158\n",
      "Iteration 13931, loss = 0.01321041\n",
      "Iteration 13932, loss = 0.01320925\n",
      "Iteration 13933, loss = 0.01320808\n",
      "Iteration 13934, loss = 0.01320692\n",
      "Iteration 13935, loss = 0.01320575\n",
      "Iteration 13936, loss = 0.01320459\n",
      "Iteration 13937, loss = 0.01320342\n",
      "Iteration 13938, loss = 0.01320226\n",
      "Iteration 13939, loss = 0.01320109\n",
      "Iteration 13940, loss = 0.01319993\n",
      "Iteration 13941, loss = 0.01319877\n",
      "Iteration 13942, loss = 0.01319760\n",
      "Iteration 13943, loss = 0.01319644\n",
      "Iteration 13944, loss = 0.01319528\n",
      "Iteration 13945, loss = 0.01319412\n",
      "Iteration 13946, loss = 0.01319296\n",
      "Iteration 13947, loss = 0.01319179\n",
      "Iteration 13948, loss = 0.01319063\n",
      "Iteration 13949, loss = 0.01318947\n",
      "Iteration 13950, loss = 0.01318831\n",
      "Iteration 13951, loss = 0.01318715\n",
      "Iteration 13952, loss = 0.01318599\n",
      "Iteration 13953, loss = 0.01318483\n",
      "Iteration 13954, loss = 0.01318367\n",
      "Iteration 13955, loss = 0.01318251\n",
      "Iteration 13956, loss = 0.01318135\n",
      "Iteration 13957, loss = 0.01318019\n",
      "Iteration 13958, loss = 0.01317903\n",
      "Iteration 13959, loss = 0.01317787\n",
      "Iteration 13960, loss = 0.01317671\n",
      "Iteration 13961, loss = 0.01317555\n",
      "Iteration 13962, loss = 0.01317440\n",
      "Iteration 13963, loss = 0.01317324\n",
      "Iteration 13964, loss = 0.01317208\n",
      "Iteration 13965, loss = 0.01317092\n",
      "Iteration 13966, loss = 0.01316976\n",
      "Iteration 13967, loss = 0.01316861\n",
      "Iteration 13968, loss = 0.01316745\n",
      "Iteration 13969, loss = 0.01316629\n",
      "Iteration 13970, loss = 0.01316514\n",
      "Iteration 13971, loss = 0.01316398\n",
      "Iteration 13972, loss = 0.01316283\n",
      "Iteration 13973, loss = 0.01316167\n",
      "Iteration 13974, loss = 0.01316052\n",
      "Iteration 13975, loss = 0.01315936\n",
      "Iteration 13976, loss = 0.01315821\n",
      "Iteration 13977, loss = 0.01315705\n",
      "Iteration 13978, loss = 0.01315590\n",
      "Iteration 13979, loss = 0.01315474\n",
      "Iteration 13980, loss = 0.01315359\n",
      "Iteration 13981, loss = 0.01315244\n",
      "Iteration 13982, loss = 0.01315128\n",
      "Iteration 13983, loss = 0.01315013\n",
      "Iteration 13984, loss = 0.01314898\n",
      "Iteration 13985, loss = 0.01314782\n",
      "Iteration 13986, loss = 0.01314667\n",
      "Iteration 13987, loss = 0.01314552\n",
      "Iteration 13988, loss = 0.01314437\n",
      "Iteration 13989, loss = 0.01314321\n",
      "Iteration 13990, loss = 0.01314206\n",
      "Iteration 13991, loss = 0.01314091\n",
      "Iteration 13992, loss = 0.01313976\n",
      "Iteration 13993, loss = 0.01313861\n",
      "Iteration 13994, loss = 0.01313746\n",
      "Iteration 13995, loss = 0.01313631\n",
      "Iteration 13996, loss = 0.01313516\n",
      "Iteration 13997, loss = 0.01313401\n",
      "Iteration 13998, loss = 0.01313286\n",
      "Iteration 13999, loss = 0.01313171\n",
      "Iteration 14000, loss = 0.01313056\n",
      "Iteration 14001, loss = 0.01312941\n",
      "Iteration 14002, loss = 0.01312826\n",
      "Iteration 14003, loss = 0.01312712\n",
      "Iteration 14004, loss = 0.01312597\n",
      "Iteration 14005, loss = 0.01312482\n",
      "Iteration 14006, loss = 0.01312367\n",
      "Iteration 14007, loss = 0.01312252\n",
      "Iteration 14008, loss = 0.01312138\n",
      "Iteration 14009, loss = 0.01312023\n",
      "Iteration 14010, loss = 0.01311908\n",
      "Iteration 14011, loss = 0.01311794\n",
      "Iteration 14012, loss = 0.01311679\n",
      "Iteration 14013, loss = 0.01311564\n",
      "Iteration 14014, loss = 0.01311450\n",
      "Iteration 14015, loss = 0.01311335\n",
      "Iteration 14016, loss = 0.01311221\n",
      "Iteration 14017, loss = 0.01311106\n",
      "Iteration 14018, loss = 0.01310992\n",
      "Iteration 14019, loss = 0.01310877\n",
      "Iteration 14020, loss = 0.01310763\n",
      "Iteration 14021, loss = 0.01310648\n",
      "Iteration 14022, loss = 0.01310534\n",
      "Iteration 14023, loss = 0.01310420\n",
      "Iteration 14024, loss = 0.01310305\n",
      "Iteration 14025, loss = 0.01310191\n",
      "Iteration 14026, loss = 0.01310077\n",
      "Iteration 14027, loss = 0.01309963\n",
      "Iteration 14028, loss = 0.01309848\n",
      "Iteration 14029, loss = 0.01309734\n",
      "Iteration 14030, loss = 0.01309620\n",
      "Iteration 14031, loss = 0.01309506\n",
      "Iteration 14032, loss = 0.01309392\n",
      "Iteration 14033, loss = 0.01309277\n",
      "Iteration 14034, loss = 0.01309163\n",
      "Iteration 14035, loss = 0.01309049\n",
      "Iteration 14036, loss = 0.01308935\n",
      "Iteration 14037, loss = 0.01308821\n",
      "Iteration 14038, loss = 0.01308707\n",
      "Iteration 14039, loss = 0.01308593\n",
      "Iteration 14040, loss = 0.01308479\n",
      "Iteration 14041, loss = 0.01308365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14042, loss = 0.01308251\n",
      "Iteration 14043, loss = 0.01308137\n",
      "Iteration 14044, loss = 0.01308024\n",
      "Iteration 14045, loss = 0.01307910\n",
      "Iteration 14046, loss = 0.01307796\n",
      "Iteration 14047, loss = 0.01307682\n",
      "Iteration 14048, loss = 0.01307568\n",
      "Iteration 14049, loss = 0.01307454\n",
      "Iteration 14050, loss = 0.01307341\n",
      "Iteration 14051, loss = 0.01307227\n",
      "Iteration 14052, loss = 0.01307113\n",
      "Iteration 14053, loss = 0.01307000\n",
      "Iteration 14054, loss = 0.01306886\n",
      "Iteration 14055, loss = 0.01306772\n",
      "Iteration 14056, loss = 0.01306659\n",
      "Iteration 14057, loss = 0.01306545\n",
      "Iteration 14058, loss = 0.01306432\n",
      "Iteration 14059, loss = 0.01306318\n",
      "Iteration 14060, loss = 0.01306205\n",
      "Iteration 14061, loss = 0.01306091\n",
      "Iteration 14062, loss = 0.01305978\n",
      "Iteration 14063, loss = 0.01305864\n",
      "Iteration 14064, loss = 0.01305751\n",
      "Iteration 14065, loss = 0.01305638\n",
      "Iteration 14066, loss = 0.01305524\n",
      "Iteration 14067, loss = 0.01305411\n",
      "Iteration 14068, loss = 0.01305298\n",
      "Iteration 14069, loss = 0.01305184\n",
      "Iteration 14070, loss = 0.01305071\n",
      "Iteration 14071, loss = 0.01304958\n",
      "Iteration 14072, loss = 0.01304845\n",
      "Iteration 14073, loss = 0.01304731\n",
      "Iteration 14074, loss = 0.01304618\n",
      "Iteration 14075, loss = 0.01304505\n",
      "Iteration 14076, loss = 0.01304392\n",
      "Iteration 14077, loss = 0.01304279\n",
      "Iteration 14078, loss = 0.01304166\n",
      "Iteration 14079, loss = 0.01304053\n",
      "Iteration 14080, loss = 0.01303940\n",
      "Iteration 14081, loss = 0.01303827\n",
      "Iteration 14082, loss = 0.01303714\n",
      "Iteration 14083, loss = 0.01303601\n",
      "Iteration 14084, loss = 0.01303488\n",
      "Iteration 14085, loss = 0.01303375\n",
      "Iteration 14086, loss = 0.01303262\n",
      "Iteration 14087, loss = 0.01303149\n",
      "Iteration 14088, loss = 0.01303036\n",
      "Iteration 14089, loss = 0.01302924\n",
      "Iteration 14090, loss = 0.01302811\n",
      "Iteration 14091, loss = 0.01302698\n",
      "Iteration 14092, loss = 0.01302585\n",
      "Iteration 14093, loss = 0.01302472\n",
      "Iteration 14094, loss = 0.01302360\n",
      "Iteration 14095, loss = 0.01302247\n",
      "Iteration 14096, loss = 0.01302134\n",
      "Iteration 14097, loss = 0.01302022\n",
      "Iteration 14098, loss = 0.01301909\n",
      "Iteration 14099, loss = 0.01301797\n",
      "Iteration 14100, loss = 0.01301684\n",
      "Iteration 14101, loss = 0.01301571\n",
      "Iteration 14102, loss = 0.01301459\n",
      "Iteration 14103, loss = 0.01301346\n",
      "Iteration 14104, loss = 0.01301234\n",
      "Iteration 14105, loss = 0.01301122\n",
      "Iteration 14106, loss = 0.01301009\n",
      "Iteration 14107, loss = 0.01300897\n",
      "Iteration 14108, loss = 0.01300784\n",
      "Iteration 14109, loss = 0.01300672\n",
      "Iteration 14110, loss = 0.01300560\n",
      "Iteration 14111, loss = 0.01300447\n",
      "Iteration 14112, loss = 0.01300335\n",
      "Iteration 14113, loss = 0.01300223\n",
      "Iteration 14114, loss = 0.01300111\n",
      "Iteration 14115, loss = 0.01299998\n",
      "Iteration 14116, loss = 0.01299886\n",
      "Iteration 14117, loss = 0.01299774\n",
      "Iteration 14118, loss = 0.01299662\n",
      "Iteration 14119, loss = 0.01299550\n",
      "Iteration 14120, loss = 0.01299438\n",
      "Iteration 14121, loss = 0.01299325\n",
      "Iteration 14122, loss = 0.01299213\n",
      "Iteration 14123, loss = 0.01299101\n",
      "Iteration 14124, loss = 0.01298989\n",
      "Iteration 14125, loss = 0.01298877\n",
      "Iteration 14126, loss = 0.01298765\n",
      "Iteration 14127, loss = 0.01298653\n",
      "Iteration 14128, loss = 0.01298542\n",
      "Iteration 14129, loss = 0.01298430\n",
      "Iteration 14130, loss = 0.01298318\n",
      "Iteration 14131, loss = 0.01298206\n",
      "Iteration 14132, loss = 0.01298094\n",
      "Iteration 14133, loss = 0.01297982\n",
      "Iteration 14134, loss = 0.01297870\n",
      "Iteration 14135, loss = 0.01297759\n",
      "Iteration 14136, loss = 0.01297647\n",
      "Iteration 14137, loss = 0.01297535\n",
      "Iteration 14138, loss = 0.01297424\n",
      "Iteration 14139, loss = 0.01297312\n",
      "Iteration 14140, loss = 0.01297200\n",
      "Iteration 14141, loss = 0.01297089\n",
      "Iteration 14142, loss = 0.01296977\n",
      "Iteration 14143, loss = 0.01296865\n",
      "Iteration 14144, loss = 0.01296754\n",
      "Iteration 14145, loss = 0.01296642\n",
      "Iteration 14146, loss = 0.01296531\n",
      "Iteration 14147, loss = 0.01296419\n",
      "Iteration 14148, loss = 0.01296308\n",
      "Iteration 14149, loss = 0.01296196\n",
      "Iteration 14150, loss = 0.01296085\n",
      "Iteration 14151, loss = 0.01295974\n",
      "Iteration 14152, loss = 0.01295862\n",
      "Iteration 14153, loss = 0.01295751\n",
      "Iteration 14154, loss = 0.01295640\n",
      "Iteration 14155, loss = 0.01295528\n",
      "Iteration 14156, loss = 0.01295417\n",
      "Iteration 14157, loss = 0.01295306\n",
      "Iteration 14158, loss = 0.01295195\n",
      "Iteration 14159, loss = 0.01295083\n",
      "Iteration 14160, loss = 0.01294972\n",
      "Iteration 14161, loss = 0.01294861\n",
      "Iteration 14162, loss = 0.01294750\n",
      "Iteration 14163, loss = 0.01294639\n",
      "Iteration 14164, loss = 0.01294528\n",
      "Iteration 14165, loss = 0.01294417\n",
      "Iteration 14166, loss = 0.01294305\n",
      "Iteration 14167, loss = 0.01294194\n",
      "Iteration 14168, loss = 0.01294083\n",
      "Iteration 14169, loss = 0.01293972\n",
      "Iteration 14170, loss = 0.01293861\n",
      "Iteration 14171, loss = 0.01293751\n",
      "Iteration 14172, loss = 0.01293640\n",
      "Iteration 14173, loss = 0.01293529\n",
      "Iteration 14174, loss = 0.01293418\n",
      "Iteration 14175, loss = 0.01293307\n",
      "Iteration 14176, loss = 0.01293196\n",
      "Iteration 14177, loss = 0.01293085\n",
      "Iteration 14178, loss = 0.01292975\n",
      "Iteration 14179, loss = 0.01292864\n",
      "Iteration 14180, loss = 0.01292753\n",
      "Iteration 14181, loss = 0.01292642\n",
      "Iteration 14182, loss = 0.01292532\n",
      "Iteration 14183, loss = 0.01292421\n",
      "Iteration 14184, loss = 0.01292310\n",
      "Iteration 14185, loss = 0.01292200\n",
      "Iteration 14186, loss = 0.01292089\n",
      "Iteration 14187, loss = 0.01291979\n",
      "Iteration 14188, loss = 0.01291868\n",
      "Iteration 14189, loss = 0.01291757\n",
      "Iteration 14190, loss = 0.01291647\n",
      "Iteration 14191, loss = 0.01291536\n",
      "Iteration 14192, loss = 0.01291426\n",
      "Iteration 14193, loss = 0.01291316\n",
      "Iteration 14194, loss = 0.01291205\n",
      "Iteration 14195, loss = 0.01291095\n",
      "Iteration 14196, loss = 0.01290984\n",
      "Iteration 14197, loss = 0.01290874\n",
      "Iteration 14198, loss = 0.01290764\n",
      "Iteration 14199, loss = 0.01290653\n",
      "Iteration 14200, loss = 0.01290543\n",
      "Iteration 14201, loss = 0.01290433\n",
      "Iteration 14202, loss = 0.01290323\n",
      "Iteration 14203, loss = 0.01290212\n",
      "Iteration 14204, loss = 0.01290102\n",
      "Iteration 14205, loss = 0.01289992\n",
      "Iteration 14206, loss = 0.01289882\n",
      "Iteration 14207, loss = 0.01289772\n",
      "Iteration 14208, loss = 0.01289662\n",
      "Iteration 14209, loss = 0.01289551\n",
      "Iteration 14210, loss = 0.01289441\n",
      "Iteration 14211, loss = 0.01289331\n",
      "Iteration 14212, loss = 0.01289221\n",
      "Iteration 14213, loss = 0.01289111\n",
      "Iteration 14214, loss = 0.01289001\n",
      "Iteration 14215, loss = 0.01288891\n",
      "Iteration 14216, loss = 0.01288782\n",
      "Iteration 14217, loss = 0.01288672\n",
      "Iteration 14218, loss = 0.01288562\n",
      "Iteration 14219, loss = 0.01288452\n",
      "Iteration 14220, loss = 0.01288342\n",
      "Iteration 14221, loss = 0.01288232\n",
      "Iteration 14222, loss = 0.01288122\n",
      "Iteration 14223, loss = 0.01288013\n",
      "Iteration 14224, loss = 0.01287903\n",
      "Iteration 14225, loss = 0.01287793\n",
      "Iteration 14226, loss = 0.01287683\n",
      "Iteration 14227, loss = 0.01287574\n",
      "Iteration 14228, loss = 0.01287464\n",
      "Iteration 14229, loss = 0.01287354\n",
      "Iteration 14230, loss = 0.01287245\n",
      "Iteration 14231, loss = 0.01287135\n",
      "Iteration 14232, loss = 0.01287026\n",
      "Iteration 14233, loss = 0.01286916\n",
      "Iteration 14234, loss = 0.01286807\n",
      "Iteration 14235, loss = 0.01286697\n",
      "Iteration 14236, loss = 0.01286588\n",
      "Iteration 14237, loss = 0.01286478\n",
      "Iteration 14238, loss = 0.01286369\n",
      "Iteration 14239, loss = 0.01286259\n",
      "Iteration 14240, loss = 0.01286150\n",
      "Iteration 14241, loss = 0.01286041\n",
      "Iteration 14242, loss = 0.01285931\n",
      "Iteration 14243, loss = 0.01285822\n",
      "Iteration 14244, loss = 0.01285713\n",
      "Iteration 14245, loss = 0.01285603\n",
      "Iteration 14246, loss = 0.01285494\n",
      "Iteration 14247, loss = 0.01285385\n",
      "Iteration 14248, loss = 0.01285276\n",
      "Iteration 14249, loss = 0.01285166\n",
      "Iteration 14250, loss = 0.01285057\n",
      "Iteration 14251, loss = 0.01284948\n",
      "Iteration 14252, loss = 0.01284839\n",
      "Iteration 14253, loss = 0.01284730\n",
      "Iteration 14254, loss = 0.01284621\n",
      "Iteration 14255, loss = 0.01284512\n",
      "Iteration 14256, loss = 0.01284403\n",
      "Iteration 14257, loss = 0.01284294\n",
      "Iteration 14258, loss = 0.01284185\n",
      "Iteration 14259, loss = 0.01284076\n",
      "Iteration 14260, loss = 0.01283967\n",
      "Iteration 14261, loss = 0.01283858\n",
      "Iteration 14262, loss = 0.01283749\n",
      "Iteration 14263, loss = 0.01283640\n",
      "Iteration 14264, loss = 0.01283531\n",
      "Iteration 14265, loss = 0.01283422\n",
      "Iteration 14266, loss = 0.01283313\n",
      "Iteration 14267, loss = 0.01283205\n",
      "Iteration 14268, loss = 0.01283096\n",
      "Iteration 14269, loss = 0.01282987\n",
      "Iteration 14270, loss = 0.01282878\n",
      "Iteration 14271, loss = 0.01282770\n",
      "Iteration 14272, loss = 0.01282661\n",
      "Iteration 14273, loss = 0.01282552\n",
      "Iteration 14274, loss = 0.01282444\n",
      "Iteration 14275, loss = 0.01282335\n",
      "Iteration 14276, loss = 0.01282226\n",
      "Iteration 14277, loss = 0.01282118\n",
      "Iteration 14278, loss = 0.01282009\n",
      "Iteration 14279, loss = 0.01281901\n",
      "Iteration 14280, loss = 0.01281792\n",
      "Iteration 14281, loss = 0.01281684\n",
      "Iteration 14282, loss = 0.01281575\n",
      "Iteration 14283, loss = 0.01281467\n",
      "Iteration 14284, loss = 0.01281358\n",
      "Iteration 14285, loss = 0.01281250\n",
      "Iteration 14286, loss = 0.01281142\n",
      "Iteration 14287, loss = 0.01281033\n",
      "Iteration 14288, loss = 0.01280925\n",
      "Iteration 14289, loss = 0.01280817\n",
      "Iteration 14290, loss = 0.01280708\n",
      "Iteration 14291, loss = 0.01280600\n",
      "Iteration 14292, loss = 0.01280492\n",
      "Iteration 14293, loss = 0.01280384\n",
      "Iteration 14294, loss = 0.01280275\n",
      "Iteration 14295, loss = 0.01280167\n",
      "Iteration 14296, loss = 0.01280059\n",
      "Iteration 14297, loss = 0.01279951\n",
      "Iteration 14298, loss = 0.01279843\n",
      "Iteration 14299, loss = 0.01279735\n",
      "Iteration 14300, loss = 0.01279627\n",
      "Iteration 14301, loss = 0.01279519\n",
      "Iteration 14302, loss = 0.01279411\n",
      "Iteration 14303, loss = 0.01279303\n",
      "Iteration 14304, loss = 0.01279195\n",
      "Iteration 14305, loss = 0.01279087\n",
      "Iteration 14306, loss = 0.01278979\n",
      "Iteration 14307, loss = 0.01278871\n",
      "Iteration 14308, loss = 0.01278763\n",
      "Iteration 14309, loss = 0.01278655\n",
      "Iteration 14310, loss = 0.01278547\n",
      "Iteration 14311, loss = 0.01278439\n",
      "Iteration 14312, loss = 0.01278332\n",
      "Iteration 14313, loss = 0.01278224\n",
      "Iteration 14314, loss = 0.01278116\n",
      "Iteration 14315, loss = 0.01278008\n",
      "Iteration 14316, loss = 0.01277901\n",
      "Iteration 14317, loss = 0.01277793\n",
      "Iteration 14318, loss = 0.01277685\n",
      "Iteration 14319, loss = 0.01277577\n",
      "Iteration 14320, loss = 0.01277470\n",
      "Iteration 14321, loss = 0.01277362\n",
      "Iteration 14322, loss = 0.01277255\n",
      "Iteration 14323, loss = 0.01277147\n",
      "Iteration 14324, loss = 0.01277039\n",
      "Iteration 14325, loss = 0.01276932\n",
      "Iteration 14326, loss = 0.01276824\n",
      "Iteration 14327, loss = 0.01276717\n",
      "Iteration 14328, loss = 0.01276610\n",
      "Iteration 14329, loss = 0.01276502\n",
      "Iteration 14330, loss = 0.01276395\n",
      "Iteration 14331, loss = 0.01276287\n",
      "Iteration 14332, loss = 0.01276180\n",
      "Iteration 14333, loss = 0.01276073\n",
      "Iteration 14334, loss = 0.01275965\n",
      "Iteration 14335, loss = 0.01275858\n",
      "Iteration 14336, loss = 0.01275751\n",
      "Iteration 14337, loss = 0.01275643\n",
      "Iteration 14338, loss = 0.01275536\n",
      "Iteration 14339, loss = 0.01275429\n",
      "Iteration 14340, loss = 0.01275322\n",
      "Iteration 14341, loss = 0.01275214\n",
      "Iteration 14342, loss = 0.01275107\n",
      "Iteration 14343, loss = 0.01275000\n",
      "Iteration 14344, loss = 0.01274893\n",
      "Iteration 14345, loss = 0.01274786\n",
      "Iteration 14346, loss = 0.01274679\n",
      "Iteration 14347, loss = 0.01274572\n",
      "Iteration 14348, loss = 0.01274465\n",
      "Iteration 14349, loss = 0.01274358\n",
      "Iteration 14350, loss = 0.01274251\n",
      "Iteration 14351, loss = 0.01274144\n",
      "Iteration 14352, loss = 0.01274037\n",
      "Iteration 14353, loss = 0.01273930\n",
      "Iteration 14354, loss = 0.01273823\n",
      "Iteration 14355, loss = 0.01273716\n",
      "Iteration 14356, loss = 0.01273609\n",
      "Iteration 14357, loss = 0.01273502\n",
      "Iteration 14358, loss = 0.01273396\n",
      "Iteration 14359, loss = 0.01273289\n",
      "Iteration 14360, loss = 0.01273182\n",
      "Iteration 14361, loss = 0.01273075\n",
      "Iteration 14362, loss = 0.01272969\n",
      "Iteration 14363, loss = 0.01272862\n",
      "Iteration 14364, loss = 0.01272755\n",
      "Iteration 14365, loss = 0.01272649\n",
      "Iteration 14366, loss = 0.01272542\n",
      "Iteration 14367, loss = 0.01272435\n",
      "Iteration 14368, loss = 0.01272329\n",
      "Iteration 14369, loss = 0.01272222\n",
      "Iteration 14370, loss = 0.01272116\n",
      "Iteration 14371, loss = 0.01272009\n",
      "Iteration 14372, loss = 0.01271902\n",
      "Iteration 14373, loss = 0.01271796\n",
      "Iteration 14374, loss = 0.01271690\n",
      "Iteration 14375, loss = 0.01271583\n",
      "Iteration 14376, loss = 0.01271477\n",
      "Iteration 14377, loss = 0.01271370\n",
      "Iteration 14378, loss = 0.01271264\n",
      "Iteration 14379, loss = 0.01271157\n",
      "Iteration 14380, loss = 0.01271051\n",
      "Iteration 14381, loss = 0.01270945\n",
      "Iteration 14382, loss = 0.01270838\n",
      "Iteration 14383, loss = 0.01270732\n",
      "Iteration 14384, loss = 0.01270626\n",
      "Iteration 14385, loss = 0.01270520\n",
      "Iteration 14386, loss = 0.01270414\n",
      "Iteration 14387, loss = 0.01270307\n",
      "Iteration 14388, loss = 0.01270201\n",
      "Iteration 14389, loss = 0.01270095\n",
      "Iteration 14390, loss = 0.01269989\n",
      "Iteration 14391, loss = 0.01269883\n",
      "Iteration 14392, loss = 0.01269777\n",
      "Iteration 14393, loss = 0.01269671\n",
      "Iteration 14394, loss = 0.01269565\n",
      "Iteration 14395, loss = 0.01269459\n",
      "Iteration 14396, loss = 0.01269353\n",
      "Iteration 14397, loss = 0.01269247\n",
      "Iteration 14398, loss = 0.01269141\n",
      "Iteration 14399, loss = 0.01269035\n",
      "Iteration 14400, loss = 0.01268929\n",
      "Iteration 14401, loss = 0.01268823\n",
      "Iteration 14402, loss = 0.01268717\n",
      "Iteration 14403, loss = 0.01268611\n",
      "Iteration 14404, loss = 0.01268505\n",
      "Iteration 14405, loss = 0.01268399\n",
      "Iteration 14406, loss = 0.01268294\n",
      "Iteration 14407, loss = 0.01268188\n",
      "Iteration 14408, loss = 0.01268082\n",
      "Iteration 14409, loss = 0.01267976\n",
      "Iteration 14410, loss = 0.01267871\n",
      "Iteration 14411, loss = 0.01267765\n",
      "Iteration 14412, loss = 0.01267659\n",
      "Iteration 14413, loss = 0.01267554\n",
      "Iteration 14414, loss = 0.01267448\n",
      "Iteration 14415, loss = 0.01267342\n",
      "Iteration 14416, loss = 0.01267237\n",
      "Iteration 14417, loss = 0.01267131\n",
      "Iteration 14418, loss = 0.01267026\n",
      "Iteration 14419, loss = 0.01266920\n",
      "Iteration 14420, loss = 0.01266815\n",
      "Iteration 14421, loss = 0.01266709\n",
      "Iteration 14422, loss = 0.01266604\n",
      "Iteration 14423, loss = 0.01266498\n",
      "Iteration 14424, loss = 0.01266393\n",
      "Iteration 14425, loss = 0.01266288\n",
      "Iteration 14426, loss = 0.01266182\n",
      "Iteration 14427, loss = 0.01266077\n",
      "Iteration 14428, loss = 0.01265972\n",
      "Iteration 14429, loss = 0.01265866\n",
      "Iteration 14430, loss = 0.01265761\n",
      "Iteration 14431, loss = 0.01265656\n",
      "Iteration 14432, loss = 0.01265550\n",
      "Iteration 14433, loss = 0.01265445\n",
      "Iteration 14434, loss = 0.01265340\n",
      "Iteration 14435, loss = 0.01265235\n",
      "Iteration 14436, loss = 0.01265130\n",
      "Iteration 14437, loss = 0.01265025\n",
      "Iteration 14438, loss = 0.01264919\n",
      "Iteration 14439, loss = 0.01264814\n",
      "Iteration 14440, loss = 0.01264709\n",
      "Iteration 14441, loss = 0.01264604\n",
      "Iteration 14442, loss = 0.01264499\n",
      "Iteration 14443, loss = 0.01264394\n",
      "Iteration 14444, loss = 0.01264289\n",
      "Iteration 14445, loss = 0.01264184\n",
      "Iteration 14446, loss = 0.01264079\n",
      "Iteration 14447, loss = 0.01263974\n",
      "Iteration 14448, loss = 0.01263869\n",
      "Iteration 14449, loss = 0.01263765\n",
      "Iteration 14450, loss = 0.01263660\n",
      "Iteration 14451, loss = 0.01263555\n",
      "Iteration 14452, loss = 0.01263450\n",
      "Iteration 14453, loss = 0.01263345\n",
      "Iteration 14454, loss = 0.01263240\n",
      "Iteration 14455, loss = 0.01263136\n",
      "Iteration 14456, loss = 0.01263031\n",
      "Iteration 14457, loss = 0.01262926\n",
      "Iteration 14458, loss = 0.01262822\n",
      "Iteration 14459, loss = 0.01262717\n",
      "Iteration 14460, loss = 0.01262612\n",
      "Iteration 14461, loss = 0.01262508\n",
      "Iteration 14462, loss = 0.01262403\n",
      "Iteration 14463, loss = 0.01262298\n",
      "Iteration 14464, loss = 0.01262194\n",
      "Iteration 14465, loss = 0.01262089\n",
      "Iteration 14466, loss = 0.01261985\n",
      "Iteration 14467, loss = 0.01261880\n",
      "Iteration 14468, loss = 0.01261776\n",
      "Iteration 14469, loss = 0.01261671\n",
      "Iteration 14470, loss = 0.01261567\n",
      "Iteration 14471, loss = 0.01261462\n",
      "Iteration 14472, loss = 0.01261358\n",
      "Iteration 14473, loss = 0.01261254\n",
      "Iteration 14474, loss = 0.01261149\n",
      "Iteration 14475, loss = 0.01261045\n",
      "Iteration 14476, loss = 0.01260941\n",
      "Iteration 14477, loss = 0.01260836\n",
      "Iteration 14478, loss = 0.01260732\n",
      "Iteration 14479, loss = 0.01260628\n",
      "Iteration 14480, loss = 0.01260523\n",
      "Iteration 14481, loss = 0.01260419\n",
      "Iteration 14482, loss = 0.01260315\n",
      "Iteration 14483, loss = 0.01260211\n",
      "Iteration 14484, loss = 0.01260107\n",
      "Iteration 14485, loss = 0.01260003\n",
      "Iteration 14486, loss = 0.01259898\n",
      "Iteration 14487, loss = 0.01259794\n",
      "Iteration 14488, loss = 0.01259690\n",
      "Iteration 14489, loss = 0.01259586\n",
      "Iteration 14490, loss = 0.01259482\n",
      "Iteration 14491, loss = 0.01259378\n",
      "Iteration 14492, loss = 0.01259274\n",
      "Iteration 14493, loss = 0.01259170\n",
      "Iteration 14494, loss = 0.01259066\n",
      "Iteration 14495, loss = 0.01258962\n",
      "Iteration 14496, loss = 0.01258858\n",
      "Iteration 14497, loss = 0.01258755\n",
      "Iteration 14498, loss = 0.01258651\n",
      "Iteration 14499, loss = 0.01258547\n",
      "Iteration 14500, loss = 0.01258443\n",
      "Iteration 14501, loss = 0.01258339\n",
      "Iteration 14502, loss = 0.01258235\n",
      "Iteration 14503, loss = 0.01258132\n",
      "Iteration 14504, loss = 0.01258028\n",
      "Iteration 14505, loss = 0.01257924\n",
      "Iteration 14506, loss = 0.01257821\n",
      "Iteration 14507, loss = 0.01257717\n",
      "Iteration 14508, loss = 0.01257613\n",
      "Iteration 14509, loss = 0.01257510\n",
      "Iteration 14510, loss = 0.01257406\n",
      "Iteration 14511, loss = 0.01257302\n",
      "Iteration 14512, loss = 0.01257199\n",
      "Iteration 14513, loss = 0.01257095\n",
      "Iteration 14514, loss = 0.01256992\n",
      "Iteration 14515, loss = 0.01256888\n",
      "Iteration 14516, loss = 0.01256785\n",
      "Iteration 14517, loss = 0.01256681\n",
      "Iteration 14518, loss = 0.01256578\n",
      "Iteration 14519, loss = 0.01256474\n",
      "Iteration 14520, loss = 0.01256371\n",
      "Iteration 14521, loss = 0.01256267\n",
      "Iteration 14522, loss = 0.01256164\n",
      "Iteration 14523, loss = 0.01256061\n",
      "Iteration 14524, loss = 0.01255957\n",
      "Iteration 14525, loss = 0.01255854\n",
      "Iteration 14526, loss = 0.01255751\n",
      "Iteration 14527, loss = 0.01255648\n",
      "Iteration 14528, loss = 0.01255544\n",
      "Iteration 14529, loss = 0.01255441\n",
      "Iteration 14530, loss = 0.01255338\n",
      "Iteration 14531, loss = 0.01255235\n",
      "Iteration 14532, loss = 0.01255131\n",
      "Iteration 14533, loss = 0.01255028\n",
      "Iteration 14534, loss = 0.01254925\n",
      "Iteration 14535, loss = 0.01254822\n",
      "Iteration 14536, loss = 0.01254719\n",
      "Iteration 14537, loss = 0.01254616\n",
      "Iteration 14538, loss = 0.01254513\n",
      "Iteration 14539, loss = 0.01254410\n",
      "Iteration 14540, loss = 0.01254307\n",
      "Iteration 14541, loss = 0.01254204\n",
      "Iteration 14542, loss = 0.01254101\n",
      "Iteration 14543, loss = 0.01253998\n",
      "Iteration 14544, loss = 0.01253895\n",
      "Iteration 14545, loss = 0.01253792\n",
      "Iteration 14546, loss = 0.01253689\n",
      "Iteration 14547, loss = 0.01253586\n",
      "Iteration 14548, loss = 0.01253484\n",
      "Iteration 14549, loss = 0.01253381\n",
      "Iteration 14550, loss = 0.01253278\n",
      "Iteration 14551, loss = 0.01253175\n",
      "Iteration 14552, loss = 0.01253072\n",
      "Iteration 14553, loss = 0.01252970\n",
      "Iteration 14554, loss = 0.01252867\n",
      "Iteration 14555, loss = 0.01252764\n",
      "Iteration 14556, loss = 0.01252662\n",
      "Iteration 14557, loss = 0.01252559\n",
      "Iteration 14558, loss = 0.01252456\n",
      "Iteration 14559, loss = 0.01252354\n",
      "Iteration 14560, loss = 0.01252251\n",
      "Iteration 14561, loss = 0.01252149\n",
      "Iteration 14562, loss = 0.01252046\n",
      "Iteration 14563, loss = 0.01251943\n",
      "Iteration 14564, loss = 0.01251841\n",
      "Iteration 14565, loss = 0.01251738\n",
      "Iteration 14566, loss = 0.01251636\n",
      "Iteration 14567, loss = 0.01251533\n",
      "Iteration 14568, loss = 0.01251431\n",
      "Iteration 14569, loss = 0.01251329\n",
      "Iteration 14570, loss = 0.01251226\n",
      "Iteration 14571, loss = 0.01251124\n",
      "Iteration 14572, loss = 0.01251022\n",
      "Iteration 14573, loss = 0.01250919\n",
      "Iteration 14574, loss = 0.01250817\n",
      "Iteration 14575, loss = 0.01250715\n",
      "Iteration 14576, loss = 0.01250612\n",
      "Iteration 14577, loss = 0.01250510\n",
      "Iteration 14578, loss = 0.01250408\n",
      "Iteration 14579, loss = 0.01250306\n",
      "Iteration 14580, loss = 0.01250203\n",
      "Iteration 14581, loss = 0.01250101\n",
      "Iteration 14582, loss = 0.01249999\n",
      "Iteration 14583, loss = 0.01249897\n",
      "Iteration 14584, loss = 0.01249795\n",
      "Iteration 14585, loss = 0.01249693\n",
      "Iteration 14586, loss = 0.01249591\n",
      "Iteration 14587, loss = 0.01249489\n",
      "Iteration 14588, loss = 0.01249387\n",
      "Iteration 14589, loss = 0.01249285\n",
      "Iteration 14590, loss = 0.01249183\n",
      "Iteration 14591, loss = 0.01249081\n",
      "Iteration 14592, loss = 0.01248979\n",
      "Iteration 14593, loss = 0.01248877\n",
      "Iteration 14594, loss = 0.01248775\n",
      "Iteration 14595, loss = 0.01248673\n",
      "Iteration 14596, loss = 0.01248571\n",
      "Iteration 14597, loss = 0.01248469\n",
      "Iteration 14598, loss = 0.01248367\n",
      "Iteration 14599, loss = 0.01248266\n",
      "Iteration 14600, loss = 0.01248164\n",
      "Iteration 14601, loss = 0.01248062\n",
      "Iteration 14602, loss = 0.01247960\n",
      "Iteration 14603, loss = 0.01247859\n",
      "Iteration 14604, loss = 0.01247757\n",
      "Iteration 14605, loss = 0.01247655\n",
      "Iteration 14606, loss = 0.01247554\n",
      "Iteration 14607, loss = 0.01247452\n",
      "Iteration 14608, loss = 0.01247350\n",
      "Iteration 14609, loss = 0.01247249\n",
      "Iteration 14610, loss = 0.01247147\n",
      "Iteration 14611, loss = 0.01247045\n",
      "Iteration 14612, loss = 0.01246944\n",
      "Iteration 14613, loss = 0.01246842\n",
      "Iteration 14614, loss = 0.01246741\n",
      "Iteration 14615, loss = 0.01246639\n",
      "Iteration 14616, loss = 0.01246538\n",
      "Iteration 14617, loss = 0.01246437\n",
      "Iteration 14618, loss = 0.01246335\n",
      "Iteration 14619, loss = 0.01246234\n",
      "Iteration 14620, loss = 0.01246132\n",
      "Iteration 14621, loss = 0.01246031\n",
      "Iteration 14622, loss = 0.01245930\n",
      "Iteration 14623, loss = 0.01245828\n",
      "Iteration 14624, loss = 0.01245727\n",
      "Iteration 14625, loss = 0.01245626\n",
      "Iteration 14626, loss = 0.01245524\n",
      "Iteration 14627, loss = 0.01245423\n",
      "Iteration 14628, loss = 0.01245322\n",
      "Iteration 14629, loss = 0.01245221\n",
      "Iteration 14630, loss = 0.01245119\n",
      "Iteration 14631, loss = 0.01245018\n",
      "Iteration 14632, loss = 0.01244917\n",
      "Iteration 14633, loss = 0.01244816\n",
      "Iteration 14634, loss = 0.01244715\n",
      "Iteration 14635, loss = 0.01244614\n",
      "Iteration 14636, loss = 0.01244513\n",
      "Iteration 14637, loss = 0.01244412\n",
      "Iteration 14638, loss = 0.01244311\n",
      "Iteration 14639, loss = 0.01244210\n",
      "Iteration 14640, loss = 0.01244109\n",
      "Iteration 14641, loss = 0.01244008\n",
      "Iteration 14642, loss = 0.01243907\n",
      "Iteration 14643, loss = 0.01243806\n",
      "Iteration 14644, loss = 0.01243705\n",
      "Iteration 14645, loss = 0.01243604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14646, loss = 0.01243503\n",
      "Iteration 14647, loss = 0.01243402\n",
      "Iteration 14648, loss = 0.01243302\n",
      "Iteration 14649, loss = 0.01243201\n",
      "Iteration 14650, loss = 0.01243100\n",
      "Iteration 14651, loss = 0.01242999\n",
      "Iteration 14652, loss = 0.01242898\n",
      "Iteration 14653, loss = 0.01242798\n",
      "Iteration 14654, loss = 0.01242697\n",
      "Iteration 14655, loss = 0.01242596\n",
      "Iteration 14656, loss = 0.01242496\n",
      "Iteration 14657, loss = 0.01242395\n",
      "Iteration 14658, loss = 0.01242294\n",
      "Iteration 14659, loss = 0.01242194\n",
      "Iteration 14660, loss = 0.01242093\n",
      "Iteration 14661, loss = 0.01241993\n",
      "Iteration 14662, loss = 0.01241892\n",
      "Iteration 14663, loss = 0.01241791\n",
      "Iteration 14664, loss = 0.01241691\n",
      "Iteration 14665, loss = 0.01241590\n",
      "Iteration 14666, loss = 0.01241490\n",
      "Iteration 14667, loss = 0.01241389\n",
      "Iteration 14668, loss = 0.01241289\n",
      "Iteration 14669, loss = 0.01241189\n",
      "Iteration 14670, loss = 0.01241088\n",
      "Iteration 14671, loss = 0.01240988\n",
      "Iteration 14672, loss = 0.01240888\n",
      "Iteration 14673, loss = 0.01240787\n",
      "Iteration 14674, loss = 0.01240687\n",
      "Iteration 14675, loss = 0.01240587\n",
      "Iteration 14676, loss = 0.01240486\n",
      "Iteration 14677, loss = 0.01240386\n",
      "Iteration 14678, loss = 0.01240286\n",
      "Iteration 14679, loss = 0.01240186\n",
      "Iteration 14680, loss = 0.01240085\n",
      "Iteration 14681, loss = 0.01239985\n",
      "Iteration 14682, loss = 0.01239885\n",
      "Iteration 14683, loss = 0.01239785\n",
      "Iteration 14684, loss = 0.01239685\n",
      "Iteration 14685, loss = 0.01239585\n",
      "Iteration 14686, loss = 0.01239485\n",
      "Iteration 14687, loss = 0.01239385\n",
      "Iteration 14688, loss = 0.01239284\n",
      "Iteration 14689, loss = 0.01239184\n",
      "Iteration 14690, loss = 0.01239084\n",
      "Iteration 14691, loss = 0.01238984\n",
      "Iteration 14692, loss = 0.01238885\n",
      "Iteration 14693, loss = 0.01238785\n",
      "Iteration 14694, loss = 0.01238685\n",
      "Iteration 14695, loss = 0.01238585\n",
      "Iteration 14696, loss = 0.01238485\n",
      "Iteration 14697, loss = 0.01238385\n",
      "Iteration 14698, loss = 0.01238285\n",
      "Iteration 14699, loss = 0.01238185\n",
      "Iteration 14700, loss = 0.01238086\n",
      "Iteration 14701, loss = 0.01237986\n",
      "Iteration 14702, loss = 0.01237886\n",
      "Iteration 14703, loss = 0.01237786\n",
      "Iteration 14704, loss = 0.01237686\n",
      "Iteration 14705, loss = 0.01237587\n",
      "Iteration 14706, loss = 0.01237487\n",
      "Iteration 14707, loss = 0.01237387\n",
      "Iteration 14708, loss = 0.01237288\n",
      "Iteration 14709, loss = 0.01237188\n",
      "Iteration 14710, loss = 0.01237089\n",
      "Iteration 14711, loss = 0.01236989\n",
      "Iteration 14712, loss = 0.01236889\n",
      "Iteration 14713, loss = 0.01236790\n",
      "Iteration 14714, loss = 0.01236690\n",
      "Iteration 14715, loss = 0.01236591\n",
      "Iteration 14716, loss = 0.01236491\n",
      "Iteration 14717, loss = 0.01236392\n",
      "Iteration 14718, loss = 0.01236292\n",
      "Iteration 14719, loss = 0.01236193\n",
      "Iteration 14720, loss = 0.01236093\n",
      "Iteration 14721, loss = 0.01235994\n",
      "Iteration 14722, loss = 0.01235895\n",
      "Iteration 14723, loss = 0.01235795\n",
      "Iteration 14724, loss = 0.01235696\n",
      "Iteration 14725, loss = 0.01235597\n",
      "Iteration 14726, loss = 0.01235497\n",
      "Iteration 14727, loss = 0.01235398\n",
      "Iteration 14728, loss = 0.01235299\n",
      "Iteration 14729, loss = 0.01235200\n",
      "Iteration 14730, loss = 0.01235100\n",
      "Iteration 14731, loss = 0.01235001\n",
      "Iteration 14732, loss = 0.01234902\n",
      "Iteration 14733, loss = 0.01234803\n",
      "Iteration 14734, loss = 0.01234704\n",
      "Iteration 14735, loss = 0.01234605\n",
      "Iteration 14736, loss = 0.01234505\n",
      "Iteration 14737, loss = 0.01234406\n",
      "Iteration 14738, loss = 0.01234307\n",
      "Iteration 14739, loss = 0.01234208\n",
      "Iteration 14740, loss = 0.01234109\n",
      "Iteration 14741, loss = 0.01234010\n",
      "Iteration 14742, loss = 0.01233911\n",
      "Iteration 14743, loss = 0.01233812\n",
      "Iteration 14744, loss = 0.01233713\n",
      "Iteration 14745, loss = 0.01233614\n",
      "Iteration 14746, loss = 0.01233515\n",
      "Iteration 14747, loss = 0.01233417\n",
      "Iteration 14748, loss = 0.01233318\n",
      "Iteration 14749, loss = 0.01233219\n",
      "Iteration 14750, loss = 0.01233120\n",
      "Iteration 14751, loss = 0.01233021\n",
      "Iteration 14752, loss = 0.01232922\n",
      "Iteration 14753, loss = 0.01232824\n",
      "Iteration 14754, loss = 0.01232725\n",
      "Iteration 14755, loss = 0.01232626\n",
      "Iteration 14756, loss = 0.01232527\n",
      "Iteration 14757, loss = 0.01232429\n",
      "Iteration 14758, loss = 0.01232330\n",
      "Iteration 14759, loss = 0.01232231\n",
      "Iteration 14760, loss = 0.01232133\n",
      "Iteration 14761, loss = 0.01232034\n",
      "Iteration 14762, loss = 0.01231935\n",
      "Iteration 14763, loss = 0.01231837\n",
      "Iteration 14764, loss = 0.01231738\n",
      "Iteration 14765, loss = 0.01231640\n",
      "Iteration 14766, loss = 0.01231541\n",
      "Iteration 14767, loss = 0.01231443\n",
      "Iteration 14768, loss = 0.01231344\n",
      "Iteration 14769, loss = 0.01231246\n",
      "Iteration 14770, loss = 0.01231147\n",
      "Iteration 14771, loss = 0.01231049\n",
      "Iteration 14772, loss = 0.01230950\n",
      "Iteration 14773, loss = 0.01230852\n",
      "Iteration 14774, loss = 0.01230754\n",
      "Iteration 14775, loss = 0.01230655\n",
      "Iteration 14776, loss = 0.01230557\n",
      "Iteration 14777, loss = 0.01230459\n",
      "Iteration 14778, loss = 0.01230360\n",
      "Iteration 14779, loss = 0.01230262\n",
      "Iteration 14780, loss = 0.01230164\n",
      "Iteration 14781, loss = 0.01230066\n",
      "Iteration 14782, loss = 0.01229967\n",
      "Iteration 14783, loss = 0.01229869\n",
      "Iteration 14784, loss = 0.01229771\n",
      "Iteration 14785, loss = 0.01229673\n",
      "Iteration 14786, loss = 0.01229575\n",
      "Iteration 14787, loss = 0.01229477\n",
      "Iteration 14788, loss = 0.01229378\n",
      "Iteration 14789, loss = 0.01229280\n",
      "Iteration 14790, loss = 0.01229182\n",
      "Iteration 14791, loss = 0.01229084\n",
      "Iteration 14792, loss = 0.01228986\n",
      "Iteration 14793, loss = 0.01228888\n",
      "Iteration 14794, loss = 0.01228790\n",
      "Iteration 14795, loss = 0.01228692\n",
      "Iteration 14796, loss = 0.01228594\n",
      "Iteration 14797, loss = 0.01228496\n",
      "Iteration 14798, loss = 0.01228398\n",
      "Iteration 14799, loss = 0.01228301\n",
      "Iteration 14800, loss = 0.01228203\n",
      "Iteration 14801, loss = 0.01228105\n",
      "Iteration 14802, loss = 0.01228007\n",
      "Iteration 14803, loss = 0.01227909\n",
      "Iteration 14804, loss = 0.01227811\n",
      "Iteration 14805, loss = 0.01227714\n",
      "Iteration 14806, loss = 0.01227616\n",
      "Iteration 14807, loss = 0.01227518\n",
      "Iteration 14808, loss = 0.01227420\n",
      "Iteration 14809, loss = 0.01227323\n",
      "Iteration 14810, loss = 0.01227225\n",
      "Iteration 14811, loss = 0.01227127\n",
      "Iteration 14812, loss = 0.01227030\n",
      "Iteration 14813, loss = 0.01226932\n",
      "Iteration 14814, loss = 0.01226834\n",
      "Iteration 14815, loss = 0.01226737\n",
      "Iteration 14816, loss = 0.01226639\n",
      "Iteration 14817, loss = 0.01226542\n",
      "Iteration 14818, loss = 0.01226444\n",
      "Iteration 14819, loss = 0.01226347\n",
      "Iteration 14820, loss = 0.01226249\n",
      "Iteration 14821, loss = 0.01226152\n",
      "Iteration 14822, loss = 0.01226054\n",
      "Iteration 14823, loss = 0.01225957\n",
      "Iteration 14824, loss = 0.01225859\n",
      "Iteration 14825, loss = 0.01225762\n",
      "Iteration 14826, loss = 0.01225664\n",
      "Iteration 14827, loss = 0.01225567\n",
      "Iteration 14828, loss = 0.01225470\n",
      "Iteration 14829, loss = 0.01225372\n",
      "Iteration 14830, loss = 0.01225275\n",
      "Iteration 14831, loss = 0.01225178\n",
      "Iteration 14832, loss = 0.01225081\n",
      "Iteration 14833, loss = 0.01224983\n",
      "Iteration 14834, loss = 0.01224886\n",
      "Iteration 14835, loss = 0.01224789\n",
      "Iteration 14836, loss = 0.01224692\n",
      "Iteration 14837, loss = 0.01224594\n",
      "Iteration 14838, loss = 0.01224497\n",
      "Iteration 14839, loss = 0.01224400\n",
      "Iteration 14840, loss = 0.01224303\n",
      "Iteration 14841, loss = 0.01224206\n",
      "Iteration 14842, loss = 0.01224109\n",
      "Iteration 14843, loss = 0.01224012\n",
      "Iteration 14844, loss = 0.01223915\n",
      "Iteration 14845, loss = 0.01223818\n",
      "Iteration 14846, loss = 0.01223721\n",
      "Iteration 14847, loss = 0.01223624\n",
      "Iteration 14848, loss = 0.01223527\n",
      "Iteration 14849, loss = 0.01223430\n",
      "Iteration 14850, loss = 0.01223333\n",
      "Iteration 14851, loss = 0.01223236\n",
      "Iteration 14852, loss = 0.01223139\n",
      "Iteration 14853, loss = 0.01223042\n",
      "Iteration 14854, loss = 0.01222945\n",
      "Iteration 14855, loss = 0.01222848\n",
      "Iteration 14856, loss = 0.01222752\n",
      "Iteration 14857, loss = 0.01222655\n",
      "Iteration 14858, loss = 0.01222558\n",
      "Iteration 14859, loss = 0.01222461\n",
      "Iteration 14860, loss = 0.01222365\n",
      "Iteration 14861, loss = 0.01222268\n",
      "Iteration 14862, loss = 0.01222171\n",
      "Iteration 14863, loss = 0.01222074\n",
      "Iteration 14864, loss = 0.01221978\n",
      "Iteration 14865, loss = 0.01221881\n",
      "Iteration 14866, loss = 0.01221784\n",
      "Iteration 14867, loss = 0.01221688\n",
      "Iteration 14868, loss = 0.01221591\n",
      "Iteration 14869, loss = 0.01221495\n",
      "Iteration 14870, loss = 0.01221398\n",
      "Iteration 14871, loss = 0.01221302\n",
      "Iteration 14872, loss = 0.01221205\n",
      "Iteration 14873, loss = 0.01221109\n",
      "Iteration 14874, loss = 0.01221012\n",
      "Iteration 14875, loss = 0.01220916\n",
      "Iteration 14876, loss = 0.01220819\n",
      "Iteration 14877, loss = 0.01220723\n",
      "Iteration 14878, loss = 0.01220626\n",
      "Iteration 14879, loss = 0.01220530\n",
      "Iteration 14880, loss = 0.01220434\n",
      "Iteration 14881, loss = 0.01220337\n",
      "Iteration 14882, loss = 0.01220241\n",
      "Iteration 14883, loss = 0.01220145\n",
      "Iteration 14884, loss = 0.01220048\n",
      "Iteration 14885, loss = 0.01219952\n",
      "Iteration 14886, loss = 0.01219856\n",
      "Iteration 14887, loss = 0.01219759\n",
      "Iteration 14888, loss = 0.01219663\n",
      "Iteration 14889, loss = 0.01219567\n",
      "Iteration 14890, loss = 0.01219471\n",
      "Iteration 14891, loss = 0.01219375\n",
      "Iteration 14892, loss = 0.01219278\n",
      "Iteration 14893, loss = 0.01219182\n",
      "Iteration 14894, loss = 0.01219086\n",
      "Iteration 14895, loss = 0.01218990\n",
      "Iteration 14896, loss = 0.01218894\n",
      "Iteration 14897, loss = 0.01218798\n",
      "Iteration 14898, loss = 0.01218702\n",
      "Iteration 14899, loss = 0.01218606\n",
      "Iteration 14900, loss = 0.01218510\n",
      "Iteration 14901, loss = 0.01218414\n",
      "Iteration 14902, loss = 0.01218318\n",
      "Iteration 14903, loss = 0.01218222\n",
      "Iteration 14904, loss = 0.01218126\n",
      "Iteration 14905, loss = 0.01218030\n",
      "Iteration 14906, loss = 0.01217934\n",
      "Iteration 14907, loss = 0.01217838\n",
      "Iteration 14908, loss = 0.01217743\n",
      "Iteration 14909, loss = 0.01217647\n",
      "Iteration 14910, loss = 0.01217551\n",
      "Iteration 14911, loss = 0.01217455\n",
      "Iteration 14912, loss = 0.01217359\n",
      "Iteration 14913, loss = 0.01217264\n",
      "Iteration 14914, loss = 0.01217168\n",
      "Iteration 14915, loss = 0.01217072\n",
      "Iteration 14916, loss = 0.01216976\n",
      "Iteration 14917, loss = 0.01216881\n",
      "Iteration 14918, loss = 0.01216785\n",
      "Iteration 14919, loss = 0.01216689\n",
      "Iteration 14920, loss = 0.01216594\n",
      "Iteration 14921, loss = 0.01216498\n",
      "Iteration 14922, loss = 0.01216403\n",
      "Iteration 14923, loss = 0.01216307\n",
      "Iteration 14924, loss = 0.01216211\n",
      "Iteration 14925, loss = 0.01216116\n",
      "Iteration 14926, loss = 0.01216020\n",
      "Iteration 14927, loss = 0.01215925\n",
      "Iteration 14928, loss = 0.01215829\n",
      "Iteration 14929, loss = 0.01215734\n",
      "Iteration 14930, loss = 0.01215638\n",
      "Iteration 14931, loss = 0.01215543\n",
      "Iteration 14932, loss = 0.01215448\n",
      "Iteration 14933, loss = 0.01215352\n",
      "Iteration 14934, loss = 0.01215257\n",
      "Iteration 14935, loss = 0.01215162\n",
      "Iteration 14936, loss = 0.01215066\n",
      "Iteration 14937, loss = 0.01214971\n",
      "Iteration 14938, loss = 0.01214876\n",
      "Iteration 14939, loss = 0.01214780\n",
      "Iteration 14940, loss = 0.01214685\n",
      "Iteration 14941, loss = 0.01214590\n",
      "Iteration 14942, loss = 0.01214495\n",
      "Iteration 14943, loss = 0.01214399\n",
      "Iteration 14944, loss = 0.01214304\n",
      "Iteration 14945, loss = 0.01214209\n",
      "Iteration 14946, loss = 0.01214114\n",
      "Iteration 14947, loss = 0.01214019\n",
      "Iteration 14948, loss = 0.01213924\n",
      "Iteration 14949, loss = 0.01213828\n",
      "Iteration 14950, loss = 0.01213733\n",
      "Iteration 14951, loss = 0.01213638\n",
      "Iteration 14952, loss = 0.01213543\n",
      "Iteration 14953, loss = 0.01213448\n",
      "Iteration 14954, loss = 0.01213353\n",
      "Iteration 14955, loss = 0.01213258\n",
      "Iteration 14956, loss = 0.01213163\n",
      "Iteration 14957, loss = 0.01213068\n",
      "Iteration 14958, loss = 0.01212973\n",
      "Iteration 14959, loss = 0.01212878\n",
      "Iteration 14960, loss = 0.01212784\n",
      "Iteration 14961, loss = 0.01212689\n",
      "Iteration 14962, loss = 0.01212594\n",
      "Iteration 14963, loss = 0.01212499\n",
      "Iteration 14964, loss = 0.01212404\n",
      "Iteration 14965, loss = 0.01212309\n",
      "Iteration 14966, loss = 0.01212215\n",
      "Iteration 14967, loss = 0.01212120\n",
      "Iteration 14968, loss = 0.01212025\n",
      "Iteration 14969, loss = 0.01211930\n",
      "Iteration 14970, loss = 0.01211836\n",
      "Iteration 14971, loss = 0.01211741\n",
      "Iteration 14972, loss = 0.01211646\n",
      "Iteration 14973, loss = 0.01211551\n",
      "Iteration 14974, loss = 0.01211457\n",
      "Iteration 14975, loss = 0.01211362\n",
      "Iteration 14976, loss = 0.01211268\n",
      "Iteration 14977, loss = 0.01211173\n",
      "Iteration 14978, loss = 0.01211078\n",
      "Iteration 14979, loss = 0.01210984\n",
      "Iteration 14980, loss = 0.01210889\n",
      "Iteration 14981, loss = 0.01210795\n",
      "Iteration 14982, loss = 0.01210700\n",
      "Iteration 14983, loss = 0.01210606\n",
      "Iteration 14984, loss = 0.01210511\n",
      "Iteration 14985, loss = 0.01210417\n",
      "Iteration 14986, loss = 0.01210322\n",
      "Iteration 14987, loss = 0.01210228\n",
      "Iteration 14988, loss = 0.01210134\n",
      "Iteration 14989, loss = 0.01210039\n",
      "Iteration 14990, loss = 0.01209945\n",
      "Iteration 14991, loss = 0.01209851\n",
      "Iteration 14992, loss = 0.01209756\n",
      "Iteration 14993, loss = 0.01209662\n",
      "Iteration 14994, loss = 0.01209568\n",
      "Iteration 14995, loss = 0.01209473\n",
      "Iteration 14996, loss = 0.01209379\n",
      "Iteration 14997, loss = 0.01209285\n",
      "Iteration 14998, loss = 0.01209191\n",
      "Iteration 14999, loss = 0.01209097\n",
      "Iteration 15000, loss = 0.01209002\n",
      "Iteration 15001, loss = 0.01208908\n",
      "Iteration 15002, loss = 0.01208814\n",
      "Iteration 15003, loss = 0.01208720\n",
      "Iteration 15004, loss = 0.01208626\n",
      "Iteration 15005, loss = 0.01208532\n",
      "Iteration 15006, loss = 0.01208438\n",
      "Iteration 15007, loss = 0.01208344\n",
      "Iteration 15008, loss = 0.01208250\n",
      "Iteration 15009, loss = 0.01208156\n",
      "Iteration 15010, loss = 0.01208062\n",
      "Iteration 15011, loss = 0.01207968\n",
      "Iteration 15012, loss = 0.01207874\n",
      "Iteration 15013, loss = 0.01207780\n",
      "Iteration 15014, loss = 0.01207686\n",
      "Iteration 15015, loss = 0.01207592\n",
      "Iteration 15016, loss = 0.01207498\n",
      "Iteration 15017, loss = 0.01207404\n",
      "Iteration 15018, loss = 0.01207310\n",
      "Iteration 15019, loss = 0.01207216\n",
      "Iteration 15020, loss = 0.01207123\n",
      "Iteration 15021, loss = 0.01207029\n",
      "Iteration 15022, loss = 0.01206935\n",
      "Iteration 15023, loss = 0.01206841\n",
      "Iteration 15024, loss = 0.01206748\n",
      "Iteration 15025, loss = 0.01206654\n",
      "Iteration 15026, loss = 0.01206560\n",
      "Iteration 15027, loss = 0.01206466\n",
      "Iteration 15028, loss = 0.01206373\n",
      "Iteration 15029, loss = 0.01206279\n",
      "Iteration 15030, loss = 0.01206185\n",
      "Iteration 15031, loss = 0.01206092\n",
      "Iteration 15032, loss = 0.01205998\n",
      "Iteration 15033, loss = 0.01205905\n",
      "Iteration 15034, loss = 0.01205811\n",
      "Iteration 15035, loss = 0.01205718\n",
      "Iteration 15036, loss = 0.01205624\n",
      "Iteration 15037, loss = 0.01205531\n",
      "Iteration 15038, loss = 0.01205437\n",
      "Iteration 15039, loss = 0.01205344\n",
      "Iteration 15040, loss = 0.01205250\n",
      "Iteration 15041, loss = 0.01205157\n",
      "Iteration 15042, loss = 0.01205063\n",
      "Iteration 15043, loss = 0.01204970\n",
      "Iteration 15044, loss = 0.01204876\n",
      "Iteration 15045, loss = 0.01204783\n",
      "Iteration 15046, loss = 0.01204690\n",
      "Iteration 15047, loss = 0.01204596\n",
      "Iteration 15048, loss = 0.01204503\n",
      "Iteration 15049, loss = 0.01204410\n",
      "Iteration 15050, loss = 0.01204317\n",
      "Iteration 15051, loss = 0.01204223\n",
      "Iteration 15052, loss = 0.01204130\n",
      "Iteration 15053, loss = 0.01204037\n",
      "Iteration 15054, loss = 0.01203944\n",
      "Iteration 15055, loss = 0.01203850\n",
      "Iteration 15056, loss = 0.01203757\n",
      "Iteration 15057, loss = 0.01203664\n",
      "Iteration 15058, loss = 0.01203571\n",
      "Iteration 15059, loss = 0.01203478\n",
      "Iteration 15060, loss = 0.01203385\n",
      "Iteration 15061, loss = 0.01203292\n",
      "Iteration 15062, loss = 0.01203199\n",
      "Iteration 15063, loss = 0.01203105\n",
      "Iteration 15064, loss = 0.01203012\n",
      "Iteration 15065, loss = 0.01202919\n",
      "Iteration 15066, loss = 0.01202826\n",
      "Iteration 15067, loss = 0.01202733\n",
      "Iteration 15068, loss = 0.01202641\n",
      "Iteration 15069, loss = 0.01202548\n",
      "Iteration 15070, loss = 0.01202455\n",
      "Iteration 15071, loss = 0.01202362\n",
      "Iteration 15072, loss = 0.01202269\n",
      "Iteration 15073, loss = 0.01202176\n",
      "Iteration 15074, loss = 0.01202083\n",
      "Iteration 15075, loss = 0.01201990\n",
      "Iteration 15076, loss = 0.01201897\n",
      "Iteration 15077, loss = 0.01201805\n",
      "Iteration 15078, loss = 0.01201712\n",
      "Iteration 15079, loss = 0.01201619\n",
      "Iteration 15080, loss = 0.01201526\n",
      "Iteration 15081, loss = 0.01201434\n",
      "Iteration 15082, loss = 0.01201341\n",
      "Iteration 15083, loss = 0.01201248\n",
      "Iteration 15084, loss = 0.01201156\n",
      "Iteration 15085, loss = 0.01201063\n",
      "Iteration 15086, loss = 0.01200970\n",
      "Iteration 15087, loss = 0.01200878\n",
      "Iteration 15088, loss = 0.01200785\n",
      "Iteration 15089, loss = 0.01200692\n",
      "Iteration 15090, loss = 0.01200600\n",
      "Iteration 15091, loss = 0.01200507\n",
      "Iteration 15092, loss = 0.01200415\n",
      "Iteration 15093, loss = 0.01200322\n",
      "Iteration 15094, loss = 0.01200230\n",
      "Iteration 15095, loss = 0.01200137\n",
      "Iteration 15096, loss = 0.01200045\n",
      "Iteration 15097, loss = 0.01199952\n",
      "Iteration 15098, loss = 0.01199860\n",
      "Iteration 15099, loss = 0.01199767\n",
      "Iteration 15100, loss = 0.01199675\n",
      "Iteration 15101, loss = 0.01199583\n",
      "Iteration 15102, loss = 0.01199490\n",
      "Iteration 15103, loss = 0.01199398\n",
      "Iteration 15104, loss = 0.01199306\n",
      "Iteration 15105, loss = 0.01199213\n",
      "Iteration 15106, loss = 0.01199121\n",
      "Iteration 15107, loss = 0.01199029\n",
      "Iteration 15108, loss = 0.01198937\n",
      "Iteration 15109, loss = 0.01198844\n",
      "Iteration 15110, loss = 0.01198752\n",
      "Iteration 15111, loss = 0.01198660\n",
      "Iteration 15112, loss = 0.01198568\n",
      "Iteration 15113, loss = 0.01198476\n",
      "Iteration 15114, loss = 0.01198383\n",
      "Iteration 15115, loss = 0.01198291\n",
      "Iteration 15116, loss = 0.01198199\n",
      "Iteration 15117, loss = 0.01198107\n",
      "Iteration 15118, loss = 0.01198015\n",
      "Iteration 15119, loss = 0.01197923\n",
      "Iteration 15120, loss = 0.01197831\n",
      "Iteration 15121, loss = 0.01197739\n",
      "Iteration 15122, loss = 0.01197647\n",
      "Iteration 15123, loss = 0.01197555\n",
      "Iteration 15124, loss = 0.01197463\n",
      "Iteration 15125, loss = 0.01197371\n",
      "Iteration 15126, loss = 0.01197279\n",
      "Iteration 15127, loss = 0.01197187\n",
      "Iteration 15128, loss = 0.01197095\n",
      "Iteration 15129, loss = 0.01197003\n",
      "Iteration 15130, loss = 0.01196911\n",
      "Iteration 15131, loss = 0.01196819\n",
      "Iteration 15132, loss = 0.01196728\n",
      "Iteration 15133, loss = 0.01196636\n",
      "Iteration 15134, loss = 0.01196544\n",
      "Iteration 15135, loss = 0.01196452\n",
      "Iteration 15136, loss = 0.01196360\n",
      "Iteration 15137, loss = 0.01196269\n",
      "Iteration 15138, loss = 0.01196177\n",
      "Iteration 15139, loss = 0.01196085\n",
      "Iteration 15140, loss = 0.01195994\n",
      "Iteration 15141, loss = 0.01195902\n",
      "Iteration 15142, loss = 0.01195810\n",
      "Iteration 15143, loss = 0.01195719\n",
      "Iteration 15144, loss = 0.01195627\n",
      "Iteration 15145, loss = 0.01195535\n",
      "Iteration 15146, loss = 0.01195444\n",
      "Iteration 15147, loss = 0.01195352\n",
      "Iteration 15148, loss = 0.01195261\n",
      "Iteration 15149, loss = 0.01195169\n",
      "Iteration 15150, loss = 0.01195078\n",
      "Iteration 15151, loss = 0.01194986\n",
      "Iteration 15152, loss = 0.01194895\n",
      "Iteration 15153, loss = 0.01194803\n",
      "Iteration 15154, loss = 0.01194712\n",
      "Iteration 15155, loss = 0.01194620\n",
      "Iteration 15156, loss = 0.01194529\n",
      "Iteration 15157, loss = 0.01194437\n",
      "Iteration 15158, loss = 0.01194346\n",
      "Iteration 15159, loss = 0.01194255\n",
      "Iteration 15160, loss = 0.01194163\n",
      "Iteration 15161, loss = 0.01194072\n",
      "Iteration 15162, loss = 0.01193981\n",
      "Iteration 15163, loss = 0.01193889\n",
      "Iteration 15164, loss = 0.01193798\n",
      "Iteration 15165, loss = 0.01193707\n",
      "Iteration 15166, loss = 0.01193616\n",
      "Iteration 15167, loss = 0.01193524\n",
      "Iteration 15168, loss = 0.01193433\n",
      "Iteration 15169, loss = 0.01193342\n",
      "Iteration 15170, loss = 0.01193251\n",
      "Iteration 15171, loss = 0.01193160\n",
      "Iteration 15172, loss = 0.01193068\n",
      "Iteration 15173, loss = 0.01192977\n",
      "Iteration 15174, loss = 0.01192886\n",
      "Iteration 15175, loss = 0.01192795\n",
      "Iteration 15176, loss = 0.01192704\n",
      "Iteration 15177, loss = 0.01192613\n",
      "Iteration 15178, loss = 0.01192522\n",
      "Iteration 15179, loss = 0.01192431\n",
      "Iteration 15180, loss = 0.01192340\n",
      "Iteration 15181, loss = 0.01192249\n",
      "Iteration 15182, loss = 0.01192158\n",
      "Iteration 15183, loss = 0.01192067\n",
      "Iteration 15184, loss = 0.01191976\n",
      "Iteration 15185, loss = 0.01191885\n",
      "Iteration 15186, loss = 0.01191794\n",
      "Iteration 15187, loss = 0.01191703\n",
      "Iteration 15188, loss = 0.01191612\n",
      "Iteration 15189, loss = 0.01191522\n",
      "Iteration 15190, loss = 0.01191431\n",
      "Iteration 15191, loss = 0.01191340\n",
      "Iteration 15192, loss = 0.01191249\n",
      "Iteration 15193, loss = 0.01191158\n",
      "Iteration 15194, loss = 0.01191068\n",
      "Iteration 15195, loss = 0.01190977\n",
      "Iteration 15196, loss = 0.01190886\n",
      "Iteration 15197, loss = 0.01190795\n",
      "Iteration 15198, loss = 0.01190705\n",
      "Iteration 15199, loss = 0.01190614\n",
      "Iteration 15200, loss = 0.01190523\n",
      "Iteration 15201, loss = 0.01190433\n",
      "Iteration 15202, loss = 0.01190342\n",
      "Iteration 15203, loss = 0.01190251\n",
      "Iteration 15204, loss = 0.01190161\n",
      "Iteration 15205, loss = 0.01190070\n",
      "Iteration 15206, loss = 0.01189980\n",
      "Iteration 15207, loss = 0.01189889\n",
      "Iteration 15208, loss = 0.01189799\n",
      "Iteration 15209, loss = 0.01189708\n",
      "Iteration 15210, loss = 0.01189618\n",
      "Iteration 15211, loss = 0.01189527\n",
      "Iteration 15212, loss = 0.01189437\n",
      "Iteration 15213, loss = 0.01189346\n",
      "Iteration 15214, loss = 0.01189256\n",
      "Iteration 15215, loss = 0.01189165\n",
      "Iteration 15216, loss = 0.01189075\n",
      "Iteration 15217, loss = 0.01188985\n",
      "Iteration 15218, loss = 0.01188894\n",
      "Iteration 15219, loss = 0.01188804\n",
      "Iteration 15220, loss = 0.01188714\n",
      "Iteration 15221, loss = 0.01188623\n",
      "Iteration 15222, loss = 0.01188533\n",
      "Iteration 15223, loss = 0.01188443\n",
      "Iteration 15224, loss = 0.01188352\n",
      "Iteration 15225, loss = 0.01188262\n",
      "Iteration 15226, loss = 0.01188172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15227, loss = 0.01188082\n",
      "Iteration 15228, loss = 0.01187992\n",
      "Iteration 15229, loss = 0.01187901\n",
      "Iteration 15230, loss = 0.01187811\n",
      "Iteration 15231, loss = 0.01187721\n",
      "Iteration 15232, loss = 0.01187631\n",
      "Iteration 15233, loss = 0.01187541\n",
      "Iteration 15234, loss = 0.01187451\n",
      "Iteration 15235, loss = 0.01187361\n",
      "Iteration 15236, loss = 0.01187271\n",
      "Iteration 15237, loss = 0.01187181\n",
      "Iteration 15238, loss = 0.01187091\n",
      "Iteration 15239, loss = 0.01187001\n",
      "Iteration 15240, loss = 0.01186911\n",
      "Iteration 15241, loss = 0.01186821\n",
      "Iteration 15242, loss = 0.01186731\n",
      "Iteration 15243, loss = 0.01186641\n",
      "Iteration 15244, loss = 0.01186551\n",
      "Iteration 15245, loss = 0.01186461\n",
      "Iteration 15246, loss = 0.01186371\n",
      "Iteration 15247, loss = 0.01186281\n",
      "Iteration 15248, loss = 0.01186191\n",
      "Iteration 15249, loss = 0.01186102\n",
      "Iteration 15250, loss = 0.01186012\n",
      "Iteration 15251, loss = 0.01185922\n",
      "Iteration 15252, loss = 0.01185832\n",
      "Iteration 15253, loss = 0.01185742\n",
      "Iteration 15254, loss = 0.01185653\n",
      "Iteration 15255, loss = 0.01185563\n",
      "Iteration 15256, loss = 0.01185473\n",
      "Iteration 15257, loss = 0.01185384\n",
      "Iteration 15258, loss = 0.01185294\n",
      "Iteration 15259, loss = 0.01185204\n",
      "Iteration 15260, loss = 0.01185115\n",
      "Iteration 15261, loss = 0.01185025\n",
      "Iteration 15262, loss = 0.01184935\n",
      "Iteration 15263, loss = 0.01184846\n",
      "Iteration 15264, loss = 0.01184756\n",
      "Iteration 15265, loss = 0.01184667\n",
      "Iteration 15266, loss = 0.01184577\n",
      "Iteration 15267, loss = 0.01184488\n",
      "Iteration 15268, loss = 0.01184398\n",
      "Iteration 15269, loss = 0.01184309\n",
      "Iteration 15270, loss = 0.01184219\n",
      "Iteration 15271, loss = 0.01184130\n",
      "Iteration 15272, loss = 0.01184040\n",
      "Iteration 15273, loss = 0.01183951\n",
      "Iteration 15274, loss = 0.01183861\n",
      "Iteration 15275, loss = 0.01183772\n",
      "Iteration 15276, loss = 0.01183683\n",
      "Iteration 15277, loss = 0.01183593\n",
      "Iteration 15278, loss = 0.01183504\n",
      "Iteration 15279, loss = 0.01183414\n",
      "Iteration 15280, loss = 0.01183325\n",
      "Iteration 15281, loss = 0.01183236\n",
      "Iteration 15282, loss = 0.01183147\n",
      "Iteration 15283, loss = 0.01183057\n",
      "Iteration 15284, loss = 0.01182968\n",
      "Iteration 15285, loss = 0.01182879\n",
      "Iteration 15286, loss = 0.01182790\n",
      "Iteration 15287, loss = 0.01182700\n",
      "Iteration 15288, loss = 0.01182611\n",
      "Iteration 15289, loss = 0.01182522\n",
      "Iteration 15290, loss = 0.01182433\n",
      "Iteration 15291, loss = 0.01182344\n",
      "Iteration 15292, loss = 0.01182255\n",
      "Iteration 15293, loss = 0.01182166\n",
      "Iteration 15294, loss = 0.01182077\n",
      "Iteration 15295, loss = 0.01181988\n",
      "Iteration 15296, loss = 0.01181898\n",
      "Iteration 15297, loss = 0.01181809\n",
      "Iteration 15298, loss = 0.01181720\n",
      "Iteration 15299, loss = 0.01181631\n",
      "Iteration 15300, loss = 0.01181542\n",
      "Iteration 15301, loss = 0.01181453\n",
      "Iteration 15302, loss = 0.01181365\n",
      "Iteration 15303, loss = 0.01181276\n",
      "Iteration 15304, loss = 0.01181187\n",
      "Iteration 15305, loss = 0.01181098\n",
      "Iteration 15306, loss = 0.01181009\n",
      "Iteration 15307, loss = 0.01180920\n",
      "Iteration 15308, loss = 0.01180831\n",
      "Iteration 15309, loss = 0.01180742\n",
      "Iteration 15310, loss = 0.01180654\n",
      "Iteration 15311, loss = 0.01180565\n",
      "Iteration 15312, loss = 0.01180476\n",
      "Iteration 15313, loss = 0.01180387\n",
      "Iteration 15314, loss = 0.01180299\n",
      "Iteration 15315, loss = 0.01180210\n",
      "Iteration 15316, loss = 0.01180121\n",
      "Iteration 15317, loss = 0.01180032\n",
      "Iteration 15318, loss = 0.01179944\n",
      "Iteration 15319, loss = 0.01179855\n",
      "Iteration 15320, loss = 0.01179766\n",
      "Iteration 15321, loss = 0.01179678\n",
      "Iteration 15322, loss = 0.01179589\n",
      "Iteration 15323, loss = 0.01179501\n",
      "Iteration 15324, loss = 0.01179412\n",
      "Iteration 15325, loss = 0.01179323\n",
      "Iteration 15326, loss = 0.01179235\n",
      "Iteration 15327, loss = 0.01179146\n",
      "Iteration 15328, loss = 0.01179058\n",
      "Iteration 15329, loss = 0.01178969\n",
      "Iteration 15330, loss = 0.01178881\n",
      "Iteration 15331, loss = 0.01178792\n",
      "Iteration 15332, loss = 0.01178704\n",
      "Iteration 15333, loss = 0.01178616\n",
      "Iteration 15334, loss = 0.01178527\n",
      "Iteration 15335, loss = 0.01178439\n",
      "Iteration 15336, loss = 0.01178350\n",
      "Iteration 15337, loss = 0.01178262\n",
      "Iteration 15338, loss = 0.01178174\n",
      "Iteration 15339, loss = 0.01178085\n",
      "Iteration 15340, loss = 0.01177997\n",
      "Iteration 15341, loss = 0.01177909\n",
      "Iteration 15342, loss = 0.01177820\n",
      "Iteration 15343, loss = 0.01177732\n",
      "Iteration 15344, loss = 0.01177644\n",
      "Iteration 15345, loss = 0.01177556\n",
      "Iteration 15346, loss = 0.01177467\n",
      "Iteration 15347, loss = 0.01177379\n",
      "Iteration 15348, loss = 0.01177291\n",
      "Iteration 15349, loss = 0.01177203\n",
      "Iteration 15350, loss = 0.01177115\n",
      "Iteration 15351, loss = 0.01177027\n",
      "Iteration 15352, loss = 0.01176939\n",
      "Iteration 15353, loss = 0.01176850\n",
      "Iteration 15354, loss = 0.01176762\n",
      "Iteration 15355, loss = 0.01176674\n",
      "Iteration 15356, loss = 0.01176586\n",
      "Iteration 15357, loss = 0.01176498\n",
      "Iteration 15358, loss = 0.01176410\n",
      "Iteration 15359, loss = 0.01176322\n",
      "Iteration 15360, loss = 0.01176234\n",
      "Iteration 15361, loss = 0.01176146\n",
      "Iteration 15362, loss = 0.01176058\n",
      "Iteration 15363, loss = 0.01175970\n",
      "Iteration 15364, loss = 0.01175882\n",
      "Iteration 15365, loss = 0.01175794\n",
      "Iteration 15366, loss = 0.01175707\n",
      "Iteration 15367, loss = 0.01175619\n",
      "Iteration 15368, loss = 0.01175531\n",
      "Iteration 15369, loss = 0.01175443\n",
      "Iteration 15370, loss = 0.01175355\n",
      "Iteration 15371, loss = 0.01175267\n",
      "Iteration 15372, loss = 0.01175180\n",
      "Iteration 15373, loss = 0.01175092\n",
      "Iteration 15374, loss = 0.01175004\n",
      "Iteration 15375, loss = 0.01174916\n",
      "Iteration 15376, loss = 0.01174829\n",
      "Iteration 15377, loss = 0.01174741\n",
      "Iteration 15378, loss = 0.01174653\n",
      "Iteration 15379, loss = 0.01174566\n",
      "Iteration 15380, loss = 0.01174478\n",
      "Iteration 15381, loss = 0.01174390\n",
      "Iteration 15382, loss = 0.01174303\n",
      "Iteration 15383, loss = 0.01174215\n",
      "Iteration 15384, loss = 0.01174127\n",
      "Iteration 15385, loss = 0.01174040\n",
      "Iteration 15386, loss = 0.01173952\n",
      "Iteration 15387, loss = 0.01173865\n",
      "Iteration 15388, loss = 0.01173777\n",
      "Iteration 15389, loss = 0.01173690\n",
      "Iteration 15390, loss = 0.01173602\n",
      "Iteration 15391, loss = 0.01173515\n",
      "Iteration 15392, loss = 0.01173427\n",
      "Iteration 15393, loss = 0.01173340\n",
      "Iteration 15394, loss = 0.01173252\n",
      "Iteration 15395, loss = 0.01173165\n",
      "Iteration 15396, loss = 0.01173078\n",
      "Iteration 15397, loss = 0.01172990\n",
      "Iteration 15398, loss = 0.01172903\n",
      "Iteration 15399, loss = 0.01172815\n",
      "Iteration 15400, loss = 0.01172728\n",
      "Iteration 15401, loss = 0.01172641\n",
      "Iteration 15402, loss = 0.01172554\n",
      "Iteration 15403, loss = 0.01172466\n",
      "Iteration 15404, loss = 0.01172379\n",
      "Iteration 15405, loss = 0.01172292\n",
      "Iteration 15406, loss = 0.01172204\n",
      "Iteration 15407, loss = 0.01172117\n",
      "Iteration 15408, loss = 0.01172030\n",
      "Iteration 15409, loss = 0.01171943\n",
      "Iteration 15410, loss = 0.01171856\n",
      "Iteration 15411, loss = 0.01171769\n",
      "Iteration 15412, loss = 0.01171681\n",
      "Iteration 15413, loss = 0.01171594\n",
      "Iteration 15414, loss = 0.01171507\n",
      "Iteration 15415, loss = 0.01171420\n",
      "Iteration 15416, loss = 0.01171333\n",
      "Iteration 15417, loss = 0.01171246\n",
      "Iteration 15418, loss = 0.01171159\n",
      "Iteration 15419, loss = 0.01171072\n",
      "Iteration 15420, loss = 0.01170985\n",
      "Iteration 15421, loss = 0.01170898\n",
      "Iteration 15422, loss = 0.01170811\n",
      "Iteration 15423, loss = 0.01170724\n",
      "Iteration 15424, loss = 0.01170637\n",
      "Iteration 15425, loss = 0.01170550\n",
      "Iteration 15426, loss = 0.01170463\n",
      "Iteration 15427, loss = 0.01170376\n",
      "Iteration 15428, loss = 0.01170289\n",
      "Iteration 15429, loss = 0.01170203\n",
      "Iteration 15430, loss = 0.01170116\n",
      "Iteration 15431, loss = 0.01170029\n",
      "Iteration 15432, loss = 0.01169942\n",
      "Iteration 15433, loss = 0.01169855\n",
      "Iteration 15434, loss = 0.01169768\n",
      "Iteration 15435, loss = 0.01169682\n",
      "Iteration 15436, loss = 0.01169595\n",
      "Iteration 15437, loss = 0.01169508\n",
      "Iteration 15438, loss = 0.01169421\n",
      "Iteration 15439, loss = 0.01169335\n",
      "Iteration 15440, loss = 0.01169248\n",
      "Iteration 15441, loss = 0.01169161\n",
      "Iteration 15442, loss = 0.01169075\n",
      "Iteration 15443, loss = 0.01168988\n",
      "Iteration 15444, loss = 0.01168902\n",
      "Iteration 15445, loss = 0.01168815\n",
      "Iteration 15446, loss = 0.01168728\n",
      "Iteration 15447, loss = 0.01168642\n",
      "Iteration 15448, loss = 0.01168555\n",
      "Iteration 15449, loss = 0.01168469\n",
      "Iteration 15450, loss = 0.01168382\n",
      "Iteration 15451, loss = 0.01168296\n",
      "Iteration 15452, loss = 0.01168209\n",
      "Iteration 15453, loss = 0.01168123\n",
      "Iteration 15454, loss = 0.01168036\n",
      "Iteration 15455, loss = 0.01167950\n",
      "Iteration 15456, loss = 0.01167863\n",
      "Iteration 15457, loss = 0.01167777\n",
      "Iteration 15458, loss = 0.01167690\n",
      "Iteration 15459, loss = 0.01167604\n",
      "Iteration 15460, loss = 0.01167518\n",
      "Iteration 15461, loss = 0.01167431\n",
      "Iteration 15462, loss = 0.01167345\n",
      "Iteration 15463, loss = 0.01167259\n",
      "Iteration 15464, loss = 0.01167172\n",
      "Iteration 15465, loss = 0.01167086\n",
      "Iteration 15466, loss = 0.01167000\n",
      "Iteration 15467, loss = 0.01166914\n",
      "Iteration 15468, loss = 0.01166827\n",
      "Iteration 15469, loss = 0.01166741\n",
      "Iteration 15470, loss = 0.01166655\n",
      "Iteration 15471, loss = 0.01166569\n",
      "Iteration 15472, loss = 0.01166483\n",
      "Iteration 15473, loss = 0.01166396\n",
      "Iteration 15474, loss = 0.01166310\n",
      "Iteration 15475, loss = 0.01166224\n",
      "Iteration 15476, loss = 0.01166138\n",
      "Iteration 15477, loss = 0.01166052\n",
      "Iteration 15478, loss = 0.01165966\n",
      "Iteration 15479, loss = 0.01165880\n",
      "Iteration 15480, loss = 0.01165794\n",
      "Iteration 15481, loss = 0.01165708\n",
      "Iteration 15482, loss = 0.01165622\n",
      "Iteration 15483, loss = 0.01165536\n",
      "Iteration 15484, loss = 0.01165450\n",
      "Iteration 15485, loss = 0.01165364\n",
      "Iteration 15486, loss = 0.01165278\n",
      "Iteration 15487, loss = 0.01165192\n",
      "Iteration 15488, loss = 0.01165106\n",
      "Iteration 15489, loss = 0.01165020\n",
      "Iteration 15490, loss = 0.01164934\n",
      "Iteration 15491, loss = 0.01164848\n",
      "Iteration 15492, loss = 0.01164762\n",
      "Iteration 15493, loss = 0.01164677\n",
      "Iteration 15494, loss = 0.01164591\n",
      "Iteration 15495, loss = 0.01164505\n",
      "Iteration 15496, loss = 0.01164419\n",
      "Iteration 15497, loss = 0.01164333\n",
      "Iteration 15498, loss = 0.01164248\n",
      "Iteration 15499, loss = 0.01164162\n",
      "Iteration 15500, loss = 0.01164076\n",
      "Iteration 15501, loss = 0.01163990\n",
      "Iteration 15502, loss = 0.01163905\n",
      "Iteration 15503, loss = 0.01163819\n",
      "Iteration 15504, loss = 0.01163733\n",
      "Iteration 15505, loss = 0.01163648\n",
      "Iteration 15506, loss = 0.01163562\n",
      "Iteration 15507, loss = 0.01163476\n",
      "Iteration 15508, loss = 0.01163391\n",
      "Iteration 15509, loss = 0.01163305\n",
      "Iteration 15510, loss = 0.01163220\n",
      "Iteration 15511, loss = 0.01163134\n",
      "Iteration 15512, loss = 0.01163049\n",
      "Iteration 15513, loss = 0.01162963\n",
      "Iteration 15514, loss = 0.01162878\n",
      "Iteration 15515, loss = 0.01162792\n",
      "Iteration 15516, loss = 0.01162707\n",
      "Iteration 15517, loss = 0.01162621\n",
      "Iteration 15518, loss = 0.01162536\n",
      "Iteration 15519, loss = 0.01162450\n",
      "Iteration 15520, loss = 0.01162365\n",
      "Iteration 15521, loss = 0.01162279\n",
      "Iteration 15522, loss = 0.01162194\n",
      "Iteration 15523, loss = 0.01162109\n",
      "Iteration 15524, loss = 0.01162023\n",
      "Iteration 15525, loss = 0.01161938\n",
      "Iteration 15526, loss = 0.01161853\n",
      "Iteration 15527, loss = 0.01161767\n",
      "Iteration 15528, loss = 0.01161682\n",
      "Iteration 15529, loss = 0.01161597\n",
      "Iteration 15530, loss = 0.01161512\n",
      "Iteration 15531, loss = 0.01161426\n",
      "Iteration 15532, loss = 0.01161341\n",
      "Iteration 15533, loss = 0.01161256\n",
      "Iteration 15534, loss = 0.01161171\n",
      "Iteration 15535, loss = 0.01161085\n",
      "Iteration 15536, loss = 0.01161000\n",
      "Iteration 15537, loss = 0.01160915\n",
      "Iteration 15538, loss = 0.01160830\n",
      "Iteration 15539, loss = 0.01160745\n",
      "Iteration 15540, loss = 0.01160660\n",
      "Iteration 15541, loss = 0.01160575\n",
      "Iteration 15542, loss = 0.01160490\n",
      "Iteration 15543, loss = 0.01160405\n",
      "Iteration 15544, loss = 0.01160320\n",
      "Iteration 15545, loss = 0.01160235\n",
      "Iteration 15546, loss = 0.01160150\n",
      "Iteration 15547, loss = 0.01160065\n",
      "Iteration 15548, loss = 0.01159980\n",
      "Iteration 15549, loss = 0.01159895\n",
      "Iteration 15550, loss = 0.01159810\n",
      "Iteration 15551, loss = 0.01159725\n",
      "Iteration 15552, loss = 0.01159640\n",
      "Iteration 15553, loss = 0.01159555\n",
      "Iteration 15554, loss = 0.01159470\n",
      "Iteration 15555, loss = 0.01159385\n",
      "Iteration 15556, loss = 0.01159300\n",
      "Iteration 15557, loss = 0.01159215\n",
      "Iteration 15558, loss = 0.01159131\n",
      "Iteration 15559, loss = 0.01159046\n",
      "Iteration 15560, loss = 0.01158961\n",
      "Iteration 15561, loss = 0.01158876\n",
      "Iteration 15562, loss = 0.01158792\n",
      "Iteration 15563, loss = 0.01158707\n",
      "Iteration 15564, loss = 0.01158622\n",
      "Iteration 15565, loss = 0.01158537\n",
      "Iteration 15566, loss = 0.01158453\n",
      "Iteration 15567, loss = 0.01158368\n",
      "Iteration 15568, loss = 0.01158283\n",
      "Iteration 15569, loss = 0.01158199\n",
      "Iteration 15570, loss = 0.01158114\n",
      "Iteration 15571, loss = 0.01158029\n",
      "Iteration 15572, loss = 0.01157945\n",
      "Iteration 15573, loss = 0.01157860\n",
      "Iteration 15574, loss = 0.01157776\n",
      "Iteration 15575, loss = 0.01157691\n",
      "Iteration 15576, loss = 0.01157607\n",
      "Iteration 15577, loss = 0.01157522\n",
      "Iteration 15578, loss = 0.01157437\n",
      "Iteration 15579, loss = 0.01157353\n",
      "Iteration 15580, loss = 0.01157269\n",
      "Iteration 15581, loss = 0.01157184\n",
      "Iteration 15582, loss = 0.01157100\n",
      "Iteration 15583, loss = 0.01157015\n",
      "Iteration 15584, loss = 0.01156931\n",
      "Iteration 15585, loss = 0.01156846\n",
      "Iteration 15586, loss = 0.01156762\n",
      "Iteration 15587, loss = 0.01156678\n",
      "Iteration 15588, loss = 0.01156593\n",
      "Iteration 15589, loss = 0.01156509\n",
      "Iteration 15590, loss = 0.01156425\n",
      "Iteration 15591, loss = 0.01156340\n",
      "Iteration 15592, loss = 0.01156256\n",
      "Iteration 15593, loss = 0.01156172\n",
      "Iteration 15594, loss = 0.01156087\n",
      "Iteration 15595, loss = 0.01156003\n",
      "Iteration 15596, loss = 0.01155919\n",
      "Iteration 15597, loss = 0.01155835\n",
      "Iteration 15598, loss = 0.01155751\n",
      "Iteration 15599, loss = 0.01155666\n",
      "Iteration 15600, loss = 0.01155582\n",
      "Iteration 15601, loss = 0.01155498\n",
      "Iteration 15602, loss = 0.01155414\n",
      "Iteration 15603, loss = 0.01155330\n",
      "Iteration 15604, loss = 0.01155246\n",
      "Iteration 15605, loss = 0.01155162\n",
      "Iteration 15606, loss = 0.01155077\n",
      "Iteration 15607, loss = 0.01154993\n",
      "Iteration 15608, loss = 0.01154909\n",
      "Iteration 15609, loss = 0.01154825\n",
      "Iteration 15610, loss = 0.01154741\n",
      "Iteration 15611, loss = 0.01154657\n",
      "Iteration 15612, loss = 0.01154573\n",
      "Iteration 15613, loss = 0.01154489\n",
      "Iteration 15614, loss = 0.01154405\n",
      "Iteration 15615, loss = 0.01154321\n",
      "Iteration 15616, loss = 0.01154238\n",
      "Iteration 15617, loss = 0.01154154\n",
      "Iteration 15618, loss = 0.01154070\n",
      "Iteration 15619, loss = 0.01153986\n",
      "Iteration 15620, loss = 0.01153902\n",
      "Iteration 15621, loss = 0.01153818\n",
      "Iteration 15622, loss = 0.01153734\n",
      "Iteration 15623, loss = 0.01153651\n",
      "Iteration 15624, loss = 0.01153567\n",
      "Iteration 15625, loss = 0.01153483\n",
      "Iteration 15626, loss = 0.01153399\n",
      "Iteration 15627, loss = 0.01153315\n",
      "Iteration 15628, loss = 0.01153232\n",
      "Iteration 15629, loss = 0.01153148\n",
      "Iteration 15630, loss = 0.01153064\n",
      "Iteration 15631, loss = 0.01152981\n",
      "Iteration 15632, loss = 0.01152897\n",
      "Iteration 15633, loss = 0.01152813\n",
      "Iteration 15634, loss = 0.01152730\n",
      "Iteration 15635, loss = 0.01152646\n",
      "Iteration 15636, loss = 0.01152562\n",
      "Iteration 15637, loss = 0.01152479\n",
      "Iteration 15638, loss = 0.01152395\n",
      "Iteration 15639, loss = 0.01152312\n",
      "Iteration 15640, loss = 0.01152228\n",
      "Iteration 15641, loss = 0.01152144\n",
      "Iteration 15642, loss = 0.01152061\n",
      "Iteration 15643, loss = 0.01151977\n",
      "Iteration 15644, loss = 0.01151894\n",
      "Iteration 15645, loss = 0.01151810\n",
      "Iteration 15646, loss = 0.01151727\n",
      "Iteration 15647, loss = 0.01151644\n",
      "Iteration 15648, loss = 0.01151560\n",
      "Iteration 15649, loss = 0.01151477\n",
      "Iteration 15650, loss = 0.01151393\n",
      "Iteration 15651, loss = 0.01151310\n",
      "Iteration 15652, loss = 0.01151227\n",
      "Iteration 15653, loss = 0.01151143\n",
      "Iteration 15654, loss = 0.01151060\n",
      "Iteration 15655, loss = 0.01150976\n",
      "Iteration 15656, loss = 0.01150893\n",
      "Iteration 15657, loss = 0.01150810\n",
      "Iteration 15658, loss = 0.01150727\n",
      "Iteration 15659, loss = 0.01150643\n",
      "Iteration 15660, loss = 0.01150560\n",
      "Iteration 15661, loss = 0.01150477\n",
      "Iteration 15662, loss = 0.01150394\n",
      "Iteration 15663, loss = 0.01150310\n",
      "Iteration 15664, loss = 0.01150227\n",
      "Iteration 15665, loss = 0.01150144\n",
      "Iteration 15666, loss = 0.01150061\n",
      "Iteration 15667, loss = 0.01149978\n",
      "Iteration 15668, loss = 0.01149895\n",
      "Iteration 15669, loss = 0.01149811\n",
      "Iteration 15670, loss = 0.01149728\n",
      "Iteration 15671, loss = 0.01149645\n",
      "Iteration 15672, loss = 0.01149562\n",
      "Iteration 15673, loss = 0.01149479\n",
      "Iteration 15674, loss = 0.01149396\n",
      "Iteration 15675, loss = 0.01149313\n",
      "Iteration 15676, loss = 0.01149230\n",
      "Iteration 15677, loss = 0.01149147\n",
      "Iteration 15678, loss = 0.01149064\n",
      "Iteration 15679, loss = 0.01148981\n",
      "Iteration 15680, loss = 0.01148898\n",
      "Iteration 15681, loss = 0.01148815\n",
      "Iteration 15682, loss = 0.01148732\n",
      "Iteration 15683, loss = 0.01148649\n",
      "Iteration 15684, loss = 0.01148567\n",
      "Iteration 15685, loss = 0.01148484\n",
      "Iteration 15686, loss = 0.01148401\n",
      "Iteration 15687, loss = 0.01148318\n",
      "Iteration 15688, loss = 0.01148235\n",
      "Iteration 15689, loss = 0.01148152\n",
      "Iteration 15690, loss = 0.01148070\n",
      "Iteration 15691, loss = 0.01147987\n",
      "Iteration 15692, loss = 0.01147904\n",
      "Iteration 15693, loss = 0.01147821\n",
      "Iteration 15694, loss = 0.01147739\n",
      "Iteration 15695, loss = 0.01147656\n",
      "Iteration 15696, loss = 0.01147573\n",
      "Iteration 15697, loss = 0.01147490\n",
      "Iteration 15698, loss = 0.01147408\n",
      "Iteration 15699, loss = 0.01147325\n",
      "Iteration 15700, loss = 0.01147242\n",
      "Iteration 15701, loss = 0.01147160\n",
      "Iteration 15702, loss = 0.01147077\n",
      "Iteration 15703, loss = 0.01146995\n",
      "Iteration 15704, loss = 0.01146912\n",
      "Iteration 15705, loss = 0.01146829\n",
      "Iteration 15706, loss = 0.01146747\n",
      "Iteration 15707, loss = 0.01146664\n",
      "Iteration 15708, loss = 0.01146582\n",
      "Iteration 15709, loss = 0.01146499\n",
      "Iteration 15710, loss = 0.01146417\n",
      "Iteration 15711, loss = 0.01146334\n",
      "Iteration 15712, loss = 0.01146252\n",
      "Iteration 15713, loss = 0.01146169\n",
      "Iteration 15714, loss = 0.01146087\n",
      "Iteration 15715, loss = 0.01146004\n",
      "Iteration 15716, loss = 0.01145922\n",
      "Iteration 15717, loss = 0.01145840\n",
      "Iteration 15718, loss = 0.01145757\n",
      "Iteration 15719, loss = 0.01145675\n",
      "Iteration 15720, loss = 0.01145593\n",
      "Iteration 15721, loss = 0.01145510\n",
      "Iteration 15722, loss = 0.01145428\n",
      "Iteration 15723, loss = 0.01145346\n",
      "Iteration 15724, loss = 0.01145263\n",
      "Iteration 15725, loss = 0.01145181\n",
      "Iteration 15726, loss = 0.01145099\n",
      "Iteration 15727, loss = 0.01145017\n",
      "Iteration 15728, loss = 0.01144934\n",
      "Iteration 15729, loss = 0.01144852\n",
      "Iteration 15730, loss = 0.01144770\n",
      "Iteration 15731, loss = 0.01144688\n",
      "Iteration 15732, loss = 0.01144606\n",
      "Iteration 15733, loss = 0.01144523\n",
      "Iteration 15734, loss = 0.01144441\n",
      "Iteration 15735, loss = 0.01144359\n",
      "Iteration 15736, loss = 0.01144277\n",
      "Iteration 15737, loss = 0.01144195\n",
      "Iteration 15738, loss = 0.01144113\n",
      "Iteration 15739, loss = 0.01144031\n",
      "Iteration 15740, loss = 0.01143949\n",
      "Iteration 15741, loss = 0.01143867\n",
      "Iteration 15742, loss = 0.01143785\n",
      "Iteration 15743, loss = 0.01143703\n",
      "Iteration 15744, loss = 0.01143621\n",
      "Iteration 15745, loss = 0.01143539\n",
      "Iteration 15746, loss = 0.01143457\n",
      "Iteration 15747, loss = 0.01143375\n",
      "Iteration 15748, loss = 0.01143293\n",
      "Iteration 15749, loss = 0.01143211\n",
      "Iteration 15750, loss = 0.01143129\n",
      "Iteration 15751, loss = 0.01143047\n",
      "Iteration 15752, loss = 0.01142965\n",
      "Iteration 15753, loss = 0.01142883\n",
      "Iteration 15754, loss = 0.01142802\n",
      "Iteration 15755, loss = 0.01142720\n",
      "Iteration 15756, loss = 0.01142638\n",
      "Iteration 15757, loss = 0.01142556\n",
      "Iteration 15758, loss = 0.01142474\n",
      "Iteration 15759, loss = 0.01142393\n",
      "Iteration 15760, loss = 0.01142311\n",
      "Iteration 15761, loss = 0.01142229\n",
      "Iteration 15762, loss = 0.01142147\n",
      "Iteration 15763, loss = 0.01142066\n",
      "Iteration 15764, loss = 0.01141984\n",
      "Iteration 15765, loss = 0.01141902\n",
      "Iteration 15766, loss = 0.01141821\n",
      "Iteration 15767, loss = 0.01141739\n",
      "Iteration 15768, loss = 0.01141657\n",
      "Iteration 15769, loss = 0.01141576\n",
      "Iteration 15770, loss = 0.01141494\n",
      "Iteration 15771, loss = 0.01141413\n",
      "Iteration 15772, loss = 0.01141331\n",
      "Iteration 15773, loss = 0.01141249\n",
      "Iteration 15774, loss = 0.01141168\n",
      "Iteration 15775, loss = 0.01141086\n",
      "Iteration 15776, loss = 0.01141005\n",
      "Iteration 15777, loss = 0.01140923\n",
      "Iteration 15778, loss = 0.01140842\n",
      "Iteration 15779, loss = 0.01140760\n",
      "Iteration 15780, loss = 0.01140679\n",
      "Iteration 15781, loss = 0.01140598\n",
      "Iteration 15782, loss = 0.01140516\n",
      "Iteration 15783, loss = 0.01140435\n",
      "Iteration 15784, loss = 0.01140353\n",
      "Iteration 15785, loss = 0.01140272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15786, loss = 0.01140191\n",
      "Iteration 15787, loss = 0.01140109\n",
      "Iteration 15788, loss = 0.01140028\n",
      "Iteration 15789, loss = 0.01139947\n",
      "Iteration 15790, loss = 0.01139865\n",
      "Iteration 15791, loss = 0.01139784\n",
      "Iteration 15792, loss = 0.01139703\n",
      "Iteration 15793, loss = 0.01139621\n",
      "Iteration 15794, loss = 0.01139540\n",
      "Iteration 15795, loss = 0.01139459\n",
      "Iteration 15796, loss = 0.01139378\n",
      "Iteration 15797, loss = 0.01139296\n",
      "Iteration 15798, loss = 0.01139215\n",
      "Iteration 15799, loss = 0.01139134\n",
      "Iteration 15800, loss = 0.01139053\n",
      "Iteration 15801, loss = 0.01138972\n",
      "Iteration 15802, loss = 0.01138891\n",
      "Iteration 15803, loss = 0.01138809\n",
      "Iteration 15804, loss = 0.01138728\n",
      "Iteration 15805, loss = 0.01138647\n",
      "Iteration 15806, loss = 0.01138566\n",
      "Iteration 15807, loss = 0.01138485\n",
      "Iteration 15808, loss = 0.01138404\n",
      "Iteration 15809, loss = 0.01138323\n",
      "Iteration 15810, loss = 0.01138242\n",
      "Iteration 15811, loss = 0.01138161\n",
      "Iteration 15812, loss = 0.01138080\n",
      "Iteration 15813, loss = 0.01137999\n",
      "Iteration 15814, loss = 0.01137918\n",
      "Iteration 15815, loss = 0.01137837\n",
      "Iteration 15816, loss = 0.01137756\n",
      "Iteration 15817, loss = 0.01137675\n",
      "Iteration 15818, loss = 0.01137594\n",
      "Iteration 15819, loss = 0.01137514\n",
      "Iteration 15820, loss = 0.01137433\n",
      "Iteration 15821, loss = 0.01137352\n",
      "Iteration 15822, loss = 0.01137271\n",
      "Iteration 15823, loss = 0.01137190\n",
      "Iteration 15824, loss = 0.01137109\n",
      "Iteration 15825, loss = 0.01137029\n",
      "Iteration 15826, loss = 0.01136948\n",
      "Iteration 15827, loss = 0.01136867\n",
      "Iteration 15828, loss = 0.01136786\n",
      "Iteration 15829, loss = 0.01136706\n",
      "Iteration 15830, loss = 0.01136625\n",
      "Iteration 15831, loss = 0.01136544\n",
      "Iteration 15832, loss = 0.01136463\n",
      "Iteration 15833, loss = 0.01136383\n",
      "Iteration 15834, loss = 0.01136302\n",
      "Iteration 15835, loss = 0.01136221\n",
      "Iteration 15836, loss = 0.01136141\n",
      "Iteration 15837, loss = 0.01136060\n",
      "Iteration 15838, loss = 0.01135979\n",
      "Iteration 15839, loss = 0.01135899\n",
      "Iteration 15840, loss = 0.01135818\n",
      "Iteration 15841, loss = 0.01135738\n",
      "Iteration 15842, loss = 0.01135657\n",
      "Iteration 15843, loss = 0.01135577\n",
      "Iteration 15844, loss = 0.01135496\n",
      "Iteration 15845, loss = 0.01135416\n",
      "Iteration 15846, loss = 0.01135335\n",
      "Iteration 15847, loss = 0.01135255\n",
      "Iteration 15848, loss = 0.01135174\n",
      "Iteration 15849, loss = 0.01135094\n",
      "Iteration 15850, loss = 0.01135013\n",
      "Iteration 15851, loss = 0.01134933\n",
      "Iteration 15852, loss = 0.01134852\n",
      "Iteration 15853, loss = 0.01134772\n",
      "Iteration 15854, loss = 0.01134692\n",
      "Iteration 15855, loss = 0.01134611\n",
      "Iteration 15856, loss = 0.01134531\n",
      "Iteration 15857, loss = 0.01134451\n",
      "Iteration 15858, loss = 0.01134370\n",
      "Iteration 15859, loss = 0.01134290\n",
      "Iteration 15860, loss = 0.01134210\n",
      "Iteration 15861, loss = 0.01134129\n",
      "Iteration 15862, loss = 0.01134049\n",
      "Iteration 15863, loss = 0.01133969\n",
      "Iteration 15864, loss = 0.01133889\n",
      "Iteration 15865, loss = 0.01133809\n",
      "Iteration 15866, loss = 0.01133728\n",
      "Iteration 15867, loss = 0.01133648\n",
      "Iteration 15868, loss = 0.01133568\n",
      "Iteration 15869, loss = 0.01133488\n",
      "Iteration 15870, loss = 0.01133408\n",
      "Iteration 15871, loss = 0.01133327\n",
      "Iteration 15872, loss = 0.01133247\n",
      "Iteration 15873, loss = 0.01133167\n",
      "Iteration 15874, loss = 0.01133087\n",
      "Iteration 15875, loss = 0.01133007\n",
      "Iteration 15876, loss = 0.01132927\n",
      "Iteration 15877, loss = 0.01132847\n",
      "Iteration 15878, loss = 0.01132767\n",
      "Iteration 15879, loss = 0.01132687\n",
      "Iteration 15880, loss = 0.01132607\n",
      "Iteration 15881, loss = 0.01132527\n",
      "Iteration 15882, loss = 0.01132447\n",
      "Iteration 15883, loss = 0.01132367\n",
      "Iteration 15884, loss = 0.01132287\n",
      "Iteration 15885, loss = 0.01132207\n",
      "Iteration 15886, loss = 0.01132127\n",
      "Iteration 15887, loss = 0.01132047\n",
      "Iteration 15888, loss = 0.01131967\n",
      "Iteration 15889, loss = 0.01131888\n",
      "Iteration 15890, loss = 0.01131808\n",
      "Iteration 15891, loss = 0.01131728\n",
      "Iteration 15892, loss = 0.01131648\n",
      "Iteration 15893, loss = 0.01131568\n",
      "Iteration 15894, loss = 0.01131488\n",
      "Iteration 15895, loss = 0.01131409\n",
      "Iteration 15896, loss = 0.01131329\n",
      "Iteration 15897, loss = 0.01131249\n",
      "Iteration 15898, loss = 0.01131169\n",
      "Iteration 15899, loss = 0.01131090\n",
      "Iteration 15900, loss = 0.01131010\n",
      "Iteration 15901, loss = 0.01130930\n",
      "Iteration 15902, loss = 0.01130851\n",
      "Iteration 15903, loss = 0.01130771\n",
      "Iteration 15904, loss = 0.01130691\n",
      "Iteration 15905, loss = 0.01130612\n",
      "Iteration 15906, loss = 0.01130532\n",
      "Iteration 15907, loss = 0.01130452\n",
      "Iteration 15908, loss = 0.01130373\n",
      "Iteration 15909, loss = 0.01130293\n",
      "Iteration 15910, loss = 0.01130214\n",
      "Iteration 15911, loss = 0.01130134\n",
      "Iteration 15912, loss = 0.01130054\n",
      "Iteration 15913, loss = 0.01129975\n",
      "Iteration 15914, loss = 0.01129895\n",
      "Iteration 15915, loss = 0.01129816\n",
      "Iteration 15916, loss = 0.01129736\n",
      "Iteration 15917, loss = 0.01129657\n",
      "Iteration 15918, loss = 0.01129577\n",
      "Iteration 15919, loss = 0.01129498\n",
      "Iteration 15920, loss = 0.01129419\n",
      "Iteration 15921, loss = 0.01129339\n",
      "Iteration 15922, loss = 0.01129260\n",
      "Iteration 15923, loss = 0.01129180\n",
      "Iteration 15924, loss = 0.01129101\n",
      "Iteration 15925, loss = 0.01129022\n",
      "Iteration 15926, loss = 0.01128942\n",
      "Iteration 15927, loss = 0.01128863\n",
      "Iteration 15928, loss = 0.01128784\n",
      "Iteration 15929, loss = 0.01128704\n",
      "Iteration 15930, loss = 0.01128625\n",
      "Iteration 15931, loss = 0.01128546\n",
      "Iteration 15932, loss = 0.01128466\n",
      "Iteration 15933, loss = 0.01128387\n",
      "Iteration 15934, loss = 0.01128308\n",
      "Iteration 15935, loss = 0.01128229\n",
      "Iteration 15936, loss = 0.01128150\n",
      "Iteration 15937, loss = 0.01128070\n",
      "Iteration 15938, loss = 0.01127991\n",
      "Iteration 15939, loss = 0.01127912\n",
      "Iteration 15940, loss = 0.01127833\n",
      "Iteration 15941, loss = 0.01127754\n",
      "Iteration 15942, loss = 0.01127675\n",
      "Iteration 15943, loss = 0.01127596\n",
      "Iteration 15944, loss = 0.01127516\n",
      "Iteration 15945, loss = 0.01127437\n",
      "Iteration 15946, loss = 0.01127358\n",
      "Iteration 15947, loss = 0.01127279\n",
      "Iteration 15948, loss = 0.01127200\n",
      "Iteration 15949, loss = 0.01127121\n",
      "Iteration 15950, loss = 0.01127042\n",
      "Iteration 15951, loss = 0.01126963\n",
      "Iteration 15952, loss = 0.01126884\n",
      "Iteration 15953, loss = 0.01126805\n",
      "Iteration 15954, loss = 0.01126726\n",
      "Iteration 15955, loss = 0.01126647\n",
      "Iteration 15956, loss = 0.01126568\n",
      "Iteration 15957, loss = 0.01126490\n",
      "Iteration 15958, loss = 0.01126411\n",
      "Iteration 15959, loss = 0.01126332\n",
      "Iteration 15960, loss = 0.01126253\n",
      "Iteration 15961, loss = 0.01126174\n",
      "Iteration 15962, loss = 0.01126095\n",
      "Iteration 15963, loss = 0.01126016\n",
      "Iteration 15964, loss = 0.01125938\n",
      "Iteration 15965, loss = 0.01125859\n",
      "Iteration 15966, loss = 0.01125780\n",
      "Iteration 15967, loss = 0.01125701\n",
      "Iteration 15968, loss = 0.01125623\n",
      "Iteration 15969, loss = 0.01125544\n",
      "Iteration 15970, loss = 0.01125465\n",
      "Iteration 15971, loss = 0.01125386\n",
      "Iteration 15972, loss = 0.01125308\n",
      "Iteration 15973, loss = 0.01125229\n",
      "Iteration 15974, loss = 0.01125150\n",
      "Iteration 15975, loss = 0.01125072\n",
      "Iteration 15976, loss = 0.01124993\n",
      "Iteration 15977, loss = 0.01124914\n",
      "Iteration 15978, loss = 0.01124836\n",
      "Iteration 15979, loss = 0.01124757\n",
      "Iteration 15980, loss = 0.01124679\n",
      "Iteration 15981, loss = 0.01124600\n",
      "Iteration 15982, loss = 0.01124521\n",
      "Iteration 15983, loss = 0.01124443\n",
      "Iteration 15984, loss = 0.01124364\n",
      "Iteration 15985, loss = 0.01124286\n",
      "Iteration 15986, loss = 0.01124207\n",
      "Iteration 15987, loss = 0.01124129\n",
      "Iteration 15988, loss = 0.01124050\n",
      "Iteration 15989, loss = 0.01123972\n",
      "Iteration 15990, loss = 0.01123893\n",
      "Iteration 15991, loss = 0.01123815\n",
      "Iteration 15992, loss = 0.01123737\n",
      "Iteration 15993, loss = 0.01123658\n",
      "Iteration 15994, loss = 0.01123580\n",
      "Iteration 15995, loss = 0.01123501\n",
      "Iteration 15996, loss = 0.01123423\n",
      "Iteration 15997, loss = 0.01123345\n",
      "Iteration 15998, loss = 0.01123266\n",
      "Iteration 15999, loss = 0.01123188\n",
      "Iteration 16000, loss = 0.01123110\n",
      "Iteration 16001, loss = 0.01123032\n",
      "Iteration 16002, loss = 0.01122953\n",
      "Iteration 16003, loss = 0.01122875\n",
      "Iteration 16004, loss = 0.01122797\n",
      "Iteration 16005, loss = 0.01122718\n",
      "Iteration 16006, loss = 0.01122640\n",
      "Iteration 16007, loss = 0.01122562\n",
      "Iteration 16008, loss = 0.01122484\n",
      "Iteration 16009, loss = 0.01122406\n",
      "Iteration 16010, loss = 0.01122328\n",
      "Iteration 16011, loss = 0.01122249\n",
      "Iteration 16012, loss = 0.01122171\n",
      "Iteration 16013, loss = 0.01122093\n",
      "Iteration 16014, loss = 0.01122015\n",
      "Iteration 16015, loss = 0.01121937\n",
      "Iteration 16016, loss = 0.01121859\n",
      "Iteration 16017, loss = 0.01121781\n",
      "Iteration 16018, loss = 0.01121703\n",
      "Iteration 16019, loss = 0.01121625\n",
      "Iteration 16020, loss = 0.01121547\n",
      "Iteration 16021, loss = 0.01121469\n",
      "Iteration 16022, loss = 0.01121391\n",
      "Iteration 16023, loss = 0.01121313\n",
      "Iteration 16024, loss = 0.01121235\n",
      "Iteration 16025, loss = 0.01121157\n",
      "Iteration 16026, loss = 0.01121079\n",
      "Iteration 16027, loss = 0.01121001\n",
      "Iteration 16028, loss = 0.01120923\n",
      "Iteration 16029, loss = 0.01120845\n",
      "Iteration 16030, loss = 0.01120767\n",
      "Iteration 16031, loss = 0.01120689\n",
      "Iteration 16032, loss = 0.01120611\n",
      "Iteration 16033, loss = 0.01120534\n",
      "Iteration 16034, loss = 0.01120456\n",
      "Iteration 16035, loss = 0.01120378\n",
      "Iteration 16036, loss = 0.01120300\n",
      "Iteration 16037, loss = 0.01120222\n",
      "Iteration 16038, loss = 0.01120144\n",
      "Iteration 16039, loss = 0.01120067\n",
      "Iteration 16040, loss = 0.01119989\n",
      "Iteration 16041, loss = 0.01119911\n",
      "Iteration 16042, loss = 0.01119834\n",
      "Iteration 16043, loss = 0.01119756\n",
      "Iteration 16044, loss = 0.01119678\n",
      "Iteration 16045, loss = 0.01119600\n",
      "Iteration 16046, loss = 0.01119523\n",
      "Iteration 16047, loss = 0.01119445\n",
      "Iteration 16048, loss = 0.01119367\n",
      "Iteration 16049, loss = 0.01119290\n",
      "Iteration 16050, loss = 0.01119212\n",
      "Iteration 16051, loss = 0.01119135\n",
      "Iteration 16052, loss = 0.01119057\n",
      "Iteration 16053, loss = 0.01118979\n",
      "Iteration 16054, loss = 0.01118902\n",
      "Iteration 16055, loss = 0.01118824\n",
      "Iteration 16056, loss = 0.01118747\n",
      "Iteration 16057, loss = 0.01118669\n",
      "Iteration 16058, loss = 0.01118592\n",
      "Iteration 16059, loss = 0.01118514\n",
      "Iteration 16060, loss = 0.01118437\n",
      "Iteration 16061, loss = 0.01118359\n",
      "Iteration 16062, loss = 0.01118282\n",
      "Iteration 16063, loss = 0.01118205\n",
      "Iteration 16064, loss = 0.01118127\n",
      "Iteration 16065, loss = 0.01118050\n",
      "Iteration 16066, loss = 0.01117972\n",
      "Iteration 16067, loss = 0.01117895\n",
      "Iteration 16068, loss = 0.01117818\n",
      "Iteration 16069, loss = 0.01117740\n",
      "Iteration 16070, loss = 0.01117663\n",
      "Iteration 16071, loss = 0.01117586\n",
      "Iteration 16072, loss = 0.01117508\n",
      "Iteration 16073, loss = 0.01117431\n",
      "Iteration 16074, loss = 0.01117354\n",
      "Iteration 16075, loss = 0.01117276\n",
      "Iteration 16076, loss = 0.01117199\n",
      "Iteration 16077, loss = 0.01117122\n",
      "Iteration 16078, loss = 0.01117045\n",
      "Iteration 16079, loss = 0.01116967\n",
      "Iteration 16080, loss = 0.01116890\n",
      "Iteration 16081, loss = 0.01116813\n",
      "Iteration 16082, loss = 0.01116736\n",
      "Iteration 16083, loss = 0.01116659\n",
      "Iteration 16084, loss = 0.01116582\n",
      "Iteration 16085, loss = 0.01116504\n",
      "Iteration 16086, loss = 0.01116427\n",
      "Iteration 16087, loss = 0.01116350\n",
      "Iteration 16088, loss = 0.01116273\n",
      "Iteration 16089, loss = 0.01116196\n",
      "Iteration 16090, loss = 0.01116119\n",
      "Iteration 16091, loss = 0.01116042\n",
      "Iteration 16092, loss = 0.01115965\n",
      "Iteration 16093, loss = 0.01115888\n",
      "Iteration 16094, loss = 0.01115811\n",
      "Iteration 16095, loss = 0.01115734\n",
      "Iteration 16096, loss = 0.01115657\n",
      "Iteration 16097, loss = 0.01115580\n",
      "Iteration 16098, loss = 0.01115503\n",
      "Iteration 16099, loss = 0.01115426\n",
      "Iteration 16100, loss = 0.01115349\n",
      "Iteration 16101, loss = 0.01115272\n",
      "Iteration 16102, loss = 0.01115195\n",
      "Iteration 16103, loss = 0.01115118\n",
      "Iteration 16104, loss = 0.01115042\n",
      "Iteration 16105, loss = 0.01114965\n",
      "Iteration 16106, loss = 0.01114888\n",
      "Iteration 16107, loss = 0.01114811\n",
      "Iteration 16108, loss = 0.01114734\n",
      "Iteration 16109, loss = 0.01114657\n",
      "Iteration 16110, loss = 0.01114581\n",
      "Iteration 16111, loss = 0.01114504\n",
      "Iteration 16112, loss = 0.01114427\n",
      "Iteration 16113, loss = 0.01114350\n",
      "Iteration 16114, loss = 0.01114274\n",
      "Iteration 16115, loss = 0.01114197\n",
      "Iteration 16116, loss = 0.01114120\n",
      "Iteration 16117, loss = 0.01114043\n",
      "Iteration 16118, loss = 0.01113967\n",
      "Iteration 16119, loss = 0.01113890\n",
      "Iteration 16120, loss = 0.01113813\n",
      "Iteration 16121, loss = 0.01113737\n",
      "Iteration 16122, loss = 0.01113660\n",
      "Iteration 16123, loss = 0.01113583\n",
      "Iteration 16124, loss = 0.01113507\n",
      "Iteration 16125, loss = 0.01113430\n",
      "Iteration 16126, loss = 0.01113354\n",
      "Iteration 16127, loss = 0.01113277\n",
      "Iteration 16128, loss = 0.01113201\n",
      "Iteration 16129, loss = 0.01113124\n",
      "Iteration 16130, loss = 0.01113048\n",
      "Iteration 16131, loss = 0.01112971\n",
      "Iteration 16132, loss = 0.01112895\n",
      "Iteration 16133, loss = 0.01112818\n",
      "Iteration 16134, loss = 0.01112742\n",
      "Iteration 16135, loss = 0.01112665\n",
      "Iteration 16136, loss = 0.01112589\n",
      "Iteration 16137, loss = 0.01112512\n",
      "Iteration 16138, loss = 0.01112436\n",
      "Iteration 16139, loss = 0.01112359\n",
      "Iteration 16140, loss = 0.01112283\n",
      "Iteration 16141, loss = 0.01112207\n",
      "Iteration 16142, loss = 0.01112130\n",
      "Iteration 16143, loss = 0.01112054\n",
      "Iteration 16144, loss = 0.01111978\n",
      "Iteration 16145, loss = 0.01111901\n",
      "Iteration 16146, loss = 0.01111825\n",
      "Iteration 16147, loss = 0.01111749\n",
      "Iteration 16148, loss = 0.01111672\n",
      "Iteration 16149, loss = 0.01111596\n",
      "Iteration 16150, loss = 0.01111520\n",
      "Iteration 16151, loss = 0.01111444\n",
      "Iteration 16152, loss = 0.01111367\n",
      "Iteration 16153, loss = 0.01111291\n",
      "Iteration 16154, loss = 0.01111215\n",
      "Iteration 16155, loss = 0.01111139\n",
      "Iteration 16156, loss = 0.01111063\n",
      "Iteration 16157, loss = 0.01110987\n",
      "Iteration 16158, loss = 0.01110910\n",
      "Iteration 16159, loss = 0.01110834\n",
      "Iteration 16160, loss = 0.01110758\n",
      "Iteration 16161, loss = 0.01110682\n",
      "Iteration 16162, loss = 0.01110606\n",
      "Iteration 16163, loss = 0.01110530\n",
      "Iteration 16164, loss = 0.01110454\n",
      "Iteration 16165, loss = 0.01110378\n",
      "Iteration 16166, loss = 0.01110302\n",
      "Iteration 16167, loss = 0.01110226\n",
      "Iteration 16168, loss = 0.01110150\n",
      "Iteration 16169, loss = 0.01110074\n",
      "Iteration 16170, loss = 0.01109998\n",
      "Iteration 16171, loss = 0.01109922\n",
      "Iteration 16172, loss = 0.01109846\n",
      "Iteration 16173, loss = 0.01109770\n",
      "Iteration 16174, loss = 0.01109694\n",
      "Iteration 16175, loss = 0.01109618\n",
      "Iteration 16176, loss = 0.01109542\n",
      "Iteration 16177, loss = 0.01109466\n",
      "Iteration 16178, loss = 0.01109390\n",
      "Iteration 16179, loss = 0.01109314\n",
      "Iteration 16180, loss = 0.01109239\n",
      "Iteration 16181, loss = 0.01109163\n",
      "Iteration 16182, loss = 0.01109087\n",
      "Iteration 16183, loss = 0.01109011\n",
      "Iteration 16184, loss = 0.01108935\n",
      "Iteration 16185, loss = 0.01108860\n",
      "Iteration 16186, loss = 0.01108784\n",
      "Iteration 16187, loss = 0.01108708\n",
      "Iteration 16188, loss = 0.01108632\n",
      "Iteration 16189, loss = 0.01108557\n",
      "Iteration 16190, loss = 0.01108481\n",
      "Iteration 16191, loss = 0.01108405\n",
      "Iteration 16192, loss = 0.01108329\n",
      "Iteration 16193, loss = 0.01108254\n",
      "Iteration 16194, loss = 0.01108178\n",
      "Iteration 16195, loss = 0.01108102\n",
      "Iteration 16196, loss = 0.01108027\n",
      "Iteration 16197, loss = 0.01107951\n",
      "Iteration 16198, loss = 0.01107876\n",
      "Iteration 16199, loss = 0.01107800\n",
      "Iteration 16200, loss = 0.01107724\n",
      "Iteration 16201, loss = 0.01107649\n",
      "Iteration 16202, loss = 0.01107573\n",
      "Iteration 16203, loss = 0.01107498\n",
      "Iteration 16204, loss = 0.01107422\n",
      "Iteration 16205, loss = 0.01107347\n",
      "Iteration 16206, loss = 0.01107271\n",
      "Iteration 16207, loss = 0.01107196\n",
      "Iteration 16208, loss = 0.01107120\n",
      "Iteration 16209, loss = 0.01107045\n",
      "Iteration 16210, loss = 0.01106969\n",
      "Iteration 16211, loss = 0.01106894\n",
      "Iteration 16212, loss = 0.01106818\n",
      "Iteration 16213, loss = 0.01106743\n",
      "Iteration 16214, loss = 0.01106668\n",
      "Iteration 16215, loss = 0.01106592\n",
      "Iteration 16216, loss = 0.01106517\n",
      "Iteration 16217, loss = 0.01106441\n",
      "Iteration 16218, loss = 0.01106366\n",
      "Iteration 16219, loss = 0.01106291\n",
      "Iteration 16220, loss = 0.01106215\n",
      "Iteration 16221, loss = 0.01106140\n",
      "Iteration 16222, loss = 0.01106065\n",
      "Iteration 16223, loss = 0.01105990\n",
      "Iteration 16224, loss = 0.01105914\n",
      "Iteration 16225, loss = 0.01105839\n",
      "Iteration 16226, loss = 0.01105764\n",
      "Iteration 16227, loss = 0.01105689\n",
      "Iteration 16228, loss = 0.01105613\n",
      "Iteration 16229, loss = 0.01105538\n",
      "Iteration 16230, loss = 0.01105463\n",
      "Iteration 16231, loss = 0.01105388\n",
      "Iteration 16232, loss = 0.01105313\n",
      "Iteration 16233, loss = 0.01105237\n",
      "Iteration 16234, loss = 0.01105162\n",
      "Iteration 16235, loss = 0.01105087\n",
      "Iteration 16236, loss = 0.01105012\n",
      "Iteration 16237, loss = 0.01104937\n",
      "Iteration 16238, loss = 0.01104862\n",
      "Iteration 16239, loss = 0.01104787\n",
      "Iteration 16240, loss = 0.01104712\n",
      "Iteration 16241, loss = 0.01104637\n",
      "Iteration 16242, loss = 0.01104562\n",
      "Iteration 16243, loss = 0.01104487\n",
      "Iteration 16244, loss = 0.01104412\n",
      "Iteration 16245, loss = 0.01104337\n",
      "Iteration 16246, loss = 0.01104262\n",
      "Iteration 16247, loss = 0.01104187\n",
      "Iteration 16248, loss = 0.01104112\n",
      "Iteration 16249, loss = 0.01104037\n",
      "Iteration 16250, loss = 0.01103962\n",
      "Iteration 16251, loss = 0.01103887\n",
      "Iteration 16252, loss = 0.01103812\n",
      "Iteration 16253, loss = 0.01103737\n",
      "Iteration 16254, loss = 0.01103662\n",
      "Iteration 16255, loss = 0.01103588\n",
      "Iteration 16256, loss = 0.01103513\n",
      "Iteration 16257, loss = 0.01103438\n",
      "Iteration 16258, loss = 0.01103363\n",
      "Iteration 16259, loss = 0.01103288\n",
      "Iteration 16260, loss = 0.01103213\n",
      "Iteration 16261, loss = 0.01103139\n",
      "Iteration 16262, loss = 0.01103064\n",
      "Iteration 16263, loss = 0.01102989\n",
      "Iteration 16264, loss = 0.01102914\n",
      "Iteration 16265, loss = 0.01102840\n",
      "Iteration 16266, loss = 0.01102765\n",
      "Iteration 16267, loss = 0.01102690\n",
      "Iteration 16268, loss = 0.01102616\n",
      "Iteration 16269, loss = 0.01102541\n",
      "Iteration 16270, loss = 0.01102466\n",
      "Iteration 16271, loss = 0.01102392\n",
      "Iteration 16272, loss = 0.01102317\n",
      "Iteration 16273, loss = 0.01102242\n",
      "Iteration 16274, loss = 0.01102168\n",
      "Iteration 16275, loss = 0.01102093\n",
      "Iteration 16276, loss = 0.01102018\n",
      "Iteration 16277, loss = 0.01101944\n",
      "Iteration 16278, loss = 0.01101869\n",
      "Iteration 16279, loss = 0.01101795\n",
      "Iteration 16280, loss = 0.01101720\n",
      "Iteration 16281, loss = 0.01101646\n",
      "Iteration 16282, loss = 0.01101571\n",
      "Iteration 16283, loss = 0.01101497\n",
      "Iteration 16284, loss = 0.01101422\n",
      "Iteration 16285, loss = 0.01101348\n",
      "Iteration 16286, loss = 0.01101273\n",
      "Iteration 16287, loss = 0.01101199\n",
      "Iteration 16288, loss = 0.01101124\n",
      "Iteration 16289, loss = 0.01101050\n",
      "Iteration 16290, loss = 0.01100976\n",
      "Iteration 16291, loss = 0.01100901\n",
      "Iteration 16292, loss = 0.01100827\n",
      "Iteration 16293, loss = 0.01100753\n",
      "Iteration 16294, loss = 0.01100678\n",
      "Iteration 16295, loss = 0.01100604\n",
      "Iteration 16296, loss = 0.01100530\n",
      "Iteration 16297, loss = 0.01100455\n",
      "Iteration 16298, loss = 0.01100381\n",
      "Iteration 16299, loss = 0.01100307\n",
      "Iteration 16300, loss = 0.01100232\n",
      "Iteration 16301, loss = 0.01100158\n",
      "Iteration 16302, loss = 0.01100084\n",
      "Iteration 16303, loss = 0.01100010\n",
      "Iteration 16304, loss = 0.01099935\n",
      "Iteration 16305, loss = 0.01099861\n",
      "Iteration 16306, loss = 0.01099787\n",
      "Iteration 16307, loss = 0.01099713\n",
      "Iteration 16308, loss = 0.01099639\n",
      "Iteration 16309, loss = 0.01099564\n",
      "Iteration 16310, loss = 0.01099490\n",
      "Iteration 16311, loss = 0.01099416\n",
      "Iteration 16312, loss = 0.01099342\n",
      "Iteration 16313, loss = 0.01099268\n",
      "Iteration 16314, loss = 0.01099194\n",
      "Iteration 16315, loss = 0.01099120\n",
      "Iteration 16316, loss = 0.01099046\n",
      "Iteration 16317, loss = 0.01098972\n",
      "Iteration 16318, loss = 0.01098898\n",
      "Iteration 16319, loss = 0.01098824\n",
      "Iteration 16320, loss = 0.01098750\n",
      "Iteration 16321, loss = 0.01098676\n",
      "Iteration 16322, loss = 0.01098602\n",
      "Iteration 16323, loss = 0.01098528\n",
      "Iteration 16324, loss = 0.01098454\n",
      "Iteration 16325, loss = 0.01098380\n",
      "Iteration 16326, loss = 0.01098306\n",
      "Iteration 16327, loss = 0.01098232\n",
      "Iteration 16328, loss = 0.01098158\n",
      "Iteration 16329, loss = 0.01098084\n",
      "Iteration 16330, loss = 0.01098010\n",
      "Iteration 16331, loss = 0.01097936\n",
      "Iteration 16332, loss = 0.01097862\n",
      "Iteration 16333, loss = 0.01097789\n",
      "Iteration 16334, loss = 0.01097715\n",
      "Iteration 16335, loss = 0.01097641\n",
      "Iteration 16336, loss = 0.01097567\n",
      "Iteration 16337, loss = 0.01097493\n",
      "Iteration 16338, loss = 0.01097419\n",
      "Iteration 16339, loss = 0.01097346\n",
      "Iteration 16340, loss = 0.01097272\n",
      "Iteration 16341, loss = 0.01097198\n",
      "Iteration 16342, loss = 0.01097124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16343, loss = 0.01097051\n",
      "Iteration 16344, loss = 0.01096977\n",
      "Iteration 16345, loss = 0.01096903\n",
      "Iteration 16346, loss = 0.01096830\n",
      "Iteration 16347, loss = 0.01096756\n",
      "Iteration 16348, loss = 0.01096682\n",
      "Iteration 16349, loss = 0.01096609\n",
      "Iteration 16350, loss = 0.01096535\n",
      "Iteration 16351, loss = 0.01096461\n",
      "Iteration 16352, loss = 0.01096388\n",
      "Iteration 16353, loss = 0.01096314\n",
      "Iteration 16354, loss = 0.01096241\n",
      "Iteration 16355, loss = 0.01096167\n",
      "Iteration 16356, loss = 0.01096094\n",
      "Iteration 16357, loss = 0.01096020\n",
      "Iteration 16358, loss = 0.01095946\n",
      "Iteration 16359, loss = 0.01095873\n",
      "Iteration 16360, loss = 0.01095799\n",
      "Iteration 16361, loss = 0.01095726\n",
      "Iteration 16362, loss = 0.01095653\n",
      "Iteration 16363, loss = 0.01095579\n",
      "Iteration 16364, loss = 0.01095506\n",
      "Iteration 16365, loss = 0.01095432\n",
      "Iteration 16366, loss = 0.01095359\n",
      "Iteration 16367, loss = 0.01095285\n",
      "Iteration 16368, loss = 0.01095212\n",
      "Iteration 16369, loss = 0.01095139\n",
      "Iteration 16370, loss = 0.01095065\n",
      "Iteration 16371, loss = 0.01094992\n",
      "Iteration 16372, loss = 0.01094918\n",
      "Iteration 16373, loss = 0.01094845\n",
      "Iteration 16374, loss = 0.01094772\n",
      "Iteration 16375, loss = 0.01094698\n",
      "Iteration 16376, loss = 0.01094625\n",
      "Iteration 16377, loss = 0.01094552\n",
      "Iteration 16378, loss = 0.01094479\n",
      "Iteration 16379, loss = 0.01094405\n",
      "Iteration 16380, loss = 0.01094332\n",
      "Iteration 16381, loss = 0.01094259\n",
      "Iteration 16382, loss = 0.01094186\n",
      "Iteration 16383, loss = 0.01094112\n",
      "Iteration 16384, loss = 0.01094039\n",
      "Iteration 16385, loss = 0.01093966\n",
      "Iteration 16386, loss = 0.01093893\n",
      "Iteration 16387, loss = 0.01093820\n",
      "Iteration 16388, loss = 0.01093747\n",
      "Iteration 16389, loss = 0.01093673\n",
      "Iteration 16390, loss = 0.01093600\n",
      "Iteration 16391, loss = 0.01093527\n",
      "Iteration 16392, loss = 0.01093454\n",
      "Iteration 16393, loss = 0.01093381\n",
      "Iteration 16394, loss = 0.01093308\n",
      "Iteration 16395, loss = 0.01093235\n",
      "Iteration 16396, loss = 0.01093162\n",
      "Iteration 16397, loss = 0.01093089\n",
      "Iteration 16398, loss = 0.01093016\n",
      "Iteration 16399, loss = 0.01092943\n",
      "Iteration 16400, loss = 0.01092870\n",
      "Iteration 16401, loss = 0.01092797\n",
      "Iteration 16402, loss = 0.01092724\n",
      "Iteration 16403, loss = 0.01092651\n",
      "Iteration 16404, loss = 0.01092578\n",
      "Iteration 16405, loss = 0.01092505\n",
      "Iteration 16406, loss = 0.01092432\n",
      "Iteration 16407, loss = 0.01092359\n",
      "Iteration 16408, loss = 0.01092286\n",
      "Iteration 16409, loss = 0.01092213\n",
      "Iteration 16410, loss = 0.01092141\n",
      "Iteration 16411, loss = 0.01092068\n",
      "Iteration 16412, loss = 0.01091995\n",
      "Iteration 16413, loss = 0.01091922\n",
      "Iteration 16414, loss = 0.01091849\n",
      "Iteration 16415, loss = 0.01091776\n",
      "Iteration 16416, loss = 0.01091704\n",
      "Iteration 16417, loss = 0.01091631\n",
      "Iteration 16418, loss = 0.01091558\n",
      "Iteration 16419, loss = 0.01091485\n",
      "Iteration 16420, loss = 0.01091413\n",
      "Iteration 16421, loss = 0.01091340\n",
      "Iteration 16422, loss = 0.01091267\n",
      "Iteration 16423, loss = 0.01091194\n",
      "Iteration 16424, loss = 0.01091122\n",
      "Iteration 16425, loss = 0.01091049\n",
      "Iteration 16426, loss = 0.01090976\n",
      "Iteration 16427, loss = 0.01090904\n",
      "Iteration 16428, loss = 0.01090831\n",
      "Iteration 16429, loss = 0.01090758\n",
      "Iteration 16430, loss = 0.01090686\n",
      "Iteration 16431, loss = 0.01090613\n",
      "Iteration 16432, loss = 0.01090541\n",
      "Iteration 16433, loss = 0.01090468\n",
      "Iteration 16434, loss = 0.01090395\n",
      "Iteration 16435, loss = 0.01090323\n",
      "Iteration 16436, loss = 0.01090250\n",
      "Iteration 16437, loss = 0.01090178\n",
      "Iteration 16438, loss = 0.01090105\n",
      "Iteration 16439, loss = 0.01090033\n",
      "Iteration 16440, loss = 0.01089960\n",
      "Iteration 16441, loss = 0.01089888\n",
      "Iteration 16442, loss = 0.01089815\n",
      "Iteration 16443, loss = 0.01089743\n",
      "Iteration 16444, loss = 0.01089670\n",
      "Iteration 16445, loss = 0.01089598\n",
      "Iteration 16446, loss = 0.01089526\n",
      "Iteration 16447, loss = 0.01089453\n",
      "Iteration 16448, loss = 0.01089381\n",
      "Iteration 16449, loss = 0.01089308\n",
      "Iteration 16450, loss = 0.01089236\n",
      "Iteration 16451, loss = 0.01089164\n",
      "Iteration 16452, loss = 0.01089091\n",
      "Iteration 16453, loss = 0.01089019\n",
      "Iteration 16454, loss = 0.01088947\n",
      "Iteration 16455, loss = 0.01088874\n",
      "Iteration 16456, loss = 0.01088802\n",
      "Iteration 16457, loss = 0.01088730\n",
      "Iteration 16458, loss = 0.01088658\n",
      "Iteration 16459, loss = 0.01088585\n",
      "Iteration 16460, loss = 0.01088513\n",
      "Iteration 16461, loss = 0.01088441\n",
      "Iteration 16462, loss = 0.01088369\n",
      "Iteration 16463, loss = 0.01088296\n",
      "Iteration 16464, loss = 0.01088224\n",
      "Iteration 16465, loss = 0.01088152\n",
      "Iteration 16466, loss = 0.01088080\n",
      "Iteration 16467, loss = 0.01088008\n",
      "Iteration 16468, loss = 0.01087936\n",
      "Iteration 16469, loss = 0.01087864\n",
      "Iteration 16470, loss = 0.01087791\n",
      "Iteration 16471, loss = 0.01087719\n",
      "Iteration 16472, loss = 0.01087647\n",
      "Iteration 16473, loss = 0.01087575\n",
      "Iteration 16474, loss = 0.01087503\n",
      "Iteration 16475, loss = 0.01087431\n",
      "Iteration 16476, loss = 0.01087359\n",
      "Iteration 16477, loss = 0.01087287\n",
      "Iteration 16478, loss = 0.01087215\n",
      "Iteration 16479, loss = 0.01087143\n",
      "Iteration 16480, loss = 0.01087071\n",
      "Iteration 16481, loss = 0.01086999\n",
      "Iteration 16482, loss = 0.01086927\n",
      "Iteration 16483, loss = 0.01086855\n",
      "Iteration 16484, loss = 0.01086783\n",
      "Iteration 16485, loss = 0.01086711\n",
      "Iteration 16486, loss = 0.01086639\n",
      "Iteration 16487, loss = 0.01086567\n",
      "Iteration 16488, loss = 0.01086495\n",
      "Iteration 16489, loss = 0.01086424\n",
      "Iteration 16490, loss = 0.01086352\n",
      "Iteration 16491, loss = 0.01086280\n",
      "Iteration 16492, loss = 0.01086208\n",
      "Iteration 16493, loss = 0.01086136\n",
      "Iteration 16494, loss = 0.01086064\n",
      "Iteration 16495, loss = 0.01085993\n",
      "Iteration 16496, loss = 0.01085921\n",
      "Iteration 16497, loss = 0.01085849\n",
      "Iteration 16498, loss = 0.01085777\n",
      "Iteration 16499, loss = 0.01085705\n",
      "Iteration 16500, loss = 0.01085634\n",
      "Iteration 16501, loss = 0.01085562\n",
      "Iteration 16502, loss = 0.01085490\n",
      "Iteration 16503, loss = 0.01085418\n",
      "Iteration 16504, loss = 0.01085347\n",
      "Iteration 16505, loss = 0.01085275\n",
      "Iteration 16506, loss = 0.01085203\n",
      "Iteration 16507, loss = 0.01085132\n",
      "Iteration 16508, loss = 0.01085060\n",
      "Iteration 16509, loss = 0.01084989\n",
      "Iteration 16510, loss = 0.01084917\n",
      "Iteration 16511, loss = 0.01084845\n",
      "Iteration 16512, loss = 0.01084774\n",
      "Iteration 16513, loss = 0.01084702\n",
      "Iteration 16514, loss = 0.01084631\n",
      "Iteration 16515, loss = 0.01084559\n",
      "Iteration 16516, loss = 0.01084487\n",
      "Iteration 16517, loss = 0.01084416\n",
      "Iteration 16518, loss = 0.01084344\n",
      "Iteration 16519, loss = 0.01084273\n",
      "Iteration 16520, loss = 0.01084201\n",
      "Iteration 16521, loss = 0.01084130\n",
      "Iteration 16522, loss = 0.01084058\n",
      "Iteration 16523, loss = 0.01083987\n",
      "Iteration 16524, loss = 0.01083915\n",
      "Iteration 16525, loss = 0.01083844\n",
      "Iteration 16526, loss = 0.01083773\n",
      "Iteration 16527, loss = 0.01083701\n",
      "Iteration 16528, loss = 0.01083630\n",
      "Iteration 16529, loss = 0.01083558\n",
      "Iteration 16530, loss = 0.01083487\n",
      "Iteration 16531, loss = 0.01083416\n",
      "Iteration 16532, loss = 0.01083344\n",
      "Iteration 16533, loss = 0.01083273\n",
      "Iteration 16534, loss = 0.01083202\n",
      "Iteration 16535, loss = 0.01083130\n",
      "Iteration 16536, loss = 0.01083059\n",
      "Iteration 16537, loss = 0.01082988\n",
      "Iteration 16538, loss = 0.01082916\n",
      "Iteration 16539, loss = 0.01082845\n",
      "Iteration 16540, loss = 0.01082774\n",
      "Iteration 16541, loss = 0.01082703\n",
      "Iteration 16542, loss = 0.01082631\n",
      "Iteration 16543, loss = 0.01082560\n",
      "Iteration 16544, loss = 0.01082489\n",
      "Iteration 16545, loss = 0.01082418\n",
      "Iteration 16546, loss = 0.01082347\n",
      "Iteration 16547, loss = 0.01082275\n",
      "Iteration 16548, loss = 0.01082204\n",
      "Iteration 16549, loss = 0.01082133\n",
      "Iteration 16550, loss = 0.01082062\n",
      "Iteration 16551, loss = 0.01081991\n",
      "Iteration 16552, loss = 0.01081920\n",
      "Iteration 16553, loss = 0.01081849\n",
      "Iteration 16554, loss = 0.01081778\n",
      "Iteration 16555, loss = 0.01081707\n",
      "Iteration 16556, loss = 0.01081636\n",
      "Iteration 16557, loss = 0.01081564\n",
      "Iteration 16558, loss = 0.01081493\n",
      "Iteration 16559, loss = 0.01081422\n",
      "Iteration 16560, loss = 0.01081351\n",
      "Iteration 16561, loss = 0.01081280\n",
      "Iteration 16562, loss = 0.01081209\n",
      "Iteration 16563, loss = 0.01081138\n",
      "Iteration 16564, loss = 0.01081067\n",
      "Iteration 16565, loss = 0.01080997\n",
      "Iteration 16566, loss = 0.01080926\n",
      "Iteration 16567, loss = 0.01080855\n",
      "Iteration 16568, loss = 0.01080784\n",
      "Iteration 16569, loss = 0.01080713\n",
      "Iteration 16570, loss = 0.01080642\n",
      "Iteration 16571, loss = 0.01080571\n",
      "Iteration 16572, loss = 0.01080500\n",
      "Iteration 16573, loss = 0.01080429\n",
      "Iteration 16574, loss = 0.01080359\n",
      "Iteration 16575, loss = 0.01080288\n",
      "Iteration 16576, loss = 0.01080217\n",
      "Iteration 16577, loss = 0.01080146\n",
      "Iteration 16578, loss = 0.01080075\n",
      "Iteration 16579, loss = 0.01080005\n",
      "Iteration 16580, loss = 0.01079934\n",
      "Iteration 16581, loss = 0.01079863\n",
      "Iteration 16582, loss = 0.01079792\n",
      "Iteration 16583, loss = 0.01079722\n",
      "Iteration 16584, loss = 0.01079651\n",
      "Iteration 16585, loss = 0.01079580\n",
      "Iteration 16586, loss = 0.01079509\n",
      "Iteration 16587, loss = 0.01079439\n",
      "Iteration 16588, loss = 0.01079368\n",
      "Iteration 16589, loss = 0.01079297\n",
      "Iteration 16590, loss = 0.01079227\n",
      "Iteration 16591, loss = 0.01079156\n",
      "Iteration 16592, loss = 0.01079085\n",
      "Iteration 16593, loss = 0.01079015\n",
      "Iteration 16594, loss = 0.01078944\n",
      "Iteration 16595, loss = 0.01078874\n",
      "Iteration 16596, loss = 0.01078803\n",
      "Iteration 16597, loss = 0.01078733\n",
      "Iteration 16598, loss = 0.01078662\n",
      "Iteration 16599, loss = 0.01078591\n",
      "Iteration 16600, loss = 0.01078521\n",
      "Iteration 16601, loss = 0.01078450\n",
      "Iteration 16602, loss = 0.01078380\n",
      "Iteration 16603, loss = 0.01078309\n",
      "Iteration 16604, loss = 0.01078239\n",
      "Iteration 16605, loss = 0.01078168\n",
      "Iteration 16606, loss = 0.01078098\n",
      "Iteration 16607, loss = 0.01078028\n",
      "Iteration 16608, loss = 0.01077957\n",
      "Iteration 16609, loss = 0.01077887\n",
      "Iteration 16610, loss = 0.01077816\n",
      "Iteration 16611, loss = 0.01077746\n",
      "Iteration 16612, loss = 0.01077676\n",
      "Iteration 16613, loss = 0.01077605\n",
      "Iteration 16614, loss = 0.01077535\n",
      "Iteration 16615, loss = 0.01077464\n",
      "Iteration 16616, loss = 0.01077394\n",
      "Iteration 16617, loss = 0.01077324\n",
      "Iteration 16618, loss = 0.01077254\n",
      "Iteration 16619, loss = 0.01077183\n",
      "Iteration 16620, loss = 0.01077113\n",
      "Iteration 16621, loss = 0.01077043\n",
      "Iteration 16622, loss = 0.01076972\n",
      "Iteration 16623, loss = 0.01076902\n",
      "Iteration 16624, loss = 0.01076832\n",
      "Iteration 16625, loss = 0.01076762\n",
      "Iteration 16626, loss = 0.01076691\n",
      "Iteration 16627, loss = 0.01076621\n",
      "Iteration 16628, loss = 0.01076551\n",
      "Iteration 16629, loss = 0.01076481\n",
      "Iteration 16630, loss = 0.01076411\n",
      "Iteration 16631, loss = 0.01076341\n",
      "Iteration 16632, loss = 0.01076270\n",
      "Iteration 16633, loss = 0.01076200\n",
      "Iteration 16634, loss = 0.01076130\n",
      "Iteration 16635, loss = 0.01076060\n",
      "Iteration 16636, loss = 0.01075990\n",
      "Iteration 16637, loss = 0.01075920\n",
      "Iteration 16638, loss = 0.01075850\n",
      "Iteration 16639, loss = 0.01075780\n",
      "Iteration 16640, loss = 0.01075710\n",
      "Iteration 16641, loss = 0.01075640\n",
      "Iteration 16642, loss = 0.01075570\n",
      "Iteration 16643, loss = 0.01075500\n",
      "Iteration 16644, loss = 0.01075430\n",
      "Iteration 16645, loss = 0.01075360\n",
      "Iteration 16646, loss = 0.01075290\n",
      "Iteration 16647, loss = 0.01075220\n",
      "Iteration 16648, loss = 0.01075150\n",
      "Iteration 16649, loss = 0.01075080\n",
      "Iteration 16650, loss = 0.01075010\n",
      "Iteration 16651, loss = 0.01074940\n",
      "Iteration 16652, loss = 0.01074870\n",
      "Iteration 16653, loss = 0.01074800\n",
      "Iteration 16654, loss = 0.01074730\n",
      "Iteration 16655, loss = 0.01074660\n",
      "Iteration 16656, loss = 0.01074591\n",
      "Iteration 16657, loss = 0.01074521\n",
      "Iteration 16658, loss = 0.01074451\n",
      "Iteration 16659, loss = 0.01074381\n",
      "Iteration 16660, loss = 0.01074311\n",
      "Iteration 16661, loss = 0.01074242\n",
      "Iteration 16662, loss = 0.01074172\n",
      "Iteration 16663, loss = 0.01074102\n",
      "Iteration 16664, loss = 0.01074032\n",
      "Iteration 16665, loss = 0.01073962\n",
      "Iteration 16666, loss = 0.01073893\n",
      "Iteration 16667, loss = 0.01073823\n",
      "Iteration 16668, loss = 0.01073753\n",
      "Iteration 16669, loss = 0.01073684\n",
      "Iteration 16670, loss = 0.01073614\n",
      "Iteration 16671, loss = 0.01073544\n",
      "Iteration 16672, loss = 0.01073475\n",
      "Iteration 16673, loss = 0.01073405\n",
      "Iteration 16674, loss = 0.01073335\n",
      "Iteration 16675, loss = 0.01073266\n",
      "Iteration 16676, loss = 0.01073196\n",
      "Iteration 16677, loss = 0.01073126\n",
      "Iteration 16678, loss = 0.01073057\n",
      "Iteration 16679, loss = 0.01072987\n",
      "Iteration 16680, loss = 0.01072918\n",
      "Iteration 16681, loss = 0.01072848\n",
      "Iteration 16682, loss = 0.01072779\n",
      "Iteration 16683, loss = 0.01072709\n",
      "Iteration 16684, loss = 0.01072639\n",
      "Iteration 16685, loss = 0.01072570\n",
      "Iteration 16686, loss = 0.01072500\n",
      "Iteration 16687, loss = 0.01072431\n",
      "Iteration 16688, loss = 0.01072361\n",
      "Iteration 16689, loss = 0.01072292\n",
      "Iteration 16690, loss = 0.01072223\n",
      "Iteration 16691, loss = 0.01072153\n",
      "Iteration 16692, loss = 0.01072084\n",
      "Iteration 16693, loss = 0.01072014\n",
      "Iteration 16694, loss = 0.01071945\n",
      "Iteration 16695, loss = 0.01071875\n",
      "Iteration 16696, loss = 0.01071806\n",
      "Iteration 16697, loss = 0.01071737\n",
      "Iteration 16698, loss = 0.01071667\n",
      "Iteration 16699, loss = 0.01071598\n",
      "Iteration 16700, loss = 0.01071529\n",
      "Iteration 16701, loss = 0.01071459\n",
      "Iteration 16702, loss = 0.01071390\n",
      "Iteration 16703, loss = 0.01071321\n",
      "Iteration 16704, loss = 0.01071251\n",
      "Iteration 16705, loss = 0.01071182\n",
      "Iteration 16706, loss = 0.01071113\n",
      "Iteration 16707, loss = 0.01071044\n",
      "Iteration 16708, loss = 0.01070974\n",
      "Iteration 16709, loss = 0.01070905\n",
      "Iteration 16710, loss = 0.01070836\n",
      "Iteration 16711, loss = 0.01070767\n",
      "Iteration 16712, loss = 0.01070698\n",
      "Iteration 16713, loss = 0.01070628\n",
      "Iteration 16714, loss = 0.01070559\n",
      "Iteration 16715, loss = 0.01070490\n",
      "Iteration 16716, loss = 0.01070421\n",
      "Iteration 16717, loss = 0.01070352\n",
      "Iteration 16718, loss = 0.01070283\n",
      "Iteration 16719, loss = 0.01070214\n",
      "Iteration 16720, loss = 0.01070144\n",
      "Iteration 16721, loss = 0.01070075\n",
      "Iteration 16722, loss = 0.01070006\n",
      "Iteration 16723, loss = 0.01069937\n",
      "Iteration 16724, loss = 0.01069868\n",
      "Iteration 16725, loss = 0.01069799\n",
      "Iteration 16726, loss = 0.01069730\n",
      "Iteration 16727, loss = 0.01069661\n",
      "Iteration 16728, loss = 0.01069592\n",
      "Iteration 16729, loss = 0.01069523\n",
      "Iteration 16730, loss = 0.01069454\n",
      "Iteration 16731, loss = 0.01069385\n",
      "Iteration 16732, loss = 0.01069316\n",
      "Iteration 16733, loss = 0.01069247\n",
      "Iteration 16734, loss = 0.01069178\n",
      "Iteration 16735, loss = 0.01069109\n",
      "Iteration 16736, loss = 0.01069040\n",
      "Iteration 16737, loss = 0.01068972\n",
      "Iteration 16738, loss = 0.01068903\n",
      "Iteration 16739, loss = 0.01068834\n",
      "Iteration 16740, loss = 0.01068765\n",
      "Iteration 16741, loss = 0.01068696\n",
      "Iteration 16742, loss = 0.01068627\n",
      "Iteration 16743, loss = 0.01068558\n",
      "Iteration 16744, loss = 0.01068490\n",
      "Iteration 16745, loss = 0.01068421\n",
      "Iteration 16746, loss = 0.01068352\n",
      "Iteration 16747, loss = 0.01068283\n",
      "Iteration 16748, loss = 0.01068214\n",
      "Iteration 16749, loss = 0.01068146\n",
      "Iteration 16750, loss = 0.01068077\n",
      "Iteration 16751, loss = 0.01068008\n",
      "Iteration 16752, loss = 0.01067939\n",
      "Iteration 16753, loss = 0.01067871\n",
      "Iteration 16754, loss = 0.01067802\n",
      "Iteration 16755, loss = 0.01067733\n",
      "Iteration 16756, loss = 0.01067665\n",
      "Iteration 16757, loss = 0.01067596\n",
      "Iteration 16758, loss = 0.01067527\n",
      "Iteration 16759, loss = 0.01067459\n",
      "Iteration 16760, loss = 0.01067390\n",
      "Iteration 16761, loss = 0.01067321\n",
      "Iteration 16762, loss = 0.01067253\n",
      "Iteration 16763, loss = 0.01067184\n",
      "Iteration 16764, loss = 0.01067116\n",
      "Iteration 16765, loss = 0.01067047\n",
      "Iteration 16766, loss = 0.01066978\n",
      "Iteration 16767, loss = 0.01066910\n",
      "Iteration 16768, loss = 0.01066841\n",
      "Iteration 16769, loss = 0.01066773\n",
      "Iteration 16770, loss = 0.01066704\n",
      "Iteration 16771, loss = 0.01066636\n",
      "Iteration 16772, loss = 0.01066567\n",
      "Iteration 16773, loss = 0.01066499\n",
      "Iteration 16774, loss = 0.01066430\n",
      "Iteration 16775, loss = 0.01066362\n",
      "Iteration 16776, loss = 0.01066293\n",
      "Iteration 16777, loss = 0.01066225\n",
      "Iteration 16778, loss = 0.01066157\n",
      "Iteration 16779, loss = 0.01066088\n",
      "Iteration 16780, loss = 0.01066020\n",
      "Iteration 16781, loss = 0.01065951\n",
      "Iteration 16782, loss = 0.01065883\n",
      "Iteration 16783, loss = 0.01065815\n",
      "Iteration 16784, loss = 0.01065746\n",
      "Iteration 16785, loss = 0.01065678\n",
      "Iteration 16786, loss = 0.01065610\n",
      "Iteration 16787, loss = 0.01065541\n",
      "Iteration 16788, loss = 0.01065473\n",
      "Iteration 16789, loss = 0.01065405\n",
      "Iteration 16790, loss = 0.01065336\n",
      "Iteration 16791, loss = 0.01065268\n",
      "Iteration 16792, loss = 0.01065200\n",
      "Iteration 16793, loss = 0.01065132\n",
      "Iteration 16794, loss = 0.01065063\n",
      "Iteration 16795, loss = 0.01064995\n",
      "Iteration 16796, loss = 0.01064927\n",
      "Iteration 16797, loss = 0.01064859\n",
      "Iteration 16798, loss = 0.01064790\n",
      "Iteration 16799, loss = 0.01064722\n",
      "Iteration 16800, loss = 0.01064654\n",
      "Iteration 16801, loss = 0.01064586\n",
      "Iteration 16802, loss = 0.01064518\n",
      "Iteration 16803, loss = 0.01064450\n",
      "Iteration 16804, loss = 0.01064382\n",
      "Iteration 16805, loss = 0.01064313\n",
      "Iteration 16806, loss = 0.01064245\n",
      "Iteration 16807, loss = 0.01064177\n",
      "Iteration 16808, loss = 0.01064109\n",
      "Iteration 16809, loss = 0.01064041\n",
      "Iteration 16810, loss = 0.01063973\n",
      "Iteration 16811, loss = 0.01063905\n",
      "Iteration 16812, loss = 0.01063837\n",
      "Iteration 16813, loss = 0.01063769\n",
      "Iteration 16814, loss = 0.01063701\n",
      "Iteration 16815, loss = 0.01063633\n",
      "Iteration 16816, loss = 0.01063565\n",
      "Iteration 16817, loss = 0.01063497\n",
      "Iteration 16818, loss = 0.01063429\n",
      "Iteration 16819, loss = 0.01063361\n",
      "Iteration 16820, loss = 0.01063293\n",
      "Iteration 16821, loss = 0.01063225\n",
      "Iteration 16822, loss = 0.01063157\n",
      "Iteration 16823, loss = 0.01063089\n",
      "Iteration 16824, loss = 0.01063021\n",
      "Iteration 16825, loss = 0.01062954\n",
      "Iteration 16826, loss = 0.01062886\n",
      "Iteration 16827, loss = 0.01062818\n",
      "Iteration 16828, loss = 0.01062750\n",
      "Iteration 16829, loss = 0.01062682\n",
      "Iteration 16830, loss = 0.01062614\n",
      "Iteration 16831, loss = 0.01062546\n",
      "Iteration 16832, loss = 0.01062479\n",
      "Iteration 16833, loss = 0.01062411\n",
      "Iteration 16834, loss = 0.01062343\n",
      "Iteration 16835, loss = 0.01062275\n",
      "Iteration 16836, loss = 0.01062208\n",
      "Iteration 16837, loss = 0.01062140\n",
      "Iteration 16838, loss = 0.01062072\n",
      "Iteration 16839, loss = 0.01062004\n",
      "Iteration 16840, loss = 0.01061937\n",
      "Iteration 16841, loss = 0.01061869\n",
      "Iteration 16842, loss = 0.01061801\n",
      "Iteration 16843, loss = 0.01061734\n",
      "Iteration 16844, loss = 0.01061666\n",
      "Iteration 16845, loss = 0.01061598\n",
      "Iteration 16846, loss = 0.01061531\n",
      "Iteration 16847, loss = 0.01061463\n",
      "Iteration 16848, loss = 0.01061395\n",
      "Iteration 16849, loss = 0.01061328\n",
      "Iteration 16850, loss = 0.01061260\n",
      "Iteration 16851, loss = 0.01061193\n",
      "Iteration 16852, loss = 0.01061125\n",
      "Iteration 16853, loss = 0.01061057\n",
      "Iteration 16854, loss = 0.01060990\n",
      "Iteration 16855, loss = 0.01060922\n",
      "Iteration 16856, loss = 0.01060855\n",
      "Iteration 16857, loss = 0.01060787\n",
      "Iteration 16858, loss = 0.01060720\n",
      "Iteration 16859, loss = 0.01060652\n",
      "Iteration 16860, loss = 0.01060585\n",
      "Iteration 16861, loss = 0.01060517\n",
      "Iteration 16862, loss = 0.01060450\n",
      "Iteration 16863, loss = 0.01060382\n",
      "Iteration 16864, loss = 0.01060315\n",
      "Iteration 16865, loss = 0.01060247\n",
      "Iteration 16866, loss = 0.01060180\n",
      "Iteration 16867, loss = 0.01060113\n",
      "Iteration 16868, loss = 0.01060045\n",
      "Iteration 16869, loss = 0.01059978\n",
      "Iteration 16870, loss = 0.01059910\n",
      "Iteration 16871, loss = 0.01059843\n",
      "Iteration 16872, loss = 0.01059776\n",
      "Iteration 16873, loss = 0.01059708\n",
      "Iteration 16874, loss = 0.01059641\n",
      "Iteration 16875, loss = 0.01059574\n",
      "Iteration 16876, loss = 0.01059506\n",
      "Iteration 16877, loss = 0.01059439\n",
      "Iteration 16878, loss = 0.01059372\n",
      "Iteration 16879, loss = 0.01059305\n",
      "Iteration 16880, loss = 0.01059237\n",
      "Iteration 16881, loss = 0.01059170\n",
      "Iteration 16882, loss = 0.01059103\n",
      "Iteration 16883, loss = 0.01059036\n",
      "Iteration 16884, loss = 0.01058968\n",
      "Iteration 16885, loss = 0.01058901\n",
      "Iteration 16886, loss = 0.01058834\n",
      "Iteration 16887, loss = 0.01058767\n",
      "Iteration 16888, loss = 0.01058700\n",
      "Iteration 16889, loss = 0.01058633\n",
      "Iteration 16890, loss = 0.01058565\n",
      "Iteration 16891, loss = 0.01058498\n",
      "Iteration 16892, loss = 0.01058431\n",
      "Iteration 16893, loss = 0.01058364\n",
      "Iteration 16894, loss = 0.01058297\n",
      "Iteration 16895, loss = 0.01058230\n",
      "Iteration 16896, loss = 0.01058163\n",
      "Iteration 16897, loss = 0.01058096\n",
      "Iteration 16898, loss = 0.01058029\n",
      "Iteration 16899, loss = 0.01057962\n",
      "Iteration 16900, loss = 0.01057894\n",
      "Iteration 16901, loss = 0.01057827\n",
      "Iteration 16902, loss = 0.01057760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16903, loss = 0.01057693\n",
      "Iteration 16904, loss = 0.01057626\n",
      "Iteration 16905, loss = 0.01057559\n",
      "Iteration 16906, loss = 0.01057493\n",
      "Iteration 16907, loss = 0.01057426\n",
      "Iteration 16908, loss = 0.01057359\n",
      "Iteration 16909, loss = 0.01057292\n",
      "Iteration 16910, loss = 0.01057225\n",
      "Iteration 16911, loss = 0.01057158\n",
      "Iteration 16912, loss = 0.01057091\n",
      "Iteration 16913, loss = 0.01057024\n",
      "Iteration 16914, loss = 0.01056957\n",
      "Iteration 16915, loss = 0.01056890\n",
      "Iteration 16916, loss = 0.01056823\n",
      "Iteration 16917, loss = 0.01056757\n",
      "Iteration 16918, loss = 0.01056690\n",
      "Iteration 16919, loss = 0.01056623\n",
      "Iteration 16920, loss = 0.01056556\n",
      "Iteration 16921, loss = 0.01056489\n",
      "Iteration 16922, loss = 0.01056423\n",
      "Iteration 16923, loss = 0.01056356\n",
      "Iteration 16924, loss = 0.01056289\n",
      "Iteration 16925, loss = 0.01056222\n",
      "Iteration 16926, loss = 0.01056155\n",
      "Iteration 16927, loss = 0.01056089\n",
      "Iteration 16928, loss = 0.01056022\n",
      "Iteration 16929, loss = 0.01055955\n",
      "Iteration 16930, loss = 0.01055889\n",
      "Iteration 16931, loss = 0.01055822\n",
      "Iteration 16932, loss = 0.01055755\n",
      "Iteration 16933, loss = 0.01055689\n",
      "Iteration 16934, loss = 0.01055622\n",
      "Iteration 16935, loss = 0.01055555\n",
      "Iteration 16936, loss = 0.01055489\n",
      "Iteration 16937, loss = 0.01055422\n",
      "Iteration 16938, loss = 0.01055355\n",
      "Iteration 16939, loss = 0.01055289\n",
      "Iteration 16940, loss = 0.01055222\n",
      "Iteration 16941, loss = 0.01055156\n",
      "Iteration 16942, loss = 0.01055089\n",
      "Iteration 16943, loss = 0.01055023\n",
      "Iteration 16944, loss = 0.01054956\n",
      "Iteration 16945, loss = 0.01054889\n",
      "Iteration 16946, loss = 0.01054823\n",
      "Iteration 16947, loss = 0.01054756\n",
      "Iteration 16948, loss = 0.01054690\n",
      "Iteration 16949, loss = 0.01054623\n",
      "Iteration 16950, loss = 0.01054557\n",
      "Iteration 16951, loss = 0.01054490\n",
      "Iteration 16952, loss = 0.01054424\n",
      "Iteration 16953, loss = 0.01054358\n",
      "Iteration 16954, loss = 0.01054291\n",
      "Iteration 16955, loss = 0.01054225\n",
      "Iteration 16956, loss = 0.01054158\n",
      "Iteration 16957, loss = 0.01054092\n",
      "Iteration 16958, loss = 0.01054026\n",
      "Iteration 16959, loss = 0.01053959\n",
      "Iteration 16960, loss = 0.01053893\n",
      "Iteration 16961, loss = 0.01053826\n",
      "Iteration 16962, loss = 0.01053760\n",
      "Iteration 16963, loss = 0.01053694\n",
      "Iteration 16964, loss = 0.01053627\n",
      "Iteration 16965, loss = 0.01053561\n",
      "Iteration 16966, loss = 0.01053495\n",
      "Iteration 16967, loss = 0.01053429\n",
      "Iteration 16968, loss = 0.01053362\n",
      "Iteration 16969, loss = 0.01053296\n",
      "Iteration 16970, loss = 0.01053230\n",
      "Iteration 16971, loss = 0.01053163\n",
      "Iteration 16972, loss = 0.01053097\n",
      "Iteration 16973, loss = 0.01053031\n",
      "Iteration 16974, loss = 0.01052965\n",
      "Iteration 16975, loss = 0.01052899\n",
      "Iteration 16976, loss = 0.01052832\n",
      "Iteration 16977, loss = 0.01052766\n",
      "Iteration 16978, loss = 0.01052700\n",
      "Iteration 16979, loss = 0.01052634\n",
      "Iteration 16980, loss = 0.01052568\n",
      "Iteration 16981, loss = 0.01052502\n",
      "Iteration 16982, loss = 0.01052435\n",
      "Iteration 16983, loss = 0.01052369\n",
      "Iteration 16984, loss = 0.01052303\n",
      "Iteration 16985, loss = 0.01052237\n",
      "Iteration 16986, loss = 0.01052171\n",
      "Iteration 16987, loss = 0.01052105\n",
      "Iteration 16988, loss = 0.01052039\n",
      "Iteration 16989, loss = 0.01051973\n",
      "Iteration 16990, loss = 0.01051907\n",
      "Iteration 16991, loss = 0.01051841\n",
      "Iteration 16992, loss = 0.01051775\n",
      "Iteration 16993, loss = 0.01051709\n",
      "Iteration 16994, loss = 0.01051643\n",
      "Iteration 16995, loss = 0.01051577\n",
      "Iteration 16996, loss = 0.01051511\n",
      "Iteration 16997, loss = 0.01051445\n",
      "Iteration 16998, loss = 0.01051379\n",
      "Iteration 16999, loss = 0.01051313\n",
      "Iteration 17000, loss = 0.01051247\n",
      "Iteration 17001, loss = 0.01051181\n",
      "Iteration 17002, loss = 0.01051115\n",
      "Iteration 17003, loss = 0.01051049\n",
      "Iteration 17004, loss = 0.01050984\n",
      "Iteration 17005, loss = 0.01050918\n",
      "Iteration 17006, loss = 0.01050852\n",
      "Iteration 17007, loss = 0.01050786\n",
      "Iteration 17008, loss = 0.01050720\n",
      "Iteration 17009, loss = 0.01050654\n",
      "Iteration 17010, loss = 0.01050588\n",
      "Iteration 17011, loss = 0.01050523\n",
      "Iteration 17012, loss = 0.01050457\n",
      "Iteration 17013, loss = 0.01050391\n",
      "Iteration 17014, loss = 0.01050325\n",
      "Iteration 17015, loss = 0.01050260\n",
      "Iteration 17016, loss = 0.01050194\n",
      "Iteration 17017, loss = 0.01050128\n",
      "Iteration 17018, loss = 0.01050062\n",
      "Iteration 17019, loss = 0.01049997\n",
      "Iteration 17020, loss = 0.01049931\n",
      "Iteration 17021, loss = 0.01049865\n",
      "Iteration 17022, loss = 0.01049800\n",
      "Iteration 17023, loss = 0.01049734\n",
      "Iteration 17024, loss = 0.01049668\n",
      "Iteration 17025, loss = 0.01049603\n",
      "Iteration 17026, loss = 0.01049537\n",
      "Iteration 17027, loss = 0.01049471\n",
      "Iteration 17028, loss = 0.01049406\n",
      "Iteration 17029, loss = 0.01049340\n",
      "Iteration 17030, loss = 0.01049274\n",
      "Iteration 17031, loss = 0.01049209\n",
      "Iteration 17032, loss = 0.01049143\n",
      "Iteration 17033, loss = 0.01049078\n",
      "Iteration 17034, loss = 0.01049012\n",
      "Iteration 17035, loss = 0.01048947\n",
      "Iteration 17036, loss = 0.01048881\n",
      "Iteration 17037, loss = 0.01048816\n",
      "Iteration 17038, loss = 0.01048750\n",
      "Iteration 17039, loss = 0.01048685\n",
      "Iteration 17040, loss = 0.01048619\n",
      "Iteration 17041, loss = 0.01048554\n",
      "Iteration 17042, loss = 0.01048488\n",
      "Iteration 17043, loss = 0.01048423\n",
      "Iteration 17044, loss = 0.01048357\n",
      "Iteration 17045, loss = 0.01048292\n",
      "Iteration 17046, loss = 0.01048226\n",
      "Iteration 17047, loss = 0.01048161\n",
      "Iteration 17048, loss = 0.01048096\n",
      "Iteration 17049, loss = 0.01048030\n",
      "Iteration 17050, loss = 0.01047965\n",
      "Iteration 17051, loss = 0.01047899\n",
      "Iteration 17052, loss = 0.01047834\n",
      "Iteration 17053, loss = 0.01047769\n",
      "Iteration 17054, loss = 0.01047703\n",
      "Iteration 17055, loss = 0.01047638\n",
      "Iteration 17056, loss = 0.01047573\n",
      "Iteration 17057, loss = 0.01047507\n",
      "Iteration 17058, loss = 0.01047442\n",
      "Iteration 17059, loss = 0.01047377\n",
      "Iteration 17060, loss = 0.01047312\n",
      "Iteration 17061, loss = 0.01047246\n",
      "Iteration 17062, loss = 0.01047181\n",
      "Iteration 17063, loss = 0.01047116\n",
      "Iteration 17064, loss = 0.01047051\n",
      "Iteration 17065, loss = 0.01046985\n",
      "Iteration 17066, loss = 0.01046920\n",
      "Iteration 17067, loss = 0.01046855\n",
      "Iteration 17068, loss = 0.01046790\n",
      "Iteration 17069, loss = 0.01046725\n",
      "Iteration 17070, loss = 0.01046659\n",
      "Iteration 17071, loss = 0.01046594\n",
      "Iteration 17072, loss = 0.01046529\n",
      "Iteration 17073, loss = 0.01046464\n",
      "Iteration 17074, loss = 0.01046399\n",
      "Iteration 17075, loss = 0.01046334\n",
      "Iteration 17076, loss = 0.01046269\n",
      "Iteration 17077, loss = 0.01046204\n",
      "Iteration 17078, loss = 0.01046138\n",
      "Iteration 17079, loss = 0.01046073\n",
      "Iteration 17080, loss = 0.01046008\n",
      "Iteration 17081, loss = 0.01045943\n",
      "Iteration 17082, loss = 0.01045878\n",
      "Iteration 17083, loss = 0.01045813\n",
      "Iteration 17084, loss = 0.01045748\n",
      "Iteration 17085, loss = 0.01045683\n",
      "Iteration 17086, loss = 0.01045618\n",
      "Iteration 17087, loss = 0.01045553\n",
      "Iteration 17088, loss = 0.01045488\n",
      "Iteration 17089, loss = 0.01045423\n",
      "Iteration 17090, loss = 0.01045358\n",
      "Iteration 17091, loss = 0.01045293\n",
      "Iteration 17092, loss = 0.01045229\n",
      "Iteration 17093, loss = 0.01045164\n",
      "Iteration 17094, loss = 0.01045099\n",
      "Iteration 17095, loss = 0.01045034\n",
      "Iteration 17096, loss = 0.01044969\n",
      "Iteration 17097, loss = 0.01044904\n",
      "Iteration 17098, loss = 0.01044839\n",
      "Iteration 17099, loss = 0.01044774\n",
      "Iteration 17100, loss = 0.01044709\n",
      "Iteration 17101, loss = 0.01044645\n",
      "Iteration 17102, loss = 0.01044580\n",
      "Iteration 17103, loss = 0.01044515\n",
      "Iteration 17104, loss = 0.01044450\n",
      "Iteration 17105, loss = 0.01044385\n",
      "Iteration 17106, loss = 0.01044321\n",
      "Iteration 17107, loss = 0.01044256\n",
      "Iteration 17108, loss = 0.01044191\n",
      "Iteration 17109, loss = 0.01044126\n",
      "Iteration 17110, loss = 0.01044062\n",
      "Iteration 17111, loss = 0.01043997\n",
      "Iteration 17112, loss = 0.01043932\n",
      "Iteration 17113, loss = 0.01043868\n",
      "Iteration 17114, loss = 0.01043803\n",
      "Iteration 17115, loss = 0.01043738\n",
      "Iteration 17116, loss = 0.01043673\n",
      "Iteration 17117, loss = 0.01043609\n",
      "Iteration 17118, loss = 0.01043544\n",
      "Iteration 17119, loss = 0.01043480\n",
      "Iteration 17120, loss = 0.01043415\n",
      "Iteration 17121, loss = 0.01043350\n",
      "Iteration 17122, loss = 0.01043286\n",
      "Iteration 17123, loss = 0.01043221\n",
      "Iteration 17124, loss = 0.01043156\n",
      "Iteration 17125, loss = 0.01043092\n",
      "Iteration 17126, loss = 0.01043027\n",
      "Iteration 17127, loss = 0.01042963\n",
      "Iteration 17128, loss = 0.01042898\n",
      "Iteration 17129, loss = 0.01042834\n",
      "Iteration 17130, loss = 0.01042769\n",
      "Iteration 17131, loss = 0.01042705\n",
      "Iteration 17132, loss = 0.01042640\n",
      "Iteration 17133, loss = 0.01042576\n",
      "Iteration 17134, loss = 0.01042511\n",
      "Iteration 17135, loss = 0.01042447\n",
      "Iteration 17136, loss = 0.01042382\n",
      "Iteration 17137, loss = 0.01042318\n",
      "Iteration 17138, loss = 0.01042253\n",
      "Iteration 17139, loss = 0.01042189\n",
      "Iteration 17140, loss = 0.01042125\n",
      "Iteration 17141, loss = 0.01042060\n",
      "Iteration 17142, loss = 0.01041996\n",
      "Iteration 17143, loss = 0.01041931\n",
      "Iteration 17144, loss = 0.01041867\n",
      "Iteration 17145, loss = 0.01041803\n",
      "Iteration 17146, loss = 0.01041738\n",
      "Iteration 17147, loss = 0.01041674\n",
      "Iteration 17148, loss = 0.01041610\n",
      "Iteration 17149, loss = 0.01041545\n",
      "Iteration 17150, loss = 0.01041481\n",
      "Iteration 17151, loss = 0.01041417\n",
      "Iteration 17152, loss = 0.01041352\n",
      "Iteration 17153, loss = 0.01041288\n",
      "Iteration 17154, loss = 0.01041224\n",
      "Iteration 17155, loss = 0.01041160\n",
      "Iteration 17156, loss = 0.01041095\n",
      "Iteration 17157, loss = 0.01041031\n",
      "Iteration 17158, loss = 0.01040967\n",
      "Iteration 17159, loss = 0.01040903\n",
      "Iteration 17160, loss = 0.01040838\n",
      "Iteration 17161, loss = 0.01040774\n",
      "Iteration 17162, loss = 0.01040710\n",
      "Iteration 17163, loss = 0.01040646\n",
      "Iteration 17164, loss = 0.01040582\n",
      "Iteration 17165, loss = 0.01040518\n",
      "Iteration 17166, loss = 0.01040454\n",
      "Iteration 17167, loss = 0.01040389\n",
      "Iteration 17168, loss = 0.01040325\n",
      "Iteration 17169, loss = 0.01040261\n",
      "Iteration 17170, loss = 0.01040197\n",
      "Iteration 17171, loss = 0.01040133\n",
      "Iteration 17172, loss = 0.01040069\n",
      "Iteration 17173, loss = 0.01040005\n",
      "Iteration 17174, loss = 0.01039941\n",
      "Iteration 17175, loss = 0.01039877\n",
      "Iteration 17176, loss = 0.01039813\n",
      "Iteration 17177, loss = 0.01039749\n",
      "Iteration 17178, loss = 0.01039685\n",
      "Iteration 17179, loss = 0.01039621\n",
      "Iteration 17180, loss = 0.01039557\n",
      "Iteration 17181, loss = 0.01039493\n",
      "Iteration 17182, loss = 0.01039429\n",
      "Iteration 17183, loss = 0.01039365\n",
      "Iteration 17184, loss = 0.01039301\n",
      "Iteration 17185, loss = 0.01039237\n",
      "Iteration 17186, loss = 0.01039173\n",
      "Iteration 17187, loss = 0.01039109\n",
      "Iteration 17188, loss = 0.01039045\n",
      "Iteration 17189, loss = 0.01038981\n",
      "Iteration 17190, loss = 0.01038917\n",
      "Iteration 17191, loss = 0.01038853\n",
      "Iteration 17192, loss = 0.01038790\n",
      "Iteration 17193, loss = 0.01038726\n",
      "Iteration 17194, loss = 0.01038662\n",
      "Iteration 17195, loss = 0.01038598\n",
      "Iteration 17196, loss = 0.01038534\n",
      "Iteration 17197, loss = 0.01038470\n",
      "Iteration 17198, loss = 0.01038407\n",
      "Iteration 17199, loss = 0.01038343\n",
      "Iteration 17200, loss = 0.01038279\n",
      "Iteration 17201, loss = 0.01038215\n",
      "Iteration 17202, loss = 0.01038151\n",
      "Iteration 17203, loss = 0.01038088\n",
      "Iteration 17204, loss = 0.01038024\n",
      "Iteration 17205, loss = 0.01037960\n",
      "Iteration 17206, loss = 0.01037897\n",
      "Iteration 17207, loss = 0.01037833\n",
      "Iteration 17208, loss = 0.01037769\n",
      "Iteration 17209, loss = 0.01037705\n",
      "Iteration 17210, loss = 0.01037642\n",
      "Iteration 17211, loss = 0.01037578\n",
      "Iteration 17212, loss = 0.01037514\n",
      "Iteration 17213, loss = 0.01037451\n",
      "Iteration 17214, loss = 0.01037387\n",
      "Iteration 17215, loss = 0.01037324\n",
      "Iteration 17216, loss = 0.01037260\n",
      "Iteration 17217, loss = 0.01037196\n",
      "Iteration 17218, loss = 0.01037133\n",
      "Iteration 17219, loss = 0.01037069\n",
      "Iteration 17220, loss = 0.01037006\n",
      "Iteration 17221, loss = 0.01036942\n",
      "Iteration 17222, loss = 0.01036878\n",
      "Iteration 17223, loss = 0.01036815\n",
      "Iteration 17224, loss = 0.01036751\n",
      "Iteration 17225, loss = 0.01036688\n",
      "Iteration 17226, loss = 0.01036624\n",
      "Iteration 17227, loss = 0.01036561\n",
      "Iteration 17228, loss = 0.01036497\n",
      "Iteration 17229, loss = 0.01036434\n",
      "Iteration 17230, loss = 0.01036370\n",
      "Iteration 17231, loss = 0.01036307\n",
      "Iteration 17232, loss = 0.01036244\n",
      "Iteration 17233, loss = 0.01036180\n",
      "Iteration 17234, loss = 0.01036117\n",
      "Iteration 17235, loss = 0.01036053\n",
      "Iteration 17236, loss = 0.01035990\n",
      "Iteration 17237, loss = 0.01035926\n",
      "Iteration 17238, loss = 0.01035863\n",
      "Iteration 17239, loss = 0.01035800\n",
      "Iteration 17240, loss = 0.01035736\n",
      "Iteration 17241, loss = 0.01035673\n",
      "Iteration 17242, loss = 0.01035610\n",
      "Iteration 17243, loss = 0.01035546\n",
      "Iteration 17244, loss = 0.01035483\n",
      "Iteration 17245, loss = 0.01035420\n",
      "Iteration 17246, loss = 0.01035356\n",
      "Iteration 17247, loss = 0.01035293\n",
      "Iteration 17248, loss = 0.01035230\n",
      "Iteration 17249, loss = 0.01035166\n",
      "Iteration 17250, loss = 0.01035103\n",
      "Iteration 17251, loss = 0.01035040\n",
      "Iteration 17252, loss = 0.01034977\n",
      "Iteration 17253, loss = 0.01034914\n",
      "Iteration 17254, loss = 0.01034850\n",
      "Iteration 17255, loss = 0.01034787\n",
      "Iteration 17256, loss = 0.01034724\n",
      "Iteration 17257, loss = 0.01034661\n",
      "Iteration 17258, loss = 0.01034598\n",
      "Iteration 17259, loss = 0.01034534\n",
      "Iteration 17260, loss = 0.01034471\n",
      "Iteration 17261, loss = 0.01034408\n",
      "Iteration 17262, loss = 0.01034345\n",
      "Iteration 17263, loss = 0.01034282\n",
      "Iteration 17264, loss = 0.01034219\n",
      "Iteration 17265, loss = 0.01034156\n",
      "Iteration 17266, loss = 0.01034092\n",
      "Iteration 17267, loss = 0.01034029\n",
      "Iteration 17268, loss = 0.01033966\n",
      "Iteration 17269, loss = 0.01033903\n",
      "Iteration 17270, loss = 0.01033840\n",
      "Iteration 17271, loss = 0.01033777\n",
      "Iteration 17272, loss = 0.01033714\n",
      "Iteration 17273, loss = 0.01033651\n",
      "Iteration 17274, loss = 0.01033588\n",
      "Iteration 17275, loss = 0.01033525\n",
      "Iteration 17276, loss = 0.01033462\n",
      "Iteration 17277, loss = 0.01033399\n",
      "Iteration 17278, loss = 0.01033336\n",
      "Iteration 17279, loss = 0.01033273\n",
      "Iteration 17280, loss = 0.01033210\n",
      "Iteration 17281, loss = 0.01033147\n",
      "Iteration 17282, loss = 0.01033084\n",
      "Iteration 17283, loss = 0.01033021\n",
      "Iteration 17284, loss = 0.01032958\n",
      "Iteration 17285, loss = 0.01032896\n",
      "Iteration 17286, loss = 0.01032833\n",
      "Iteration 17287, loss = 0.01032770\n",
      "Iteration 17288, loss = 0.01032707\n",
      "Iteration 17289, loss = 0.01032644\n",
      "Iteration 17290, loss = 0.01032581\n",
      "Iteration 17291, loss = 0.01032518\n",
      "Iteration 17292, loss = 0.01032456\n",
      "Iteration 17293, loss = 0.01032393\n",
      "Iteration 17294, loss = 0.01032330\n",
      "Iteration 17295, loss = 0.01032267\n",
      "Iteration 17296, loss = 0.01032204\n",
      "Iteration 17297, loss = 0.01032142\n",
      "Iteration 17298, loss = 0.01032079\n",
      "Iteration 17299, loss = 0.01032016\n",
      "Iteration 17300, loss = 0.01031953\n",
      "Iteration 17301, loss = 0.01031891\n",
      "Iteration 17302, loss = 0.01031828\n",
      "Iteration 17303, loss = 0.01031765\n",
      "Iteration 17304, loss = 0.01031702\n",
      "Iteration 17305, loss = 0.01031640\n",
      "Iteration 17306, loss = 0.01031577\n",
      "Iteration 17307, loss = 0.01031514\n",
      "Iteration 17308, loss = 0.01031452\n",
      "Iteration 17309, loss = 0.01031389\n",
      "Iteration 17310, loss = 0.01031326\n",
      "Iteration 17311, loss = 0.01031264\n",
      "Iteration 17312, loss = 0.01031201\n",
      "Iteration 17313, loss = 0.01031139\n",
      "Iteration 17314, loss = 0.01031076\n",
      "Iteration 17315, loss = 0.01031013\n",
      "Iteration 17316, loss = 0.01030951\n",
      "Iteration 17317, loss = 0.01030888\n",
      "Iteration 17318, loss = 0.01030826\n",
      "Iteration 17319, loss = 0.01030763\n",
      "Iteration 17320, loss = 0.01030701\n",
      "Iteration 17321, loss = 0.01030638\n",
      "Iteration 17322, loss = 0.01030576\n",
      "Iteration 17323, loss = 0.01030513\n",
      "Iteration 17324, loss = 0.01030450\n",
      "Iteration 17325, loss = 0.01030388\n",
      "Iteration 17326, loss = 0.01030326\n",
      "Iteration 17327, loss = 0.01030263\n",
      "Iteration 17328, loss = 0.01030201\n",
      "Iteration 17329, loss = 0.01030138\n",
      "Iteration 17330, loss = 0.01030076\n",
      "Iteration 17331, loss = 0.01030013\n",
      "Iteration 17332, loss = 0.01029951\n",
      "Iteration 17333, loss = 0.01029888\n",
      "Iteration 17334, loss = 0.01029826\n",
      "Iteration 17335, loss = 0.01029764\n",
      "Iteration 17336, loss = 0.01029701\n",
      "Iteration 17337, loss = 0.01029639\n",
      "Iteration 17338, loss = 0.01029577\n",
      "Iteration 17339, loss = 0.01029514\n",
      "Iteration 17340, loss = 0.01029452\n",
      "Iteration 17341, loss = 0.01029390\n",
      "Iteration 17342, loss = 0.01029327\n",
      "Iteration 17343, loss = 0.01029265\n",
      "Iteration 17344, loss = 0.01029203\n",
      "Iteration 17345, loss = 0.01029140\n",
      "Iteration 17346, loss = 0.01029078\n",
      "Iteration 17347, loss = 0.01029016\n",
      "Iteration 17348, loss = 0.01028954\n",
      "Iteration 17349, loss = 0.01028891\n",
      "Iteration 17350, loss = 0.01028829\n",
      "Iteration 17351, loss = 0.01028767\n",
      "Iteration 17352, loss = 0.01028705\n",
      "Iteration 17353, loss = 0.01028642\n",
      "Iteration 17354, loss = 0.01028580\n",
      "Iteration 17355, loss = 0.01028518\n",
      "Iteration 17356, loss = 0.01028456\n",
      "Iteration 17357, loss = 0.01028394\n",
      "Iteration 17358, loss = 0.01028332\n",
      "Iteration 17359, loss = 0.01028269\n",
      "Iteration 17360, loss = 0.01028207\n",
      "Iteration 17361, loss = 0.01028145\n",
      "Iteration 17362, loss = 0.01028083\n",
      "Iteration 17363, loss = 0.01028021\n",
      "Iteration 17364, loss = 0.01027959\n",
      "Iteration 17365, loss = 0.01027897\n",
      "Iteration 17366, loss = 0.01027835\n",
      "Iteration 17367, loss = 0.01027773\n",
      "Iteration 17368, loss = 0.01027711\n",
      "Iteration 17369, loss = 0.01027648\n",
      "Iteration 17370, loss = 0.01027586\n",
      "Iteration 17371, loss = 0.01027524\n",
      "Iteration 17372, loss = 0.01027462\n",
      "Iteration 17373, loss = 0.01027400\n",
      "Iteration 17374, loss = 0.01027338\n",
      "Iteration 17375, loss = 0.01027276\n",
      "Iteration 17376, loss = 0.01027214\n",
      "Iteration 17377, loss = 0.01027152\n",
      "Iteration 17378, loss = 0.01027091\n",
      "Iteration 17379, loss = 0.01027029\n",
      "Iteration 17380, loss = 0.01026967\n",
      "Iteration 17381, loss = 0.01026905\n",
      "Iteration 17382, loss = 0.01026843\n",
      "Iteration 17383, loss = 0.01026781\n",
      "Iteration 17384, loss = 0.01026719\n",
      "Iteration 17385, loss = 0.01026657\n",
      "Iteration 17386, loss = 0.01026595\n",
      "Iteration 17387, loss = 0.01026533\n",
      "Iteration 17388, loss = 0.01026472\n",
      "Iteration 17389, loss = 0.01026410\n",
      "Iteration 17390, loss = 0.01026348\n",
      "Iteration 17391, loss = 0.01026286\n",
      "Iteration 17392, loss = 0.01026224\n",
      "Iteration 17393, loss = 0.01026162\n",
      "Iteration 17394, loss = 0.01026101\n",
      "Iteration 17395, loss = 0.01026039\n",
      "Iteration 17396, loss = 0.01025977\n",
      "Iteration 17397, loss = 0.01025915\n",
      "Iteration 17398, loss = 0.01025854\n",
      "Iteration 17399, loss = 0.01025792\n",
      "Iteration 17400, loss = 0.01025730\n",
      "Iteration 17401, loss = 0.01025668\n",
      "Iteration 17402, loss = 0.01025607\n",
      "Iteration 17403, loss = 0.01025545\n",
      "Iteration 17404, loss = 0.01025483\n",
      "Iteration 17405, loss = 0.01025422\n",
      "Iteration 17406, loss = 0.01025360\n",
      "Iteration 17407, loss = 0.01025298\n",
      "Iteration 17408, loss = 0.01025237\n",
      "Iteration 17409, loss = 0.01025175\n",
      "Iteration 17410, loss = 0.01025113\n",
      "Iteration 17411, loss = 0.01025052\n",
      "Iteration 17412, loss = 0.01024990\n",
      "Iteration 17413, loss = 0.01024928\n",
      "Iteration 17414, loss = 0.01024867\n",
      "Iteration 17415, loss = 0.01024805\n",
      "Iteration 17416, loss = 0.01024744\n",
      "Iteration 17417, loss = 0.01024682\n",
      "Iteration 17418, loss = 0.01024621\n",
      "Iteration 17419, loss = 0.01024559\n",
      "Iteration 17420, loss = 0.01024497\n",
      "Iteration 17421, loss = 0.01024436\n",
      "Iteration 17422, loss = 0.01024374\n",
      "Iteration 17423, loss = 0.01024313\n",
      "Iteration 17424, loss = 0.01024251\n",
      "Iteration 17425, loss = 0.01024190\n",
      "Iteration 17426, loss = 0.01024128\n",
      "Iteration 17427, loss = 0.01024067\n",
      "Iteration 17428, loss = 0.01024006\n",
      "Iteration 17429, loss = 0.01023944\n",
      "Iteration 17430, loss = 0.01023883\n",
      "Iteration 17431, loss = 0.01023821\n",
      "Iteration 17432, loss = 0.01023760\n",
      "Iteration 17433, loss = 0.01023698\n",
      "Iteration 17434, loss = 0.01023637\n",
      "Iteration 17435, loss = 0.01023576\n",
      "Iteration 17436, loss = 0.01023514\n",
      "Iteration 17437, loss = 0.01023453\n",
      "Iteration 17438, loss = 0.01023392\n",
      "Iteration 17439, loss = 0.01023330\n",
      "Iteration 17440, loss = 0.01023269\n",
      "Iteration 17441, loss = 0.01023208\n",
      "Iteration 17442, loss = 0.01023146\n",
      "Iteration 17443, loss = 0.01023085\n",
      "Iteration 17444, loss = 0.01023024\n",
      "Iteration 17445, loss = 0.01022962\n",
      "Iteration 17446, loss = 0.01022901\n",
      "Iteration 17447, loss = 0.01022840\n",
      "Iteration 17448, loss = 0.01022779\n",
      "Iteration 17449, loss = 0.01022717\n",
      "Iteration 17450, loss = 0.01022656\n",
      "Iteration 17451, loss = 0.01022595\n",
      "Iteration 17452, loss = 0.01022534\n",
      "Iteration 17453, loss = 0.01022472\n",
      "Iteration 17454, loss = 0.01022411\n",
      "Iteration 17455, loss = 0.01022350\n",
      "Iteration 17456, loss = 0.01022289\n",
      "Iteration 17457, loss = 0.01022228\n",
      "Iteration 17458, loss = 0.01022167\n",
      "Iteration 17459, loss = 0.01022105\n",
      "Iteration 17460, loss = 0.01022044\n",
      "Iteration 17461, loss = 0.01021983\n",
      "Iteration 17462, loss = 0.01021922\n",
      "Iteration 17463, loss = 0.01021861\n",
      "Iteration 17464, loss = 0.01021800\n",
      "Iteration 17465, loss = 0.01021739\n",
      "Iteration 17466, loss = 0.01021678\n",
      "Iteration 17467, loss = 0.01021617\n",
      "Iteration 17468, loss = 0.01021556\n",
      "Iteration 17469, loss = 0.01021495\n",
      "Iteration 17470, loss = 0.01021433\n",
      "Iteration 17471, loss = 0.01021372\n",
      "Iteration 17472, loss = 0.01021311\n",
      "Iteration 17473, loss = 0.01021250\n",
      "Iteration 17474, loss = 0.01021189\n",
      "Iteration 17475, loss = 0.01021128\n",
      "Iteration 17476, loss = 0.01021067\n",
      "Iteration 17477, loss = 0.01021006\n",
      "Iteration 17478, loss = 0.01020946\n",
      "Iteration 17479, loss = 0.01020885\n",
      "Iteration 17480, loss = 0.01020824\n",
      "Iteration 17481, loss = 0.01020763\n",
      "Iteration 17482, loss = 0.01020702\n",
      "Iteration 17483, loss = 0.01020641\n",
      "Iteration 17484, loss = 0.01020580\n",
      "Iteration 17485, loss = 0.01020519\n",
      "Iteration 17486, loss = 0.01020458\n",
      "Iteration 17487, loss = 0.01020397\n",
      "Iteration 17488, loss = 0.01020336\n",
      "Iteration 17489, loss = 0.01020276\n",
      "Iteration 17490, loss = 0.01020215\n",
      "Iteration 17491, loss = 0.01020154\n",
      "Iteration 17492, loss = 0.01020093\n",
      "Iteration 17493, loss = 0.01020032\n",
      "Iteration 17494, loss = 0.01019971\n",
      "Iteration 17495, loss = 0.01019911\n",
      "Iteration 17496, loss = 0.01019850\n",
      "Iteration 17497, loss = 0.01019789\n",
      "Iteration 17498, loss = 0.01019728\n",
      "Iteration 17499, loss = 0.01019668\n",
      "Iteration 17500, loss = 0.01019607\n",
      "Iteration 17501, loss = 0.01019546\n",
      "Iteration 17502, loss = 0.01019485\n",
      "Iteration 17503, loss = 0.01019425\n",
      "Iteration 17504, loss = 0.01019364\n",
      "Iteration 17505, loss = 0.01019303\n",
      "Iteration 17506, loss = 0.01019243\n",
      "Iteration 17507, loss = 0.01019182\n",
      "Iteration 17508, loss = 0.01019121\n",
      "Iteration 17509, loss = 0.01019061\n",
      "Iteration 17510, loss = 0.01019000\n",
      "Iteration 17511, loss = 0.01018939\n",
      "Iteration 17512, loss = 0.01018879\n",
      "Iteration 17513, loss = 0.01018818\n",
      "Iteration 17514, loss = 0.01018758\n",
      "Iteration 17515, loss = 0.01018697\n",
      "Iteration 17516, loss = 0.01018636\n",
      "Iteration 17517, loss = 0.01018576\n",
      "Iteration 17518, loss = 0.01018515\n",
      "Iteration 17519, loss = 0.01018455\n",
      "Iteration 17520, loss = 0.01018394\n",
      "Iteration 17521, loss = 0.01018334\n",
      "Iteration 17522, loss = 0.01018273\n",
      "Iteration 17523, loss = 0.01018212\n",
      "Iteration 17524, loss = 0.01018152\n",
      "Iteration 17525, loss = 0.01018091\n",
      "Iteration 17526, loss = 0.01018031\n",
      "Iteration 17527, loss = 0.01017971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17528, loss = 0.01017910\n",
      "Iteration 17529, loss = 0.01017850\n",
      "Iteration 17530, loss = 0.01017789\n",
      "Iteration 17531, loss = 0.01017729\n",
      "Iteration 17532, loss = 0.01017668\n",
      "Iteration 17533, loss = 0.01017608\n",
      "Iteration 17534, loss = 0.01017547\n",
      "Iteration 17535, loss = 0.01017487\n",
      "Iteration 17536, loss = 0.01017427\n",
      "Iteration 17537, loss = 0.01017366\n",
      "Iteration 17538, loss = 0.01017306\n",
      "Iteration 17539, loss = 0.01017246\n",
      "Iteration 17540, loss = 0.01017185\n",
      "Iteration 17541, loss = 0.01017125\n",
      "Iteration 17542, loss = 0.01017064\n",
      "Iteration 17543, loss = 0.01017004\n",
      "Iteration 17544, loss = 0.01016944\n",
      "Iteration 17545, loss = 0.01016884\n",
      "Iteration 17546, loss = 0.01016823\n",
      "Iteration 17547, loss = 0.01016763\n",
      "Iteration 17548, loss = 0.01016703\n",
      "Iteration 17549, loss = 0.01016642\n",
      "Iteration 17550, loss = 0.01016582\n",
      "Iteration 17551, loss = 0.01016522\n",
      "Iteration 17552, loss = 0.01016462\n",
      "Iteration 17553, loss = 0.01016401\n",
      "Iteration 17554, loss = 0.01016341\n",
      "Iteration 17555, loss = 0.01016281\n",
      "Iteration 17556, loss = 0.01016221\n",
      "Iteration 17557, loss = 0.01016161\n",
      "Iteration 17558, loss = 0.01016100\n",
      "Iteration 17559, loss = 0.01016040\n",
      "Iteration 17560, loss = 0.01015980\n",
      "Iteration 17561, loss = 0.01015920\n",
      "Iteration 17562, loss = 0.01015860\n",
      "Iteration 17563, loss = 0.01015800\n",
      "Iteration 17564, loss = 0.01015740\n",
      "Iteration 17565, loss = 0.01015679\n",
      "Iteration 17566, loss = 0.01015619\n",
      "Iteration 17567, loss = 0.01015559\n",
      "Iteration 17568, loss = 0.01015499\n",
      "Iteration 17569, loss = 0.01015439\n",
      "Iteration 17570, loss = 0.01015379\n",
      "Iteration 17571, loss = 0.01015319\n",
      "Iteration 17572, loss = 0.01015259\n",
      "Iteration 17573, loss = 0.01015199\n",
      "Iteration 17574, loss = 0.01015139\n",
      "Iteration 17575, loss = 0.01015079\n",
      "Iteration 17576, loss = 0.01015019\n",
      "Iteration 17577, loss = 0.01014959\n",
      "Iteration 17578, loss = 0.01014899\n",
      "Iteration 17579, loss = 0.01014839\n",
      "Iteration 17580, loss = 0.01014779\n",
      "Iteration 17581, loss = 0.01014719\n",
      "Iteration 17582, loss = 0.01014659\n",
      "Iteration 17583, loss = 0.01014599\n",
      "Iteration 17584, loss = 0.01014539\n",
      "Iteration 17585, loss = 0.01014479\n",
      "Iteration 17586, loss = 0.01014419\n",
      "Iteration 17587, loss = 0.01014359\n",
      "Iteration 17588, loss = 0.01014300\n",
      "Iteration 17589, loss = 0.01014240\n",
      "Iteration 17590, loss = 0.01014180\n",
      "Iteration 17591, loss = 0.01014120\n",
      "Iteration 17592, loss = 0.01014060\n",
      "Iteration 17593, loss = 0.01014000\n",
      "Iteration 17594, loss = 0.01013940\n",
      "Iteration 17595, loss = 0.01013881\n",
      "Iteration 17596, loss = 0.01013821\n",
      "Iteration 17597, loss = 0.01013761\n",
      "Iteration 17598, loss = 0.01013701\n",
      "Iteration 17599, loss = 0.01013641\n",
      "Iteration 17600, loss = 0.01013582\n",
      "Iteration 17601, loss = 0.01013522\n",
      "Iteration 17602, loss = 0.01013462\n",
      "Iteration 17603, loss = 0.01013402\n",
      "Iteration 17604, loss = 0.01013343\n",
      "Iteration 17605, loss = 0.01013283\n",
      "Iteration 17606, loss = 0.01013223\n",
      "Iteration 17607, loss = 0.01013163\n",
      "Iteration 17608, loss = 0.01013104\n",
      "Iteration 17609, loss = 0.01013044\n",
      "Iteration 17610, loss = 0.01012984\n",
      "Iteration 17611, loss = 0.01012925\n",
      "Iteration 17612, loss = 0.01012865\n",
      "Iteration 17613, loss = 0.01012805\n",
      "Iteration 17614, loss = 0.01012746\n",
      "Iteration 17615, loss = 0.01012686\n",
      "Iteration 17616, loss = 0.01012627\n",
      "Iteration 17617, loss = 0.01012567\n",
      "Iteration 17618, loss = 0.01012507\n",
      "Iteration 17619, loss = 0.01012448\n",
      "Iteration 17620, loss = 0.01012388\n",
      "Iteration 17621, loss = 0.01012329\n",
      "Iteration 17622, loss = 0.01012269\n",
      "Iteration 17623, loss = 0.01012209\n",
      "Iteration 17624, loss = 0.01012150\n",
      "Iteration 17625, loss = 0.01012090\n",
      "Iteration 17626, loss = 0.01012031\n",
      "Iteration 17627, loss = 0.01011971\n",
      "Iteration 17628, loss = 0.01011912\n",
      "Iteration 17629, loss = 0.01011852\n",
      "Iteration 17630, loss = 0.01011793\n",
      "Iteration 17631, loss = 0.01011733\n",
      "Iteration 17632, loss = 0.01011674\n",
      "Iteration 17633, loss = 0.01011614\n",
      "Iteration 17634, loss = 0.01011555\n",
      "Iteration 17635, loss = 0.01011496\n",
      "Iteration 17636, loss = 0.01011436\n",
      "Iteration 17637, loss = 0.01011377\n",
      "Iteration 17638, loss = 0.01011317\n",
      "Iteration 17639, loss = 0.01011258\n",
      "Iteration 17640, loss = 0.01011198\n",
      "Iteration 17641, loss = 0.01011139\n",
      "Iteration 17642, loss = 0.01011080\n",
      "Iteration 17643, loss = 0.01011020\n",
      "Iteration 17644, loss = 0.01010961\n",
      "Iteration 17645, loss = 0.01010902\n",
      "Iteration 17646, loss = 0.01010842\n",
      "Iteration 17647, loss = 0.01010783\n",
      "Iteration 17648, loss = 0.01010724\n",
      "Iteration 17649, loss = 0.01010664\n",
      "Iteration 17650, loss = 0.01010605\n",
      "Iteration 17651, loss = 0.01010546\n",
      "Iteration 17652, loss = 0.01010486\n",
      "Iteration 17653, loss = 0.01010427\n",
      "Iteration 17654, loss = 0.01010368\n",
      "Iteration 17655, loss = 0.01010309\n",
      "Iteration 17656, loss = 0.01010249\n",
      "Iteration 17657, loss = 0.01010190\n",
      "Iteration 17658, loss = 0.01010131\n",
      "Iteration 17659, loss = 0.01010072\n",
      "Iteration 17660, loss = 0.01010013\n",
      "Iteration 17661, loss = 0.01009953\n",
      "Iteration 17662, loss = 0.01009894\n",
      "Iteration 17663, loss = 0.01009835\n",
      "Iteration 17664, loss = 0.01009776\n",
      "Iteration 17665, loss = 0.01009717\n",
      "Iteration 17666, loss = 0.01009658\n",
      "Iteration 17667, loss = 0.01009598\n",
      "Iteration 17668, loss = 0.01009539\n",
      "Iteration 17669, loss = 0.01009480\n",
      "Iteration 17670, loss = 0.01009421\n",
      "Iteration 17671, loss = 0.01009362\n",
      "Iteration 17672, loss = 0.01009303\n",
      "Iteration 17673, loss = 0.01009244\n",
      "Iteration 17674, loss = 0.01009185\n",
      "Iteration 17675, loss = 0.01009126\n",
      "Iteration 17676, loss = 0.01009067\n",
      "Iteration 17677, loss = 0.01009008\n",
      "Iteration 17678, loss = 0.01008948\n",
      "Iteration 17679, loss = 0.01008889\n",
      "Iteration 17680, loss = 0.01008830\n",
      "Iteration 17681, loss = 0.01008771\n",
      "Iteration 17682, loss = 0.01008712\n",
      "Iteration 17683, loss = 0.01008653\n",
      "Iteration 17684, loss = 0.01008594\n",
      "Iteration 17685, loss = 0.01008536\n",
      "Iteration 17686, loss = 0.01008477\n",
      "Iteration 17687, loss = 0.01008418\n",
      "Iteration 17688, loss = 0.01008359\n",
      "Iteration 17689, loss = 0.01008300\n",
      "Iteration 17690, loss = 0.01008241\n",
      "Iteration 17691, loss = 0.01008182\n",
      "Iteration 17692, loss = 0.01008123\n",
      "Iteration 17693, loss = 0.01008064\n",
      "Iteration 17694, loss = 0.01008005\n",
      "Iteration 17695, loss = 0.01007946\n",
      "Iteration 17696, loss = 0.01007887\n",
      "Iteration 17697, loss = 0.01007829\n",
      "Iteration 17698, loss = 0.01007770\n",
      "Iteration 17699, loss = 0.01007711\n",
      "Iteration 17700, loss = 0.01007652\n",
      "Iteration 17701, loss = 0.01007593\n",
      "Iteration 17702, loss = 0.01007534\n",
      "Iteration 17703, loss = 0.01007476\n",
      "Iteration 17704, loss = 0.01007417\n",
      "Iteration 17705, loss = 0.01007358\n",
      "Iteration 17706, loss = 0.01007299\n",
      "Iteration 17707, loss = 0.01007241\n",
      "Iteration 17708, loss = 0.01007182\n",
      "Iteration 17709, loss = 0.01007123\n",
      "Iteration 17710, loss = 0.01007064\n",
      "Iteration 17711, loss = 0.01007006\n",
      "Iteration 17712, loss = 0.01006947\n",
      "Iteration 17713, loss = 0.01006888\n",
      "Iteration 17714, loss = 0.01006829\n",
      "Iteration 17715, loss = 0.01006771\n",
      "Iteration 17716, loss = 0.01006712\n",
      "Iteration 17717, loss = 0.01006653\n",
      "Iteration 17718, loss = 0.01006595\n",
      "Iteration 17719, loss = 0.01006536\n",
      "Iteration 17720, loss = 0.01006477\n",
      "Iteration 17721, loss = 0.01006419\n",
      "Iteration 17722, loss = 0.01006360\n",
      "Iteration 17723, loss = 0.01006302\n",
      "Iteration 17724, loss = 0.01006243\n",
      "Iteration 17725, loss = 0.01006184\n",
      "Iteration 17726, loss = 0.01006126\n",
      "Iteration 17727, loss = 0.01006067\n",
      "Iteration 17728, loss = 0.01006009\n",
      "Iteration 17729, loss = 0.01005950\n",
      "Iteration 17730, loss = 0.01005892\n",
      "Iteration 17731, loss = 0.01005833\n",
      "Iteration 17732, loss = 0.01005775\n",
      "Iteration 17733, loss = 0.01005716\n",
      "Iteration 17734, loss = 0.01005657\n",
      "Iteration 17735, loss = 0.01005599\n",
      "Iteration 17736, loss = 0.01005540\n",
      "Iteration 17737, loss = 0.01005482\n",
      "Iteration 17738, loss = 0.01005424\n",
      "Iteration 17739, loss = 0.01005365\n",
      "Iteration 17740, loss = 0.01005307\n",
      "Iteration 17741, loss = 0.01005248\n",
      "Iteration 17742, loss = 0.01005190\n",
      "Iteration 17743, loss = 0.01005131\n",
      "Iteration 17744, loss = 0.01005073\n",
      "Iteration 17745, loss = 0.01005014\n",
      "Iteration 17746, loss = 0.01004956\n",
      "Iteration 17747, loss = 0.01004898\n",
      "Iteration 17748, loss = 0.01004839\n",
      "Iteration 17749, loss = 0.01004781\n",
      "Iteration 17750, loss = 0.01004723\n",
      "Iteration 17751, loss = 0.01004664\n",
      "Iteration 17752, loss = 0.01004606\n",
      "Iteration 17753, loss = 0.01004548\n",
      "Iteration 17754, loss = 0.01004489\n",
      "Iteration 17755, loss = 0.01004431\n",
      "Iteration 17756, loss = 0.01004373\n",
      "Iteration 17757, loss = 0.01004314\n",
      "Iteration 17758, loss = 0.01004256\n",
      "Iteration 17759, loss = 0.01004198\n",
      "Iteration 17760, loss = 0.01004139\n",
      "Iteration 17761, loss = 0.01004081\n",
      "Iteration 17762, loss = 0.01004023\n",
      "Iteration 17763, loss = 0.01003965\n",
      "Iteration 17764, loss = 0.01003906\n",
      "Iteration 17765, loss = 0.01003848\n",
      "Iteration 17766, loss = 0.01003790\n",
      "Iteration 17767, loss = 0.01003732\n",
      "Iteration 17768, loss = 0.01003674\n",
      "Iteration 17769, loss = 0.01003615\n",
      "Iteration 17770, loss = 0.01003557\n",
      "Iteration 17771, loss = 0.01003499\n",
      "Iteration 17772, loss = 0.01003441\n",
      "Iteration 17773, loss = 0.01003383\n",
      "Iteration 17774, loss = 0.01003325\n",
      "Iteration 17775, loss = 0.01003266\n",
      "Iteration 17776, loss = 0.01003208\n",
      "Iteration 17777, loss = 0.01003150\n",
      "Iteration 17778, loss = 0.01003092\n",
      "Iteration 17779, loss = 0.01003034\n",
      "Iteration 17780, loss = 0.01002976\n",
      "Iteration 17781, loss = 0.01002918\n",
      "Iteration 17782, loss = 0.01002860\n",
      "Iteration 17783, loss = 0.01002802\n",
      "Iteration 17784, loss = 0.01002744\n",
      "Iteration 17785, loss = 0.01002686\n",
      "Iteration 17786, loss = 0.01002628\n",
      "Iteration 17787, loss = 0.01002570\n",
      "Iteration 17788, loss = 0.01002512\n",
      "Iteration 17789, loss = 0.01002454\n",
      "Iteration 17790, loss = 0.01002396\n",
      "Iteration 17791, loss = 0.01002338\n",
      "Iteration 17792, loss = 0.01002280\n",
      "Iteration 17793, loss = 0.01002222\n",
      "Iteration 17794, loss = 0.01002164\n",
      "Iteration 17795, loss = 0.01002106\n",
      "Iteration 17796, loss = 0.01002048\n",
      "Iteration 17797, loss = 0.01001990\n",
      "Iteration 17798, loss = 0.01001932\n",
      "Iteration 17799, loss = 0.01001874\n",
      "Iteration 17800, loss = 0.01001816\n",
      "Iteration 17801, loss = 0.01001758\n",
      "Iteration 17802, loss = 0.01001700\n",
      "Iteration 17803, loss = 0.01001643\n",
      "Iteration 17804, loss = 0.01001585\n",
      "Iteration 17805, loss = 0.01001527\n",
      "Iteration 17806, loss = 0.01001469\n",
      "Iteration 17807, loss = 0.01001411\n",
      "Iteration 17808, loss = 0.01001353\n",
      "Iteration 17809, loss = 0.01001295\n",
      "Iteration 17810, loss = 0.01001238\n",
      "Iteration 17811, loss = 0.01001180\n",
      "Iteration 17812, loss = 0.01001122\n",
      "Iteration 17813, loss = 0.01001064\n",
      "Iteration 17814, loss = 0.01001007\n",
      "Iteration 17815, loss = 0.01000949\n",
      "Iteration 17816, loss = 0.01000891\n",
      "Iteration 17817, loss = 0.01000833\n",
      "Iteration 17818, loss = 0.01000776\n",
      "Iteration 17819, loss = 0.01000718\n",
      "Iteration 17820, loss = 0.01000660\n",
      "Iteration 17821, loss = 0.01000602\n",
      "Iteration 17822, loss = 0.01000545\n",
      "Iteration 17823, loss = 0.01000487\n",
      "Iteration 17824, loss = 0.01000429\n",
      "Iteration 17825, loss = 0.01000372\n",
      "Iteration 17826, loss = 0.01000314\n",
      "Iteration 17827, loss = 0.01000256\n",
      "Iteration 17828, loss = 0.01000199\n",
      "Iteration 17829, loss = 0.01000141\n",
      "Iteration 17830, loss = 0.01000083\n",
      "Iteration 17831, loss = 0.01000026\n",
      "Iteration 17832, loss = 0.00999968\n",
      "Iteration 17833, loss = 0.00999911\n",
      "Iteration 17834, loss = 0.00999853\n",
      "Iteration 17835, loss = 0.00999795\n",
      "Iteration 17836, loss = 0.00999738\n",
      "Iteration 17837, loss = 0.00999680\n",
      "Iteration 17838, loss = 0.00999623\n",
      "Iteration 17839, loss = 0.00999565\n",
      "Iteration 17840, loss = 0.00999508\n",
      "Iteration 17841, loss = 0.00999450\n",
      "Iteration 17842, loss = 0.00999393\n",
      "Iteration 17843, loss = 0.00999335\n",
      "Iteration 17844, loss = 0.00999278\n",
      "Iteration 17845, loss = 0.00999220\n",
      "Iteration 17846, loss = 0.00999163\n",
      "Iteration 17847, loss = 0.00999105\n",
      "Iteration 17848, loss = 0.00999048\n",
      "Iteration 17849, loss = 0.00998990\n",
      "Iteration 17850, loss = 0.00998933\n",
      "Iteration 17851, loss = 0.00998875\n",
      "Iteration 17852, loss = 0.00998818\n",
      "Iteration 17853, loss = 0.00998760\n",
      "Iteration 17854, loss = 0.00998703\n",
      "Iteration 17855, loss = 0.00998646\n",
      "Iteration 17856, loss = 0.00998588\n",
      "Iteration 17857, loss = 0.00998531\n",
      "Iteration 17858, loss = 0.00998473\n",
      "Iteration 17859, loss = 0.00998416\n",
      "Iteration 17860, loss = 0.00998359\n",
      "Iteration 17861, loss = 0.00998301\n",
      "Iteration 17862, loss = 0.00998244\n",
      "Iteration 17863, loss = 0.00998187\n",
      "Iteration 17864, loss = 0.00998129\n",
      "Iteration 17865, loss = 0.00998072\n",
      "Iteration 17866, loss = 0.00998015\n",
      "Iteration 17867, loss = 0.00997957\n",
      "Iteration 17868, loss = 0.00997900\n",
      "Iteration 17869, loss = 0.00997843\n",
      "Iteration 17870, loss = 0.00997786\n",
      "Iteration 17871, loss = 0.00997728\n",
      "Iteration 17872, loss = 0.00997671\n",
      "Iteration 17873, loss = 0.00997614\n",
      "Iteration 17874, loss = 0.00997557\n",
      "Iteration 17875, loss = 0.00997499\n",
      "Iteration 17876, loss = 0.00997442\n",
      "Iteration 17877, loss = 0.00997385\n",
      "Iteration 17878, loss = 0.00997328\n",
      "Iteration 17879, loss = 0.00997271\n",
      "Iteration 17880, loss = 0.00997213\n",
      "Iteration 17881, loss = 0.00997156\n",
      "Iteration 17882, loss = 0.00997099\n",
      "Iteration 17883, loss = 0.00997042\n",
      "Iteration 17884, loss = 0.00996985\n",
      "Iteration 17885, loss = 0.00996928\n",
      "Iteration 17886, loss = 0.00996871\n",
      "Iteration 17887, loss = 0.00996814\n",
      "Iteration 17888, loss = 0.00996756\n",
      "Iteration 17889, loss = 0.00996699\n",
      "Iteration 17890, loss = 0.00996642\n",
      "Iteration 17891, loss = 0.00996585\n",
      "Iteration 17892, loss = 0.00996528\n",
      "Iteration 17893, loss = 0.00996471\n",
      "Iteration 17894, loss = 0.00996414\n",
      "Iteration 17895, loss = 0.00996357\n",
      "Iteration 17896, loss = 0.00996300\n",
      "Iteration 17897, loss = 0.00996243\n",
      "Iteration 17898, loss = 0.00996186\n",
      "Iteration 17899, loss = 0.00996129\n",
      "Iteration 17900, loss = 0.00996072\n",
      "Iteration 17901, loss = 0.00996015\n",
      "Iteration 17902, loss = 0.00995958\n",
      "Iteration 17903, loss = 0.00995901\n",
      "Iteration 17904, loss = 0.00995844\n",
      "Iteration 17905, loss = 0.00995787\n",
      "Iteration 17906, loss = 0.00995730\n",
      "Iteration 17907, loss = 0.00995673\n",
      "Iteration 17908, loss = 0.00995616\n",
      "Iteration 17909, loss = 0.00995559\n",
      "Iteration 17910, loss = 0.00995502\n",
      "Iteration 17911, loss = 0.00995445\n",
      "Iteration 17912, loss = 0.00995389\n",
      "Iteration 17913, loss = 0.00995332\n",
      "Iteration 17914, loss = 0.00995275\n",
      "Iteration 17915, loss = 0.00995218\n",
      "Iteration 17916, loss = 0.00995161\n",
      "Iteration 17917, loss = 0.00995104\n",
      "Iteration 17918, loss = 0.00995047\n",
      "Iteration 17919, loss = 0.00994991\n",
      "Iteration 17920, loss = 0.00994934\n",
      "Iteration 17921, loss = 0.00994877\n",
      "Iteration 17922, loss = 0.00994820\n",
      "Iteration 17923, loss = 0.00994763\n",
      "Iteration 17924, loss = 0.00994707\n",
      "Iteration 17925, loss = 0.00994650\n",
      "Iteration 17926, loss = 0.00994593\n",
      "Iteration 17927, loss = 0.00994536\n",
      "Iteration 17928, loss = 0.00994480\n",
      "Iteration 17929, loss = 0.00994423\n",
      "Iteration 17930, loss = 0.00994366\n",
      "Iteration 17931, loss = 0.00994309\n",
      "Iteration 17932, loss = 0.00994253\n",
      "Iteration 17933, loss = 0.00994196\n",
      "Iteration 17934, loss = 0.00994139\n",
      "Iteration 17935, loss = 0.00994083\n",
      "Iteration 17936, loss = 0.00994026\n",
      "Iteration 17937, loss = 0.00993969\n",
      "Iteration 17938, loss = 0.00993913\n",
      "Iteration 17939, loss = 0.00993856\n",
      "Iteration 17940, loss = 0.00993799\n",
      "Iteration 17941, loss = 0.00993743\n",
      "Iteration 17942, loss = 0.00993686\n",
      "Iteration 17943, loss = 0.00993629\n",
      "Iteration 17944, loss = 0.00993573\n",
      "Iteration 17945, loss = 0.00993516\n",
      "Iteration 17946, loss = 0.00993460\n",
      "Iteration 17947, loss = 0.00993403\n",
      "Iteration 17948, loss = 0.00993347\n",
      "Iteration 17949, loss = 0.00993290\n",
      "Iteration 17950, loss = 0.00993233\n",
      "Iteration 17951, loss = 0.00993177\n",
      "Iteration 17952, loss = 0.00993120\n",
      "Iteration 17953, loss = 0.00993064\n",
      "Iteration 17954, loss = 0.00993007\n",
      "Iteration 17955, loss = 0.00992951\n",
      "Iteration 17956, loss = 0.00992894\n",
      "Iteration 17957, loss = 0.00992838\n",
      "Iteration 17958, loss = 0.00992781\n",
      "Iteration 17959, loss = 0.00992725\n",
      "Iteration 17960, loss = 0.00992668\n",
      "Iteration 17961, loss = 0.00992612\n",
      "Iteration 17962, loss = 0.00992556\n",
      "Iteration 17963, loss = 0.00992499\n",
      "Iteration 17964, loss = 0.00992443\n",
      "Iteration 17965, loss = 0.00992386\n",
      "Iteration 17966, loss = 0.00992330\n",
      "Iteration 17967, loss = 0.00992273\n",
      "Iteration 17968, loss = 0.00992217\n",
      "Iteration 17969, loss = 0.00992161\n",
      "Iteration 17970, loss = 0.00992104\n",
      "Iteration 17971, loss = 0.00992048\n",
      "Iteration 17972, loss = 0.00991992\n",
      "Iteration 17973, loss = 0.00991935\n",
      "Iteration 17974, loss = 0.00991879\n",
      "Iteration 17975, loss = 0.00991823\n",
      "Iteration 17976, loss = 0.00991766\n",
      "Iteration 17977, loss = 0.00991710\n",
      "Iteration 17978, loss = 0.00991654\n",
      "Iteration 17979, loss = 0.00991597\n",
      "Iteration 17980, loss = 0.00991541\n",
      "Iteration 17981, loss = 0.00991485\n",
      "Iteration 17982, loss = 0.00991428\n",
      "Iteration 17983, loss = 0.00991372\n",
      "Iteration 17984, loss = 0.00991316\n",
      "Iteration 17985, loss = 0.00991260\n",
      "Iteration 17986, loss = 0.00991203\n",
      "Iteration 17987, loss = 0.00991147\n",
      "Iteration 17988, loss = 0.00991091\n",
      "Iteration 17989, loss = 0.00991035\n",
      "Iteration 17990, loss = 0.00990979\n",
      "Iteration 17991, loss = 0.00990922\n",
      "Iteration 17992, loss = 0.00990866\n",
      "Iteration 17993, loss = 0.00990810\n",
      "Iteration 17994, loss = 0.00990754\n",
      "Iteration 17995, loss = 0.00990698\n",
      "Iteration 17996, loss = 0.00990642\n",
      "Iteration 17997, loss = 0.00990585\n",
      "Iteration 17998, loss = 0.00990529\n",
      "Iteration 17999, loss = 0.00990473\n",
      "Iteration 18000, loss = 0.00990417\n",
      "Iteration 18001, loss = 0.00990361\n",
      "Iteration 18002, loss = 0.00990305\n",
      "Iteration 18003, loss = 0.00990249\n",
      "Iteration 18004, loss = 0.00990193\n",
      "Iteration 18005, loss = 0.00990137\n",
      "Iteration 18006, loss = 0.00990081\n",
      "Iteration 18007, loss = 0.00990025\n",
      "Iteration 18008, loss = 0.00989969\n",
      "Iteration 18009, loss = 0.00989913\n",
      "Iteration 18010, loss = 0.00989857\n",
      "Iteration 18011, loss = 0.00989801\n",
      "Iteration 18012, loss = 0.00989745\n",
      "Iteration 18013, loss = 0.00989689\n",
      "Iteration 18014, loss = 0.00989633\n",
      "Iteration 18015, loss = 0.00989577\n",
      "Iteration 18016, loss = 0.00989521\n",
      "Iteration 18017, loss = 0.00989465\n",
      "Iteration 18018, loss = 0.00989409\n",
      "Iteration 18019, loss = 0.00989353\n",
      "Iteration 18020, loss = 0.00989297\n",
      "Iteration 18021, loss = 0.00989241\n",
      "Iteration 18022, loss = 0.00989185\n",
      "Iteration 18023, loss = 0.00989129\n",
      "Iteration 18024, loss = 0.00989073\n",
      "Iteration 18025, loss = 0.00989017\n",
      "Iteration 18026, loss = 0.00988961\n",
      "Iteration 18027, loss = 0.00988905\n",
      "Iteration 18028, loss = 0.00988850\n",
      "Iteration 18029, loss = 0.00988794\n",
      "Iteration 18030, loss = 0.00988738\n",
      "Iteration 18031, loss = 0.00988682\n",
      "Iteration 18032, loss = 0.00988626\n",
      "Iteration 18033, loss = 0.00988570\n",
      "Iteration 18034, loss = 0.00988515\n",
      "Iteration 18035, loss = 0.00988459\n",
      "Iteration 18036, loss = 0.00988403\n",
      "Iteration 18037, loss = 0.00988347\n",
      "Iteration 18038, loss = 0.00988291\n",
      "Iteration 18039, loss = 0.00988236\n",
      "Iteration 18040, loss = 0.00988180\n",
      "Iteration 18041, loss = 0.00988124\n",
      "Iteration 18042, loss = 0.00988068\n",
      "Iteration 18043, loss = 0.00988013\n",
      "Iteration 18044, loss = 0.00987957\n",
      "Iteration 18045, loss = 0.00987901\n",
      "Iteration 18046, loss = 0.00987846\n",
      "Iteration 18047, loss = 0.00987790\n",
      "Iteration 18048, loss = 0.00987734\n",
      "Iteration 18049, loss = 0.00987678\n",
      "Iteration 18050, loss = 0.00987623\n",
      "Iteration 18051, loss = 0.00987567\n",
      "Iteration 18052, loss = 0.00987511\n",
      "Iteration 18053, loss = 0.00987456\n",
      "Iteration 18054, loss = 0.00987400\n",
      "Iteration 18055, loss = 0.00987345\n",
      "Iteration 18056, loss = 0.00987289\n",
      "Iteration 18057, loss = 0.00987233\n",
      "Iteration 18058, loss = 0.00987178\n",
      "Iteration 18059, loss = 0.00987122\n",
      "Iteration 18060, loss = 0.00987067\n",
      "Iteration 18061, loss = 0.00987011\n",
      "Iteration 18062, loss = 0.00986955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18063, loss = 0.00986900\n",
      "Iteration 18064, loss = 0.00986844\n",
      "Iteration 18065, loss = 0.00986789\n",
      "Iteration 18066, loss = 0.00986733\n",
      "Iteration 18067, loss = 0.00986678\n",
      "Iteration 18068, loss = 0.00986622\n",
      "Iteration 18069, loss = 0.00986567\n",
      "Iteration 18070, loss = 0.00986511\n",
      "Iteration 18071, loss = 0.00986456\n",
      "Iteration 18072, loss = 0.00986400\n",
      "Iteration 18073, loss = 0.00986345\n",
      "Iteration 18074, loss = 0.00986289\n",
      "Iteration 18075, loss = 0.00986234\n",
      "Iteration 18076, loss = 0.00986178\n",
      "Iteration 18077, loss = 0.00986123\n",
      "Iteration 18078, loss = 0.00986067\n",
      "Iteration 18079, loss = 0.00986012\n",
      "Iteration 18080, loss = 0.00985957\n",
      "Iteration 18081, loss = 0.00985901\n",
      "Iteration 18082, loss = 0.00985846\n",
      "Iteration 18083, loss = 0.00985790\n",
      "Iteration 18084, loss = 0.00985735\n",
      "Iteration 18085, loss = 0.00985680\n",
      "Iteration 18086, loss = 0.00985624\n",
      "Iteration 18087, loss = 0.00985569\n",
      "Iteration 18088, loss = 0.00985514\n",
      "Iteration 18089, loss = 0.00985458\n",
      "Iteration 18090, loss = 0.00985403\n",
      "Iteration 18091, loss = 0.00985348\n",
      "Iteration 18092, loss = 0.00985292\n",
      "Iteration 18093, loss = 0.00985237\n",
      "Iteration 18094, loss = 0.00985182\n",
      "Iteration 18095, loss = 0.00985126\n",
      "Iteration 18096, loss = 0.00985071\n",
      "Iteration 18097, loss = 0.00985016\n",
      "Iteration 18098, loss = 0.00984961\n",
      "Iteration 18099, loss = 0.00984905\n",
      "Iteration 18100, loss = 0.00984850\n",
      "Iteration 18101, loss = 0.00984795\n",
      "Iteration 18102, loss = 0.00984740\n",
      "Iteration 18103, loss = 0.00984684\n",
      "Iteration 18104, loss = 0.00984629\n",
      "Iteration 18105, loss = 0.00984574\n",
      "Iteration 18106, loss = 0.00984519\n",
      "Iteration 18107, loss = 0.00984464\n",
      "Iteration 18108, loss = 0.00984408\n",
      "Iteration 18109, loss = 0.00984353\n",
      "Iteration 18110, loss = 0.00984298\n",
      "Iteration 18111, loss = 0.00984243\n",
      "Iteration 18112, loss = 0.00984188\n",
      "Iteration 18113, loss = 0.00984133\n",
      "Iteration 18114, loss = 0.00984078\n",
      "Iteration 18115, loss = 0.00984022\n",
      "Iteration 18116, loss = 0.00983967\n",
      "Iteration 18117, loss = 0.00983912\n",
      "Iteration 18118, loss = 0.00983857\n",
      "Iteration 18119, loss = 0.00983802\n",
      "Iteration 18120, loss = 0.00983747\n",
      "Iteration 18121, loss = 0.00983692\n",
      "Iteration 18122, loss = 0.00983637\n",
      "Iteration 18123, loss = 0.00983582\n",
      "Iteration 18124, loss = 0.00983527\n",
      "Iteration 18125, loss = 0.00983472\n",
      "Iteration 18126, loss = 0.00983417\n",
      "Iteration 18127, loss = 0.00983362\n",
      "Iteration 18128, loss = 0.00983307\n",
      "Iteration 18129, loss = 0.00983252\n",
      "Iteration 18130, loss = 0.00983197\n",
      "Iteration 18131, loss = 0.00983142\n",
      "Iteration 18132, loss = 0.00983087\n",
      "Iteration 18133, loss = 0.00983032\n",
      "Iteration 18134, loss = 0.00982977\n",
      "Iteration 18135, loss = 0.00982922\n",
      "Iteration 18136, loss = 0.00982867\n",
      "Iteration 18137, loss = 0.00982812\n",
      "Iteration 18138, loss = 0.00982757\n",
      "Iteration 18139, loss = 0.00982702\n",
      "Iteration 18140, loss = 0.00982647\n",
      "Iteration 18141, loss = 0.00982593\n",
      "Iteration 18142, loss = 0.00982538\n",
      "Iteration 18143, loss = 0.00982483\n",
      "Iteration 18144, loss = 0.00982428\n",
      "Iteration 18145, loss = 0.00982373\n",
      "Iteration 18146, loss = 0.00982318\n",
      "Iteration 18147, loss = 0.00982263\n",
      "Iteration 18148, loss = 0.00982208\n",
      "Iteration 18149, loss = 0.00982154\n",
      "Iteration 18150, loss = 0.00982099\n",
      "Iteration 18151, loss = 0.00982044\n",
      "Iteration 18152, loss = 0.00981989\n",
      "Iteration 18153, loss = 0.00981934\n",
      "Iteration 18154, loss = 0.00981880\n",
      "Iteration 18155, loss = 0.00981825\n",
      "Iteration 18156, loss = 0.00981770\n",
      "Iteration 18157, loss = 0.00981715\n",
      "Iteration 18158, loss = 0.00981661\n",
      "Iteration 18159, loss = 0.00981606\n",
      "Iteration 18160, loss = 0.00981551\n",
      "Iteration 18161, loss = 0.00981496\n",
      "Iteration 18162, loss = 0.00981442\n",
      "Iteration 18163, loss = 0.00981387\n",
      "Iteration 18164, loss = 0.00981332\n",
      "Iteration 18165, loss = 0.00981278\n",
      "Iteration 18166, loss = 0.00981223\n",
      "Iteration 18167, loss = 0.00981168\n",
      "Iteration 18168, loss = 0.00981114\n",
      "Iteration 18169, loss = 0.00981059\n",
      "Iteration 18170, loss = 0.00981004\n",
      "Iteration 18171, loss = 0.00980950\n",
      "Iteration 18172, loss = 0.00980895\n",
      "Iteration 18173, loss = 0.00980840\n",
      "Iteration 18174, loss = 0.00980786\n",
      "Iteration 18175, loss = 0.00980731\n",
      "Iteration 18176, loss = 0.00980677\n",
      "Iteration 18177, loss = 0.00980622\n",
      "Iteration 18178, loss = 0.00980567\n",
      "Iteration 18179, loss = 0.00980513\n",
      "Iteration 18180, loss = 0.00980458\n",
      "Iteration 18181, loss = 0.00980404\n",
      "Iteration 18182, loss = 0.00980349\n",
      "Iteration 18183, loss = 0.00980295\n",
      "Iteration 18184, loss = 0.00980240\n",
      "Iteration 18185, loss = 0.00980186\n",
      "Iteration 18186, loss = 0.00980131\n",
      "Iteration 18187, loss = 0.00980077\n",
      "Iteration 18188, loss = 0.00980022\n",
      "Iteration 18189, loss = 0.00979968\n",
      "Iteration 18190, loss = 0.00979913\n",
      "Iteration 18191, loss = 0.00979859\n",
      "Iteration 18192, loss = 0.00979804\n",
      "Iteration 18193, loss = 0.00979750\n",
      "Iteration 18194, loss = 0.00979695\n",
      "Iteration 18195, loss = 0.00979641\n",
      "Iteration 18196, loss = 0.00979586\n",
      "Iteration 18197, loss = 0.00979532\n",
      "Iteration 18198, loss = 0.00979478\n",
      "Iteration 18199, loss = 0.00979423\n",
      "Iteration 18200, loss = 0.00979369\n",
      "Iteration 18201, loss = 0.00979314\n",
      "Iteration 18202, loss = 0.00979260\n",
      "Iteration 18203, loss = 0.00979206\n",
      "Iteration 18204, loss = 0.00979151\n",
      "Iteration 18205, loss = 0.00979097\n",
      "Iteration 18206, loss = 0.00979043\n",
      "Iteration 18207, loss = 0.00978988\n",
      "Iteration 18208, loss = 0.00978934\n",
      "Iteration 18209, loss = 0.00978880\n",
      "Iteration 18210, loss = 0.00978825\n",
      "Iteration 18211, loss = 0.00978771\n",
      "Iteration 18212, loss = 0.00978717\n",
      "Iteration 18213, loss = 0.00978662\n",
      "Iteration 18214, loss = 0.00978608\n",
      "Iteration 18215, loss = 0.00978554\n",
      "Iteration 18216, loss = 0.00978500\n",
      "Iteration 18217, loss = 0.00978445\n",
      "Iteration 18218, loss = 0.00978391\n",
      "Iteration 18219, loss = 0.00978337\n",
      "Iteration 18220, loss = 0.00978283\n",
      "Iteration 18221, loss = 0.00978229\n",
      "Iteration 18222, loss = 0.00978174\n",
      "Iteration 18223, loss = 0.00978120\n",
      "Iteration 18224, loss = 0.00978066\n",
      "Iteration 18225, loss = 0.00978012\n",
      "Iteration 18226, loss = 0.00977958\n",
      "Iteration 18227, loss = 0.00977903\n",
      "Iteration 18228, loss = 0.00977849\n",
      "Iteration 18229, loss = 0.00977795\n",
      "Iteration 18230, loss = 0.00977741\n",
      "Iteration 18231, loss = 0.00977687\n",
      "Iteration 18232, loss = 0.00977633\n",
      "Iteration 18233, loss = 0.00977579\n",
      "Iteration 18234, loss = 0.00977524\n",
      "Iteration 18235, loss = 0.00977470\n",
      "Iteration 18236, loss = 0.00977416\n",
      "Iteration 18237, loss = 0.00977362\n",
      "Iteration 18238, loss = 0.00977308\n",
      "Iteration 18239, loss = 0.00977254\n",
      "Iteration 18240, loss = 0.00977200\n",
      "Iteration 18241, loss = 0.00977146\n",
      "Iteration 18242, loss = 0.00977092\n",
      "Iteration 18243, loss = 0.00977038\n",
      "Iteration 18244, loss = 0.00976984\n",
      "Iteration 18245, loss = 0.00976930\n",
      "Iteration 18246, loss = 0.00976876\n",
      "Iteration 18247, loss = 0.00976822\n",
      "Iteration 18248, loss = 0.00976768\n",
      "Iteration 18249, loss = 0.00976714\n",
      "Iteration 18250, loss = 0.00976660\n",
      "Iteration 18251, loss = 0.00976606\n",
      "Iteration 18252, loss = 0.00976552\n",
      "Iteration 18253, loss = 0.00976498\n",
      "Iteration 18254, loss = 0.00976444\n",
      "Iteration 18255, loss = 0.00976390\n",
      "Iteration 18256, loss = 0.00976336\n",
      "Iteration 18257, loss = 0.00976282\n",
      "Iteration 18258, loss = 0.00976228\n",
      "Iteration 18259, loss = 0.00976174\n",
      "Iteration 18260, loss = 0.00976121\n",
      "Iteration 18261, loss = 0.00976067\n",
      "Iteration 18262, loss = 0.00976013\n",
      "Iteration 18263, loss = 0.00975959\n",
      "Iteration 18264, loss = 0.00975905\n",
      "Iteration 18265, loss = 0.00975851\n",
      "Iteration 18266, loss = 0.00975797\n",
      "Iteration 18267, loss = 0.00975743\n",
      "Iteration 18268, loss = 0.00975690\n",
      "Iteration 18269, loss = 0.00975636\n",
      "Iteration 18270, loss = 0.00975582\n",
      "Iteration 18271, loss = 0.00975528\n",
      "Iteration 18272, loss = 0.00975474\n",
      "Iteration 18273, loss = 0.00975421\n",
      "Iteration 18274, loss = 0.00975367\n",
      "Iteration 18275, loss = 0.00975313\n",
      "Iteration 18276, loss = 0.00975259\n",
      "Iteration 18277, loss = 0.00975206\n",
      "Iteration 18278, loss = 0.00975152\n",
      "Iteration 18279, loss = 0.00975098\n",
      "Iteration 18280, loss = 0.00975044\n",
      "Iteration 18281, loss = 0.00974991\n",
      "Iteration 18282, loss = 0.00974937\n",
      "Iteration 18283, loss = 0.00974883\n",
      "Iteration 18284, loss = 0.00974830\n",
      "Iteration 18285, loss = 0.00974776\n",
      "Iteration 18286, loss = 0.00974722\n",
      "Iteration 18287, loss = 0.00974669\n",
      "Iteration 18288, loss = 0.00974615\n",
      "Iteration 18289, loss = 0.00974561\n",
      "Iteration 18290, loss = 0.00974508\n",
      "Iteration 18291, loss = 0.00974454\n",
      "Iteration 18292, loss = 0.00974400\n",
      "Iteration 18293, loss = 0.00974347\n",
      "Iteration 18294, loss = 0.00974293\n",
      "Iteration 18295, loss = 0.00974239\n",
      "Iteration 18296, loss = 0.00974186\n",
      "Iteration 18297, loss = 0.00974132\n",
      "Iteration 18298, loss = 0.00974079\n",
      "Iteration 18299, loss = 0.00974025\n",
      "Iteration 18300, loss = 0.00973972\n",
      "Iteration 18301, loss = 0.00973918\n",
      "Iteration 18302, loss = 0.00973864\n",
      "Iteration 18303, loss = 0.00973811\n",
      "Iteration 18304, loss = 0.00973757\n",
      "Iteration 18305, loss = 0.00973704\n",
      "Iteration 18306, loss = 0.00973650\n",
      "Iteration 18307, loss = 0.00973597\n",
      "Iteration 18308, loss = 0.00973543\n",
      "Iteration 18309, loss = 0.00973490\n",
      "Iteration 18310, loss = 0.00973436\n",
      "Iteration 18311, loss = 0.00973383\n",
      "Iteration 18312, loss = 0.00973329\n",
      "Iteration 18313, loss = 0.00973276\n",
      "Iteration 18314, loss = 0.00973223\n",
      "Iteration 18315, loss = 0.00973169\n",
      "Iteration 18316, loss = 0.00973116\n",
      "Iteration 18317, loss = 0.00973062\n",
      "Iteration 18318, loss = 0.00973009\n",
      "Iteration 18319, loss = 0.00972955\n",
      "Iteration 18320, loss = 0.00972902\n",
      "Iteration 18321, loss = 0.00972849\n",
      "Iteration 18322, loss = 0.00972795\n",
      "Iteration 18323, loss = 0.00972742\n",
      "Iteration 18324, loss = 0.00972688\n",
      "Iteration 18325, loss = 0.00972635\n",
      "Iteration 18326, loss = 0.00972582\n",
      "Iteration 18327, loss = 0.00972528\n",
      "Iteration 18328, loss = 0.00972475\n",
      "Iteration 18329, loss = 0.00972422\n",
      "Iteration 18330, loss = 0.00972368\n",
      "Iteration 18331, loss = 0.00972315\n",
      "Iteration 18332, loss = 0.00972262\n",
      "Iteration 18333, loss = 0.00972209\n",
      "Iteration 18334, loss = 0.00972155\n",
      "Iteration 18335, loss = 0.00972102\n",
      "Iteration 18336, loss = 0.00972049\n",
      "Iteration 18337, loss = 0.00971995\n",
      "Iteration 18338, loss = 0.00971942\n",
      "Iteration 18339, loss = 0.00971889\n",
      "Iteration 18340, loss = 0.00971836\n",
      "Iteration 18341, loss = 0.00971783\n",
      "Iteration 18342, loss = 0.00971729\n",
      "Iteration 18343, loss = 0.00971676\n",
      "Iteration 18344, loss = 0.00971623\n",
      "Iteration 18345, loss = 0.00971570\n",
      "Iteration 18346, loss = 0.00971517\n",
      "Iteration 18347, loss = 0.00971463\n",
      "Iteration 18348, loss = 0.00971410\n",
      "Iteration 18349, loss = 0.00971357\n",
      "Iteration 18350, loss = 0.00971304\n",
      "Iteration 18351, loss = 0.00971251\n",
      "Iteration 18352, loss = 0.00971198\n",
      "Iteration 18353, loss = 0.00971144\n",
      "Iteration 18354, loss = 0.00971091\n",
      "Iteration 18355, loss = 0.00971038\n",
      "Iteration 18356, loss = 0.00970985\n",
      "Iteration 18357, loss = 0.00970932\n",
      "Iteration 18358, loss = 0.00970879\n",
      "Iteration 18359, loss = 0.00970826\n",
      "Iteration 18360, loss = 0.00970773\n",
      "Iteration 18361, loss = 0.00970720\n",
      "Iteration 18362, loss = 0.00970667\n",
      "Iteration 18363, loss = 0.00970614\n",
      "Iteration 18364, loss = 0.00970561\n",
      "Iteration 18365, loss = 0.00970507\n",
      "Iteration 18366, loss = 0.00970454\n",
      "Iteration 18367, loss = 0.00970401\n",
      "Iteration 18368, loss = 0.00970348\n",
      "Iteration 18369, loss = 0.00970295\n",
      "Iteration 18370, loss = 0.00970242\n",
      "Iteration 18371, loss = 0.00970189\n",
      "Iteration 18372, loss = 0.00970136\n",
      "Iteration 18373, loss = 0.00970084\n",
      "Iteration 18374, loss = 0.00970031\n",
      "Iteration 18375, loss = 0.00969978\n",
      "Iteration 18376, loss = 0.00969925\n",
      "Iteration 18377, loss = 0.00969872\n",
      "Iteration 18378, loss = 0.00969819\n",
      "Iteration 18379, loss = 0.00969766\n",
      "Iteration 18380, loss = 0.00969713\n",
      "Iteration 18381, loss = 0.00969660\n",
      "Iteration 18382, loss = 0.00969607\n",
      "Iteration 18383, loss = 0.00969554\n",
      "Iteration 18384, loss = 0.00969501\n",
      "Iteration 18385, loss = 0.00969449\n",
      "Iteration 18386, loss = 0.00969396\n",
      "Iteration 18387, loss = 0.00969343\n",
      "Iteration 18388, loss = 0.00969290\n",
      "Iteration 18389, loss = 0.00969237\n",
      "Iteration 18390, loss = 0.00969184\n",
      "Iteration 18391, loss = 0.00969131\n",
      "Iteration 18392, loss = 0.00969079\n",
      "Iteration 18393, loss = 0.00969026\n",
      "Iteration 18394, loss = 0.00968973\n",
      "Iteration 18395, loss = 0.00968920\n",
      "Iteration 18396, loss = 0.00968868\n",
      "Iteration 18397, loss = 0.00968815\n",
      "Iteration 18398, loss = 0.00968762\n",
      "Iteration 18399, loss = 0.00968709\n",
      "Iteration 18400, loss = 0.00968656\n",
      "Iteration 18401, loss = 0.00968604\n",
      "Iteration 18402, loss = 0.00968551\n",
      "Iteration 18403, loss = 0.00968498\n",
      "Iteration 18404, loss = 0.00968446\n",
      "Iteration 18405, loss = 0.00968393\n",
      "Iteration 18406, loss = 0.00968340\n",
      "Iteration 18407, loss = 0.00968287\n",
      "Iteration 18408, loss = 0.00968235\n",
      "Iteration 18409, loss = 0.00968182\n",
      "Iteration 18410, loss = 0.00968129\n",
      "Iteration 18411, loss = 0.00968077\n",
      "Iteration 18412, loss = 0.00968024\n",
      "Iteration 18413, loss = 0.00967971\n",
      "Iteration 18414, loss = 0.00967919\n",
      "Iteration 18415, loss = 0.00967866\n",
      "Iteration 18416, loss = 0.00967814\n",
      "Iteration 18417, loss = 0.00967761\n",
      "Iteration 18418, loss = 0.00967708\n",
      "Iteration 18419, loss = 0.00967656\n",
      "Iteration 18420, loss = 0.00967603\n",
      "Iteration 18421, loss = 0.00967551\n",
      "Iteration 18422, loss = 0.00967498\n",
      "Iteration 18423, loss = 0.00967445\n",
      "Iteration 18424, loss = 0.00967393\n",
      "Iteration 18425, loss = 0.00967340\n",
      "Iteration 18426, loss = 0.00967288\n",
      "Iteration 18427, loss = 0.00967235\n",
      "Iteration 18428, loss = 0.00967183\n",
      "Iteration 18429, loss = 0.00967130\n",
      "Iteration 18430, loss = 0.00967078\n",
      "Iteration 18431, loss = 0.00967025\n",
      "Iteration 18432, loss = 0.00966973\n",
      "Iteration 18433, loss = 0.00966920\n",
      "Iteration 18434, loss = 0.00966868\n",
      "Iteration 18435, loss = 0.00966815\n",
      "Iteration 18436, loss = 0.00966763\n",
      "Iteration 18437, loss = 0.00966710\n",
      "Iteration 18438, loss = 0.00966658\n",
      "Iteration 18439, loss = 0.00966605\n",
      "Iteration 18440, loss = 0.00966553\n",
      "Iteration 18441, loss = 0.00966501\n",
      "Iteration 18442, loss = 0.00966448\n",
      "Iteration 18443, loss = 0.00966396\n",
      "Iteration 18444, loss = 0.00966343\n",
      "Iteration 18445, loss = 0.00966291\n",
      "Iteration 18446, loss = 0.00966239\n",
      "Iteration 18447, loss = 0.00966186\n",
      "Iteration 18448, loss = 0.00966134\n",
      "Iteration 18449, loss = 0.00966082\n",
      "Iteration 18450, loss = 0.00966029\n",
      "Iteration 18451, loss = 0.00965977\n",
      "Iteration 18452, loss = 0.00965925\n",
      "Iteration 18453, loss = 0.00965872\n",
      "Iteration 18454, loss = 0.00965820\n",
      "Iteration 18455, loss = 0.00965768\n",
      "Iteration 18456, loss = 0.00965715\n",
      "Iteration 18457, loss = 0.00965663\n",
      "Iteration 18458, loss = 0.00965611\n",
      "Iteration 18459, loss = 0.00965558\n",
      "Iteration 18460, loss = 0.00965506\n",
      "Iteration 18461, loss = 0.00965454\n",
      "Iteration 18462, loss = 0.00965402\n",
      "Iteration 18463, loss = 0.00965349\n",
      "Iteration 18464, loss = 0.00965297\n",
      "Iteration 18465, loss = 0.00965245\n",
      "Iteration 18466, loss = 0.00965193\n",
      "Iteration 18467, loss = 0.00965140\n",
      "Iteration 18468, loss = 0.00965088\n",
      "Iteration 18469, loss = 0.00965036\n",
      "Iteration 18470, loss = 0.00964984\n",
      "Iteration 18471, loss = 0.00964932\n",
      "Iteration 18472, loss = 0.00964880\n",
      "Iteration 18473, loss = 0.00964827\n",
      "Iteration 18474, loss = 0.00964775\n",
      "Iteration 18475, loss = 0.00964723\n",
      "Iteration 18476, loss = 0.00964671\n",
      "Iteration 18477, loss = 0.00964619\n",
      "Iteration 18478, loss = 0.00964567\n",
      "Iteration 18479, loss = 0.00964515\n",
      "Iteration 18480, loss = 0.00964462\n",
      "Iteration 18481, loss = 0.00964410\n",
      "Iteration 18482, loss = 0.00964358\n",
      "Iteration 18483, loss = 0.00964306\n",
      "Iteration 18484, loss = 0.00964254\n",
      "Iteration 18485, loss = 0.00964202\n",
      "Iteration 18486, loss = 0.00964150\n",
      "Iteration 18487, loss = 0.00964098\n",
      "Iteration 18488, loss = 0.00964046\n",
      "Iteration 18489, loss = 0.00963994\n",
      "Iteration 18490, loss = 0.00963942\n",
      "Iteration 18491, loss = 0.00963890\n",
      "Iteration 18492, loss = 0.00963838\n",
      "Iteration 18493, loss = 0.00963786\n",
      "Iteration 18494, loss = 0.00963734\n",
      "Iteration 18495, loss = 0.00963682\n",
      "Iteration 18496, loss = 0.00963630\n",
      "Iteration 18497, loss = 0.00963578\n",
      "Iteration 18498, loss = 0.00963526\n",
      "Iteration 18499, loss = 0.00963474\n",
      "Iteration 18500, loss = 0.00963422\n",
      "Iteration 18501, loss = 0.00963370\n",
      "Iteration 18502, loss = 0.00963318\n",
      "Iteration 18503, loss = 0.00963266\n",
      "Iteration 18504, loss = 0.00963214\n",
      "Iteration 18505, loss = 0.00963162\n",
      "Iteration 18506, loss = 0.00963110\n",
      "Iteration 18507, loss = 0.00963058\n",
      "Iteration 18508, loss = 0.00963006\n",
      "Iteration 18509, loss = 0.00962955\n",
      "Iteration 18510, loss = 0.00962903\n",
      "Iteration 18511, loss = 0.00962851\n",
      "Iteration 18512, loss = 0.00962799\n",
      "Iteration 18513, loss = 0.00962747\n",
      "Iteration 18514, loss = 0.00962695\n",
      "Iteration 18515, loss = 0.00962643\n",
      "Iteration 18516, loss = 0.00962592\n",
      "Iteration 18517, loss = 0.00962540\n",
      "Iteration 18518, loss = 0.00962488\n",
      "Iteration 18519, loss = 0.00962436\n",
      "Iteration 18520, loss = 0.00962384\n",
      "Iteration 18521, loss = 0.00962333\n",
      "Iteration 18522, loss = 0.00962281\n",
      "Iteration 18523, loss = 0.00962229\n",
      "Iteration 18524, loss = 0.00962177\n",
      "Iteration 18525, loss = 0.00962126\n",
      "Iteration 18526, loss = 0.00962074\n",
      "Iteration 18527, loss = 0.00962022\n",
      "Iteration 18528, loss = 0.00961970\n",
      "Iteration 18529, loss = 0.00961919\n",
      "Iteration 18530, loss = 0.00961867\n",
      "Iteration 18531, loss = 0.00961815\n",
      "Iteration 18532, loss = 0.00961763\n",
      "Iteration 18533, loss = 0.00961712\n",
      "Iteration 18534, loss = 0.00961660\n",
      "Iteration 18535, loss = 0.00961608\n",
      "Iteration 18536, loss = 0.00961557\n",
      "Iteration 18537, loss = 0.00961505\n",
      "Iteration 18538, loss = 0.00961453\n",
      "Iteration 18539, loss = 0.00961402\n",
      "Iteration 18540, loss = 0.00961350\n",
      "Iteration 18541, loss = 0.00961298\n",
      "Iteration 18542, loss = 0.00961247\n",
      "Iteration 18543, loss = 0.00961195\n",
      "Iteration 18544, loss = 0.00961144\n",
      "Iteration 18545, loss = 0.00961092\n",
      "Iteration 18546, loss = 0.00961040\n",
      "Iteration 18547, loss = 0.00960989\n",
      "Iteration 18548, loss = 0.00960937\n",
      "Iteration 18549, loss = 0.00960886\n",
      "Iteration 18550, loss = 0.00960834\n",
      "Iteration 18551, loss = 0.00960783\n",
      "Iteration 18552, loss = 0.00960731\n",
      "Iteration 18553, loss = 0.00960680\n",
      "Iteration 18554, loss = 0.00960628\n",
      "Iteration 18555, loss = 0.00960576\n",
      "Iteration 18556, loss = 0.00960525\n",
      "Iteration 18557, loss = 0.00960473\n",
      "Iteration 18558, loss = 0.00960422\n",
      "Iteration 18559, loss = 0.00960370\n",
      "Iteration 18560, loss = 0.00960319\n",
      "Iteration 18561, loss = 0.00960267\n",
      "Iteration 18562, loss = 0.00960216\n",
      "Iteration 18563, loss = 0.00960165\n",
      "Iteration 18564, loss = 0.00960113\n",
      "Iteration 18565, loss = 0.00960062\n",
      "Iteration 18566, loss = 0.00960010\n",
      "Iteration 18567, loss = 0.00959959\n",
      "Iteration 18568, loss = 0.00959907\n",
      "Iteration 18569, loss = 0.00959856\n",
      "Iteration 18570, loss = 0.00959805\n",
      "Iteration 18571, loss = 0.00959753\n",
      "Iteration 18572, loss = 0.00959702\n",
      "Iteration 18573, loss = 0.00959650\n",
      "Iteration 18574, loss = 0.00959599\n",
      "Iteration 18575, loss = 0.00959548\n",
      "Iteration 18576, loss = 0.00959496\n",
      "Iteration 18577, loss = 0.00959445\n",
      "Iteration 18578, loss = 0.00959394\n",
      "Iteration 18579, loss = 0.00959342\n",
      "Iteration 18580, loss = 0.00959291\n",
      "Iteration 18581, loss = 0.00959240\n",
      "Iteration 18582, loss = 0.00959188\n",
      "Iteration 18583, loss = 0.00959137\n",
      "Iteration 18584, loss = 0.00959086\n",
      "Iteration 18585, loss = 0.00959034\n",
      "Iteration 18586, loss = 0.00958983\n",
      "Iteration 18587, loss = 0.00958932\n",
      "Iteration 18588, loss = 0.00958881\n",
      "Iteration 18589, loss = 0.00958829\n",
      "Iteration 18590, loss = 0.00958778\n",
      "Iteration 18591, loss = 0.00958727\n",
      "Iteration 18592, loss = 0.00958676\n",
      "Iteration 18593, loss = 0.00958624\n",
      "Iteration 18594, loss = 0.00958573\n",
      "Iteration 18595, loss = 0.00958522\n",
      "Iteration 18596, loss = 0.00958471\n",
      "Iteration 18597, loss = 0.00958420\n",
      "Iteration 18598, loss = 0.00958368\n",
      "Iteration 18599, loss = 0.00958317\n",
      "Iteration 18600, loss = 0.00958266\n",
      "Iteration 18601, loss = 0.00958215\n",
      "Iteration 18602, loss = 0.00958164\n",
      "Iteration 18603, loss = 0.00958112\n",
      "Iteration 18604, loss = 0.00958061\n",
      "Iteration 18605, loss = 0.00958010\n",
      "Iteration 18606, loss = 0.00957959\n",
      "Iteration 18607, loss = 0.00957908\n",
      "Iteration 18608, loss = 0.00957857\n",
      "Iteration 18609, loss = 0.00957806\n",
      "Iteration 18610, loss = 0.00957755\n",
      "Iteration 18611, loss = 0.00957704\n",
      "Iteration 18612, loss = 0.00957652\n",
      "Iteration 18613, loss = 0.00957601\n",
      "Iteration 18614, loss = 0.00957550\n",
      "Iteration 18615, loss = 0.00957499\n",
      "Iteration 18616, loss = 0.00957448\n",
      "Iteration 18617, loss = 0.00957397\n",
      "Iteration 18618, loss = 0.00957346\n",
      "Iteration 18619, loss = 0.00957295\n",
      "Iteration 18620, loss = 0.00957244\n",
      "Iteration 18621, loss = 0.00957193\n",
      "Iteration 18622, loss = 0.00957142\n",
      "Iteration 18623, loss = 0.00957091\n",
      "Iteration 18624, loss = 0.00957040\n",
      "Iteration 18625, loss = 0.00956989\n",
      "Iteration 18626, loss = 0.00956938\n",
      "Iteration 18627, loss = 0.00956887\n",
      "Iteration 18628, loss = 0.00956836\n",
      "Iteration 18629, loss = 0.00956785\n",
      "Iteration 18630, loss = 0.00956734\n",
      "Iteration 18631, loss = 0.00956683\n",
      "Iteration 18632, loss = 0.00956632\n",
      "Iteration 18633, loss = 0.00956582\n",
      "Iteration 18634, loss = 0.00956531\n",
      "Iteration 18635, loss = 0.00956480\n",
      "Iteration 18636, loss = 0.00956429\n",
      "Iteration 18637, loss = 0.00956378\n",
      "Iteration 18638, loss = 0.00956327\n",
      "Iteration 18639, loss = 0.00956276\n",
      "Iteration 18640, loss = 0.00956225\n",
      "Iteration 18641, loss = 0.00956174\n",
      "Iteration 18642, loss = 0.00956124\n",
      "Iteration 18643, loss = 0.00956073\n",
      "Iteration 18644, loss = 0.00956022\n",
      "Iteration 18645, loss = 0.00955971\n",
      "Iteration 18646, loss = 0.00955920\n",
      "Iteration 18647, loss = 0.00955869\n",
      "Iteration 18648, loss = 0.00955819\n",
      "Iteration 18649, loss = 0.00955768\n",
      "Iteration 18650, loss = 0.00955717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18651, loss = 0.00955666\n",
      "Iteration 18652, loss = 0.00955616\n",
      "Iteration 18653, loss = 0.00955565\n",
      "Iteration 18654, loss = 0.00955514\n",
      "Iteration 18655, loss = 0.00955463\n",
      "Iteration 18656, loss = 0.00955412\n",
      "Iteration 18657, loss = 0.00955362\n",
      "Iteration 18658, loss = 0.00955311\n",
      "Iteration 18659, loss = 0.00955260\n",
      "Iteration 18660, loss = 0.00955210\n",
      "Iteration 18661, loss = 0.00955159\n",
      "Iteration 18662, loss = 0.00955108\n",
      "Iteration 18663, loss = 0.00955057\n",
      "Iteration 18664, loss = 0.00955007\n",
      "Iteration 18665, loss = 0.00954956\n",
      "Iteration 18666, loss = 0.00954905\n",
      "Iteration 18667, loss = 0.00954855\n",
      "Iteration 18668, loss = 0.00954804\n",
      "Iteration 18669, loss = 0.00954754\n",
      "Iteration 18670, loss = 0.00954703\n",
      "Iteration 18671, loss = 0.00954652\n",
      "Iteration 18672, loss = 0.00954602\n",
      "Iteration 18673, loss = 0.00954551\n",
      "Iteration 18674, loss = 0.00954500\n",
      "Iteration 18675, loss = 0.00954450\n",
      "Iteration 18676, loss = 0.00954399\n",
      "Iteration 18677, loss = 0.00954349\n",
      "Iteration 18678, loss = 0.00954298\n",
      "Iteration 18679, loss = 0.00954247\n",
      "Iteration 18680, loss = 0.00954197\n",
      "Iteration 18681, loss = 0.00954146\n",
      "Iteration 18682, loss = 0.00954096\n",
      "Iteration 18683, loss = 0.00954045\n",
      "Iteration 18684, loss = 0.00953995\n",
      "Iteration 18685, loss = 0.00953944\n",
      "Iteration 18686, loss = 0.00953894\n",
      "Iteration 18687, loss = 0.00953843\n",
      "Iteration 18688, loss = 0.00953793\n",
      "Iteration 18689, loss = 0.00953742\n",
      "Iteration 18690, loss = 0.00953692\n",
      "Iteration 18691, loss = 0.00953641\n",
      "Iteration 18692, loss = 0.00953591\n",
      "Iteration 18693, loss = 0.00953540\n",
      "Iteration 18694, loss = 0.00953490\n",
      "Iteration 18695, loss = 0.00953439\n",
      "Iteration 18696, loss = 0.00953389\n",
      "Iteration 18697, loss = 0.00953339\n",
      "Iteration 18698, loss = 0.00953288\n",
      "Iteration 18699, loss = 0.00953238\n",
      "Iteration 18700, loss = 0.00953187\n",
      "Iteration 18701, loss = 0.00953137\n",
      "Iteration 18702, loss = 0.00953086\n",
      "Iteration 18703, loss = 0.00953036\n",
      "Iteration 18704, loss = 0.00952986\n",
      "Iteration 18705, loss = 0.00952935\n",
      "Iteration 18706, loss = 0.00952885\n",
      "Iteration 18707, loss = 0.00952835\n",
      "Iteration 18708, loss = 0.00952784\n",
      "Iteration 18709, loss = 0.00952734\n",
      "Iteration 18710, loss = 0.00952684\n",
      "Iteration 18711, loss = 0.00952633\n",
      "Iteration 18712, loss = 0.00952583\n",
      "Iteration 18713, loss = 0.00952533\n",
      "Iteration 18714, loss = 0.00952482\n",
      "Iteration 18715, loss = 0.00952432\n",
      "Iteration 18716, loss = 0.00952382\n",
      "Iteration 18717, loss = 0.00952331\n",
      "Iteration 18718, loss = 0.00952281\n",
      "Iteration 18719, loss = 0.00952231\n",
      "Iteration 18720, loss = 0.00952181\n",
      "Iteration 18721, loss = 0.00952130\n",
      "Iteration 18722, loss = 0.00952080\n",
      "Iteration 18723, loss = 0.00952030\n",
      "Iteration 18724, loss = 0.00951980\n",
      "Iteration 18725, loss = 0.00951929\n",
      "Iteration 18726, loss = 0.00951879\n",
      "Iteration 18727, loss = 0.00951829\n",
      "Iteration 18728, loss = 0.00951779\n",
      "Iteration 18729, loss = 0.00951729\n",
      "Iteration 18730, loss = 0.00951678\n",
      "Iteration 18731, loss = 0.00951628\n",
      "Iteration 18732, loss = 0.00951578\n",
      "Iteration 18733, loss = 0.00951528\n",
      "Iteration 18734, loss = 0.00951478\n",
      "Iteration 18735, loss = 0.00951428\n",
      "Iteration 18736, loss = 0.00951377\n",
      "Iteration 18737, loss = 0.00951327\n",
      "Iteration 18738, loss = 0.00951277\n",
      "Iteration 18739, loss = 0.00951227\n",
      "Iteration 18740, loss = 0.00951177\n",
      "Iteration 18741, loss = 0.00951127\n",
      "Iteration 18742, loss = 0.00951077\n",
      "Iteration 18743, loss = 0.00951027\n",
      "Iteration 18744, loss = 0.00950977\n",
      "Iteration 18745, loss = 0.00950927\n",
      "Iteration 18746, loss = 0.00950877\n",
      "Iteration 18747, loss = 0.00950826\n",
      "Iteration 18748, loss = 0.00950776\n",
      "Iteration 18749, loss = 0.00950726\n",
      "Iteration 18750, loss = 0.00950676\n",
      "Iteration 18751, loss = 0.00950626\n",
      "Iteration 18752, loss = 0.00950576\n",
      "Iteration 18753, loss = 0.00950526\n",
      "Iteration 18754, loss = 0.00950476\n",
      "Iteration 18755, loss = 0.00950426\n",
      "Iteration 18756, loss = 0.00950376\n",
      "Iteration 18757, loss = 0.00950326\n",
      "Iteration 18758, loss = 0.00950276\n",
      "Iteration 18759, loss = 0.00950226\n",
      "Iteration 18760, loss = 0.00950176\n",
      "Iteration 18761, loss = 0.00950126\n",
      "Iteration 18762, loss = 0.00950076\n",
      "Iteration 18763, loss = 0.00950027\n",
      "Iteration 18764, loss = 0.00949977\n",
      "Iteration 18765, loss = 0.00949927\n",
      "Iteration 18766, loss = 0.00949877\n",
      "Iteration 18767, loss = 0.00949827\n",
      "Iteration 18768, loss = 0.00949777\n",
      "Iteration 18769, loss = 0.00949727\n",
      "Iteration 18770, loss = 0.00949677\n",
      "Iteration 18771, loss = 0.00949627\n",
      "Iteration 18772, loss = 0.00949577\n",
      "Iteration 18773, loss = 0.00949528\n",
      "Iteration 18774, loss = 0.00949478\n",
      "Iteration 18775, loss = 0.00949428\n",
      "Iteration 18776, loss = 0.00949378\n",
      "Iteration 18777, loss = 0.00949328\n",
      "Iteration 18778, loss = 0.00949278\n",
      "Iteration 18779, loss = 0.00949229\n",
      "Iteration 18780, loss = 0.00949179\n",
      "Iteration 18781, loss = 0.00949129\n",
      "Iteration 18782, loss = 0.00949079\n",
      "Iteration 18783, loss = 0.00949029\n",
      "Iteration 18784, loss = 0.00948980\n",
      "Iteration 18785, loss = 0.00948930\n",
      "Iteration 18786, loss = 0.00948880\n",
      "Iteration 18787, loss = 0.00948830\n",
      "Iteration 18788, loss = 0.00948781\n",
      "Iteration 18789, loss = 0.00948731\n",
      "Iteration 18790, loss = 0.00948681\n",
      "Iteration 18791, loss = 0.00948631\n",
      "Iteration 18792, loss = 0.00948582\n",
      "Iteration 18793, loss = 0.00948532\n",
      "Iteration 18794, loss = 0.00948482\n",
      "Iteration 18795, loss = 0.00948432\n",
      "Iteration 18796, loss = 0.00948383\n",
      "Iteration 18797, loss = 0.00948333\n",
      "Iteration 18798, loss = 0.00948283\n",
      "Iteration 18799, loss = 0.00948234\n",
      "Iteration 18800, loss = 0.00948184\n",
      "Iteration 18801, loss = 0.00948134\n",
      "Iteration 18802, loss = 0.00948085\n",
      "Iteration 18803, loss = 0.00948035\n",
      "Iteration 18804, loss = 0.00947986\n",
      "Iteration 18805, loss = 0.00947936\n",
      "Iteration 18806, loss = 0.00947886\n",
      "Iteration 18807, loss = 0.00947837\n",
      "Iteration 18808, loss = 0.00947787\n",
      "Iteration 18809, loss = 0.00947737\n",
      "Iteration 18810, loss = 0.00947688\n",
      "Iteration 18811, loss = 0.00947638\n",
      "Iteration 18812, loss = 0.00947589\n",
      "Iteration 18813, loss = 0.00947539\n",
      "Iteration 18814, loss = 0.00947490\n",
      "Iteration 18815, loss = 0.00947440\n",
      "Iteration 18816, loss = 0.00947391\n",
      "Iteration 18817, loss = 0.00947341\n",
      "Iteration 18818, loss = 0.00947291\n",
      "Iteration 18819, loss = 0.00947242\n",
      "Iteration 18820, loss = 0.00947192\n",
      "Iteration 18821, loss = 0.00947143\n",
      "Iteration 18822, loss = 0.00947093\n",
      "Iteration 18823, loss = 0.00947044\n",
      "Iteration 18824, loss = 0.00946994\n",
      "Iteration 18825, loss = 0.00946945\n",
      "Iteration 18826, loss = 0.00946895\n",
      "Iteration 18827, loss = 0.00946846\n",
      "Iteration 18828, loss = 0.00946797\n",
      "Iteration 18829, loss = 0.00946747\n",
      "Iteration 18830, loss = 0.00946698\n",
      "Iteration 18831, loss = 0.00946648\n",
      "Iteration 18832, loss = 0.00946599\n",
      "Iteration 18833, loss = 0.00946549\n",
      "Iteration 18834, loss = 0.00946500\n",
      "Iteration 18835, loss = 0.00946451\n",
      "Iteration 18836, loss = 0.00946401\n",
      "Iteration 18837, loss = 0.00946352\n",
      "Iteration 18838, loss = 0.00946302\n",
      "Iteration 18839, loss = 0.00946253\n",
      "Iteration 18840, loss = 0.00946204\n",
      "Iteration 18841, loss = 0.00946154\n",
      "Iteration 18842, loss = 0.00946105\n",
      "Iteration 18843, loss = 0.00946056\n",
      "Iteration 18844, loss = 0.00946006\n",
      "Iteration 18845, loss = 0.00945957\n",
      "Iteration 18846, loss = 0.00945908\n",
      "Iteration 18847, loss = 0.00945858\n",
      "Iteration 18848, loss = 0.00945809\n",
      "Iteration 18849, loss = 0.00945760\n",
      "Iteration 18850, loss = 0.00945710\n",
      "Iteration 18851, loss = 0.00945661\n",
      "Iteration 18852, loss = 0.00945612\n",
      "Iteration 18853, loss = 0.00945563\n",
      "Iteration 18854, loss = 0.00945513\n",
      "Iteration 18855, loss = 0.00945464\n",
      "Iteration 18856, loss = 0.00945415\n",
      "Iteration 18857, loss = 0.00945366\n",
      "Iteration 18858, loss = 0.00945316\n",
      "Iteration 18859, loss = 0.00945267\n",
      "Iteration 18860, loss = 0.00945218\n",
      "Iteration 18861, loss = 0.00945169\n",
      "Iteration 18862, loss = 0.00945119\n",
      "Iteration 18863, loss = 0.00945070\n",
      "Iteration 18864, loss = 0.00945021\n",
      "Iteration 18865, loss = 0.00944972\n",
      "Iteration 18866, loss = 0.00944923\n",
      "Iteration 18867, loss = 0.00944873\n",
      "Iteration 18868, loss = 0.00944824\n",
      "Iteration 18869, loss = 0.00944775\n",
      "Iteration 18870, loss = 0.00944726\n",
      "Iteration 18871, loss = 0.00944677\n",
      "Iteration 18872, loss = 0.00944628\n",
      "Iteration 18873, loss = 0.00944579\n",
      "Iteration 18874, loss = 0.00944529\n",
      "Iteration 18875, loss = 0.00944480\n",
      "Iteration 18876, loss = 0.00944431\n",
      "Iteration 18877, loss = 0.00944382\n",
      "Iteration 18878, loss = 0.00944333\n",
      "Iteration 18879, loss = 0.00944284\n",
      "Iteration 18880, loss = 0.00944235\n",
      "Iteration 18881, loss = 0.00944186\n",
      "Iteration 18882, loss = 0.00944137\n",
      "Iteration 18883, loss = 0.00944088\n",
      "Iteration 18884, loss = 0.00944039\n",
      "Iteration 18885, loss = 0.00943990\n",
      "Iteration 18886, loss = 0.00943941\n",
      "Iteration 18887, loss = 0.00943892\n",
      "Iteration 18888, loss = 0.00943843\n",
      "Iteration 18889, loss = 0.00943794\n",
      "Iteration 18890, loss = 0.00943745\n",
      "Iteration 18891, loss = 0.00943696\n",
      "Iteration 18892, loss = 0.00943647\n",
      "Iteration 18893, loss = 0.00943598\n",
      "Iteration 18894, loss = 0.00943549\n",
      "Iteration 18895, loss = 0.00943500\n",
      "Iteration 18896, loss = 0.00943451\n",
      "Iteration 18897, loss = 0.00943402\n",
      "Iteration 18898, loss = 0.00943353\n",
      "Iteration 18899, loss = 0.00943304\n",
      "Iteration 18900, loss = 0.00943255\n",
      "Iteration 18901, loss = 0.00943206\n",
      "Iteration 18902, loss = 0.00943157\n",
      "Iteration 18903, loss = 0.00943108\n",
      "Iteration 18904, loss = 0.00943059\n",
      "Iteration 18905, loss = 0.00943010\n",
      "Iteration 18906, loss = 0.00942962\n",
      "Iteration 18907, loss = 0.00942913\n",
      "Iteration 18908, loss = 0.00942864\n",
      "Iteration 18909, loss = 0.00942815\n",
      "Iteration 18910, loss = 0.00942766\n",
      "Iteration 18911, loss = 0.00942717\n",
      "Iteration 18912, loss = 0.00942668\n",
      "Iteration 18913, loss = 0.00942620\n",
      "Iteration 18914, loss = 0.00942571\n",
      "Iteration 18915, loss = 0.00942522\n",
      "Iteration 18916, loss = 0.00942473\n",
      "Iteration 18917, loss = 0.00942424\n",
      "Iteration 18918, loss = 0.00942375\n",
      "Iteration 18919, loss = 0.00942327\n",
      "Iteration 18920, loss = 0.00942278\n",
      "Iteration 18921, loss = 0.00942229\n",
      "Iteration 18922, loss = 0.00942180\n",
      "Iteration 18923, loss = 0.00942132\n",
      "Iteration 18924, loss = 0.00942083\n",
      "Iteration 18925, loss = 0.00942034\n",
      "Iteration 18926, loss = 0.00941985\n",
      "Iteration 18927, loss = 0.00941937\n",
      "Iteration 18928, loss = 0.00941888\n",
      "Iteration 18929, loss = 0.00941839\n",
      "Iteration 18930, loss = 0.00941790\n",
      "Iteration 18931, loss = 0.00941742\n",
      "Iteration 18932, loss = 0.00941693\n",
      "Iteration 18933, loss = 0.00941644\n",
      "Iteration 18934, loss = 0.00941596\n",
      "Iteration 18935, loss = 0.00941547\n",
      "Iteration 18936, loss = 0.00941498\n",
      "Iteration 18937, loss = 0.00941450\n",
      "Iteration 18938, loss = 0.00941401\n",
      "Iteration 18939, loss = 0.00941352\n",
      "Iteration 18940, loss = 0.00941304\n",
      "Iteration 18941, loss = 0.00941255\n",
      "Iteration 18942, loss = 0.00941206\n",
      "Iteration 18943, loss = 0.00941158\n",
      "Iteration 18944, loss = 0.00941109\n",
      "Iteration 18945, loss = 0.00941061\n",
      "Iteration 18946, loss = 0.00941012\n",
      "Iteration 18947, loss = 0.00940963\n",
      "Iteration 18948, loss = 0.00940915\n",
      "Iteration 18949, loss = 0.00940866\n",
      "Iteration 18950, loss = 0.00940818\n",
      "Iteration 18951, loss = 0.00940769\n",
      "Iteration 18952, loss = 0.00940721\n",
      "Iteration 18953, loss = 0.00940672\n",
      "Iteration 18954, loss = 0.00940623\n",
      "Iteration 18955, loss = 0.00940575\n",
      "Iteration 18956, loss = 0.00940526\n",
      "Iteration 18957, loss = 0.00940478\n",
      "Iteration 18958, loss = 0.00940429\n",
      "Iteration 18959, loss = 0.00940381\n",
      "Iteration 18960, loss = 0.00940332\n",
      "Iteration 18961, loss = 0.00940284\n",
      "Iteration 18962, loss = 0.00940235\n",
      "Iteration 18963, loss = 0.00940187\n",
      "Iteration 18964, loss = 0.00940138\n",
      "Iteration 18965, loss = 0.00940090\n",
      "Iteration 18966, loss = 0.00940041\n",
      "Iteration 18967, loss = 0.00939993\n",
      "Iteration 18968, loss = 0.00939945\n",
      "Iteration 18969, loss = 0.00939896\n",
      "Iteration 18970, loss = 0.00939848\n",
      "Iteration 18971, loss = 0.00939799\n",
      "Iteration 18972, loss = 0.00939751\n",
      "Iteration 18973, loss = 0.00939703\n",
      "Iteration 18974, loss = 0.00939654\n",
      "Iteration 18975, loss = 0.00939606\n",
      "Iteration 18976, loss = 0.00939557\n",
      "Iteration 18977, loss = 0.00939509\n",
      "Iteration 18978, loss = 0.00939461\n",
      "Iteration 18979, loss = 0.00939412\n",
      "Iteration 18980, loss = 0.00939364\n",
      "Iteration 18981, loss = 0.00939316\n",
      "Iteration 18982, loss = 0.00939267\n",
      "Iteration 18983, loss = 0.00939219\n",
      "Iteration 18984, loss = 0.00939171\n",
      "Iteration 18985, loss = 0.00939122\n",
      "Iteration 18986, loss = 0.00939074\n",
      "Iteration 18987, loss = 0.00939026\n",
      "Iteration 18988, loss = 0.00938977\n",
      "Iteration 18989, loss = 0.00938929\n",
      "Iteration 18990, loss = 0.00938881\n",
      "Iteration 18991, loss = 0.00938832\n",
      "Iteration 18992, loss = 0.00938784\n",
      "Iteration 18993, loss = 0.00938736\n",
      "Iteration 18994, loss = 0.00938688\n",
      "Iteration 18995, loss = 0.00938639\n",
      "Iteration 18996, loss = 0.00938591\n",
      "Iteration 18997, loss = 0.00938543\n",
      "Iteration 18998, loss = 0.00938495\n",
      "Iteration 18999, loss = 0.00938446\n",
      "Iteration 19000, loss = 0.00938398\n",
      "Iteration 19001, loss = 0.00938350\n",
      "Iteration 19002, loss = 0.00938302\n",
      "Iteration 19003, loss = 0.00938254\n",
      "Iteration 19004, loss = 0.00938205\n",
      "Iteration 19005, loss = 0.00938157\n",
      "Iteration 19006, loss = 0.00938109\n",
      "Iteration 19007, loss = 0.00938061\n",
      "Iteration 19008, loss = 0.00938013\n",
      "Iteration 19009, loss = 0.00937965\n",
      "Iteration 19010, loss = 0.00937917\n",
      "Iteration 19011, loss = 0.00937868\n",
      "Iteration 19012, loss = 0.00937820\n",
      "Iteration 19013, loss = 0.00937772\n",
      "Iteration 19014, loss = 0.00937724\n",
      "Iteration 19015, loss = 0.00937676\n",
      "Iteration 19016, loss = 0.00937628\n",
      "Iteration 19017, loss = 0.00937580\n",
      "Iteration 19018, loss = 0.00937532\n",
      "Iteration 19019, loss = 0.00937484\n",
      "Iteration 19020, loss = 0.00937436\n",
      "Iteration 19021, loss = 0.00937387\n",
      "Iteration 19022, loss = 0.00937339\n",
      "Iteration 19023, loss = 0.00937291\n",
      "Iteration 19024, loss = 0.00937243\n",
      "Iteration 19025, loss = 0.00937195\n",
      "Iteration 19026, loss = 0.00937147\n",
      "Iteration 19027, loss = 0.00937099\n",
      "Iteration 19028, loss = 0.00937051\n",
      "Iteration 19029, loss = 0.00937003\n",
      "Iteration 19030, loss = 0.00936955\n",
      "Iteration 19031, loss = 0.00936907\n",
      "Iteration 19032, loss = 0.00936859\n",
      "Iteration 19033, loss = 0.00936811\n",
      "Iteration 19034, loss = 0.00936763\n",
      "Iteration 19035, loss = 0.00936715\n",
      "Iteration 19036, loss = 0.00936667\n",
      "Iteration 19037, loss = 0.00936619\n",
      "Iteration 19038, loss = 0.00936571\n",
      "Iteration 19039, loss = 0.00936523\n",
      "Iteration 19040, loss = 0.00936476\n",
      "Iteration 19041, loss = 0.00936428\n",
      "Iteration 19042, loss = 0.00936380\n",
      "Iteration 19043, loss = 0.00936332\n",
      "Iteration 19044, loss = 0.00936284\n",
      "Iteration 19045, loss = 0.00936236\n",
      "Iteration 19046, loss = 0.00936188\n",
      "Iteration 19047, loss = 0.00936140\n",
      "Iteration 19048, loss = 0.00936092\n",
      "Iteration 19049, loss = 0.00936045\n",
      "Iteration 19050, loss = 0.00935997\n",
      "Iteration 19051, loss = 0.00935949\n",
      "Iteration 19052, loss = 0.00935901\n",
      "Iteration 19053, loss = 0.00935853\n",
      "Iteration 19054, loss = 0.00935805\n",
      "Iteration 19055, loss = 0.00935757\n",
      "Iteration 19056, loss = 0.00935710\n",
      "Iteration 19057, loss = 0.00935662\n",
      "Iteration 19058, loss = 0.00935614\n",
      "Iteration 19059, loss = 0.00935566\n",
      "Iteration 19060, loss = 0.00935518\n",
      "Iteration 19061, loss = 0.00935471\n",
      "Iteration 19062, loss = 0.00935423\n",
      "Iteration 19063, loss = 0.00935375\n",
      "Iteration 19064, loss = 0.00935327\n",
      "Iteration 19065, loss = 0.00935280\n",
      "Iteration 19066, loss = 0.00935232\n",
      "Iteration 19067, loss = 0.00935184\n",
      "Iteration 19068, loss = 0.00935136\n",
      "Iteration 19069, loss = 0.00935089\n",
      "Iteration 19070, loss = 0.00935041\n",
      "Iteration 19071, loss = 0.00934993\n",
      "Iteration 19072, loss = 0.00934946\n",
      "Iteration 19073, loss = 0.00934898\n",
      "Iteration 19074, loss = 0.00934850\n",
      "Iteration 19075, loss = 0.00934802\n",
      "Iteration 19076, loss = 0.00934755\n",
      "Iteration 19077, loss = 0.00934707\n",
      "Iteration 19078, loss = 0.00934659\n",
      "Iteration 19079, loss = 0.00934612\n",
      "Iteration 19080, loss = 0.00934564\n",
      "Iteration 19081, loss = 0.00934516\n",
      "Iteration 19082, loss = 0.00934469\n",
      "Iteration 19083, loss = 0.00934421\n",
      "Iteration 19084, loss = 0.00934374\n",
      "Iteration 19085, loss = 0.00934326\n",
      "Iteration 19086, loss = 0.00934278\n",
      "Iteration 19087, loss = 0.00934231\n",
      "Iteration 19088, loss = 0.00934183\n",
      "Iteration 19089, loss = 0.00934136\n",
      "Iteration 19090, loss = 0.00934088\n",
      "Iteration 19091, loss = 0.00934040\n",
      "Iteration 19092, loss = 0.00933993\n",
      "Iteration 19093, loss = 0.00933945\n",
      "Iteration 19094, loss = 0.00933898\n",
      "Iteration 19095, loss = 0.00933850\n",
      "Iteration 19096, loss = 0.00933803\n",
      "Iteration 19097, loss = 0.00933755\n",
      "Iteration 19098, loss = 0.00933708\n",
      "Iteration 19099, loss = 0.00933660\n",
      "Iteration 19100, loss = 0.00933613\n",
      "Iteration 19101, loss = 0.00933565\n",
      "Iteration 19102, loss = 0.00933518\n",
      "Iteration 19103, loss = 0.00933470\n",
      "Iteration 19104, loss = 0.00933423\n",
      "Iteration 19105, loss = 0.00933375\n",
      "Iteration 19106, loss = 0.00933328\n",
      "Iteration 19107, loss = 0.00933280\n",
      "Iteration 19108, loss = 0.00933233\n",
      "Iteration 19109, loss = 0.00933185\n",
      "Iteration 19110, loss = 0.00933138\n",
      "Iteration 19111, loss = 0.00933090\n",
      "Iteration 19112, loss = 0.00933043\n",
      "Iteration 19113, loss = 0.00932996\n",
      "Iteration 19114, loss = 0.00932948\n",
      "Iteration 19115, loss = 0.00932901\n",
      "Iteration 19116, loss = 0.00932853\n",
      "Iteration 19117, loss = 0.00932806\n",
      "Iteration 19118, loss = 0.00932759\n",
      "Iteration 19119, loss = 0.00932711\n",
      "Iteration 19120, loss = 0.00932664\n",
      "Iteration 19121, loss = 0.00932617\n",
      "Iteration 19122, loss = 0.00932569\n",
      "Iteration 19123, loss = 0.00932522\n",
      "Iteration 19124, loss = 0.00932474\n",
      "Iteration 19125, loss = 0.00932427\n",
      "Iteration 19126, loss = 0.00932380\n",
      "Iteration 19127, loss = 0.00932332\n",
      "Iteration 19128, loss = 0.00932285\n",
      "Iteration 19129, loss = 0.00932238\n",
      "Iteration 19130, loss = 0.00932191\n",
      "Iteration 19131, loss = 0.00932143\n",
      "Iteration 19132, loss = 0.00932096\n",
      "Iteration 19133, loss = 0.00932049\n",
      "Iteration 19134, loss = 0.00932001\n",
      "Iteration 19135, loss = 0.00931954\n",
      "Iteration 19136, loss = 0.00931907\n",
      "Iteration 19137, loss = 0.00931860\n",
      "Iteration 19138, loss = 0.00931812\n",
      "Iteration 19139, loss = 0.00931765\n",
      "Iteration 19140, loss = 0.00931718\n",
      "Iteration 19141, loss = 0.00931671\n",
      "Iteration 19142, loss = 0.00931623\n",
      "Iteration 19143, loss = 0.00931576\n",
      "Iteration 19144, loss = 0.00931529\n",
      "Iteration 19145, loss = 0.00931482\n",
      "Iteration 19146, loss = 0.00931435\n",
      "Iteration 19147, loss = 0.00931387\n",
      "Iteration 19148, loss = 0.00931340\n",
      "Iteration 19149, loss = 0.00931293\n",
      "Iteration 19150, loss = 0.00931246\n",
      "Iteration 19151, loss = 0.00931199\n",
      "Iteration 19152, loss = 0.00931152\n",
      "Iteration 19153, loss = 0.00931105\n",
      "Iteration 19154, loss = 0.00931057\n",
      "Iteration 19155, loss = 0.00931010\n",
      "Iteration 19156, loss = 0.00930963\n",
      "Iteration 19157, loss = 0.00930916\n",
      "Iteration 19158, loss = 0.00930869\n",
      "Iteration 19159, loss = 0.00930822\n",
      "Iteration 19160, loss = 0.00930775\n",
      "Iteration 19161, loss = 0.00930728\n",
      "Iteration 19162, loss = 0.00930681\n",
      "Iteration 19163, loss = 0.00930633\n",
      "Iteration 19164, loss = 0.00930586\n",
      "Iteration 19165, loss = 0.00930539\n",
      "Iteration 19166, loss = 0.00930492\n",
      "Iteration 19167, loss = 0.00930445\n",
      "Iteration 19168, loss = 0.00930398\n",
      "Iteration 19169, loss = 0.00930351\n",
      "Iteration 19170, loss = 0.00930304\n",
      "Iteration 19171, loss = 0.00930257\n",
      "Iteration 19172, loss = 0.00930210\n",
      "Iteration 19173, loss = 0.00930163\n",
      "Iteration 19174, loss = 0.00930116\n",
      "Iteration 19175, loss = 0.00930069\n",
      "Iteration 19176, loss = 0.00930022\n",
      "Iteration 19177, loss = 0.00929975\n",
      "Iteration 19178, loss = 0.00929928\n",
      "Iteration 19179, loss = 0.00929881\n",
      "Iteration 19180, loss = 0.00929834\n",
      "Iteration 19181, loss = 0.00929787\n",
      "Iteration 19182, loss = 0.00929740\n",
      "Iteration 19183, loss = 0.00929693\n",
      "Iteration 19184, loss = 0.00929646\n",
      "Iteration 19185, loss = 0.00929600\n",
      "Iteration 19186, loss = 0.00929553\n",
      "Iteration 19187, loss = 0.00929506\n",
      "Iteration 19188, loss = 0.00929459\n",
      "Iteration 19189, loss = 0.00929412\n",
      "Iteration 19190, loss = 0.00929365\n",
      "Iteration 19191, loss = 0.00929318\n",
      "Iteration 19192, loss = 0.00929271\n",
      "Iteration 19193, loss = 0.00929224\n",
      "Iteration 19194, loss = 0.00929178\n",
      "Iteration 19195, loss = 0.00929131\n",
      "Iteration 19196, loss = 0.00929084\n",
      "Iteration 19197, loss = 0.00929037\n",
      "Iteration 19198, loss = 0.00928990\n",
      "Iteration 19199, loss = 0.00928943\n",
      "Iteration 19200, loss = 0.00928897\n",
      "Iteration 19201, loss = 0.00928850\n",
      "Iteration 19202, loss = 0.00928803\n",
      "Iteration 19203, loss = 0.00928756\n",
      "Iteration 19204, loss = 0.00928709\n",
      "Iteration 19205, loss = 0.00928663\n",
      "Iteration 19206, loss = 0.00928616\n",
      "Iteration 19207, loss = 0.00928569\n",
      "Iteration 19208, loss = 0.00928522\n",
      "Iteration 19209, loss = 0.00928475\n",
      "Iteration 19210, loss = 0.00928429\n",
      "Iteration 19211, loss = 0.00928382\n",
      "Iteration 19212, loss = 0.00928335\n",
      "Iteration 19213, loss = 0.00928288\n",
      "Iteration 19214, loss = 0.00928242\n",
      "Iteration 19215, loss = 0.00928195\n",
      "Iteration 19216, loss = 0.00928148\n",
      "Iteration 19217, loss = 0.00928102\n",
      "Iteration 19218, loss = 0.00928055\n",
      "Iteration 19219, loss = 0.00928008\n",
      "Iteration 19220, loss = 0.00927962\n",
      "Iteration 19221, loss = 0.00927915\n",
      "Iteration 19222, loss = 0.00927868\n",
      "Iteration 19223, loss = 0.00927822\n",
      "Iteration 19224, loss = 0.00927775\n",
      "Iteration 19225, loss = 0.00927728\n",
      "Iteration 19226, loss = 0.00927682\n",
      "Iteration 19227, loss = 0.00927635\n",
      "Iteration 19228, loss = 0.00927588\n",
      "Iteration 19229, loss = 0.00927542\n",
      "Iteration 19230, loss = 0.00927495\n",
      "Iteration 19231, loss = 0.00927448\n",
      "Iteration 19232, loss = 0.00927402\n",
      "Iteration 19233, loss = 0.00927355\n",
      "Iteration 19234, loss = 0.00927309\n",
      "Iteration 19235, loss = 0.00927262\n",
      "Iteration 19236, loss = 0.00927216\n",
      "Iteration 19237, loss = 0.00927169\n",
      "Iteration 19238, loss = 0.00927122\n",
      "Iteration 19239, loss = 0.00927076\n",
      "Iteration 19240, loss = 0.00927029\n",
      "Iteration 19241, loss = 0.00926983\n",
      "Iteration 19242, loss = 0.00926936\n",
      "Iteration 19243, loss = 0.00926890\n",
      "Iteration 19244, loss = 0.00926843\n",
      "Iteration 19245, loss = 0.00926797\n",
      "Iteration 19246, loss = 0.00926750\n",
      "Iteration 19247, loss = 0.00926704\n",
      "Iteration 19248, loss = 0.00926657\n",
      "Iteration 19249, loss = 0.00926611\n",
      "Iteration 19250, loss = 0.00926564\n",
      "Iteration 19251, loss = 0.00926518\n",
      "Iteration 19252, loss = 0.00926471\n",
      "Iteration 19253, loss = 0.00926425\n",
      "Iteration 19254, loss = 0.00926378\n",
      "Iteration 19255, loss = 0.00926332\n",
      "Iteration 19256, loss = 0.00926285\n",
      "Iteration 19257, loss = 0.00926239\n",
      "Iteration 19258, loss = 0.00926193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19259, loss = 0.00926146\n",
      "Iteration 19260, loss = 0.00926100\n",
      "Iteration 19261, loss = 0.00926053\n",
      "Iteration 19262, loss = 0.00926007\n",
      "Iteration 19263, loss = 0.00925961\n",
      "Iteration 19264, loss = 0.00925914\n",
      "Iteration 19265, loss = 0.00925868\n",
      "Iteration 19266, loss = 0.00925821\n",
      "Iteration 19267, loss = 0.00925775\n",
      "Iteration 19268, loss = 0.00925729\n",
      "Iteration 19269, loss = 0.00925682\n",
      "Iteration 19270, loss = 0.00925636\n",
      "Iteration 19271, loss = 0.00925590\n",
      "Iteration 19272, loss = 0.00925543\n",
      "Iteration 19273, loss = 0.00925497\n",
      "Iteration 19274, loss = 0.00925451\n",
      "Iteration 19275, loss = 0.00925404\n",
      "Iteration 19276, loss = 0.00925358\n",
      "Iteration 19277, loss = 0.00925312\n",
      "Iteration 19278, loss = 0.00925265\n",
      "Iteration 19279, loss = 0.00925219\n",
      "Iteration 19280, loss = 0.00925173\n",
      "Iteration 19281, loss = 0.00925127\n",
      "Iteration 19282, loss = 0.00925080\n",
      "Iteration 19283, loss = 0.00925034\n",
      "Iteration 19284, loss = 0.00924988\n",
      "Iteration 19285, loss = 0.00924942\n",
      "Iteration 19286, loss = 0.00924895\n",
      "Iteration 19287, loss = 0.00924849\n",
      "Iteration 19288, loss = 0.00924803\n",
      "Iteration 19289, loss = 0.00924757\n",
      "Iteration 19290, loss = 0.00924710\n",
      "Iteration 19291, loss = 0.00924664\n",
      "Iteration 19292, loss = 0.00924618\n",
      "Iteration 19293, loss = 0.00924572\n",
      "Iteration 19294, loss = 0.00924526\n",
      "Iteration 19295, loss = 0.00924479\n",
      "Iteration 19296, loss = 0.00924433\n",
      "Iteration 19297, loss = 0.00924387\n",
      "Iteration 19298, loss = 0.00924341\n",
      "Iteration 19299, loss = 0.00924295\n",
      "Iteration 19300, loss = 0.00924249\n",
      "Iteration 19301, loss = 0.00924203\n",
      "Iteration 19302, loss = 0.00924156\n",
      "Iteration 19303, loss = 0.00924110\n",
      "Iteration 19304, loss = 0.00924064\n",
      "Iteration 19305, loss = 0.00924018\n",
      "Iteration 19306, loss = 0.00923972\n",
      "Iteration 19307, loss = 0.00923926\n",
      "Iteration 19308, loss = 0.00923880\n",
      "Iteration 19309, loss = 0.00923834\n",
      "Iteration 19310, loss = 0.00923788\n",
      "Iteration 19311, loss = 0.00923742\n",
      "Iteration 19312, loss = 0.00923695\n",
      "Iteration 19313, loss = 0.00923649\n",
      "Iteration 19314, loss = 0.00923603\n",
      "Iteration 19315, loss = 0.00923557\n",
      "Iteration 19316, loss = 0.00923511\n",
      "Iteration 19317, loss = 0.00923465\n",
      "Iteration 19318, loss = 0.00923419\n",
      "Iteration 19319, loss = 0.00923373\n",
      "Iteration 19320, loss = 0.00923327\n",
      "Iteration 19321, loss = 0.00923281\n",
      "Iteration 19322, loss = 0.00923235\n",
      "Iteration 19323, loss = 0.00923189\n",
      "Iteration 19324, loss = 0.00923143\n",
      "Iteration 19325, loss = 0.00923097\n",
      "Iteration 19326, loss = 0.00923051\n",
      "Iteration 19327, loss = 0.00923005\n",
      "Iteration 19328, loss = 0.00922959\n",
      "Iteration 19329, loss = 0.00922913\n",
      "Iteration 19330, loss = 0.00922867\n",
      "Iteration 19331, loss = 0.00922821\n",
      "Iteration 19332, loss = 0.00922776\n",
      "Iteration 19333, loss = 0.00922730\n",
      "Iteration 19334, loss = 0.00922684\n",
      "Iteration 19335, loss = 0.00922638\n",
      "Iteration 19336, loss = 0.00922592\n",
      "Iteration 19337, loss = 0.00922546\n",
      "Iteration 19338, loss = 0.00922500\n",
      "Iteration 19339, loss = 0.00922454\n",
      "Iteration 19340, loss = 0.00922408\n",
      "Iteration 19341, loss = 0.00922362\n",
      "Iteration 19342, loss = 0.00922317\n",
      "Iteration 19343, loss = 0.00922271\n",
      "Iteration 19344, loss = 0.00922225\n",
      "Iteration 19345, loss = 0.00922179\n",
      "Iteration 19346, loss = 0.00922133\n",
      "Iteration 19347, loss = 0.00922087\n",
      "Iteration 19348, loss = 0.00922042\n",
      "Iteration 19349, loss = 0.00921996\n",
      "Iteration 19350, loss = 0.00921950\n",
      "Iteration 19351, loss = 0.00921904\n",
      "Iteration 19352, loss = 0.00921858\n",
      "Iteration 19353, loss = 0.00921812\n",
      "Iteration 19354, loss = 0.00921767\n",
      "Iteration 19355, loss = 0.00921721\n",
      "Iteration 19356, loss = 0.00921675\n",
      "Iteration 19357, loss = 0.00921629\n",
      "Iteration 19358, loss = 0.00921584\n",
      "Iteration 19359, loss = 0.00921538\n",
      "Iteration 19360, loss = 0.00921492\n",
      "Iteration 19361, loss = 0.00921446\n",
      "Iteration 19362, loss = 0.00921401\n",
      "Iteration 19363, loss = 0.00921355\n",
      "Iteration 19364, loss = 0.00921309\n",
      "Iteration 19365, loss = 0.00921263\n",
      "Iteration 19366, loss = 0.00921218\n",
      "Iteration 19367, loss = 0.00921172\n",
      "Iteration 19368, loss = 0.00921126\n",
      "Iteration 19369, loss = 0.00921081\n",
      "Iteration 19370, loss = 0.00921035\n",
      "Iteration 19371, loss = 0.00920989\n",
      "Iteration 19372, loss = 0.00920944\n",
      "Iteration 19373, loss = 0.00920898\n",
      "Iteration 19374, loss = 0.00920852\n",
      "Iteration 19375, loss = 0.00920807\n",
      "Iteration 19376, loss = 0.00920761\n",
      "Iteration 19377, loss = 0.00920715\n",
      "Iteration 19378, loss = 0.00920670\n",
      "Iteration 19379, loss = 0.00920624\n",
      "Iteration 19380, loss = 0.00920579\n",
      "Iteration 19381, loss = 0.00920533\n",
      "Iteration 19382, loss = 0.00920487\n",
      "Iteration 19383, loss = 0.00920442\n",
      "Iteration 19384, loss = 0.00920396\n",
      "Iteration 19385, loss = 0.00920351\n",
      "Iteration 19386, loss = 0.00920305\n",
      "Iteration 19387, loss = 0.00920259\n",
      "Iteration 19388, loss = 0.00920214\n",
      "Iteration 19389, loss = 0.00920168\n",
      "Iteration 19390, loss = 0.00920123\n",
      "Iteration 19391, loss = 0.00920077\n",
      "Iteration 19392, loss = 0.00920032\n",
      "Iteration 19393, loss = 0.00919986\n",
      "Iteration 19394, loss = 0.00919941\n",
      "Iteration 19395, loss = 0.00919895\n",
      "Iteration 19396, loss = 0.00919850\n",
      "Iteration 19397, loss = 0.00919804\n",
      "Iteration 19398, loss = 0.00919759\n",
      "Iteration 19399, loss = 0.00919713\n",
      "Iteration 19400, loss = 0.00919668\n",
      "Iteration 19401, loss = 0.00919622\n",
      "Iteration 19402, loss = 0.00919577\n",
      "Iteration 19403, loss = 0.00919531\n",
      "Iteration 19404, loss = 0.00919486\n",
      "Iteration 19405, loss = 0.00919440\n",
      "Iteration 19406, loss = 0.00919395\n",
      "Iteration 19407, loss = 0.00919349\n",
      "Iteration 19408, loss = 0.00919304\n",
      "Iteration 19409, loss = 0.00919259\n",
      "Iteration 19410, loss = 0.00919213\n",
      "Iteration 19411, loss = 0.00919168\n",
      "Iteration 19412, loss = 0.00919122\n",
      "Iteration 19413, loss = 0.00919077\n",
      "Iteration 19414, loss = 0.00919032\n",
      "Iteration 19415, loss = 0.00918986\n",
      "Iteration 19416, loss = 0.00918941\n",
      "Iteration 19417, loss = 0.00918895\n",
      "Iteration 19418, loss = 0.00918850\n",
      "Iteration 19419, loss = 0.00918805\n",
      "Iteration 19420, loss = 0.00918759\n",
      "Iteration 19421, loss = 0.00918714\n",
      "Iteration 19422, loss = 0.00918669\n",
      "Iteration 19423, loss = 0.00918623\n",
      "Iteration 19424, loss = 0.00918578\n",
      "Iteration 19425, loss = 0.00918533\n",
      "Iteration 19426, loss = 0.00918487\n",
      "Iteration 19427, loss = 0.00918442\n",
      "Iteration 19428, loss = 0.00918397\n",
      "Iteration 19429, loss = 0.00918351\n",
      "Iteration 19430, loss = 0.00918306\n",
      "Iteration 19431, loss = 0.00918261\n",
      "Iteration 19432, loss = 0.00918216\n",
      "Iteration 19433, loss = 0.00918170\n",
      "Iteration 19434, loss = 0.00918125\n",
      "Iteration 19435, loss = 0.00918080\n",
      "Iteration 19436, loss = 0.00918035\n",
      "Iteration 19437, loss = 0.00917989\n",
      "Iteration 19438, loss = 0.00917944\n",
      "Iteration 19439, loss = 0.00917899\n",
      "Iteration 19440, loss = 0.00917854\n",
      "Iteration 19441, loss = 0.00917808\n",
      "Iteration 19442, loss = 0.00917763\n",
      "Iteration 19443, loss = 0.00917718\n",
      "Iteration 19444, loss = 0.00917673\n",
      "Iteration 19445, loss = 0.00917628\n",
      "Iteration 19446, loss = 0.00917582\n",
      "Iteration 19447, loss = 0.00917537\n",
      "Iteration 19448, loss = 0.00917492\n",
      "Iteration 19449, loss = 0.00917447\n",
      "Iteration 19450, loss = 0.00917402\n",
      "Iteration 19451, loss = 0.00917357\n",
      "Iteration 19452, loss = 0.00917311\n",
      "Iteration 19453, loss = 0.00917266\n",
      "Iteration 19454, loss = 0.00917221\n",
      "Iteration 19455, loss = 0.00917176\n",
      "Iteration 19456, loss = 0.00917131\n",
      "Iteration 19457, loss = 0.00917086\n",
      "Iteration 19458, loss = 0.00917041\n",
      "Iteration 19459, loss = 0.00916996\n",
      "Iteration 19460, loss = 0.00916951\n",
      "Iteration 19461, loss = 0.00916905\n",
      "Iteration 19462, loss = 0.00916860\n",
      "Iteration 19463, loss = 0.00916815\n",
      "Iteration 19464, loss = 0.00916770\n",
      "Iteration 19465, loss = 0.00916725\n",
      "Iteration 19466, loss = 0.00916680\n",
      "Iteration 19467, loss = 0.00916635\n",
      "Iteration 19468, loss = 0.00916590\n",
      "Iteration 19469, loss = 0.00916545\n",
      "Iteration 19470, loss = 0.00916500\n",
      "Iteration 19471, loss = 0.00916455\n",
      "Iteration 19472, loss = 0.00916410\n",
      "Iteration 19473, loss = 0.00916365\n",
      "Iteration 19474, loss = 0.00916320\n",
      "Iteration 19475, loss = 0.00916275\n",
      "Iteration 19476, loss = 0.00916230\n",
      "Iteration 19477, loss = 0.00916185\n",
      "Iteration 19478, loss = 0.00916140\n",
      "Iteration 19479, loss = 0.00916095\n",
      "Iteration 19480, loss = 0.00916050\n",
      "Iteration 19481, loss = 0.00916005\n",
      "Iteration 19482, loss = 0.00915960\n",
      "Iteration 19483, loss = 0.00915915\n",
      "Iteration 19484, loss = 0.00915870\n",
      "Iteration 19485, loss = 0.00915825\n",
      "Iteration 19486, loss = 0.00915780\n",
      "Iteration 19487, loss = 0.00915735\n",
      "Iteration 19488, loss = 0.00915691\n",
      "Iteration 19489, loss = 0.00915646\n",
      "Iteration 19490, loss = 0.00915601\n",
      "Iteration 19491, loss = 0.00915556\n",
      "Iteration 19492, loss = 0.00915511\n",
      "Iteration 19493, loss = 0.00915466\n",
      "Iteration 19494, loss = 0.00915421\n",
      "Iteration 19495, loss = 0.00915376\n",
      "Iteration 19496, loss = 0.00915331\n",
      "Iteration 19497, loss = 0.00915287\n",
      "Iteration 19498, loss = 0.00915242\n",
      "Iteration 19499, loss = 0.00915197\n",
      "Iteration 19500, loss = 0.00915152\n",
      "Iteration 19501, loss = 0.00915107\n",
      "Iteration 19502, loss = 0.00915062\n",
      "Iteration 19503, loss = 0.00915018\n",
      "Iteration 19504, loss = 0.00914973\n",
      "Iteration 19505, loss = 0.00914928\n",
      "Iteration 19506, loss = 0.00914883\n",
      "Iteration 19507, loss = 0.00914838\n",
      "Iteration 19508, loss = 0.00914794\n",
      "Iteration 19509, loss = 0.00914749\n",
      "Iteration 19510, loss = 0.00914704\n",
      "Iteration 19511, loss = 0.00914659\n",
      "Iteration 19512, loss = 0.00914615\n",
      "Iteration 19513, loss = 0.00914570\n",
      "Iteration 19514, loss = 0.00914525\n",
      "Iteration 19515, loss = 0.00914480\n",
      "Iteration 19516, loss = 0.00914436\n",
      "Iteration 19517, loss = 0.00914391\n",
      "Iteration 19518, loss = 0.00914346\n",
      "Iteration 19519, loss = 0.00914301\n",
      "Iteration 19520, loss = 0.00914257\n",
      "Iteration 19521, loss = 0.00914212\n",
      "Iteration 19522, loss = 0.00914167\n",
      "Iteration 19523, loss = 0.00914123\n",
      "Iteration 19524, loss = 0.00914078\n",
      "Iteration 19525, loss = 0.00914033\n",
      "Iteration 19526, loss = 0.00913989\n",
      "Iteration 19527, loss = 0.00913944\n",
      "Iteration 19528, loss = 0.00913899\n",
      "Iteration 19529, loss = 0.00913855\n",
      "Iteration 19530, loss = 0.00913810\n",
      "Iteration 19531, loss = 0.00913765\n",
      "Iteration 19532, loss = 0.00913721\n",
      "Iteration 19533, loss = 0.00913676\n",
      "Iteration 19534, loss = 0.00913631\n",
      "Iteration 19535, loss = 0.00913587\n",
      "Iteration 19536, loss = 0.00913542\n",
      "Iteration 19537, loss = 0.00913498\n",
      "Iteration 19538, loss = 0.00913453\n",
      "Iteration 19539, loss = 0.00913408\n",
      "Iteration 19540, loss = 0.00913364\n",
      "Iteration 19541, loss = 0.00913319\n",
      "Iteration 19542, loss = 0.00913275\n",
      "Iteration 19543, loss = 0.00913230\n",
      "Iteration 19544, loss = 0.00913186\n",
      "Iteration 19545, loss = 0.00913141\n",
      "Iteration 19546, loss = 0.00913097\n",
      "Iteration 19547, loss = 0.00913052\n",
      "Iteration 19548, loss = 0.00913007\n",
      "Iteration 19549, loss = 0.00912963\n",
      "Iteration 19550, loss = 0.00912918\n",
      "Iteration 19551, loss = 0.00912874\n",
      "Iteration 19552, loss = 0.00912829\n",
      "Iteration 19553, loss = 0.00912785\n",
      "Iteration 19554, loss = 0.00912740\n",
      "Iteration 19555, loss = 0.00912696\n",
      "Iteration 19556, loss = 0.00912651\n",
      "Iteration 19557, loss = 0.00912607\n",
      "Iteration 19558, loss = 0.00912563\n",
      "Iteration 19559, loss = 0.00912518\n",
      "Iteration 19560, loss = 0.00912474\n",
      "Iteration 19561, loss = 0.00912429\n",
      "Iteration 19562, loss = 0.00912385\n",
      "Iteration 19563, loss = 0.00912340\n",
      "Iteration 19564, loss = 0.00912296\n",
      "Iteration 19565, loss = 0.00912251\n",
      "Iteration 19566, loss = 0.00912207\n",
      "Iteration 19567, loss = 0.00912163\n",
      "Iteration 19568, loss = 0.00912118\n",
      "Iteration 19569, loss = 0.00912074\n",
      "Iteration 19570, loss = 0.00912029\n",
      "Iteration 19571, loss = 0.00911985\n",
      "Iteration 19572, loss = 0.00911941\n",
      "Iteration 19573, loss = 0.00911896\n",
      "Iteration 19574, loss = 0.00911852\n",
      "Iteration 19575, loss = 0.00911808\n",
      "Iteration 19576, loss = 0.00911763\n",
      "Iteration 19577, loss = 0.00911719\n",
      "Iteration 19578, loss = 0.00911675\n",
      "Iteration 19579, loss = 0.00911630\n",
      "Iteration 19580, loss = 0.00911586\n",
      "Iteration 19581, loss = 0.00911542\n",
      "Iteration 19582, loss = 0.00911497\n",
      "Iteration 19583, loss = 0.00911453\n",
      "Iteration 19584, loss = 0.00911409\n",
      "Iteration 19585, loss = 0.00911364\n",
      "Iteration 19586, loss = 0.00911320\n",
      "Iteration 19587, loss = 0.00911276\n",
      "Iteration 19588, loss = 0.00911232\n",
      "Iteration 19589, loss = 0.00911187\n",
      "Iteration 19590, loss = 0.00911143\n",
      "Iteration 19591, loss = 0.00911099\n",
      "Iteration 19592, loss = 0.00911054\n",
      "Iteration 19593, loss = 0.00911010\n",
      "Iteration 19594, loss = 0.00910966\n",
      "Iteration 19595, loss = 0.00910922\n",
      "Iteration 19596, loss = 0.00910878\n",
      "Iteration 19597, loss = 0.00910833\n",
      "Iteration 19598, loss = 0.00910789\n",
      "Iteration 19599, loss = 0.00910745\n",
      "Iteration 19600, loss = 0.00910701\n",
      "Iteration 19601, loss = 0.00910657\n",
      "Iteration 19602, loss = 0.00910612\n",
      "Iteration 19603, loss = 0.00910568\n",
      "Iteration 19604, loss = 0.00910524\n",
      "Iteration 19605, loss = 0.00910480\n",
      "Iteration 19606, loss = 0.00910436\n",
      "Iteration 19607, loss = 0.00910391\n",
      "Iteration 19608, loss = 0.00910347\n",
      "Iteration 19609, loss = 0.00910303\n",
      "Iteration 19610, loss = 0.00910259\n",
      "Iteration 19611, loss = 0.00910215\n",
      "Iteration 19612, loss = 0.00910171\n",
      "Iteration 19613, loss = 0.00910127\n",
      "Iteration 19614, loss = 0.00910083\n",
      "Iteration 19615, loss = 0.00910038\n",
      "Iteration 19616, loss = 0.00909994\n",
      "Iteration 19617, loss = 0.00909950\n",
      "Iteration 19618, loss = 0.00909906\n",
      "Iteration 19619, loss = 0.00909862\n",
      "Iteration 19620, loss = 0.00909818\n",
      "Iteration 19621, loss = 0.00909774\n",
      "Iteration 19622, loss = 0.00909730\n",
      "Iteration 19623, loss = 0.00909686\n",
      "Iteration 19624, loss = 0.00909642\n",
      "Iteration 19625, loss = 0.00909598\n",
      "Iteration 19626, loss = 0.00909554\n",
      "Iteration 19627, loss = 0.00909510\n",
      "Iteration 19628, loss = 0.00909466\n",
      "Iteration 19629, loss = 0.00909422\n",
      "Iteration 19630, loss = 0.00909378\n",
      "Iteration 19631, loss = 0.00909334\n",
      "Iteration 19632, loss = 0.00909290\n",
      "Iteration 19633, loss = 0.00909246\n",
      "Iteration 19634, loss = 0.00909202\n",
      "Iteration 19635, loss = 0.00909158\n",
      "Iteration 19636, loss = 0.00909114\n",
      "Iteration 19637, loss = 0.00909070\n",
      "Iteration 19638, loss = 0.00909026\n",
      "Iteration 19639, loss = 0.00908982\n",
      "Iteration 19640, loss = 0.00908938\n",
      "Iteration 19641, loss = 0.00908894\n",
      "Iteration 19642, loss = 0.00908850\n",
      "Iteration 19643, loss = 0.00908806\n",
      "Iteration 19644, loss = 0.00908762\n",
      "Iteration 19645, loss = 0.00908718\n",
      "Iteration 19646, loss = 0.00908674\n",
      "Iteration 19647, loss = 0.00908630\n",
      "Iteration 19648, loss = 0.00908587\n",
      "Iteration 19649, loss = 0.00908543\n",
      "Iteration 19650, loss = 0.00908499\n",
      "Iteration 19651, loss = 0.00908455\n",
      "Iteration 19652, loss = 0.00908411\n",
      "Iteration 19653, loss = 0.00908367\n",
      "Iteration 19654, loss = 0.00908323\n",
      "Iteration 19655, loss = 0.00908279\n",
      "Iteration 19656, loss = 0.00908236\n",
      "Iteration 19657, loss = 0.00908192\n",
      "Iteration 19658, loss = 0.00908148\n",
      "Iteration 19659, loss = 0.00908104\n",
      "Iteration 19660, loss = 0.00908060\n",
      "Iteration 19661, loss = 0.00908016\n",
      "Iteration 19662, loss = 0.00907973\n",
      "Iteration 19663, loss = 0.00907929\n",
      "Iteration 19664, loss = 0.00907885\n",
      "Iteration 19665, loss = 0.00907841\n",
      "Iteration 19666, loss = 0.00907797\n",
      "Iteration 19667, loss = 0.00907754\n",
      "Iteration 19668, loss = 0.00907710\n",
      "Iteration 19669, loss = 0.00907666\n",
      "Iteration 19670, loss = 0.00907622\n",
      "Iteration 19671, loss = 0.00907579\n",
      "Iteration 19672, loss = 0.00907535\n",
      "Iteration 19673, loss = 0.00907491\n",
      "Iteration 19674, loss = 0.00907447\n",
      "Iteration 19675, loss = 0.00907404\n",
      "Iteration 19676, loss = 0.00907360\n",
      "Iteration 19677, loss = 0.00907316\n",
      "Iteration 19678, loss = 0.00907272\n",
      "Iteration 19679, loss = 0.00907229\n",
      "Iteration 19680, loss = 0.00907185\n",
      "Iteration 19681, loss = 0.00907141\n",
      "Iteration 19682, loss = 0.00907098\n",
      "Iteration 19683, loss = 0.00907054\n",
      "Iteration 19684, loss = 0.00907010\n",
      "Iteration 19685, loss = 0.00906967\n",
      "Iteration 19686, loss = 0.00906923\n",
      "Iteration 19687, loss = 0.00906879\n",
      "Iteration 19688, loss = 0.00906836\n",
      "Iteration 19689, loss = 0.00906792\n",
      "Iteration 19690, loss = 0.00906748\n",
      "Iteration 19691, loss = 0.00906705\n",
      "Iteration 19692, loss = 0.00906661\n",
      "Iteration 19693, loss = 0.00906617\n",
      "Iteration 19694, loss = 0.00906574\n",
      "Iteration 19695, loss = 0.00906530\n",
      "Iteration 19696, loss = 0.00906487\n",
      "Iteration 19697, loss = 0.00906443\n",
      "Iteration 19698, loss = 0.00906399\n",
      "Iteration 19699, loss = 0.00906356\n",
      "Iteration 19700, loss = 0.00906312\n",
      "Iteration 19701, loss = 0.00906269\n",
      "Iteration 19702, loss = 0.00906225\n",
      "Iteration 19703, loss = 0.00906182\n",
      "Iteration 19704, loss = 0.00906138\n",
      "Iteration 19705, loss = 0.00906095\n",
      "Iteration 19706, loss = 0.00906051\n",
      "Iteration 19707, loss = 0.00906007\n",
      "Iteration 19708, loss = 0.00905964\n",
      "Iteration 19709, loss = 0.00905920\n",
      "Iteration 19710, loss = 0.00905877\n",
      "Iteration 19711, loss = 0.00905833\n",
      "Iteration 19712, loss = 0.00905790\n",
      "Iteration 19713, loss = 0.00905746\n",
      "Iteration 19714, loss = 0.00905703\n",
      "Iteration 19715, loss = 0.00905659\n",
      "Iteration 19716, loss = 0.00905616\n",
      "Iteration 19717, loss = 0.00905572\n",
      "Iteration 19718, loss = 0.00905529\n",
      "Iteration 19719, loss = 0.00905486\n",
      "Iteration 19720, loss = 0.00905442\n",
      "Iteration 19721, loss = 0.00905399\n",
      "Iteration 19722, loss = 0.00905355\n",
      "Iteration 19723, loss = 0.00905312\n",
      "Iteration 19724, loss = 0.00905268\n",
      "Iteration 19725, loss = 0.00905225\n",
      "Iteration 19726, loss = 0.00905182\n",
      "Iteration 19727, loss = 0.00905138\n",
      "Iteration 19728, loss = 0.00905095\n",
      "Iteration 19729, loss = 0.00905051\n",
      "Iteration 19730, loss = 0.00905008\n",
      "Iteration 19731, loss = 0.00904965\n",
      "Iteration 19732, loss = 0.00904921\n",
      "Iteration 19733, loss = 0.00904878\n",
      "Iteration 19734, loss = 0.00904834\n",
      "Iteration 19735, loss = 0.00904791\n",
      "Iteration 19736, loss = 0.00904748\n",
      "Iteration 19737, loss = 0.00904704\n",
      "Iteration 19738, loss = 0.00904661\n",
      "Iteration 19739, loss = 0.00904618\n",
      "Iteration 19740, loss = 0.00904574\n",
      "Iteration 19741, loss = 0.00904531\n",
      "Iteration 19742, loss = 0.00904488\n",
      "Iteration 19743, loss = 0.00904444\n",
      "Iteration 19744, loss = 0.00904401\n",
      "Iteration 19745, loss = 0.00904358\n",
      "Iteration 19746, loss = 0.00904314\n",
      "Iteration 19747, loss = 0.00904271\n",
      "Iteration 19748, loss = 0.00904228\n",
      "Iteration 19749, loss = 0.00904185\n",
      "Iteration 19750, loss = 0.00904141\n",
      "Iteration 19751, loss = 0.00904098\n",
      "Iteration 19752, loss = 0.00904055\n",
      "Iteration 19753, loss = 0.00904012\n",
      "Iteration 19754, loss = 0.00903968\n",
      "Iteration 19755, loss = 0.00903925\n",
      "Iteration 19756, loss = 0.00903882\n",
      "Iteration 19757, loss = 0.00903839\n",
      "Iteration 19758, loss = 0.00903795\n",
      "Iteration 19759, loss = 0.00903752\n",
      "Iteration 19760, loss = 0.00903709\n",
      "Iteration 19761, loss = 0.00903666\n",
      "Iteration 19762, loss = 0.00903623\n",
      "Iteration 19763, loss = 0.00903579\n",
      "Iteration 19764, loss = 0.00903536\n",
      "Iteration 19765, loss = 0.00903493\n",
      "Iteration 19766, loss = 0.00903450\n",
      "Iteration 19767, loss = 0.00903407\n",
      "Iteration 19768, loss = 0.00903364\n",
      "Iteration 19769, loss = 0.00903320\n",
      "Iteration 19770, loss = 0.00903277\n",
      "Iteration 19771, loss = 0.00903234\n",
      "Iteration 19772, loss = 0.00903191\n",
      "Iteration 19773, loss = 0.00903148\n",
      "Iteration 19774, loss = 0.00903105\n",
      "Iteration 19775, loss = 0.00903062\n",
      "Iteration 19776, loss = 0.00903018\n",
      "Iteration 19777, loss = 0.00902975\n",
      "Iteration 19778, loss = 0.00902932\n",
      "Iteration 19779, loss = 0.00902889\n",
      "Iteration 19780, loss = 0.00902846\n",
      "Iteration 19781, loss = 0.00902803\n",
      "Iteration 19782, loss = 0.00902760\n",
      "Iteration 19783, loss = 0.00902717\n",
      "Iteration 19784, loss = 0.00902674\n",
      "Iteration 19785, loss = 0.00902631\n",
      "Iteration 19786, loss = 0.00902588\n",
      "Iteration 19787, loss = 0.00902545\n",
      "Iteration 19788, loss = 0.00902502\n",
      "Iteration 19789, loss = 0.00902459\n",
      "Iteration 19790, loss = 0.00902416\n",
      "Iteration 19791, loss = 0.00902373\n",
      "Iteration 19792, loss = 0.00902330\n",
      "Iteration 19793, loss = 0.00902287\n",
      "Iteration 19794, loss = 0.00902244\n",
      "Iteration 19795, loss = 0.00902201\n",
      "Iteration 19796, loss = 0.00902158\n",
      "Iteration 19797, loss = 0.00902115\n",
      "Iteration 19798, loss = 0.00902072\n",
      "Iteration 19799, loss = 0.00902029\n",
      "Iteration 19800, loss = 0.00901986\n",
      "Iteration 19801, loss = 0.00901943\n",
      "Iteration 19802, loss = 0.00901900\n",
      "Iteration 19803, loss = 0.00901857\n",
      "Iteration 19804, loss = 0.00901814\n",
      "Iteration 19805, loss = 0.00901771\n",
      "Iteration 19806, loss = 0.00901728\n",
      "Iteration 19807, loss = 0.00901685\n",
      "Iteration 19808, loss = 0.00901642\n",
      "Iteration 19809, loss = 0.00901599\n",
      "Iteration 19810, loss = 0.00901556\n",
      "Iteration 19811, loss = 0.00901513\n",
      "Iteration 19812, loss = 0.00901471\n",
      "Iteration 19813, loss = 0.00901428\n",
      "Iteration 19814, loss = 0.00901385\n",
      "Iteration 19815, loss = 0.00901342\n",
      "Iteration 19816, loss = 0.00901299\n",
      "Iteration 19817, loss = 0.00901256\n",
      "Iteration 19818, loss = 0.00901213\n",
      "Iteration 19819, loss = 0.00901170\n",
      "Iteration 19820, loss = 0.00901128\n",
      "Iteration 19821, loss = 0.00901085\n",
      "Iteration 19822, loss = 0.00901042\n",
      "Iteration 19823, loss = 0.00900999\n",
      "Iteration 19824, loss = 0.00900956\n",
      "Iteration 19825, loss = 0.00900913\n",
      "Iteration 19826, loss = 0.00900871\n",
      "Iteration 19827, loss = 0.00900828\n",
      "Iteration 19828, loss = 0.00900785\n",
      "Iteration 19829, loss = 0.00900742\n",
      "Iteration 19830, loss = 0.00900700\n",
      "Iteration 19831, loss = 0.00900657\n",
      "Iteration 19832, loss = 0.00900614\n",
      "Iteration 19833, loss = 0.00900571\n",
      "Iteration 19834, loss = 0.00900528\n",
      "Iteration 19835, loss = 0.00900486\n",
      "Iteration 19836, loss = 0.00900443\n",
      "Iteration 19837, loss = 0.00900400\n",
      "Iteration 19838, loss = 0.00900357\n",
      "Iteration 19839, loss = 0.00900315\n",
      "Iteration 19840, loss = 0.00900272\n",
      "Iteration 19841, loss = 0.00900229\n",
      "Iteration 19842, loss = 0.00900187\n",
      "Iteration 19843, loss = 0.00900144\n",
      "Iteration 19844, loss = 0.00900101\n",
      "Iteration 19845, loss = 0.00900058\n",
      "Iteration 19846, loss = 0.00900016\n",
      "Iteration 19847, loss = 0.00899973\n",
      "Iteration 19848, loss = 0.00899930\n",
      "Iteration 19849, loss = 0.00899888\n",
      "Iteration 19850, loss = 0.00899845\n",
      "Iteration 19851, loss = 0.00899802\n",
      "Iteration 19852, loss = 0.00899760\n",
      "Iteration 19853, loss = 0.00899717\n",
      "Iteration 19854, loss = 0.00899675\n",
      "Iteration 19855, loss = 0.00899632\n",
      "Iteration 19856, loss = 0.00899589\n",
      "Iteration 19857, loss = 0.00899547\n",
      "Iteration 19858, loss = 0.00899504\n",
      "Iteration 19859, loss = 0.00899461\n",
      "Iteration 19860, loss = 0.00899419\n",
      "Iteration 19861, loss = 0.00899376\n",
      "Iteration 19862, loss = 0.00899334\n",
      "Iteration 19863, loss = 0.00899291\n",
      "Iteration 19864, loss = 0.00899248\n",
      "Iteration 19865, loss = 0.00899206\n",
      "Iteration 19866, loss = 0.00899163\n",
      "Iteration 19867, loss = 0.00899121\n",
      "Iteration 19868, loss = 0.00899078\n",
      "Iteration 19869, loss = 0.00899036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19870, loss = 0.00898993\n",
      "Iteration 19871, loss = 0.00898951\n",
      "Iteration 19872, loss = 0.00898908\n",
      "Iteration 19873, loss = 0.00898865\n",
      "Iteration 19874, loss = 0.00898823\n",
      "Iteration 19875, loss = 0.00898780\n",
      "Iteration 19876, loss = 0.00898738\n",
      "Iteration 19877, loss = 0.00898695\n",
      "Iteration 19878, loss = 0.00898653\n",
      "Iteration 19879, loss = 0.00898610\n",
      "Iteration 19880, loss = 0.00898568\n",
      "Iteration 19881, loss = 0.00898525\n",
      "Iteration 19882, loss = 0.00898483\n",
      "Iteration 19883, loss = 0.00898441\n",
      "Iteration 19884, loss = 0.00898398\n",
      "Iteration 19885, loss = 0.00898356\n",
      "Iteration 19886, loss = 0.00898313\n",
      "Iteration 19887, loss = 0.00898271\n",
      "Iteration 19888, loss = 0.00898228\n",
      "Iteration 19889, loss = 0.00898186\n",
      "Iteration 19890, loss = 0.00898143\n",
      "Iteration 19891, loss = 0.00898101\n",
      "Iteration 19892, loss = 0.00898059\n",
      "Iteration 19893, loss = 0.00898016\n",
      "Iteration 19894, loss = 0.00897974\n",
      "Iteration 19895, loss = 0.00897931\n",
      "Iteration 19896, loss = 0.00897889\n",
      "Iteration 19897, loss = 0.00897847\n",
      "Iteration 19898, loss = 0.00897804\n",
      "Iteration 19899, loss = 0.00897762\n",
      "Iteration 19900, loss = 0.00897720\n",
      "Iteration 19901, loss = 0.00897677\n",
      "Iteration 19902, loss = 0.00897635\n",
      "Iteration 19903, loss = 0.00897592\n",
      "Iteration 19904, loss = 0.00897550\n",
      "Iteration 19905, loss = 0.00897508\n",
      "Iteration 19906, loss = 0.00897465\n",
      "Iteration 19907, loss = 0.00897423\n",
      "Iteration 19908, loss = 0.00897381\n",
      "Iteration 19909, loss = 0.00897338\n",
      "Iteration 19910, loss = 0.00897296\n",
      "Iteration 19911, loss = 0.00897254\n",
      "Iteration 19912, loss = 0.00897212\n",
      "Iteration 19913, loss = 0.00897169\n",
      "Iteration 19914, loss = 0.00897127\n",
      "Iteration 19915, loss = 0.00897085\n",
      "Iteration 19916, loss = 0.00897042\n",
      "Iteration 19917, loss = 0.00897000\n",
      "Iteration 19918, loss = 0.00896958\n",
      "Iteration 19919, loss = 0.00896916\n",
      "Iteration 19920, loss = 0.00896873\n",
      "Iteration 19921, loss = 0.00896831\n",
      "Iteration 19922, loss = 0.00896789\n",
      "Iteration 19923, loss = 0.00896747\n",
      "Iteration 19924, loss = 0.00896705\n",
      "Iteration 19925, loss = 0.00896662\n",
      "Iteration 19926, loss = 0.00896620\n",
      "Iteration 19927, loss = 0.00896578\n",
      "Iteration 19928, loss = 0.00896536\n",
      "Iteration 19929, loss = 0.00896493\n",
      "Iteration 19930, loss = 0.00896451\n",
      "Iteration 19931, loss = 0.00896409\n",
      "Iteration 19932, loss = 0.00896367\n",
      "Iteration 19933, loss = 0.00896325\n",
      "Iteration 19934, loss = 0.00896283\n",
      "Iteration 19935, loss = 0.00896240\n",
      "Iteration 19936, loss = 0.00896198\n",
      "Iteration 19937, loss = 0.00896156\n",
      "Iteration 19938, loss = 0.00896114\n",
      "Iteration 19939, loss = 0.00896072\n",
      "Iteration 19940, loss = 0.00896030\n",
      "Iteration 19941, loss = 0.00895988\n",
      "Iteration 19942, loss = 0.00895946\n",
      "Iteration 19943, loss = 0.00895903\n",
      "Iteration 19944, loss = 0.00895861\n",
      "Iteration 19945, loss = 0.00895819\n",
      "Iteration 19946, loss = 0.00895777\n",
      "Iteration 19947, loss = 0.00895735\n",
      "Iteration 19948, loss = 0.00895693\n",
      "Iteration 19949, loss = 0.00895651\n",
      "Iteration 19950, loss = 0.00895609\n",
      "Iteration 19951, loss = 0.00895567\n",
      "Iteration 19952, loss = 0.00895525\n",
      "Iteration 19953, loss = 0.00895483\n",
      "Iteration 19954, loss = 0.00895441\n",
      "Iteration 19955, loss = 0.00895399\n",
      "Iteration 19956, loss = 0.00895356\n",
      "Iteration 19957, loss = 0.00895314\n",
      "Iteration 19958, loss = 0.00895272\n",
      "Iteration 19959, loss = 0.00895230\n",
      "Iteration 19960, loss = 0.00895188\n",
      "Iteration 19961, loss = 0.00895146\n",
      "Iteration 19962, loss = 0.00895104\n",
      "Iteration 19963, loss = 0.00895062\n",
      "Iteration 19964, loss = 0.00895020\n",
      "Iteration 19965, loss = 0.00894978\n",
      "Iteration 19966, loss = 0.00894936\n",
      "Iteration 19967, loss = 0.00894894\n",
      "Iteration 19968, loss = 0.00894853\n",
      "Iteration 19969, loss = 0.00894811\n",
      "Iteration 19970, loss = 0.00894769\n",
      "Iteration 19971, loss = 0.00894727\n",
      "Iteration 19972, loss = 0.00894685\n",
      "Iteration 19973, loss = 0.00894643\n",
      "Iteration 19974, loss = 0.00894601\n",
      "Iteration 19975, loss = 0.00894559\n",
      "Iteration 19976, loss = 0.00894517\n",
      "Iteration 19977, loss = 0.00894475\n",
      "Iteration 19978, loss = 0.00894433\n",
      "Iteration 19979, loss = 0.00894391\n",
      "Iteration 19980, loss = 0.00894349\n",
      "Iteration 19981, loss = 0.00894308\n",
      "Iteration 19982, loss = 0.00894266\n",
      "Iteration 19983, loss = 0.00894224\n",
      "Iteration 19984, loss = 0.00894182\n",
      "Iteration 19985, loss = 0.00894140\n",
      "Iteration 19986, loss = 0.00894098\n",
      "Iteration 19987, loss = 0.00894056\n",
      "Iteration 19988, loss = 0.00894014\n",
      "Iteration 19989, loss = 0.00893973\n",
      "Iteration 19990, loss = 0.00893931\n",
      "Iteration 19991, loss = 0.00893889\n",
      "Iteration 19992, loss = 0.00893847\n",
      "Iteration 19993, loss = 0.00893805\n",
      "Iteration 19994, loss = 0.00893763\n",
      "Iteration 19995, loss = 0.00893722\n",
      "Iteration 19996, loss = 0.00893680\n",
      "Iteration 19997, loss = 0.00893638\n",
      "Iteration 19998, loss = 0.00893596\n",
      "Iteration 19999, loss = 0.00893555\n",
      "Iteration 20000, loss = 0.00893513\n",
      "Iteration 20001, loss = 0.00893471\n",
      "Iteration 20002, loss = 0.00893429\n",
      "Iteration 20003, loss = 0.00893387\n",
      "Iteration 20004, loss = 0.00893346\n",
      "Iteration 20005, loss = 0.00893304\n",
      "Iteration 20006, loss = 0.00893262\n",
      "Iteration 20007, loss = 0.00893220\n",
      "Iteration 20008, loss = 0.00893179\n",
      "Iteration 20009, loss = 0.00893137\n",
      "Iteration 20010, loss = 0.00893095\n",
      "Iteration 20011, loss = 0.00893054\n",
      "Iteration 20012, loss = 0.00893012\n",
      "Iteration 20013, loss = 0.00892970\n",
      "Iteration 20014, loss = 0.00892928\n",
      "Iteration 20015, loss = 0.00892887\n",
      "Iteration 20016, loss = 0.00892845\n",
      "Iteration 20017, loss = 0.00892803\n",
      "Iteration 20018, loss = 0.00892762\n",
      "Iteration 20019, loss = 0.00892720\n",
      "Iteration 20020, loss = 0.00892678\n",
      "Iteration 20021, loss = 0.00892637\n",
      "Iteration 20022, loss = 0.00892595\n",
      "Iteration 20023, loss = 0.00892553\n",
      "Iteration 20024, loss = 0.00892512\n",
      "Iteration 20025, loss = 0.00892470\n",
      "Iteration 20026, loss = 0.00892429\n",
      "Iteration 20027, loss = 0.00892387\n",
      "Iteration 20028, loss = 0.00892345\n",
      "Iteration 20029, loss = 0.00892304\n",
      "Iteration 20030, loss = 0.00892262\n",
      "Iteration 20031, loss = 0.00892221\n",
      "Iteration 20032, loss = 0.00892179\n",
      "Iteration 20033, loss = 0.00892137\n",
      "Iteration 20034, loss = 0.00892096\n",
      "Iteration 20035, loss = 0.00892054\n",
      "Iteration 20036, loss = 0.00892013\n",
      "Iteration 20037, loss = 0.00891971\n",
      "Iteration 20038, loss = 0.00891929\n",
      "Iteration 20039, loss = 0.00891888\n",
      "Iteration 20040, loss = 0.00891846\n",
      "Iteration 20041, loss = 0.00891805\n",
      "Iteration 20042, loss = 0.00891763\n",
      "Iteration 20043, loss = 0.00891722\n",
      "Iteration 20044, loss = 0.00891680\n",
      "Iteration 20045, loss = 0.00891639\n",
      "Iteration 20046, loss = 0.00891597\n",
      "Iteration 20047, loss = 0.00891556\n",
      "Iteration 20048, loss = 0.00891514\n",
      "Iteration 20049, loss = 0.00891473\n",
      "Iteration 20050, loss = 0.00891431\n",
      "Iteration 20051, loss = 0.00891390\n",
      "Iteration 20052, loss = 0.00891348\n",
      "Iteration 20053, loss = 0.00891307\n",
      "Iteration 20054, loss = 0.00891265\n",
      "Iteration 20055, loss = 0.00891224\n",
      "Iteration 20056, loss = 0.00891182\n",
      "Iteration 20057, loss = 0.00891141\n",
      "Iteration 20058, loss = 0.00891100\n",
      "Iteration 20059, loss = 0.00891058\n",
      "Iteration 20060, loss = 0.00891017\n",
      "Iteration 20061, loss = 0.00890975\n",
      "Iteration 20062, loss = 0.00890934\n",
      "Iteration 20063, loss = 0.00890892\n",
      "Iteration 20064, loss = 0.00890851\n",
      "Iteration 20065, loss = 0.00890810\n",
      "Iteration 20066, loss = 0.00890768\n",
      "Iteration 20067, loss = 0.00890727\n",
      "Iteration 20068, loss = 0.00890685\n",
      "Iteration 20069, loss = 0.00890644\n",
      "Iteration 20070, loss = 0.00890603\n",
      "Iteration 20071, loss = 0.00890561\n",
      "Iteration 20072, loss = 0.00890520\n",
      "Iteration 20073, loss = 0.00890479\n",
      "Iteration 20074, loss = 0.00890437\n",
      "Iteration 20075, loss = 0.00890396\n",
      "Iteration 20076, loss = 0.00890355\n",
      "Iteration 20077, loss = 0.00890313\n",
      "Iteration 20078, loss = 0.00890272\n",
      "Iteration 20079, loss = 0.00890231\n",
      "Iteration 20080, loss = 0.00890189\n",
      "Iteration 20081, loss = 0.00890148\n",
      "Iteration 20082, loss = 0.00890107\n",
      "Iteration 20083, loss = 0.00890065\n",
      "Iteration 20084, loss = 0.00890024\n",
      "Iteration 20085, loss = 0.00889983\n",
      "Iteration 20086, loss = 0.00889942\n",
      "Iteration 20087, loss = 0.00889900\n",
      "Iteration 20088, loss = 0.00889859\n",
      "Iteration 20089, loss = 0.00889818\n",
      "Iteration 20090, loss = 0.00889776\n",
      "Iteration 20091, loss = 0.00889735\n",
      "Iteration 20092, loss = 0.00889694\n",
      "Iteration 20093, loss = 0.00889653\n",
      "Iteration 20094, loss = 0.00889611\n",
      "Iteration 20095, loss = 0.00889570\n",
      "Iteration 20096, loss = 0.00889529\n",
      "Iteration 20097, loss = 0.00889488\n",
      "Iteration 20098, loss = 0.00889447\n",
      "Iteration 20099, loss = 0.00889405\n",
      "Iteration 20100, loss = 0.00889364\n",
      "Iteration 20101, loss = 0.00889323\n",
      "Iteration 20102, loss = 0.00889282\n",
      "Iteration 20103, loss = 0.00889241\n",
      "Iteration 20104, loss = 0.00889199\n",
      "Iteration 20105, loss = 0.00889158\n",
      "Iteration 20106, loss = 0.00889117\n",
      "Iteration 20107, loss = 0.00889076\n",
      "Iteration 20108, loss = 0.00889035\n",
      "Iteration 20109, loss = 0.00888994\n",
      "Iteration 20110, loss = 0.00888952\n",
      "Iteration 20111, loss = 0.00888911\n",
      "Iteration 20112, loss = 0.00888870\n",
      "Iteration 20113, loss = 0.00888829\n",
      "Iteration 20114, loss = 0.00888788\n",
      "Iteration 20115, loss = 0.00888747\n",
      "Iteration 20116, loss = 0.00888706\n",
      "Iteration 20117, loss = 0.00888665\n",
      "Iteration 20118, loss = 0.00888624\n",
      "Iteration 20119, loss = 0.00888582\n",
      "Iteration 20120, loss = 0.00888541\n",
      "Iteration 20121, loss = 0.00888500\n",
      "Iteration 20122, loss = 0.00888459\n",
      "Iteration 20123, loss = 0.00888418\n",
      "Iteration 20124, loss = 0.00888377\n",
      "Iteration 20125, loss = 0.00888336\n",
      "Iteration 20126, loss = 0.00888295\n",
      "Iteration 20127, loss = 0.00888254\n",
      "Iteration 20128, loss = 0.00888213\n",
      "Iteration 20129, loss = 0.00888172\n",
      "Iteration 20130, loss = 0.00888131\n",
      "Iteration 20131, loss = 0.00888090\n",
      "Iteration 20132, loss = 0.00888049\n",
      "Iteration 20133, loss = 0.00888008\n",
      "Iteration 20134, loss = 0.00887967\n",
      "Iteration 20135, loss = 0.00887926\n",
      "Iteration 20136, loss = 0.00887885\n",
      "Iteration 20137, loss = 0.00887844\n",
      "Iteration 20138, loss = 0.00887803\n",
      "Iteration 20139, loss = 0.00887762\n",
      "Iteration 20140, loss = 0.00887721\n",
      "Iteration 20141, loss = 0.00887680\n",
      "Iteration 20142, loss = 0.00887639\n",
      "Iteration 20143, loss = 0.00887598\n",
      "Iteration 20144, loss = 0.00887557\n",
      "Iteration 20145, loss = 0.00887516\n",
      "Iteration 20146, loss = 0.00887475\n",
      "Iteration 20147, loss = 0.00887434\n",
      "Iteration 20148, loss = 0.00887393\n",
      "Iteration 20149, loss = 0.00887352\n",
      "Iteration 20150, loss = 0.00887311\n",
      "Iteration 20151, loss = 0.00887270\n",
      "Iteration 20152, loss = 0.00887230\n",
      "Iteration 20153, loss = 0.00887189\n",
      "Iteration 20154, loss = 0.00887148\n",
      "Iteration 20155, loss = 0.00887107\n",
      "Iteration 20156, loss = 0.00887066\n",
      "Iteration 20157, loss = 0.00887025\n",
      "Iteration 20158, loss = 0.00886984\n",
      "Iteration 20159, loss = 0.00886943\n",
      "Iteration 20160, loss = 0.00886903\n",
      "Iteration 20161, loss = 0.00886862\n",
      "Iteration 20162, loss = 0.00886821\n",
      "Iteration 20163, loss = 0.00886780\n",
      "Iteration 20164, loss = 0.00886739\n",
      "Iteration 20165, loss = 0.00886698\n",
      "Iteration 20166, loss = 0.00886657\n",
      "Iteration 20167, loss = 0.00886617\n",
      "Iteration 20168, loss = 0.00886576\n",
      "Iteration 20169, loss = 0.00886535\n",
      "Iteration 20170, loss = 0.00886494\n",
      "Iteration 20171, loss = 0.00886453\n",
      "Iteration 20172, loss = 0.00886413\n",
      "Iteration 20173, loss = 0.00886372\n",
      "Iteration 20174, loss = 0.00886331\n",
      "Iteration 20175, loss = 0.00886290\n",
      "Iteration 20176, loss = 0.00886250\n",
      "Iteration 20177, loss = 0.00886209\n",
      "Iteration 20178, loss = 0.00886168\n",
      "Iteration 20179, loss = 0.00886127\n",
      "Iteration 20180, loss = 0.00886086\n",
      "Iteration 20181, loss = 0.00886046\n",
      "Iteration 20182, loss = 0.00886005\n",
      "Iteration 20183, loss = 0.00885964\n",
      "Iteration 20184, loss = 0.00885924\n",
      "Iteration 20185, loss = 0.00885883\n",
      "Iteration 20186, loss = 0.00885842\n",
      "Iteration 20187, loss = 0.00885801\n",
      "Iteration 20188, loss = 0.00885761\n",
      "Iteration 20189, loss = 0.00885720\n",
      "Iteration 20190, loss = 0.00885679\n",
      "Iteration 20191, loss = 0.00885639\n",
      "Iteration 20192, loss = 0.00885598\n",
      "Iteration 20193, loss = 0.00885557\n",
      "Iteration 20194, loss = 0.00885517\n",
      "Iteration 20195, loss = 0.00885476\n",
      "Iteration 20196, loss = 0.00885435\n",
      "Iteration 20197, loss = 0.00885395\n",
      "Iteration 20198, loss = 0.00885354\n",
      "Iteration 20199, loss = 0.00885313\n",
      "Iteration 20200, loss = 0.00885273\n",
      "Iteration 20201, loss = 0.00885232\n",
      "Iteration 20202, loss = 0.00885191\n",
      "Iteration 20203, loss = 0.00885151\n",
      "Iteration 20204, loss = 0.00885110\n",
      "Iteration 20205, loss = 0.00885070\n",
      "Iteration 20206, loss = 0.00885029\n",
      "Iteration 20207, loss = 0.00884988\n",
      "Iteration 20208, loss = 0.00884948\n",
      "Iteration 20209, loss = 0.00884907\n",
      "Iteration 20210, loss = 0.00884867\n",
      "Iteration 20211, loss = 0.00884826\n",
      "Iteration 20212, loss = 0.00884786\n",
      "Iteration 20213, loss = 0.00884745\n",
      "Iteration 20214, loss = 0.00884704\n",
      "Iteration 20215, loss = 0.00884664\n",
      "Iteration 20216, loss = 0.00884623\n",
      "Iteration 20217, loss = 0.00884583\n",
      "Iteration 20218, loss = 0.00884542\n",
      "Iteration 20219, loss = 0.00884502\n",
      "Iteration 20220, loss = 0.00884461\n",
      "Iteration 20221, loss = 0.00884421\n",
      "Iteration 20222, loss = 0.00884380\n",
      "Iteration 20223, loss = 0.00884340\n",
      "Iteration 20224, loss = 0.00884299\n",
      "Iteration 20225, loss = 0.00884259\n",
      "Iteration 20226, loss = 0.00884218\n",
      "Iteration 20227, loss = 0.00884178\n",
      "Iteration 20228, loss = 0.00884137\n",
      "Iteration 20229, loss = 0.00884097\n",
      "Iteration 20230, loss = 0.00884056\n",
      "Iteration 20231, loss = 0.00884016\n",
      "Iteration 20232, loss = 0.00883975\n",
      "Iteration 20233, loss = 0.00883935\n",
      "Iteration 20234, loss = 0.00883894\n",
      "Iteration 20235, loss = 0.00883854\n",
      "Iteration 20236, loss = 0.00883814\n",
      "Iteration 20237, loss = 0.00883773\n",
      "Iteration 20238, loss = 0.00883733\n",
      "Iteration 20239, loss = 0.00883692\n",
      "Iteration 20240, loss = 0.00883652\n",
      "Iteration 20241, loss = 0.00883611\n",
      "Iteration 20242, loss = 0.00883571\n",
      "Iteration 20243, loss = 0.00883531\n",
      "Iteration 20244, loss = 0.00883490\n",
      "Iteration 20245, loss = 0.00883450\n",
      "Iteration 20246, loss = 0.00883410\n",
      "Iteration 20247, loss = 0.00883369\n",
      "Iteration 20248, loss = 0.00883329\n",
      "Iteration 20249, loss = 0.00883288\n",
      "Iteration 20250, loss = 0.00883248\n",
      "Iteration 20251, loss = 0.00883208\n",
      "Iteration 20252, loss = 0.00883167\n",
      "Iteration 20253, loss = 0.00883127\n",
      "Iteration 20254, loss = 0.00883087\n",
      "Iteration 20255, loss = 0.00883046\n",
      "Iteration 20256, loss = 0.00883006\n",
      "Iteration 20257, loss = 0.00882966\n",
      "Iteration 20258, loss = 0.00882925\n",
      "Iteration 20259, loss = 0.00882885\n",
      "Iteration 20260, loss = 0.00882845\n",
      "Iteration 20261, loss = 0.00882805\n",
      "Iteration 20262, loss = 0.00882764\n",
      "Iteration 20263, loss = 0.00882724\n",
      "Iteration 20264, loss = 0.00882684\n",
      "Iteration 20265, loss = 0.00882643\n",
      "Iteration 20266, loss = 0.00882603\n",
      "Iteration 20267, loss = 0.00882563\n",
      "Iteration 20268, loss = 0.00882523\n",
      "Iteration 20269, loss = 0.00882482\n",
      "Iteration 20270, loss = 0.00882442\n",
      "Iteration 20271, loss = 0.00882402\n",
      "Iteration 20272, loss = 0.00882362\n",
      "Iteration 20273, loss = 0.00882321\n",
      "Iteration 20274, loss = 0.00882281\n",
      "Iteration 20275, loss = 0.00882241\n",
      "Iteration 20276, loss = 0.00882201\n",
      "Iteration 20277, loss = 0.00882161\n",
      "Iteration 20278, loss = 0.00882120\n",
      "Iteration 20279, loss = 0.00882080\n",
      "Iteration 20280, loss = 0.00882040\n",
      "Iteration 20281, loss = 0.00882000\n",
      "Iteration 20282, loss = 0.00881960\n",
      "Iteration 20283, loss = 0.00881919\n",
      "Iteration 20284, loss = 0.00881879\n",
      "Iteration 20285, loss = 0.00881839\n",
      "Iteration 20286, loss = 0.00881799\n",
      "Iteration 20287, loss = 0.00881759\n",
      "Iteration 20288, loss = 0.00881719\n",
      "Iteration 20289, loss = 0.00881678\n",
      "Iteration 20290, loss = 0.00881638\n",
      "Iteration 20291, loss = 0.00881598\n",
      "Iteration 20292, loss = 0.00881558\n",
      "Iteration 20293, loss = 0.00881518\n",
      "Iteration 20294, loss = 0.00881478\n",
      "Iteration 20295, loss = 0.00881438\n",
      "Iteration 20296, loss = 0.00881398\n",
      "Iteration 20297, loss = 0.00881358\n",
      "Iteration 20298, loss = 0.00881317\n",
      "Iteration 20299, loss = 0.00881277\n",
      "Iteration 20300, loss = 0.00881237\n",
      "Iteration 20301, loss = 0.00881197\n",
      "Iteration 20302, loss = 0.00881157\n",
      "Iteration 20303, loss = 0.00881117\n",
      "Iteration 20304, loss = 0.00881077\n",
      "Iteration 20305, loss = 0.00881037\n",
      "Iteration 20306, loss = 0.00880997\n",
      "Iteration 20307, loss = 0.00880957\n",
      "Iteration 20308, loss = 0.00880917\n",
      "Iteration 20309, loss = 0.00880877\n",
      "Iteration 20310, loss = 0.00880837\n",
      "Iteration 20311, loss = 0.00880797\n",
      "Iteration 20312, loss = 0.00880757\n",
      "Iteration 20313, loss = 0.00880717\n",
      "Iteration 20314, loss = 0.00880677\n",
      "Iteration 20315, loss = 0.00880637\n",
      "Iteration 20316, loss = 0.00880597\n",
      "Iteration 20317, loss = 0.00880557\n",
      "Iteration 20318, loss = 0.00880517\n",
      "Iteration 20319, loss = 0.00880477\n",
      "Iteration 20320, loss = 0.00880437\n",
      "Iteration 20321, loss = 0.00880397\n",
      "Iteration 20322, loss = 0.00880357\n",
      "Iteration 20323, loss = 0.00880317\n",
      "Iteration 20324, loss = 0.00880277\n",
      "Iteration 20325, loss = 0.00880237\n",
      "Iteration 20326, loss = 0.00880197\n",
      "Iteration 20327, loss = 0.00880157\n",
      "Iteration 20328, loss = 0.00880117\n",
      "Iteration 20329, loss = 0.00880077\n",
      "Iteration 20330, loss = 0.00880037\n",
      "Iteration 20331, loss = 0.00879998\n",
      "Iteration 20332, loss = 0.00879958\n",
      "Iteration 20333, loss = 0.00879918\n",
      "Iteration 20334, loss = 0.00879878\n",
      "Iteration 20335, loss = 0.00879838\n",
      "Iteration 20336, loss = 0.00879798\n",
      "Iteration 20337, loss = 0.00879758\n",
      "Iteration 20338, loss = 0.00879718\n",
      "Iteration 20339, loss = 0.00879678\n",
      "Iteration 20340, loss = 0.00879639\n",
      "Iteration 20341, loss = 0.00879599\n",
      "Iteration 20342, loss = 0.00879559\n",
      "Iteration 20343, loss = 0.00879519\n",
      "Iteration 20344, loss = 0.00879479\n",
      "Iteration 20345, loss = 0.00879439\n",
      "Iteration 20346, loss = 0.00879400\n",
      "Iteration 20347, loss = 0.00879360\n",
      "Iteration 20348, loss = 0.00879320\n",
      "Iteration 20349, loss = 0.00879280\n",
      "Iteration 20350, loss = 0.00879240\n",
      "Iteration 20351, loss = 0.00879200\n",
      "Iteration 20352, loss = 0.00879161\n",
      "Iteration 20353, loss = 0.00879121\n",
      "Iteration 20354, loss = 0.00879081\n",
      "Iteration 20355, loss = 0.00879041\n",
      "Iteration 20356, loss = 0.00879002\n",
      "Iteration 20357, loss = 0.00878962\n",
      "Iteration 20358, loss = 0.00878922\n",
      "Iteration 20359, loss = 0.00878882\n",
      "Iteration 20360, loss = 0.00878842\n",
      "Iteration 20361, loss = 0.00878803\n",
      "Iteration 20362, loss = 0.00878763\n",
      "Iteration 20363, loss = 0.00878723\n",
      "Iteration 20364, loss = 0.00878684\n",
      "Iteration 20365, loss = 0.00878644\n",
      "Iteration 20366, loss = 0.00878604\n",
      "Iteration 20367, loss = 0.00878564\n",
      "Iteration 20368, loss = 0.00878525\n",
      "Iteration 20369, loss = 0.00878485\n",
      "Iteration 20370, loss = 0.00878445\n",
      "Iteration 20371, loss = 0.00878406\n",
      "Iteration 20372, loss = 0.00878366\n",
      "Iteration 20373, loss = 0.00878326\n",
      "Iteration 20374, loss = 0.00878287\n",
      "Iteration 20375, loss = 0.00878247\n",
      "Iteration 20376, loss = 0.00878207\n",
      "Iteration 20377, loss = 0.00878168\n",
      "Iteration 20378, loss = 0.00878128\n",
      "Iteration 20379, loss = 0.00878088\n",
      "Iteration 20380, loss = 0.00878049\n",
      "Iteration 20381, loss = 0.00878009\n",
      "Iteration 20382, loss = 0.00877969\n",
      "Iteration 20383, loss = 0.00877930\n",
      "Iteration 20384, loss = 0.00877890\n",
      "Iteration 20385, loss = 0.00877850\n",
      "Iteration 20386, loss = 0.00877811\n",
      "Iteration 20387, loss = 0.00877771\n",
      "Iteration 20388, loss = 0.00877732\n",
      "Iteration 20389, loss = 0.00877692\n",
      "Iteration 20390, loss = 0.00877652\n",
      "Iteration 20391, loss = 0.00877613\n",
      "Iteration 20392, loss = 0.00877573\n",
      "Iteration 20393, loss = 0.00877534\n",
      "Iteration 20394, loss = 0.00877494\n",
      "Iteration 20395, loss = 0.00877455\n",
      "Iteration 20396, loss = 0.00877415\n",
      "Iteration 20397, loss = 0.00877375\n",
      "Iteration 20398, loss = 0.00877336\n",
      "Iteration 20399, loss = 0.00877296\n",
      "Iteration 20400, loss = 0.00877257\n",
      "Iteration 20401, loss = 0.00877217\n",
      "Iteration 20402, loss = 0.00877178\n",
      "Iteration 20403, loss = 0.00877138\n",
      "Iteration 20404, loss = 0.00877099\n",
      "Iteration 20405, loss = 0.00877059\n",
      "Iteration 20406, loss = 0.00877020\n",
      "Iteration 20407, loss = 0.00876980\n",
      "Iteration 20408, loss = 0.00876941\n",
      "Iteration 20409, loss = 0.00876901\n",
      "Iteration 20410, loss = 0.00876862\n",
      "Iteration 20411, loss = 0.00876822\n",
      "Iteration 20412, loss = 0.00876783\n",
      "Iteration 20413, loss = 0.00876743\n",
      "Iteration 20414, loss = 0.00876704\n",
      "Iteration 20415, loss = 0.00876664\n",
      "Iteration 20416, loss = 0.00876625\n",
      "Iteration 20417, loss = 0.00876586\n",
      "Iteration 20418, loss = 0.00876546\n",
      "Iteration 20419, loss = 0.00876507\n",
      "Iteration 20420, loss = 0.00876467\n",
      "Iteration 20421, loss = 0.00876428\n",
      "Iteration 20422, loss = 0.00876388\n",
      "Iteration 20423, loss = 0.00876349\n",
      "Iteration 20424, loss = 0.00876310\n",
      "Iteration 20425, loss = 0.00876270\n",
      "Iteration 20426, loss = 0.00876231\n",
      "Iteration 20427, loss = 0.00876191\n",
      "Iteration 20428, loss = 0.00876152\n",
      "Iteration 20429, loss = 0.00876113\n",
      "Iteration 20430, loss = 0.00876073\n",
      "Iteration 20431, loss = 0.00876034\n",
      "Iteration 20432, loss = 0.00875995\n",
      "Iteration 20433, loss = 0.00875955\n",
      "Iteration 20434, loss = 0.00875916\n",
      "Iteration 20435, loss = 0.00875876\n",
      "Iteration 20436, loss = 0.00875837\n",
      "Iteration 20437, loss = 0.00875798\n",
      "Iteration 20438, loss = 0.00875758\n",
      "Iteration 20439, loss = 0.00875719\n",
      "Iteration 20440, loss = 0.00875680\n",
      "Iteration 20441, loss = 0.00875640\n",
      "Iteration 20442, loss = 0.00875601\n",
      "Iteration 20443, loss = 0.00875562\n",
      "Iteration 20444, loss = 0.00875523\n",
      "Iteration 20445, loss = 0.00875483\n",
      "Iteration 20446, loss = 0.00875444\n",
      "Iteration 20447, loss = 0.00875405\n",
      "Iteration 20448, loss = 0.00875365\n",
      "Iteration 20449, loss = 0.00875326\n",
      "Iteration 20450, loss = 0.00875287\n",
      "Iteration 20451, loss = 0.00875248\n",
      "Iteration 20452, loss = 0.00875208\n",
      "Iteration 20453, loss = 0.00875169\n",
      "Iteration 20454, loss = 0.00875130\n",
      "Iteration 20455, loss = 0.00875091\n",
      "Iteration 20456, loss = 0.00875051\n",
      "Iteration 20457, loss = 0.00875012\n",
      "Iteration 20458, loss = 0.00874973\n",
      "Iteration 20459, loss = 0.00874934\n",
      "Iteration 20460, loss = 0.00874895\n",
      "Iteration 20461, loss = 0.00874855\n",
      "Iteration 20462, loss = 0.00874816\n",
      "Iteration 20463, loss = 0.00874777\n",
      "Iteration 20464, loss = 0.00874738\n",
      "Iteration 20465, loss = 0.00874699\n",
      "Iteration 20466, loss = 0.00874659\n",
      "Iteration 20467, loss = 0.00874620\n",
      "Iteration 20468, loss = 0.00874581\n",
      "Iteration 20469, loss = 0.00874542\n",
      "Iteration 20470, loss = 0.00874503\n",
      "Iteration 20471, loss = 0.00874464\n",
      "Iteration 20472, loss = 0.00874424\n",
      "Iteration 20473, loss = 0.00874385\n",
      "Iteration 20474, loss = 0.00874346\n",
      "Iteration 20475, loss = 0.00874307\n",
      "Iteration 20476, loss = 0.00874268\n",
      "Iteration 20477, loss = 0.00874229\n",
      "Iteration 20478, loss = 0.00874190\n",
      "Iteration 20479, loss = 0.00874150\n",
      "Iteration 20480, loss = 0.00874111\n",
      "Iteration 20481, loss = 0.00874072\n",
      "Iteration 20482, loss = 0.00874033\n",
      "Iteration 20483, loss = 0.00873994\n",
      "Iteration 20484, loss = 0.00873955\n",
      "Iteration 20485, loss = 0.00873916\n",
      "Iteration 20486, loss = 0.00873877\n",
      "Iteration 20487, loss = 0.00873838\n",
      "Iteration 20488, loss = 0.00873799\n",
      "Iteration 20489, loss = 0.00873760\n",
      "Iteration 20490, loss = 0.00873721\n",
      "Iteration 20491, loss = 0.00873682\n",
      "Iteration 20492, loss = 0.00873642\n",
      "Iteration 20493, loss = 0.00873603\n",
      "Iteration 20494, loss = 0.00873564\n",
      "Iteration 20495, loss = 0.00873525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20496, loss = 0.00873486\n",
      "Iteration 20497, loss = 0.00873447\n",
      "Iteration 20498, loss = 0.00873408\n",
      "Iteration 20499, loss = 0.00873369\n",
      "Iteration 20500, loss = 0.00873330\n",
      "Iteration 20501, loss = 0.00873291\n",
      "Iteration 20502, loss = 0.00873252\n",
      "Iteration 20503, loss = 0.00873213\n",
      "Iteration 20504, loss = 0.00873174\n",
      "Iteration 20505, loss = 0.00873135\n",
      "Iteration 20506, loss = 0.00873096\n",
      "Iteration 20507, loss = 0.00873057\n",
      "Iteration 20508, loss = 0.00873018\n",
      "Iteration 20509, loss = 0.00872980\n",
      "Iteration 20510, loss = 0.00872941\n",
      "Iteration 20511, loss = 0.00872902\n",
      "Iteration 20512, loss = 0.00872863\n",
      "Iteration 20513, loss = 0.00872824\n",
      "Iteration 20514, loss = 0.00872785\n",
      "Iteration 20515, loss = 0.00872746\n",
      "Iteration 20516, loss = 0.00872707\n",
      "Iteration 20517, loss = 0.00872668\n",
      "Iteration 20518, loss = 0.00872629\n",
      "Iteration 20519, loss = 0.00872590\n",
      "Iteration 20520, loss = 0.00872551\n",
      "Iteration 20521, loss = 0.00872512\n",
      "Iteration 20522, loss = 0.00872474\n",
      "Iteration 20523, loss = 0.00872435\n",
      "Iteration 20524, loss = 0.00872396\n",
      "Iteration 20525, loss = 0.00872357\n",
      "Iteration 20526, loss = 0.00872318\n",
      "Iteration 20527, loss = 0.00872279\n",
      "Iteration 20528, loss = 0.00872240\n",
      "Iteration 20529, loss = 0.00872202\n",
      "Iteration 20530, loss = 0.00872163\n",
      "Iteration 20531, loss = 0.00872124\n",
      "Iteration 20532, loss = 0.00872085\n",
      "Iteration 20533, loss = 0.00872046\n",
      "Iteration 20534, loss = 0.00872007\n",
      "Iteration 20535, loss = 0.00871969\n",
      "Iteration 20536, loss = 0.00871930\n",
      "Iteration 20537, loss = 0.00871891\n",
      "Iteration 20538, loss = 0.00871852\n",
      "Iteration 20539, loss = 0.00871813\n",
      "Iteration 20540, loss = 0.00871775\n",
      "Iteration 20541, loss = 0.00871736\n",
      "Iteration 20542, loss = 0.00871697\n",
      "Iteration 20543, loss = 0.00871658\n",
      "Iteration 20544, loss = 0.00871619\n",
      "Iteration 20545, loss = 0.00871581\n",
      "Iteration 20546, loss = 0.00871542\n",
      "Iteration 20547, loss = 0.00871503\n",
      "Iteration 20548, loss = 0.00871464\n",
      "Iteration 20549, loss = 0.00871426\n",
      "Iteration 20550, loss = 0.00871387\n",
      "Iteration 20551, loss = 0.00871348\n",
      "Iteration 20552, loss = 0.00871309\n",
      "Iteration 20553, loss = 0.00871271\n",
      "Iteration 20554, loss = 0.00871232\n",
      "Iteration 20555, loss = 0.00871193\n",
      "Iteration 20556, loss = 0.00871155\n",
      "Iteration 20557, loss = 0.00871116\n",
      "Iteration 20558, loss = 0.00871077\n",
      "Iteration 20559, loss = 0.00871039\n",
      "Iteration 20560, loss = 0.00871000\n",
      "Iteration 20561, loss = 0.00870961\n",
      "Iteration 20562, loss = 0.00870923\n",
      "Iteration 20563, loss = 0.00870884\n",
      "Iteration 20564, loss = 0.00870845\n",
      "Iteration 20565, loss = 0.00870807\n",
      "Iteration 20566, loss = 0.00870768\n",
      "Iteration 20567, loss = 0.00870729\n",
      "Iteration 20568, loss = 0.00870691\n",
      "Iteration 20569, loss = 0.00870652\n",
      "Iteration 20570, loss = 0.00870613\n",
      "Iteration 20571, loss = 0.00870575\n",
      "Iteration 20572, loss = 0.00870536\n",
      "Iteration 20573, loss = 0.00870497\n",
      "Iteration 20574, loss = 0.00870459\n",
      "Iteration 20575, loss = 0.00870420\n",
      "Iteration 20576, loss = 0.00870382\n",
      "Iteration 20577, loss = 0.00870343\n",
      "Iteration 20578, loss = 0.00870304\n",
      "Iteration 20579, loss = 0.00870266\n",
      "Iteration 20580, loss = 0.00870227\n",
      "Iteration 20581, loss = 0.00870189\n",
      "Iteration 20582, loss = 0.00870150\n",
      "Iteration 20583, loss = 0.00870112\n",
      "Iteration 20584, loss = 0.00870073\n",
      "Iteration 20585, loss = 0.00870034\n",
      "Iteration 20586, loss = 0.00869996\n",
      "Iteration 20587, loss = 0.00869957\n",
      "Iteration 20588, loss = 0.00869919\n",
      "Iteration 20589, loss = 0.00869880\n",
      "Iteration 20590, loss = 0.00869842\n",
      "Iteration 20591, loss = 0.00869803\n",
      "Iteration 20592, loss = 0.00869765\n",
      "Iteration 20593, loss = 0.00869726\n",
      "Iteration 20594, loss = 0.00869688\n",
      "Iteration 20595, loss = 0.00869649\n",
      "Iteration 20596, loss = 0.00869611\n",
      "Iteration 20597, loss = 0.00869572\n",
      "Iteration 20598, loss = 0.00869534\n",
      "Iteration 20599, loss = 0.00869495\n",
      "Iteration 20600, loss = 0.00869457\n",
      "Iteration 20601, loss = 0.00869418\n",
      "Iteration 20602, loss = 0.00869380\n",
      "Iteration 20603, loss = 0.00869341\n",
      "Iteration 20604, loss = 0.00869303\n",
      "Iteration 20605, loss = 0.00869264\n",
      "Iteration 20606, loss = 0.00869226\n",
      "Iteration 20607, loss = 0.00869188\n",
      "Iteration 20608, loss = 0.00869149\n",
      "Iteration 20609, loss = 0.00869111\n",
      "Iteration 20610, loss = 0.00869072\n",
      "Iteration 20611, loss = 0.00869034\n",
      "Iteration 20612, loss = 0.00868995\n",
      "Iteration 20613, loss = 0.00868957\n",
      "Iteration 20614, loss = 0.00868919\n",
      "Iteration 20615, loss = 0.00868880\n",
      "Iteration 20616, loss = 0.00868842\n",
      "Iteration 20617, loss = 0.00868803\n",
      "Iteration 20618, loss = 0.00868765\n",
      "Iteration 20619, loss = 0.00868727\n",
      "Iteration 20620, loss = 0.00868688\n",
      "Iteration 20621, loss = 0.00868650\n",
      "Iteration 20622, loss = 0.00868612\n",
      "Iteration 20623, loss = 0.00868573\n",
      "Iteration 20624, loss = 0.00868535\n",
      "Iteration 20625, loss = 0.00868496\n",
      "Iteration 20626, loss = 0.00868458\n",
      "Iteration 20627, loss = 0.00868420\n",
      "Iteration 20628, loss = 0.00868381\n",
      "Iteration 20629, loss = 0.00868343\n",
      "Iteration 20630, loss = 0.00868305\n",
      "Iteration 20631, loss = 0.00868266\n",
      "Iteration 20632, loss = 0.00868228\n",
      "Iteration 20633, loss = 0.00868190\n",
      "Iteration 20634, loss = 0.00868152\n",
      "Iteration 20635, loss = 0.00868113\n",
      "Iteration 20636, loss = 0.00868075\n",
      "Iteration 20637, loss = 0.00868037\n",
      "Iteration 20638, loss = 0.00867998\n",
      "Iteration 20639, loss = 0.00867960\n",
      "Iteration 20640, loss = 0.00867922\n",
      "Iteration 20641, loss = 0.00867884\n",
      "Iteration 20642, loss = 0.00867845\n",
      "Iteration 20643, loss = 0.00867807\n",
      "Iteration 20644, loss = 0.00867769\n",
      "Iteration 20645, loss = 0.00867731\n",
      "Iteration 20646, loss = 0.00867692\n",
      "Iteration 20647, loss = 0.00867654\n",
      "Iteration 20648, loss = 0.00867616\n",
      "Iteration 20649, loss = 0.00867578\n",
      "Iteration 20650, loss = 0.00867539\n",
      "Iteration 20651, loss = 0.00867501\n",
      "Iteration 20652, loss = 0.00867463\n",
      "Iteration 20653, loss = 0.00867425\n",
      "Iteration 20654, loss = 0.00867387\n",
      "Iteration 20655, loss = 0.00867348\n",
      "Iteration 20656, loss = 0.00867310\n",
      "Iteration 20657, loss = 0.00867272\n",
      "Iteration 20658, loss = 0.00867234\n",
      "Iteration 20659, loss = 0.00867196\n",
      "Iteration 20660, loss = 0.00867158\n",
      "Iteration 20661, loss = 0.00867119\n",
      "Iteration 20662, loss = 0.00867081\n",
      "Iteration 20663, loss = 0.00867043\n",
      "Iteration 20664, loss = 0.00867005\n",
      "Iteration 20665, loss = 0.00866967\n",
      "Iteration 20666, loss = 0.00866929\n",
      "Iteration 20667, loss = 0.00866891\n",
      "Iteration 20668, loss = 0.00866852\n",
      "Iteration 20669, loss = 0.00866814\n",
      "Iteration 20670, loss = 0.00866776\n",
      "Iteration 20671, loss = 0.00866738\n",
      "Iteration 20672, loss = 0.00866700\n",
      "Iteration 20673, loss = 0.00866662\n",
      "Iteration 20674, loss = 0.00866624\n",
      "Iteration 20675, loss = 0.00866586\n",
      "Iteration 20676, loss = 0.00866548\n",
      "Iteration 20677, loss = 0.00866509\n",
      "Iteration 20678, loss = 0.00866471\n",
      "Iteration 20679, loss = 0.00866433\n",
      "Iteration 20680, loss = 0.00866395\n",
      "Iteration 20681, loss = 0.00866357\n",
      "Iteration 20682, loss = 0.00866319\n",
      "Iteration 20683, loss = 0.00866281\n",
      "Iteration 20684, loss = 0.00866243\n",
      "Iteration 20685, loss = 0.00866205\n",
      "Iteration 20686, loss = 0.00866167\n",
      "Iteration 20687, loss = 0.00866129\n",
      "Iteration 20688, loss = 0.00866091\n",
      "Iteration 20689, loss = 0.00866053\n",
      "Iteration 20690, loss = 0.00866015\n",
      "Iteration 20691, loss = 0.00865977\n",
      "Iteration 20692, loss = 0.00865939\n",
      "Iteration 20693, loss = 0.00865901\n",
      "Iteration 20694, loss = 0.00865863\n",
      "Iteration 20695, loss = 0.00865825\n",
      "Iteration 20696, loss = 0.00865787\n",
      "Iteration 20697, loss = 0.00865749\n",
      "Iteration 20698, loss = 0.00865711\n",
      "Iteration 20699, loss = 0.00865673\n",
      "Iteration 20700, loss = 0.00865635\n",
      "Iteration 20701, loss = 0.00865597\n",
      "Iteration 20702, loss = 0.00865559\n",
      "Iteration 20703, loss = 0.00865521\n",
      "Iteration 20704, loss = 0.00865483\n",
      "Iteration 20705, loss = 0.00865445\n",
      "Iteration 20706, loss = 0.00865407\n",
      "Iteration 20707, loss = 0.00865369\n",
      "Iteration 20708, loss = 0.00865332\n",
      "Iteration 20709, loss = 0.00865294\n",
      "Iteration 20710, loss = 0.00865256\n",
      "Iteration 20711, loss = 0.00865218\n",
      "Iteration 20712, loss = 0.00865180\n",
      "Iteration 20713, loss = 0.00865142\n",
      "Iteration 20714, loss = 0.00865104\n",
      "Iteration 20715, loss = 0.00865066\n",
      "Iteration 20716, loss = 0.00865028\n",
      "Iteration 20717, loss = 0.00864990\n",
      "Iteration 20718, loss = 0.00864953\n",
      "Iteration 20719, loss = 0.00864915\n",
      "Iteration 20720, loss = 0.00864877\n",
      "Iteration 20721, loss = 0.00864839\n",
      "Iteration 20722, loss = 0.00864801\n",
      "Iteration 20723, loss = 0.00864763\n",
      "Iteration 20724, loss = 0.00864725\n",
      "Iteration 20725, loss = 0.00864688\n",
      "Iteration 20726, loss = 0.00864650\n",
      "Iteration 20727, loss = 0.00864612\n",
      "Iteration 20728, loss = 0.00864574\n",
      "Iteration 20729, loss = 0.00864536\n",
      "Iteration 20730, loss = 0.00864499\n",
      "Iteration 20731, loss = 0.00864461\n",
      "Iteration 20732, loss = 0.00864423\n",
      "Iteration 20733, loss = 0.00864385\n",
      "Iteration 20734, loss = 0.00864347\n",
      "Iteration 20735, loss = 0.00864310\n",
      "Iteration 20736, loss = 0.00864272\n",
      "Iteration 20737, loss = 0.00864234\n",
      "Iteration 20738, loss = 0.00864196\n",
      "Iteration 20739, loss = 0.00864158\n",
      "Iteration 20740, loss = 0.00864121\n",
      "Iteration 20741, loss = 0.00864083\n",
      "Iteration 20742, loss = 0.00864045\n",
      "Iteration 20743, loss = 0.00864007\n",
      "Iteration 20744, loss = 0.00863970\n",
      "Iteration 20745, loss = 0.00863932\n",
      "Iteration 20746, loss = 0.00863894\n",
      "Iteration 20747, loss = 0.00863857\n",
      "Iteration 20748, loss = 0.00863819\n",
      "Iteration 20749, loss = 0.00863781\n",
      "Iteration 20750, loss = 0.00863743\n",
      "Iteration 20751, loss = 0.00863706\n",
      "Iteration 20752, loss = 0.00863668\n",
      "Iteration 20753, loss = 0.00863630\n",
      "Iteration 20754, loss = 0.00863593\n",
      "Iteration 20755, loss = 0.00863555\n",
      "Iteration 20756, loss = 0.00863517\n",
      "Iteration 20757, loss = 0.00863480\n",
      "Iteration 20758, loss = 0.00863442\n",
      "Iteration 20759, loss = 0.00863404\n",
      "Iteration 20760, loss = 0.00863367\n",
      "Iteration 20761, loss = 0.00863329\n",
      "Iteration 20762, loss = 0.00863291\n",
      "Iteration 20763, loss = 0.00863254\n",
      "Iteration 20764, loss = 0.00863216\n",
      "Iteration 20765, loss = 0.00863178\n",
      "Iteration 20766, loss = 0.00863141\n",
      "Iteration 20767, loss = 0.00863103\n",
      "Iteration 20768, loss = 0.00863066\n",
      "Iteration 20769, loss = 0.00863028\n",
      "Iteration 20770, loss = 0.00862990\n",
      "Iteration 20771, loss = 0.00862953\n",
      "Iteration 20772, loss = 0.00862915\n",
      "Iteration 20773, loss = 0.00862878\n",
      "Iteration 20774, loss = 0.00862840\n",
      "Iteration 20775, loss = 0.00862802\n",
      "Iteration 20776, loss = 0.00862765\n",
      "Iteration 20777, loss = 0.00862727\n",
      "Iteration 20778, loss = 0.00862690\n",
      "Iteration 20779, loss = 0.00862652\n",
      "Iteration 20780, loss = 0.00862615\n",
      "Iteration 20781, loss = 0.00862577\n",
      "Iteration 20782, loss = 0.00862539\n",
      "Iteration 20783, loss = 0.00862502\n",
      "Iteration 20784, loss = 0.00862464\n",
      "Iteration 20785, loss = 0.00862427\n",
      "Iteration 20786, loss = 0.00862389\n",
      "Iteration 20787, loss = 0.00862352\n",
      "Iteration 20788, loss = 0.00862314\n",
      "Iteration 20789, loss = 0.00862277\n",
      "Iteration 20790, loss = 0.00862239\n",
      "Iteration 20791, loss = 0.00862202\n",
      "Iteration 20792, loss = 0.00862164\n",
      "Iteration 20793, loss = 0.00862127\n",
      "Iteration 20794, loss = 0.00862089\n",
      "Iteration 20795, loss = 0.00862052\n",
      "Iteration 20796, loss = 0.00862014\n",
      "Iteration 20797, loss = 0.00861977\n",
      "Iteration 20798, loss = 0.00861939\n",
      "Iteration 20799, loss = 0.00861902\n",
      "Iteration 20800, loss = 0.00861864\n",
      "Iteration 20801, loss = 0.00861827\n",
      "Iteration 20802, loss = 0.00861790\n",
      "Iteration 20803, loss = 0.00861752\n",
      "Iteration 20804, loss = 0.00861715\n",
      "Iteration 20805, loss = 0.00861677\n",
      "Iteration 20806, loss = 0.00861640\n",
      "Iteration 20807, loss = 0.00861602\n",
      "Iteration 20808, loss = 0.00861565\n",
      "Iteration 20809, loss = 0.00861528\n",
      "Iteration 20810, loss = 0.00861490\n",
      "Iteration 20811, loss = 0.00861453\n",
      "Iteration 20812, loss = 0.00861415\n",
      "Iteration 20813, loss = 0.00861378\n",
      "Iteration 20814, loss = 0.00861341\n",
      "Iteration 20815, loss = 0.00861303\n",
      "Iteration 20816, loss = 0.00861266\n",
      "Iteration 20817, loss = 0.00861228\n",
      "Iteration 20818, loss = 0.00861191\n",
      "Iteration 20819, loss = 0.00861154\n",
      "Iteration 20820, loss = 0.00861116\n",
      "Iteration 20821, loss = 0.00861079\n",
      "Iteration 20822, loss = 0.00861042\n",
      "Iteration 20823, loss = 0.00861004\n",
      "Iteration 20824, loss = 0.00860967\n",
      "Iteration 20825, loss = 0.00860930\n",
      "Iteration 20826, loss = 0.00860892\n",
      "Iteration 20827, loss = 0.00860855\n",
      "Iteration 20828, loss = 0.00860818\n",
      "Iteration 20829, loss = 0.00860780\n",
      "Iteration 20830, loss = 0.00860743\n",
      "Iteration 20831, loss = 0.00860706\n",
      "Iteration 20832, loss = 0.00860668\n",
      "Iteration 20833, loss = 0.00860631\n",
      "Iteration 20834, loss = 0.00860594\n",
      "Iteration 20835, loss = 0.00860557\n",
      "Iteration 20836, loss = 0.00860519\n",
      "Iteration 20837, loss = 0.00860482\n",
      "Iteration 20838, loss = 0.00860445\n",
      "Iteration 20839, loss = 0.00860407\n",
      "Iteration 20840, loss = 0.00860370\n",
      "Iteration 20841, loss = 0.00860333\n",
      "Iteration 20842, loss = 0.00860296\n",
      "Iteration 20843, loss = 0.00860258\n",
      "Iteration 20844, loss = 0.00860221\n",
      "Iteration 20845, loss = 0.00860184\n",
      "Iteration 20846, loss = 0.00860147\n",
      "Iteration 20847, loss = 0.00860110\n",
      "Iteration 20848, loss = 0.00860072\n",
      "Iteration 20849, loss = 0.00860035\n",
      "Iteration 20850, loss = 0.00859998\n",
      "Iteration 20851, loss = 0.00859961\n",
      "Iteration 20852, loss = 0.00859923\n",
      "Iteration 20853, loss = 0.00859886\n",
      "Iteration 20854, loss = 0.00859849\n",
      "Iteration 20855, loss = 0.00859812\n",
      "Iteration 20856, loss = 0.00859775\n",
      "Iteration 20857, loss = 0.00859738\n",
      "Iteration 20858, loss = 0.00859700\n",
      "Iteration 20859, loss = 0.00859663\n",
      "Iteration 20860, loss = 0.00859626\n",
      "Iteration 20861, loss = 0.00859589\n",
      "Iteration 20862, loss = 0.00859552\n",
      "Iteration 20863, loss = 0.00859515\n",
      "Iteration 20864, loss = 0.00859478\n",
      "Iteration 20865, loss = 0.00859440\n",
      "Iteration 20866, loss = 0.00859403\n",
      "Iteration 20867, loss = 0.00859366\n",
      "Iteration 20868, loss = 0.00859329\n",
      "Iteration 20869, loss = 0.00859292\n",
      "Iteration 20870, loss = 0.00859255\n",
      "Iteration 20871, loss = 0.00859218\n",
      "Iteration 20872, loss = 0.00859181\n",
      "Iteration 20873, loss = 0.00859143\n",
      "Iteration 20874, loss = 0.00859106\n",
      "Iteration 20875, loss = 0.00859069\n",
      "Iteration 20876, loss = 0.00859032\n",
      "Iteration 20877, loss = 0.00858995\n",
      "Iteration 20878, loss = 0.00858958\n",
      "Iteration 20879, loss = 0.00858921\n",
      "Iteration 20880, loss = 0.00858884\n",
      "Iteration 20881, loss = 0.00858847\n",
      "Iteration 20882, loss = 0.00858810\n",
      "Iteration 20883, loss = 0.00858773\n",
      "Iteration 20884, loss = 0.00858736\n",
      "Iteration 20885, loss = 0.00858699\n",
      "Iteration 20886, loss = 0.00858662\n",
      "Iteration 20887, loss = 0.00858625\n",
      "Iteration 20888, loss = 0.00858588\n",
      "Iteration 20889, loss = 0.00858551\n",
      "Iteration 20890, loss = 0.00858514\n",
      "Iteration 20891, loss = 0.00858477\n",
      "Iteration 20892, loss = 0.00858440\n",
      "Iteration 20893, loss = 0.00858403\n",
      "Iteration 20894, loss = 0.00858366\n",
      "Iteration 20895, loss = 0.00858329\n",
      "Iteration 20896, loss = 0.00858292\n",
      "Iteration 20897, loss = 0.00858255\n",
      "Iteration 20898, loss = 0.00858218\n",
      "Iteration 20899, loss = 0.00858181\n",
      "Iteration 20900, loss = 0.00858144\n",
      "Iteration 20901, loss = 0.00858107\n",
      "Iteration 20902, loss = 0.00858070\n",
      "Iteration 20903, loss = 0.00858033\n",
      "Iteration 20904, loss = 0.00857996\n",
      "Iteration 20905, loss = 0.00857959\n",
      "Iteration 20906, loss = 0.00857922\n",
      "Iteration 20907, loss = 0.00857885\n",
      "Iteration 20908, loss = 0.00857848\n",
      "Iteration 20909, loss = 0.00857811\n",
      "Iteration 20910, loss = 0.00857774\n",
      "Iteration 20911, loss = 0.00857738\n",
      "Iteration 20912, loss = 0.00857701\n",
      "Iteration 20913, loss = 0.00857664\n",
      "Iteration 20914, loss = 0.00857627\n",
      "Iteration 20915, loss = 0.00857590\n",
      "Iteration 20916, loss = 0.00857553\n",
      "Iteration 20917, loss = 0.00857516\n",
      "Iteration 20918, loss = 0.00857479\n",
      "Iteration 20919, loss = 0.00857442\n",
      "Iteration 20920, loss = 0.00857406\n",
      "Iteration 20921, loss = 0.00857369\n",
      "Iteration 20922, loss = 0.00857332\n",
      "Iteration 20923, loss = 0.00857295\n",
      "Iteration 20924, loss = 0.00857258\n",
      "Iteration 20925, loss = 0.00857221\n",
      "Iteration 20926, loss = 0.00857185\n",
      "Iteration 20927, loss = 0.00857148\n",
      "Iteration 20928, loss = 0.00857111\n",
      "Iteration 20929, loss = 0.00857074\n",
      "Iteration 20930, loss = 0.00857037\n",
      "Iteration 20931, loss = 0.00857000\n",
      "Iteration 20932, loss = 0.00856964\n",
      "Iteration 20933, loss = 0.00856927\n",
      "Iteration 20934, loss = 0.00856890\n",
      "Iteration 20935, loss = 0.00856853\n",
      "Iteration 20936, loss = 0.00856816\n",
      "Iteration 20937, loss = 0.00856780\n",
      "Iteration 20938, loss = 0.00856743\n",
      "Iteration 20939, loss = 0.00856706\n",
      "Iteration 20940, loss = 0.00856669\n",
      "Iteration 20941, loss = 0.00856633\n",
      "Iteration 20942, loss = 0.00856596\n",
      "Iteration 20943, loss = 0.00856559\n",
      "Iteration 20944, loss = 0.00856522\n",
      "Iteration 20945, loss = 0.00856486\n",
      "Iteration 20946, loss = 0.00856449\n",
      "Iteration 20947, loss = 0.00856412\n",
      "Iteration 20948, loss = 0.00856375\n",
      "Iteration 20949, loss = 0.00856339\n",
      "Iteration 20950, loss = 0.00856302\n",
      "Iteration 20951, loss = 0.00856265\n",
      "Iteration 20952, loss = 0.00856229\n",
      "Iteration 20953, loss = 0.00856192\n",
      "Iteration 20954, loss = 0.00856155\n",
      "Iteration 20955, loss = 0.00856118\n",
      "Iteration 20956, loss = 0.00856082\n",
      "Iteration 20957, loss = 0.00856045\n",
      "Iteration 20958, loss = 0.00856008\n",
      "Iteration 20959, loss = 0.00855972\n",
      "Iteration 20960, loss = 0.00855935\n",
      "Iteration 20961, loss = 0.00855898\n",
      "Iteration 20962, loss = 0.00855862\n",
      "Iteration 20963, loss = 0.00855825\n",
      "Iteration 20964, loss = 0.00855788\n",
      "Iteration 20965, loss = 0.00855752\n",
      "Iteration 20966, loss = 0.00855715\n",
      "Iteration 20967, loss = 0.00855679\n",
      "Iteration 20968, loss = 0.00855642\n",
      "Iteration 20969, loss = 0.00855605\n",
      "Iteration 20970, loss = 0.00855569\n",
      "Iteration 20971, loss = 0.00855532\n",
      "Iteration 20972, loss = 0.00855495\n",
      "Iteration 20973, loss = 0.00855459\n",
      "Iteration 20974, loss = 0.00855422\n",
      "Iteration 20975, loss = 0.00855386\n",
      "Iteration 20976, loss = 0.00855349\n",
      "Iteration 20977, loss = 0.00855312\n",
      "Iteration 20978, loss = 0.00855276\n",
      "Iteration 20979, loss = 0.00855239\n",
      "Iteration 20980, loss = 0.00855203\n",
      "Iteration 20981, loss = 0.00855166\n",
      "Iteration 20982, loss = 0.00855130\n",
      "Iteration 20983, loss = 0.00855093\n",
      "Iteration 20984, loss = 0.00855056\n",
      "Iteration 20985, loss = 0.00855020\n",
      "Iteration 20986, loss = 0.00854983\n",
      "Iteration 20987, loss = 0.00854947\n",
      "Iteration 20988, loss = 0.00854910\n",
      "Iteration 20989, loss = 0.00854874\n",
      "Iteration 20990, loss = 0.00854837\n",
      "Iteration 20991, loss = 0.00854801\n",
      "Iteration 20992, loss = 0.00854764\n",
      "Iteration 20993, loss = 0.00854728\n",
      "Iteration 20994, loss = 0.00854691\n",
      "Iteration 20995, loss = 0.00854655\n",
      "Iteration 20996, loss = 0.00854618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20997, loss = 0.00854582\n",
      "Iteration 20998, loss = 0.00854545\n",
      "Iteration 20999, loss = 0.00854509\n",
      "Iteration 21000, loss = 0.00854472\n",
      "Iteration 21001, loss = 0.00854436\n",
      "Iteration 21002, loss = 0.00854399\n",
      "Iteration 21003, loss = 0.00854363\n",
      "Iteration 21004, loss = 0.00854326\n",
      "Iteration 21005, loss = 0.00854290\n",
      "Iteration 21006, loss = 0.00854254\n",
      "Iteration 21007, loss = 0.00854217\n",
      "Iteration 21008, loss = 0.00854181\n",
      "Iteration 21009, loss = 0.00854144\n",
      "Iteration 21010, loss = 0.00854108\n",
      "Iteration 21011, loss = 0.00854071\n",
      "Iteration 21012, loss = 0.00854035\n",
      "Iteration 21013, loss = 0.00853999\n",
      "Iteration 21014, loss = 0.00853962\n",
      "Iteration 21015, loss = 0.00853926\n",
      "Iteration 21016, loss = 0.00853889\n",
      "Iteration 21017, loss = 0.00853853\n",
      "Iteration 21018, loss = 0.00853817\n",
      "Iteration 21019, loss = 0.00853780\n",
      "Iteration 21020, loss = 0.00853744\n",
      "Iteration 21021, loss = 0.00853707\n",
      "Iteration 21022, loss = 0.00853671\n",
      "Iteration 21023, loss = 0.00853635\n",
      "Iteration 21024, loss = 0.00853598\n",
      "Iteration 21025, loss = 0.00853562\n",
      "Iteration 21026, loss = 0.00853526\n",
      "Iteration 21027, loss = 0.00853489\n",
      "Iteration 21028, loss = 0.00853453\n",
      "Iteration 21029, loss = 0.00853417\n",
      "Iteration 21030, loss = 0.00853380\n",
      "Iteration 21031, loss = 0.00853344\n",
      "Iteration 21032, loss = 0.00853308\n",
      "Iteration 21033, loss = 0.00853271\n",
      "Iteration 21034, loss = 0.00853235\n",
      "Iteration 21035, loss = 0.00853199\n",
      "Iteration 21036, loss = 0.00853162\n",
      "Iteration 21037, loss = 0.00853126\n",
      "Iteration 21038, loss = 0.00853090\n",
      "Iteration 21039, loss = 0.00853054\n",
      "Iteration 21040, loss = 0.00853017\n",
      "Iteration 21041, loss = 0.00852981\n",
      "Iteration 21042, loss = 0.00852945\n",
      "Iteration 21043, loss = 0.00852908\n",
      "Iteration 21044, loss = 0.00852872\n",
      "Iteration 21045, loss = 0.00852836\n",
      "Iteration 21046, loss = 0.00852800\n",
      "Iteration 21047, loss = 0.00852763\n",
      "Iteration 21048, loss = 0.00852727\n",
      "Iteration 21049, loss = 0.00852691\n",
      "Iteration 21050, loss = 0.00852655\n",
      "Iteration 21051, loss = 0.00852618\n",
      "Iteration 21052, loss = 0.00852582\n",
      "Iteration 21053, loss = 0.00852546\n",
      "Iteration 21054, loss = 0.00852510\n",
      "Iteration 21055, loss = 0.00852474\n",
      "Iteration 21056, loss = 0.00852437\n",
      "Iteration 21057, loss = 0.00852401\n",
      "Iteration 21058, loss = 0.00852365\n",
      "Iteration 21059, loss = 0.00852329\n",
      "Iteration 21060, loss = 0.00852293\n",
      "Iteration 21061, loss = 0.00852256\n",
      "Iteration 21062, loss = 0.00852220\n",
      "Iteration 21063, loss = 0.00852184\n",
      "Iteration 21064, loss = 0.00852148\n",
      "Iteration 21065, loss = 0.00852112\n",
      "Iteration 21066, loss = 0.00852076\n",
      "Iteration 21067, loss = 0.00852039\n",
      "Iteration 21068, loss = 0.00852003\n",
      "Iteration 21069, loss = 0.00851967\n",
      "Iteration 21070, loss = 0.00851931\n",
      "Iteration 21071, loss = 0.00851895\n",
      "Iteration 21072, loss = 0.00851859\n",
      "Iteration 21073, loss = 0.00851823\n",
      "Iteration 21074, loss = 0.00851787\n",
      "Iteration 21075, loss = 0.00851750\n",
      "Iteration 21076, loss = 0.00851714\n",
      "Iteration 21077, loss = 0.00851678\n",
      "Iteration 21078, loss = 0.00851642\n",
      "Iteration 21079, loss = 0.00851606\n",
      "Iteration 21080, loss = 0.00851570\n",
      "Iteration 21081, loss = 0.00851534\n",
      "Iteration 21082, loss = 0.00851498\n",
      "Iteration 21083, loss = 0.00851462\n",
      "Iteration 21084, loss = 0.00851426\n",
      "Iteration 21085, loss = 0.00851390\n",
      "Iteration 21086, loss = 0.00851353\n",
      "Iteration 21087, loss = 0.00851317\n",
      "Iteration 21088, loss = 0.00851281\n",
      "Iteration 21089, loss = 0.00851245\n",
      "Iteration 21090, loss = 0.00851209\n",
      "Iteration 21091, loss = 0.00851173\n",
      "Iteration 21092, loss = 0.00851137\n",
      "Iteration 21093, loss = 0.00851101\n",
      "Iteration 21094, loss = 0.00851065\n",
      "Iteration 21095, loss = 0.00851029\n",
      "Iteration 21096, loss = 0.00850993\n",
      "Iteration 21097, loss = 0.00850957\n",
      "Iteration 21098, loss = 0.00850921\n",
      "Iteration 21099, loss = 0.00850885\n",
      "Iteration 21100, loss = 0.00850849\n",
      "Iteration 21101, loss = 0.00850813\n",
      "Iteration 21102, loss = 0.00850777\n",
      "Iteration 21103, loss = 0.00850741\n",
      "Iteration 21104, loss = 0.00850705\n",
      "Iteration 21105, loss = 0.00850669\n",
      "Iteration 21106, loss = 0.00850633\n",
      "Iteration 21107, loss = 0.00850597\n",
      "Iteration 21108, loss = 0.00850561\n",
      "Iteration 21109, loss = 0.00850525\n",
      "Iteration 21110, loss = 0.00850489\n",
      "Iteration 21111, loss = 0.00850453\n",
      "Iteration 21112, loss = 0.00850417\n",
      "Iteration 21113, loss = 0.00850382\n",
      "Iteration 21114, loss = 0.00850346\n",
      "Iteration 21115, loss = 0.00850310\n",
      "Iteration 21116, loss = 0.00850274\n",
      "Iteration 21117, loss = 0.00850238\n",
      "Iteration 21118, loss = 0.00850202\n",
      "Iteration 21119, loss = 0.00850166\n",
      "Iteration 21120, loss = 0.00850130\n",
      "Iteration 21121, loss = 0.00850094\n",
      "Iteration 21122, loss = 0.00850058\n",
      "Iteration 21123, loss = 0.00850022\n",
      "Iteration 21124, loss = 0.00849987\n",
      "Iteration 21125, loss = 0.00849951\n",
      "Iteration 21126, loss = 0.00849915\n",
      "Iteration 21127, loss = 0.00849879\n",
      "Iteration 21128, loss = 0.00849843\n",
      "Iteration 21129, loss = 0.00849807\n",
      "Iteration 21130, loss = 0.00849771\n",
      "Iteration 21131, loss = 0.00849736\n",
      "Iteration 21132, loss = 0.00849700\n",
      "Iteration 21133, loss = 0.00849664\n",
      "Iteration 21134, loss = 0.00849628\n",
      "Iteration 21135, loss = 0.00849592\n",
      "Iteration 21136, loss = 0.00849556\n",
      "Iteration 21137, loss = 0.00849521\n",
      "Iteration 21138, loss = 0.00849485\n",
      "Iteration 21139, loss = 0.00849449\n",
      "Iteration 21140, loss = 0.00849413\n",
      "Iteration 21141, loss = 0.00849377\n",
      "Iteration 21142, loss = 0.00849341\n",
      "Iteration 21143, loss = 0.00849306\n",
      "Iteration 21144, loss = 0.00849270\n",
      "Iteration 21145, loss = 0.00849234\n",
      "Iteration 21146, loss = 0.00849198\n",
      "Iteration 21147, loss = 0.00849163\n",
      "Iteration 21148, loss = 0.00849127\n",
      "Iteration 21149, loss = 0.00849091\n",
      "Iteration 21150, loss = 0.00849055\n",
      "Iteration 21151, loss = 0.00849020\n",
      "Iteration 21152, loss = 0.00848984\n",
      "Iteration 21153, loss = 0.00848948\n",
      "Iteration 21154, loss = 0.00848912\n",
      "Iteration 21155, loss = 0.00848877\n",
      "Iteration 21156, loss = 0.00848841\n",
      "Iteration 21157, loss = 0.00848805\n",
      "Iteration 21158, loss = 0.00848769\n",
      "Iteration 21159, loss = 0.00848734\n",
      "Iteration 21160, loss = 0.00848698\n",
      "Iteration 21161, loss = 0.00848662\n",
      "Iteration 21162, loss = 0.00848627\n",
      "Iteration 21163, loss = 0.00848591\n",
      "Iteration 21164, loss = 0.00848555\n",
      "Iteration 21165, loss = 0.00848519\n",
      "Iteration 21166, loss = 0.00848484\n",
      "Iteration 21167, loss = 0.00848448\n",
      "Iteration 21168, loss = 0.00848412\n",
      "Iteration 21169, loss = 0.00848377\n",
      "Iteration 21170, loss = 0.00848341\n",
      "Iteration 21171, loss = 0.00848305\n",
      "Iteration 21172, loss = 0.00848270\n",
      "Iteration 21173, loss = 0.00848234\n",
      "Iteration 21174, loss = 0.00848198\n",
      "Iteration 21175, loss = 0.00848163\n",
      "Iteration 21176, loss = 0.00848127\n",
      "Iteration 21177, loss = 0.00848092\n",
      "Iteration 21178, loss = 0.00848056\n",
      "Iteration 21179, loss = 0.00848020\n",
      "Iteration 21180, loss = 0.00847985\n",
      "Iteration 21181, loss = 0.00847949\n",
      "Iteration 21182, loss = 0.00847913\n",
      "Iteration 21183, loss = 0.00847878\n",
      "Iteration 21184, loss = 0.00847842\n",
      "Iteration 21185, loss = 0.00847807\n",
      "Iteration 21186, loss = 0.00847771\n",
      "Iteration 21187, loss = 0.00847735\n",
      "Iteration 21188, loss = 0.00847700\n",
      "Iteration 21189, loss = 0.00847664\n",
      "Iteration 21190, loss = 0.00847629\n",
      "Iteration 21191, loss = 0.00847593\n",
      "Iteration 21192, loss = 0.00847558\n",
      "Iteration 21193, loss = 0.00847522\n",
      "Iteration 21194, loss = 0.00847486\n",
      "Iteration 21195, loss = 0.00847451\n",
      "Iteration 21196, loss = 0.00847415\n",
      "Iteration 21197, loss = 0.00847380\n",
      "Iteration 21198, loss = 0.00847344\n",
      "Iteration 21199, loss = 0.00847309\n",
      "Iteration 21200, loss = 0.00847273\n",
      "Iteration 21201, loss = 0.00847238\n",
      "Iteration 21202, loss = 0.00847202\n",
      "Iteration 21203, loss = 0.00847167\n",
      "Iteration 21204, loss = 0.00847131\n",
      "Iteration 21205, loss = 0.00847096\n",
      "Iteration 21206, loss = 0.00847060\n",
      "Iteration 21207, loss = 0.00847025\n",
      "Iteration 21208, loss = 0.00846989\n",
      "Iteration 21209, loss = 0.00846954\n",
      "Iteration 21210, loss = 0.00846918\n",
      "Iteration 21211, loss = 0.00846883\n",
      "Iteration 21212, loss = 0.00846847\n",
      "Iteration 21213, loss = 0.00846812\n",
      "Iteration 21214, loss = 0.00846776\n",
      "Iteration 21215, loss = 0.00846741\n",
      "Iteration 21216, loss = 0.00846705\n",
      "Iteration 21217, loss = 0.00846670\n",
      "Iteration 21218, loss = 0.00846635\n",
      "Iteration 21219, loss = 0.00846599\n",
      "Iteration 21220, loss = 0.00846564\n",
      "Iteration 21221, loss = 0.00846528\n",
      "Iteration 21222, loss = 0.00846493\n",
      "Iteration 21223, loss = 0.00846457\n",
      "Iteration 21224, loss = 0.00846422\n",
      "Iteration 21225, loss = 0.00846387\n",
      "Iteration 21226, loss = 0.00846351\n",
      "Iteration 21227, loss = 0.00846316\n",
      "Iteration 21228, loss = 0.00846280\n",
      "Iteration 21229, loss = 0.00846245\n",
      "Iteration 21230, loss = 0.00846210\n",
      "Iteration 21231, loss = 0.00846174\n",
      "Iteration 21232, loss = 0.00846139\n",
      "Iteration 21233, loss = 0.00846103\n",
      "Iteration 21234, loss = 0.00846068\n",
      "Iteration 21235, loss = 0.00846033\n",
      "Iteration 21236, loss = 0.00845997\n",
      "Iteration 21237, loss = 0.00845962\n",
      "Iteration 21238, loss = 0.00845927\n",
      "Iteration 21239, loss = 0.00845891\n",
      "Iteration 21240, loss = 0.00845856\n",
      "Iteration 21241, loss = 0.00845821\n",
      "Iteration 21242, loss = 0.00845785\n",
      "Iteration 21243, loss = 0.00845750\n",
      "Iteration 21244, loss = 0.00845715\n",
      "Iteration 21245, loss = 0.00845679\n",
      "Iteration 21246, loss = 0.00845644\n",
      "Iteration 21247, loss = 0.00845609\n",
      "Iteration 21248, loss = 0.00845573\n",
      "Iteration 21249, loss = 0.00845538\n",
      "Iteration 21250, loss = 0.00845503\n",
      "Iteration 21251, loss = 0.00845467\n",
      "Iteration 21252, loss = 0.00845432\n",
      "Iteration 21253, loss = 0.00845397\n",
      "Iteration 21254, loss = 0.00845362\n",
      "Iteration 21255, loss = 0.00845326\n",
      "Iteration 21256, loss = 0.00845291\n",
      "Iteration 21257, loss = 0.00845256\n",
      "Iteration 21258, loss = 0.00845221\n",
      "Iteration 21259, loss = 0.00845185\n",
      "Iteration 21260, loss = 0.00845150\n",
      "Iteration 21261, loss = 0.00845115\n",
      "Iteration 21262, loss = 0.00845080\n",
      "Iteration 21263, loss = 0.00845044\n",
      "Iteration 21264, loss = 0.00845009\n",
      "Iteration 21265, loss = 0.00844974\n",
      "Iteration 21266, loss = 0.00844939\n",
      "Iteration 21267, loss = 0.00844903\n",
      "Iteration 21268, loss = 0.00844868\n",
      "Iteration 21269, loss = 0.00844833\n",
      "Iteration 21270, loss = 0.00844798\n",
      "Iteration 21271, loss = 0.00844763\n",
      "Iteration 21272, loss = 0.00844727\n",
      "Iteration 21273, loss = 0.00844692\n",
      "Iteration 21274, loss = 0.00844657\n",
      "Iteration 21275, loss = 0.00844622\n",
      "Iteration 21276, loss = 0.00844587\n",
      "Iteration 21277, loss = 0.00844552\n",
      "Iteration 21278, loss = 0.00844516\n",
      "Iteration 21279, loss = 0.00844481\n",
      "Iteration 21280, loss = 0.00844446\n",
      "Iteration 21281, loss = 0.00844411\n",
      "Iteration 21282, loss = 0.00844376\n",
      "Iteration 21283, loss = 0.00844341\n",
      "Iteration 21284, loss = 0.00844305\n",
      "Iteration 21285, loss = 0.00844270\n",
      "Iteration 21286, loss = 0.00844235\n",
      "Iteration 21287, loss = 0.00844200\n",
      "Iteration 21288, loss = 0.00844165\n",
      "Iteration 21289, loss = 0.00844130\n",
      "Iteration 21290, loss = 0.00844095\n",
      "Iteration 21291, loss = 0.00844060\n",
      "Iteration 21292, loss = 0.00844025\n",
      "Iteration 21293, loss = 0.00843989\n",
      "Iteration 21294, loss = 0.00843954\n",
      "Iteration 21295, loss = 0.00843919\n",
      "Iteration 21296, loss = 0.00843884\n",
      "Iteration 21297, loss = 0.00843849\n",
      "Iteration 21298, loss = 0.00843814\n",
      "Iteration 21299, loss = 0.00843779\n",
      "Iteration 21300, loss = 0.00843744\n",
      "Iteration 21301, loss = 0.00843709\n",
      "Iteration 21302, loss = 0.00843674\n",
      "Iteration 21303, loss = 0.00843639\n",
      "Iteration 21304, loss = 0.00843604\n",
      "Iteration 21305, loss = 0.00843569\n",
      "Iteration 21306, loss = 0.00843534\n",
      "Iteration 21307, loss = 0.00843499\n",
      "Iteration 21308, loss = 0.00843463\n",
      "Iteration 21309, loss = 0.00843428\n",
      "Iteration 21310, loss = 0.00843393\n",
      "Iteration 21311, loss = 0.00843358\n",
      "Iteration 21312, loss = 0.00843323\n",
      "Iteration 21313, loss = 0.00843288\n",
      "Iteration 21314, loss = 0.00843253\n",
      "Iteration 21315, loss = 0.00843218\n",
      "Iteration 21316, loss = 0.00843183\n",
      "Iteration 21317, loss = 0.00843148\n",
      "Iteration 21318, loss = 0.00843113\n",
      "Iteration 21319, loss = 0.00843078\n",
      "Iteration 21320, loss = 0.00843043\n",
      "Iteration 21321, loss = 0.00843009\n",
      "Iteration 21322, loss = 0.00842974\n",
      "Iteration 21323, loss = 0.00842939\n",
      "Iteration 21324, loss = 0.00842904\n",
      "Iteration 21325, loss = 0.00842869\n",
      "Iteration 21326, loss = 0.00842834\n",
      "Iteration 21327, loss = 0.00842799\n",
      "Iteration 21328, loss = 0.00842764\n",
      "Iteration 21329, loss = 0.00842729\n",
      "Iteration 21330, loss = 0.00842694\n",
      "Iteration 21331, loss = 0.00842659\n",
      "Iteration 21332, loss = 0.00842624\n",
      "Iteration 21333, loss = 0.00842589\n",
      "Iteration 21334, loss = 0.00842554\n",
      "Iteration 21335, loss = 0.00842519\n",
      "Iteration 21336, loss = 0.00842485\n",
      "Iteration 21337, loss = 0.00842450\n",
      "Iteration 21338, loss = 0.00842415\n",
      "Iteration 21339, loss = 0.00842380\n",
      "Iteration 21340, loss = 0.00842345\n",
      "Iteration 21341, loss = 0.00842310\n",
      "Iteration 21342, loss = 0.00842275\n",
      "Iteration 21343, loss = 0.00842240\n",
      "Iteration 21344, loss = 0.00842205\n",
      "Iteration 21345, loss = 0.00842171\n",
      "Iteration 21346, loss = 0.00842136\n",
      "Iteration 21347, loss = 0.00842101\n",
      "Iteration 21348, loss = 0.00842066\n",
      "Iteration 21349, loss = 0.00842031\n",
      "Iteration 21350, loss = 0.00841996\n",
      "Iteration 21351, loss = 0.00841962\n",
      "Iteration 21352, loss = 0.00841927\n",
      "Iteration 21353, loss = 0.00841892\n",
      "Iteration 21354, loss = 0.00841857\n",
      "Iteration 21355, loss = 0.00841822\n",
      "Iteration 21356, loss = 0.00841787\n",
      "Iteration 21357, loss = 0.00841753\n",
      "Iteration 21358, loss = 0.00841718\n",
      "Iteration 21359, loss = 0.00841683\n",
      "Iteration 21360, loss = 0.00841648\n",
      "Iteration 21361, loss = 0.00841613\n",
      "Iteration 21362, loss = 0.00841579\n",
      "Iteration 21363, loss = 0.00841544\n",
      "Iteration 21364, loss = 0.00841509\n",
      "Iteration 21365, loss = 0.00841474\n",
      "Iteration 21366, loss = 0.00841440\n",
      "Iteration 21367, loss = 0.00841405\n",
      "Iteration 21368, loss = 0.00841370\n",
      "Iteration 21369, loss = 0.00841335\n",
      "Iteration 21370, loss = 0.00841301\n",
      "Iteration 21371, loss = 0.00841266\n",
      "Iteration 21372, loss = 0.00841231\n",
      "Iteration 21373, loss = 0.00841196\n",
      "Iteration 21374, loss = 0.00841162\n",
      "Iteration 21375, loss = 0.00841127\n",
      "Iteration 21376, loss = 0.00841092\n",
      "Iteration 21377, loss = 0.00841057\n",
      "Iteration 21378, loss = 0.00841023\n",
      "Iteration 21379, loss = 0.00840988\n",
      "Iteration 21380, loss = 0.00840953\n",
      "Iteration 21381, loss = 0.00840919\n",
      "Iteration 21382, loss = 0.00840884\n",
      "Iteration 21383, loss = 0.00840849\n",
      "Iteration 21384, loss = 0.00840815\n",
      "Iteration 21385, loss = 0.00840780\n",
      "Iteration 21386, loss = 0.00840745\n",
      "Iteration 21387, loss = 0.00840711\n",
      "Iteration 21388, loss = 0.00840676\n",
      "Iteration 21389, loss = 0.00840641\n",
      "Iteration 21390, loss = 0.00840607\n",
      "Iteration 21391, loss = 0.00840572\n",
      "Iteration 21392, loss = 0.00840537\n",
      "Iteration 21393, loss = 0.00840503\n",
      "Iteration 21394, loss = 0.00840468\n",
      "Iteration 21395, loss = 0.00840433\n",
      "Iteration 21396, loss = 0.00840399\n",
      "Iteration 21397, loss = 0.00840364\n",
      "Iteration 21398, loss = 0.00840329\n",
      "Iteration 21399, loss = 0.00840295\n",
      "Iteration 21400, loss = 0.00840260\n",
      "Iteration 21401, loss = 0.00840226\n",
      "Iteration 21402, loss = 0.00840191\n",
      "Iteration 21403, loss = 0.00840156\n",
      "Iteration 21404, loss = 0.00840122\n",
      "Iteration 21405, loss = 0.00840087\n",
      "Iteration 21406, loss = 0.00840053\n",
      "Iteration 21407, loss = 0.00840018\n",
      "Iteration 21408, loss = 0.00839983\n",
      "Iteration 21409, loss = 0.00839949\n",
      "Iteration 21410, loss = 0.00839914\n",
      "Iteration 21411, loss = 0.00839880\n",
      "Iteration 21412, loss = 0.00839845\n",
      "Iteration 21413, loss = 0.00839811\n",
      "Iteration 21414, loss = 0.00839776\n",
      "Iteration 21415, loss = 0.00839742\n",
      "Iteration 21416, loss = 0.00839707\n",
      "Iteration 21417, loss = 0.00839672\n",
      "Iteration 21418, loss = 0.00839638\n",
      "Iteration 21419, loss = 0.00839603\n",
      "Iteration 21420, loss = 0.00839569\n",
      "Iteration 21421, loss = 0.00839534\n",
      "Iteration 21422, loss = 0.00839500\n",
      "Iteration 21423, loss = 0.00839465\n",
      "Iteration 21424, loss = 0.00839431\n",
      "Iteration 21425, loss = 0.00839396\n",
      "Iteration 21426, loss = 0.00839362\n",
      "Iteration 21427, loss = 0.00839327\n",
      "Iteration 21428, loss = 0.00839293\n",
      "Iteration 21429, loss = 0.00839258\n",
      "Iteration 21430, loss = 0.00839224\n",
      "Iteration 21431, loss = 0.00839189\n",
      "Iteration 21432, loss = 0.00839155\n",
      "Iteration 21433, loss = 0.00839120\n",
      "Iteration 21434, loss = 0.00839086\n",
      "Iteration 21435, loss = 0.00839052\n",
      "Iteration 21436, loss = 0.00839017\n",
      "Iteration 21437, loss = 0.00838983\n",
      "Iteration 21438, loss = 0.00838948\n",
      "Iteration 21439, loss = 0.00838914\n",
      "Iteration 21440, loss = 0.00838879\n",
      "Iteration 21441, loss = 0.00838845\n",
      "Iteration 21442, loss = 0.00838810\n",
      "Iteration 21443, loss = 0.00838776\n",
      "Iteration 21444, loss = 0.00838742\n",
      "Iteration 21445, loss = 0.00838707\n",
      "Iteration 21446, loss = 0.00838673\n",
      "Iteration 21447, loss = 0.00838638\n",
      "Iteration 21448, loss = 0.00838604\n",
      "Iteration 21449, loss = 0.00838570\n",
      "Iteration 21450, loss = 0.00838535\n",
      "Iteration 21451, loss = 0.00838501\n",
      "Iteration 21452, loss = 0.00838466\n",
      "Iteration 21453, loss = 0.00838432\n",
      "Iteration 21454, loss = 0.00838398\n",
      "Iteration 21455, loss = 0.00838363\n",
      "Iteration 21456, loss = 0.00838329\n",
      "Iteration 21457, loss = 0.00838295\n",
      "Iteration 21458, loss = 0.00838260\n",
      "Iteration 21459, loss = 0.00838226\n",
      "Iteration 21460, loss = 0.00838192\n",
      "Iteration 21461, loss = 0.00838157\n",
      "Iteration 21462, loss = 0.00838123\n",
      "Iteration 21463, loss = 0.00838089\n",
      "Iteration 21464, loss = 0.00838054\n",
      "Iteration 21465, loss = 0.00838020\n",
      "Iteration 21466, loss = 0.00837986\n",
      "Iteration 21467, loss = 0.00837951\n",
      "Iteration 21468, loss = 0.00837917\n",
      "Iteration 21469, loss = 0.00837883\n",
      "Iteration 21470, loss = 0.00837848\n",
      "Iteration 21471, loss = 0.00837814\n",
      "Iteration 21472, loss = 0.00837780\n",
      "Iteration 21473, loss = 0.00837745\n",
      "Iteration 21474, loss = 0.00837711\n",
      "Iteration 21475, loss = 0.00837677\n",
      "Iteration 21476, loss = 0.00837643\n",
      "Iteration 21477, loss = 0.00837608\n",
      "Iteration 21478, loss = 0.00837574\n",
      "Iteration 21479, loss = 0.00837540\n",
      "Iteration 21480, loss = 0.00837506\n",
      "Iteration 21481, loss = 0.00837471\n",
      "Iteration 21482, loss = 0.00837437\n",
      "Iteration 21483, loss = 0.00837403\n",
      "Iteration 21484, loss = 0.00837369\n",
      "Iteration 21485, loss = 0.00837334\n",
      "Iteration 21486, loss = 0.00837300\n",
      "Iteration 21487, loss = 0.00837266\n",
      "Iteration 21488, loss = 0.00837232\n",
      "Iteration 21489, loss = 0.00837197\n",
      "Iteration 21490, loss = 0.00837163\n",
      "Iteration 21491, loss = 0.00837129\n",
      "Iteration 21492, loss = 0.00837095\n",
      "Iteration 21493, loss = 0.00837061\n",
      "Iteration 21494, loss = 0.00837026\n",
      "Iteration 21495, loss = 0.00836992\n",
      "Iteration 21496, loss = 0.00836958\n",
      "Iteration 21497, loss = 0.00836924\n",
      "Iteration 21498, loss = 0.00836890\n",
      "Iteration 21499, loss = 0.00836856\n",
      "Iteration 21500, loss = 0.00836821\n",
      "Iteration 21501, loss = 0.00836787\n",
      "Iteration 21502, loss = 0.00836753\n",
      "Iteration 21503, loss = 0.00836719\n",
      "Iteration 21504, loss = 0.00836685\n",
      "Iteration 21505, loss = 0.00836651\n",
      "Iteration 21506, loss = 0.00836616\n",
      "Iteration 21507, loss = 0.00836582\n",
      "Iteration 21508, loss = 0.00836548\n",
      "Iteration 21509, loss = 0.00836514\n",
      "Iteration 21510, loss = 0.00836480\n",
      "Iteration 21511, loss = 0.00836446\n",
      "Iteration 21512, loss = 0.00836412\n",
      "Iteration 21513, loss = 0.00836378\n",
      "Iteration 21514, loss = 0.00836343\n",
      "Iteration 21515, loss = 0.00836309\n",
      "Iteration 21516, loss = 0.00836275\n",
      "Iteration 21517, loss = 0.00836241\n",
      "Iteration 21518, loss = 0.00836207\n",
      "Iteration 21519, loss = 0.00836173\n",
      "Iteration 21520, loss = 0.00836139\n",
      "Iteration 21521, loss = 0.00836105\n",
      "Iteration 21522, loss = 0.00836071\n",
      "Iteration 21523, loss = 0.00836037\n",
      "Iteration 21524, loss = 0.00836003\n",
      "Iteration 21525, loss = 0.00835969\n",
      "Iteration 21526, loss = 0.00835935\n",
      "Iteration 21527, loss = 0.00835900\n",
      "Iteration 21528, loss = 0.00835866\n",
      "Iteration 21529, loss = 0.00835832\n",
      "Iteration 21530, loss = 0.00835798\n",
      "Iteration 21531, loss = 0.00835764\n",
      "Iteration 21532, loss = 0.00835730\n",
      "Iteration 21533, loss = 0.00835696\n",
      "Iteration 21534, loss = 0.00835662\n",
      "Iteration 21535, loss = 0.00835628\n",
      "Iteration 21536, loss = 0.00835594\n",
      "Iteration 21537, loss = 0.00835560\n",
      "Iteration 21538, loss = 0.00835526\n",
      "Iteration 21539, loss = 0.00835492\n",
      "Iteration 21540, loss = 0.00835458\n",
      "Iteration 21541, loss = 0.00835424\n",
      "Iteration 21542, loss = 0.00835390\n",
      "Iteration 21543, loss = 0.00835356\n",
      "Iteration 21544, loss = 0.00835322\n",
      "Iteration 21545, loss = 0.00835288\n",
      "Iteration 21546, loss = 0.00835254\n",
      "Iteration 21547, loss = 0.00835220\n",
      "Iteration 21548, loss = 0.00835186\n",
      "Iteration 21549, loss = 0.00835152\n",
      "Iteration 21550, loss = 0.00835119\n",
      "Iteration 21551, loss = 0.00835085\n",
      "Iteration 21552, loss = 0.00835051\n",
      "Iteration 21553, loss = 0.00835017\n",
      "Iteration 21554, loss = 0.00834983\n",
      "Iteration 21555, loss = 0.00834949\n",
      "Iteration 21556, loss = 0.00834915\n",
      "Iteration 21557, loss = 0.00834881\n",
      "Iteration 21558, loss = 0.00834847\n",
      "Iteration 21559, loss = 0.00834813\n",
      "Iteration 21560, loss = 0.00834779\n",
      "Iteration 21561, loss = 0.00834745\n",
      "Iteration 21562, loss = 0.00834712\n",
      "Iteration 21563, loss = 0.00834678\n",
      "Iteration 21564, loss = 0.00834644\n",
      "Iteration 21565, loss = 0.00834610\n",
      "Iteration 21566, loss = 0.00834576\n",
      "Iteration 21567, loss = 0.00834542\n",
      "Iteration 21568, loss = 0.00834508\n",
      "Iteration 21569, loss = 0.00834474\n",
      "Iteration 21570, loss = 0.00834440\n",
      "Iteration 21571, loss = 0.00834407\n",
      "Iteration 21572, loss = 0.00834373\n",
      "Iteration 21573, loss = 0.00834339\n",
      "Iteration 21574, loss = 0.00834305\n",
      "Iteration 21575, loss = 0.00834271\n",
      "Iteration 21576, loss = 0.00834237\n",
      "Iteration 21577, loss = 0.00834204\n",
      "Iteration 21578, loss = 0.00834170\n",
      "Iteration 21579, loss = 0.00834136\n",
      "Iteration 21580, loss = 0.00834102\n",
      "Iteration 21581, loss = 0.00834068\n",
      "Iteration 21582, loss = 0.00834034\n",
      "Iteration 21583, loss = 0.00834001\n",
      "Iteration 21584, loss = 0.00833967\n",
      "Iteration 21585, loss = 0.00833933\n",
      "Iteration 21586, loss = 0.00833899\n",
      "Iteration 21587, loss = 0.00833865\n",
      "Iteration 21588, loss = 0.00833832\n",
      "Iteration 21589, loss = 0.00833798\n",
      "Iteration 21590, loss = 0.00833764\n",
      "Iteration 21591, loss = 0.00833730\n",
      "Iteration 21592, loss = 0.00833697\n",
      "Iteration 21593, loss = 0.00833663\n",
      "Iteration 21594, loss = 0.00833629\n",
      "Iteration 21595, loss = 0.00833595\n",
      "Iteration 21596, loss = 0.00833562\n",
      "Iteration 21597, loss = 0.00833528\n",
      "Iteration 21598, loss = 0.00833494\n",
      "Iteration 21599, loss = 0.00833460\n",
      "Iteration 21600, loss = 0.00833427\n",
      "Iteration 21601, loss = 0.00833393\n",
      "Iteration 21602, loss = 0.00833359\n",
      "Iteration 21603, loss = 0.00833325\n",
      "Iteration 21604, loss = 0.00833292\n",
      "Iteration 21605, loss = 0.00833258\n",
      "Iteration 21606, loss = 0.00833224\n",
      "Iteration 21607, loss = 0.00833191\n",
      "Iteration 21608, loss = 0.00833157\n",
      "Iteration 21609, loss = 0.00833123\n",
      "Iteration 21610, loss = 0.00833090\n",
      "Iteration 21611, loss = 0.00833056\n",
      "Iteration 21612, loss = 0.00833022\n",
      "Iteration 21613, loss = 0.00832989\n",
      "Iteration 21614, loss = 0.00832955\n",
      "Iteration 21615, loss = 0.00832921\n",
      "Iteration 21616, loss = 0.00832888\n",
      "Iteration 21617, loss = 0.00832854\n",
      "Iteration 21618, loss = 0.00832820\n",
      "Iteration 21619, loss = 0.00832787\n",
      "Iteration 21620, loss = 0.00832753\n",
      "Iteration 21621, loss = 0.00832719\n",
      "Iteration 21622, loss = 0.00832686\n",
      "Iteration 21623, loss = 0.00832652\n",
      "Iteration 21624, loss = 0.00832618\n",
      "Iteration 21625, loss = 0.00832585\n",
      "Iteration 21626, loss = 0.00832551\n",
      "Iteration 21627, loss = 0.00832518\n",
      "Iteration 21628, loss = 0.00832484\n",
      "Iteration 21629, loss = 0.00832450\n",
      "Iteration 21630, loss = 0.00832417\n",
      "Iteration 21631, loss = 0.00832383\n",
      "Iteration 21632, loss = 0.00832350\n",
      "Iteration 21633, loss = 0.00832316\n",
      "Iteration 21634, loss = 0.00832282\n",
      "Iteration 21635, loss = 0.00832249\n",
      "Iteration 21636, loss = 0.00832215\n",
      "Iteration 21637, loss = 0.00832182\n",
      "Iteration 21638, loss = 0.00832148\n",
      "Iteration 21639, loss = 0.00832114\n",
      "Iteration 21640, loss = 0.00832081\n",
      "Iteration 21641, loss = 0.00832047\n",
      "Iteration 21642, loss = 0.00832014\n",
      "Iteration 21643, loss = 0.00831980\n",
      "Iteration 21644, loss = 0.00831947\n",
      "Iteration 21645, loss = 0.00831913\n",
      "Iteration 21646, loss = 0.00831880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21647, loss = 0.00831846\n",
      "Iteration 21648, loss = 0.00831813\n",
      "Iteration 21649, loss = 0.00831779\n",
      "Iteration 21650, loss = 0.00831746\n",
      "Iteration 21651, loss = 0.00831712\n",
      "Iteration 21652, loss = 0.00831679\n",
      "Iteration 21653, loss = 0.00831645\n",
      "Iteration 21654, loss = 0.00831612\n",
      "Iteration 21655, loss = 0.00831578\n",
      "Iteration 21656, loss = 0.00831545\n",
      "Iteration 21657, loss = 0.00831511\n",
      "Iteration 21658, loss = 0.00831478\n",
      "Iteration 21659, loss = 0.00831444\n",
      "Iteration 21660, loss = 0.00831411\n",
      "Iteration 21661, loss = 0.00831377\n",
      "Iteration 21662, loss = 0.00831344\n",
      "Iteration 21663, loss = 0.00831310\n",
      "Iteration 21664, loss = 0.00831277\n",
      "Iteration 21665, loss = 0.00831243\n",
      "Iteration 21666, loss = 0.00831210\n",
      "Iteration 21667, loss = 0.00831176\n",
      "Iteration 21668, loss = 0.00831143\n",
      "Iteration 21669, loss = 0.00831110\n",
      "Iteration 21670, loss = 0.00831076\n",
      "Iteration 21671, loss = 0.00831043\n",
      "Iteration 21672, loss = 0.00831009\n",
      "Iteration 21673, loss = 0.00830976\n",
      "Iteration 21674, loss = 0.00830942\n",
      "Iteration 21675, loss = 0.00830909\n",
      "Iteration 21676, loss = 0.00830876\n",
      "Iteration 21677, loss = 0.00830842\n",
      "Iteration 21678, loss = 0.00830809\n",
      "Iteration 21679, loss = 0.00830775\n",
      "Iteration 21680, loss = 0.00830742\n",
      "Iteration 21681, loss = 0.00830709\n",
      "Iteration 21682, loss = 0.00830675\n",
      "Iteration 21683, loss = 0.00830642\n",
      "Iteration 21684, loss = 0.00830609\n",
      "Iteration 21685, loss = 0.00830575\n",
      "Iteration 21686, loss = 0.00830542\n",
      "Iteration 21687, loss = 0.00830508\n",
      "Iteration 21688, loss = 0.00830475\n",
      "Iteration 21689, loss = 0.00830442\n",
      "Iteration 21690, loss = 0.00830408\n",
      "Iteration 21691, loss = 0.00830375\n",
      "Iteration 21692, loss = 0.00830342\n",
      "Iteration 21693, loss = 0.00830308\n",
      "Iteration 21694, loss = 0.00830275\n",
      "Iteration 21695, loss = 0.00830242\n",
      "Iteration 21696, loss = 0.00830208\n",
      "Iteration 21697, loss = 0.00830175\n",
      "Iteration 21698, loss = 0.00830142\n",
      "Iteration 21699, loss = 0.00830109\n",
      "Iteration 21700, loss = 0.00830075\n",
      "Iteration 21701, loss = 0.00830042\n",
      "Iteration 21702, loss = 0.00830009\n",
      "Iteration 21703, loss = 0.00829975\n",
      "Iteration 21704, loss = 0.00829942\n",
      "Iteration 21705, loss = 0.00829909\n",
      "Iteration 21706, loss = 0.00829875\n",
      "Iteration 21707, loss = 0.00829842\n",
      "Iteration 21708, loss = 0.00829809\n",
      "Iteration 21709, loss = 0.00829776\n",
      "Iteration 21710, loss = 0.00829742\n",
      "Iteration 21711, loss = 0.00829709\n",
      "Iteration 21712, loss = 0.00829676\n",
      "Iteration 21713, loss = 0.00829643\n",
      "Iteration 21714, loss = 0.00829609\n",
      "Iteration 21715, loss = 0.00829576\n",
      "Iteration 21716, loss = 0.00829543\n",
      "Iteration 21717, loss = 0.00829510\n",
      "Iteration 21718, loss = 0.00829476\n",
      "Iteration 21719, loss = 0.00829443\n",
      "Iteration 21720, loss = 0.00829410\n",
      "Iteration 21721, loss = 0.00829377\n",
      "Iteration 21722, loss = 0.00829344\n",
      "Iteration 21723, loss = 0.00829310\n",
      "Iteration 21724, loss = 0.00829277\n",
      "Iteration 21725, loss = 0.00829244\n",
      "Iteration 21726, loss = 0.00829211\n",
      "Iteration 21727, loss = 0.00829178\n",
      "Iteration 21728, loss = 0.00829144\n",
      "Iteration 21729, loss = 0.00829111\n",
      "Iteration 21730, loss = 0.00829078\n",
      "Iteration 21731, loss = 0.00829045\n",
      "Iteration 21732, loss = 0.00829012\n",
      "Iteration 21733, loss = 0.00828979\n",
      "Iteration 21734, loss = 0.00828945\n",
      "Iteration 21735, loss = 0.00828912\n",
      "Iteration 21736, loss = 0.00828879\n",
      "Iteration 21737, loss = 0.00828846\n",
      "Iteration 21738, loss = 0.00828813\n",
      "Iteration 21739, loss = 0.00828780\n",
      "Iteration 21740, loss = 0.00828747\n",
      "Iteration 21741, loss = 0.00828713\n",
      "Iteration 21742, loss = 0.00828680\n",
      "Iteration 21743, loss = 0.00828647\n",
      "Iteration 21744, loss = 0.00828614\n",
      "Iteration 21745, loss = 0.00828581\n",
      "Iteration 21746, loss = 0.00828548\n",
      "Iteration 21747, loss = 0.00828515\n",
      "Iteration 21748, loss = 0.00828482\n",
      "Iteration 21749, loss = 0.00828449\n",
      "Iteration 21750, loss = 0.00828415\n",
      "Iteration 21751, loss = 0.00828382\n",
      "Iteration 21752, loss = 0.00828349\n",
      "Iteration 21753, loss = 0.00828316\n",
      "Iteration 21754, loss = 0.00828283\n",
      "Iteration 21755, loss = 0.00828250\n",
      "Iteration 21756, loss = 0.00828217\n",
      "Iteration 21757, loss = 0.00828184\n",
      "Iteration 21758, loss = 0.00828151\n",
      "Iteration 21759, loss = 0.00828118\n",
      "Iteration 21760, loss = 0.00828085\n",
      "Iteration 21761, loss = 0.00828052\n",
      "Iteration 21762, loss = 0.00828019\n",
      "Iteration 21763, loss = 0.00827986\n",
      "Iteration 21764, loss = 0.00827953\n",
      "Iteration 21765, loss = 0.00827920\n",
      "Iteration 21766, loss = 0.00827887\n",
      "Iteration 21767, loss = 0.00827854\n",
      "Iteration 21768, loss = 0.00827821\n",
      "Iteration 21769, loss = 0.00827788\n",
      "Iteration 21770, loss = 0.00827755\n",
      "Iteration 21771, loss = 0.00827722\n",
      "Iteration 21772, loss = 0.00827689\n",
      "Iteration 21773, loss = 0.00827656\n",
      "Iteration 21774, loss = 0.00827623\n",
      "Iteration 21775, loss = 0.00827590\n",
      "Iteration 21776, loss = 0.00827557\n",
      "Iteration 21777, loss = 0.00827524\n",
      "Iteration 21778, loss = 0.00827491\n",
      "Iteration 21779, loss = 0.00827458\n",
      "Iteration 21780, loss = 0.00827425\n",
      "Iteration 21781, loss = 0.00827392\n",
      "Iteration 21782, loss = 0.00827359\n",
      "Iteration 21783, loss = 0.00827326\n",
      "Iteration 21784, loss = 0.00827293\n",
      "Iteration 21785, loss = 0.00827260\n",
      "Iteration 21786, loss = 0.00827227\n",
      "Iteration 21787, loss = 0.00827194\n",
      "Iteration 21788, loss = 0.00827161\n",
      "Iteration 21789, loss = 0.00827128\n",
      "Iteration 21790, loss = 0.00827095\n",
      "Iteration 21791, loss = 0.00827062\n",
      "Iteration 21792, loss = 0.00827029\n",
      "Iteration 21793, loss = 0.00826997\n",
      "Iteration 21794, loss = 0.00826964\n",
      "Iteration 21795, loss = 0.00826931\n",
      "Iteration 21796, loss = 0.00826898\n",
      "Iteration 21797, loss = 0.00826865\n",
      "Iteration 21798, loss = 0.00826832\n",
      "Iteration 21799, loss = 0.00826799\n",
      "Iteration 21800, loss = 0.00826766\n",
      "Iteration 21801, loss = 0.00826733\n",
      "Iteration 21802, loss = 0.00826701\n",
      "Iteration 21803, loss = 0.00826668\n",
      "Iteration 21804, loss = 0.00826635\n",
      "Iteration 21805, loss = 0.00826602\n",
      "Iteration 21806, loss = 0.00826569\n",
      "Iteration 21807, loss = 0.00826536\n",
      "Iteration 21808, loss = 0.00826503\n",
      "Iteration 21809, loss = 0.00826471\n",
      "Iteration 21810, loss = 0.00826438\n",
      "Iteration 21811, loss = 0.00826405\n",
      "Iteration 21812, loss = 0.00826372\n",
      "Iteration 21813, loss = 0.00826339\n",
      "Iteration 21814, loss = 0.00826306\n",
      "Iteration 21815, loss = 0.00826274\n",
      "Iteration 21816, loss = 0.00826241\n",
      "Iteration 21817, loss = 0.00826208\n",
      "Iteration 21818, loss = 0.00826175\n",
      "Iteration 21819, loss = 0.00826142\n",
      "Iteration 21820, loss = 0.00826110\n",
      "Iteration 21821, loss = 0.00826077\n",
      "Iteration 21822, loss = 0.00826044\n",
      "Iteration 21823, loss = 0.00826011\n",
      "Iteration 21824, loss = 0.00825978\n",
      "Iteration 21825, loss = 0.00825946\n",
      "Iteration 21826, loss = 0.00825913\n",
      "Iteration 21827, loss = 0.00825880\n",
      "Iteration 21828, loss = 0.00825847\n",
      "Iteration 21829, loss = 0.00825815\n",
      "Iteration 21830, loss = 0.00825782\n",
      "Iteration 21831, loss = 0.00825749\n",
      "Iteration 21832, loss = 0.00825716\n",
      "Iteration 21833, loss = 0.00825684\n",
      "Iteration 21834, loss = 0.00825651\n",
      "Iteration 21835, loss = 0.00825618\n",
      "Iteration 21836, loss = 0.00825585\n",
      "Iteration 21837, loss = 0.00825553\n",
      "Iteration 21838, loss = 0.00825520\n",
      "Iteration 21839, loss = 0.00825487\n",
      "Iteration 21840, loss = 0.00825454\n",
      "Iteration 21841, loss = 0.00825422\n",
      "Iteration 21842, loss = 0.00825389\n",
      "Iteration 21843, loss = 0.00825356\n",
      "Iteration 21844, loss = 0.00825324\n",
      "Iteration 21845, loss = 0.00825291\n",
      "Iteration 21846, loss = 0.00825258\n",
      "Iteration 21847, loss = 0.00825226\n",
      "Iteration 21848, loss = 0.00825193\n",
      "Iteration 21849, loss = 0.00825160\n",
      "Iteration 21850, loss = 0.00825128\n",
      "Iteration 21851, loss = 0.00825095\n",
      "Iteration 21852, loss = 0.00825062\n",
      "Iteration 21853, loss = 0.00825030\n",
      "Iteration 21854, loss = 0.00824997\n",
      "Iteration 21855, loss = 0.00824964\n",
      "Iteration 21856, loss = 0.00824932\n",
      "Iteration 21857, loss = 0.00824899\n",
      "Iteration 21858, loss = 0.00824866\n",
      "Iteration 21859, loss = 0.00824834\n",
      "Iteration 21860, loss = 0.00824801\n",
      "Iteration 21861, loss = 0.00824768\n",
      "Iteration 21862, loss = 0.00824736\n",
      "Iteration 21863, loss = 0.00824703\n",
      "Iteration 21864, loss = 0.00824671\n",
      "Iteration 21865, loss = 0.00824638\n",
      "Iteration 21866, loss = 0.00824605\n",
      "Iteration 21867, loss = 0.00824573\n",
      "Iteration 21868, loss = 0.00824540\n",
      "Iteration 21869, loss = 0.00824508\n",
      "Iteration 21870, loss = 0.00824475\n",
      "Iteration 21871, loss = 0.00824442\n",
      "Iteration 21872, loss = 0.00824410\n",
      "Iteration 21873, loss = 0.00824377\n",
      "Iteration 21874, loss = 0.00824345\n",
      "Iteration 21875, loss = 0.00824312\n",
      "Iteration 21876, loss = 0.00824280\n",
      "Iteration 21877, loss = 0.00824247\n",
      "Iteration 21878, loss = 0.00824214\n",
      "Iteration 21879, loss = 0.00824182\n",
      "Iteration 21880, loss = 0.00824149\n",
      "Iteration 21881, loss = 0.00824117\n",
      "Iteration 21882, loss = 0.00824084\n",
      "Iteration 21883, loss = 0.00824052\n",
      "Iteration 21884, loss = 0.00824019\n",
      "Iteration 21885, loss = 0.00823987\n",
      "Iteration 21886, loss = 0.00823954\n",
      "Iteration 21887, loss = 0.00823922\n",
      "Iteration 21888, loss = 0.00823889\n",
      "Iteration 21889, loss = 0.00823857\n",
      "Iteration 21890, loss = 0.00823824\n",
      "Iteration 21891, loss = 0.00823792\n",
      "Iteration 21892, loss = 0.00823759\n",
      "Iteration 21893, loss = 0.00823727\n",
      "Iteration 21894, loss = 0.00823694\n",
      "Iteration 21895, loss = 0.00823662\n",
      "Iteration 21896, loss = 0.00823629\n",
      "Iteration 21897, loss = 0.00823597\n",
      "Iteration 21898, loss = 0.00823564\n",
      "Iteration 21899, loss = 0.00823532\n",
      "Iteration 21900, loss = 0.00823499\n",
      "Iteration 21901, loss = 0.00823467\n",
      "Iteration 21902, loss = 0.00823434\n",
      "Iteration 21903, loss = 0.00823402\n",
      "Iteration 21904, loss = 0.00823369\n",
      "Iteration 21905, loss = 0.00823337\n",
      "Iteration 21906, loss = 0.00823304\n",
      "Iteration 21907, loss = 0.00823272\n",
      "Iteration 21908, loss = 0.00823240\n",
      "Iteration 21909, loss = 0.00823207\n",
      "Iteration 21910, loss = 0.00823175\n",
      "Iteration 21911, loss = 0.00823142\n",
      "Iteration 21912, loss = 0.00823110\n",
      "Iteration 21913, loss = 0.00823078\n",
      "Iteration 21914, loss = 0.00823045\n",
      "Iteration 21915, loss = 0.00823013\n",
      "Iteration 21916, loss = 0.00822980\n",
      "Iteration 21917, loss = 0.00822948\n",
      "Iteration 21918, loss = 0.00822916\n",
      "Iteration 21919, loss = 0.00822883\n",
      "Iteration 21920, loss = 0.00822851\n",
      "Iteration 21921, loss = 0.00822818\n",
      "Iteration 21922, loss = 0.00822786\n",
      "Iteration 21923, loss = 0.00822754\n",
      "Iteration 21924, loss = 0.00822721\n",
      "Iteration 21925, loss = 0.00822689\n",
      "Iteration 21926, loss = 0.00822657\n",
      "Iteration 21927, loss = 0.00822624\n",
      "Iteration 21928, loss = 0.00822592\n",
      "Iteration 21929, loss = 0.00822559\n",
      "Iteration 21930, loss = 0.00822527\n",
      "Iteration 21931, loss = 0.00822495\n",
      "Iteration 21932, loss = 0.00822462\n",
      "Iteration 21933, loss = 0.00822430\n",
      "Iteration 21934, loss = 0.00822398\n",
      "Iteration 21935, loss = 0.00822365\n",
      "Iteration 21936, loss = 0.00822333\n",
      "Iteration 21937, loss = 0.00822301\n",
      "Iteration 21938, loss = 0.00822269\n",
      "Iteration 21939, loss = 0.00822236\n",
      "Iteration 21940, loss = 0.00822204\n",
      "Iteration 21941, loss = 0.00822172\n",
      "Iteration 21942, loss = 0.00822139\n",
      "Iteration 21943, loss = 0.00822107\n",
      "Iteration 21944, loss = 0.00822075\n",
      "Iteration 21945, loss = 0.00822042\n",
      "Iteration 21946, loss = 0.00822010\n",
      "Iteration 21947, loss = 0.00821978\n",
      "Iteration 21948, loss = 0.00821946\n",
      "Iteration 21949, loss = 0.00821913\n",
      "Iteration 21950, loss = 0.00821881\n",
      "Iteration 21951, loss = 0.00821849\n",
      "Iteration 21952, loss = 0.00821817\n",
      "Iteration 21953, loss = 0.00821784\n",
      "Iteration 21954, loss = 0.00821752\n",
      "Iteration 21955, loss = 0.00821720\n",
      "Iteration 21956, loss = 0.00821688\n",
      "Iteration 21957, loss = 0.00821655\n",
      "Iteration 21958, loss = 0.00821623\n",
      "Iteration 21959, loss = 0.00821591\n",
      "Iteration 21960, loss = 0.00821559\n",
      "Iteration 21961, loss = 0.00821526\n",
      "Iteration 21962, loss = 0.00821494\n",
      "Iteration 21963, loss = 0.00821462\n",
      "Iteration 21964, loss = 0.00821430\n",
      "Iteration 21965, loss = 0.00821398\n",
      "Iteration 21966, loss = 0.00821365\n",
      "Iteration 21967, loss = 0.00821333\n",
      "Iteration 21968, loss = 0.00821301\n",
      "Iteration 21969, loss = 0.00821269\n",
      "Iteration 21970, loss = 0.00821237\n",
      "Iteration 21971, loss = 0.00821205\n",
      "Iteration 21972, loss = 0.00821172\n",
      "Iteration 21973, loss = 0.00821140\n",
      "Iteration 21974, loss = 0.00821108\n",
      "Iteration 21975, loss = 0.00821076\n",
      "Iteration 21976, loss = 0.00821044\n",
      "Iteration 21977, loss = 0.00821012\n",
      "Iteration 21978, loss = 0.00820979\n",
      "Iteration 21979, loss = 0.00820947\n",
      "Iteration 21980, loss = 0.00820915\n",
      "Iteration 21981, loss = 0.00820883\n",
      "Iteration 21982, loss = 0.00820851\n",
      "Iteration 21983, loss = 0.00820819\n",
      "Iteration 21984, loss = 0.00820787\n",
      "Iteration 21985, loss = 0.00820754\n",
      "Iteration 21986, loss = 0.00820722\n",
      "Iteration 21987, loss = 0.00820690\n",
      "Iteration 21988, loss = 0.00820658\n",
      "Iteration 21989, loss = 0.00820626\n",
      "Iteration 21990, loss = 0.00820594\n",
      "Iteration 21991, loss = 0.00820562\n",
      "Iteration 21992, loss = 0.00820530\n",
      "Iteration 21993, loss = 0.00820498\n",
      "Iteration 21994, loss = 0.00820466\n",
      "Iteration 21995, loss = 0.00820434\n",
      "Iteration 21996, loss = 0.00820401\n",
      "Iteration 21997, loss = 0.00820369\n",
      "Iteration 21998, loss = 0.00820337\n",
      "Iteration 21999, loss = 0.00820305\n",
      "Iteration 22000, loss = 0.00820273\n",
      "Iteration 22001, loss = 0.00820241\n",
      "Iteration 22002, loss = 0.00820209\n",
      "Iteration 22003, loss = 0.00820177\n",
      "Iteration 22004, loss = 0.00820145\n",
      "Iteration 22005, loss = 0.00820113\n",
      "Iteration 22006, loss = 0.00820081\n",
      "Iteration 22007, loss = 0.00820049\n",
      "Iteration 22008, loss = 0.00820017\n",
      "Iteration 22009, loss = 0.00819985\n",
      "Iteration 22010, loss = 0.00819953\n",
      "Iteration 22011, loss = 0.00819921\n",
      "Iteration 22012, loss = 0.00819889\n",
      "Iteration 22013, loss = 0.00819857\n",
      "Iteration 22014, loss = 0.00819825\n",
      "Iteration 22015, loss = 0.00819793\n",
      "Iteration 22016, loss = 0.00819761\n",
      "Iteration 22017, loss = 0.00819729\n",
      "Iteration 22018, loss = 0.00819697\n",
      "Iteration 22019, loss = 0.00819665\n",
      "Iteration 22020, loss = 0.00819633\n",
      "Iteration 22021, loss = 0.00819601\n",
      "Iteration 22022, loss = 0.00819569\n",
      "Iteration 22023, loss = 0.00819537\n",
      "Iteration 22024, loss = 0.00819505\n",
      "Iteration 22025, loss = 0.00819473\n",
      "Iteration 22026, loss = 0.00819441\n",
      "Iteration 22027, loss = 0.00819409\n",
      "Iteration 22028, loss = 0.00819377\n",
      "Iteration 22029, loss = 0.00819345\n",
      "Iteration 22030, loss = 0.00819313\n",
      "Iteration 22031, loss = 0.00819281\n",
      "Iteration 22032, loss = 0.00819249\n",
      "Iteration 22033, loss = 0.00819217\n",
      "Iteration 22034, loss = 0.00819186\n",
      "Iteration 22035, loss = 0.00819154\n",
      "Iteration 22036, loss = 0.00819122\n",
      "Iteration 22037, loss = 0.00819090\n",
      "Iteration 22038, loss = 0.00819058\n",
      "Iteration 22039, loss = 0.00819026\n",
      "Iteration 22040, loss = 0.00818994\n",
      "Iteration 22041, loss = 0.00818962\n",
      "Iteration 22042, loss = 0.00818930\n",
      "Iteration 22043, loss = 0.00818898\n",
      "Iteration 22044, loss = 0.00818867\n",
      "Iteration 22045, loss = 0.00818835\n",
      "Iteration 22046, loss = 0.00818803\n",
      "Iteration 22047, loss = 0.00818771\n",
      "Iteration 22048, loss = 0.00818739\n",
      "Iteration 22049, loss = 0.00818707\n",
      "Iteration 22050, loss = 0.00818675\n",
      "Iteration 22051, loss = 0.00818643\n",
      "Iteration 22052, loss = 0.00818612\n",
      "Iteration 22053, loss = 0.00818580\n",
      "Iteration 22054, loss = 0.00818548\n",
      "Iteration 22055, loss = 0.00818516\n",
      "Iteration 22056, loss = 0.00818484\n",
      "Iteration 22057, loss = 0.00818452\n",
      "Iteration 22058, loss = 0.00818421\n",
      "Iteration 22059, loss = 0.00818389\n",
      "Iteration 22060, loss = 0.00818357\n",
      "Iteration 22061, loss = 0.00818325\n",
      "Iteration 22062, loss = 0.00818293\n",
      "Iteration 22063, loss = 0.00818262\n",
      "Iteration 22064, loss = 0.00818230\n",
      "Iteration 22065, loss = 0.00818198\n",
      "Iteration 22066, loss = 0.00818166\n",
      "Iteration 22067, loss = 0.00818134\n",
      "Iteration 22068, loss = 0.00818103\n",
      "Iteration 22069, loss = 0.00818071\n",
      "Iteration 22070, loss = 0.00818039\n",
      "Iteration 22071, loss = 0.00818007\n",
      "Iteration 22072, loss = 0.00817975\n",
      "Iteration 22073, loss = 0.00817944\n",
      "Iteration 22074, loss = 0.00817912\n",
      "Iteration 22075, loss = 0.00817880\n",
      "Iteration 22076, loss = 0.00817848\n",
      "Iteration 22077, loss = 0.00817817\n",
      "Iteration 22078, loss = 0.00817785\n",
      "Iteration 22079, loss = 0.00817753\n",
      "Iteration 22080, loss = 0.00817721\n",
      "Iteration 22081, loss = 0.00817690\n",
      "Iteration 22082, loss = 0.00817658\n",
      "Iteration 22083, loss = 0.00817626\n",
      "Iteration 22084, loss = 0.00817595\n",
      "Iteration 22085, loss = 0.00817563\n",
      "Iteration 22086, loss = 0.00817531\n",
      "Iteration 22087, loss = 0.00817499\n",
      "Iteration 22088, loss = 0.00817468\n",
      "Iteration 22089, loss = 0.00817436\n",
      "Iteration 22090, loss = 0.00817404\n",
      "Iteration 22091, loss = 0.00817373\n",
      "Iteration 22092, loss = 0.00817341\n",
      "Iteration 22093, loss = 0.00817309\n",
      "Iteration 22094, loss = 0.00817278\n",
      "Iteration 22095, loss = 0.00817246\n",
      "Iteration 22096, loss = 0.00817214\n",
      "Iteration 22097, loss = 0.00817182\n",
      "Iteration 22098, loss = 0.00817151\n",
      "Iteration 22099, loss = 0.00817119\n",
      "Iteration 22100, loss = 0.00817087\n",
      "Iteration 22101, loss = 0.00817056\n",
      "Iteration 22102, loss = 0.00817024\n",
      "Iteration 22103, loss = 0.00816993\n",
      "Iteration 22104, loss = 0.00816961\n",
      "Iteration 22105, loss = 0.00816929\n",
      "Iteration 22106, loss = 0.00816898\n",
      "Iteration 22107, loss = 0.00816866\n",
      "Iteration 22108, loss = 0.00816834\n",
      "Iteration 22109, loss = 0.00816803\n",
      "Iteration 22110, loss = 0.00816771\n",
      "Iteration 22111, loss = 0.00816740\n",
      "Iteration 22112, loss = 0.00816708\n",
      "Iteration 22113, loss = 0.00816676\n",
      "Iteration 22114, loss = 0.00816645\n",
      "Iteration 22115, loss = 0.00816613\n",
      "Iteration 22116, loss = 0.00816581\n",
      "Iteration 22117, loss = 0.00816550\n",
      "Iteration 22118, loss = 0.00816518\n",
      "Iteration 22119, loss = 0.00816487\n",
      "Iteration 22120, loss = 0.00816455\n",
      "Iteration 22121, loss = 0.00816424\n",
      "Iteration 22122, loss = 0.00816392\n",
      "Iteration 22123, loss = 0.00816360\n",
      "Iteration 22124, loss = 0.00816329\n",
      "Iteration 22125, loss = 0.00816297\n",
      "Iteration 22126, loss = 0.00816266\n",
      "Iteration 22127, loss = 0.00816234\n",
      "Iteration 22128, loss = 0.00816203\n",
      "Iteration 22129, loss = 0.00816171\n",
      "Iteration 22130, loss = 0.00816140\n",
      "Iteration 22131, loss = 0.00816108\n",
      "Iteration 22132, loss = 0.00816076\n",
      "Iteration 22133, loss = 0.00816045\n",
      "Iteration 22134, loss = 0.00816013\n",
      "Iteration 22135, loss = 0.00815982\n",
      "Iteration 22136, loss = 0.00815950\n",
      "Iteration 22137, loss = 0.00815919\n",
      "Iteration 22138, loss = 0.00815887\n",
      "Iteration 22139, loss = 0.00815856\n",
      "Iteration 22140, loss = 0.00815824\n",
      "Iteration 22141, loss = 0.00815793\n",
      "Iteration 22142, loss = 0.00815761\n",
      "Iteration 22143, loss = 0.00815730\n",
      "Iteration 22144, loss = 0.00815698\n",
      "Iteration 22145, loss = 0.00815667\n",
      "Iteration 22146, loss = 0.00815635\n",
      "Iteration 22147, loss = 0.00815604\n",
      "Iteration 22148, loss = 0.00815572\n",
      "Iteration 22149, loss = 0.00815541\n",
      "Iteration 22150, loss = 0.00815510\n",
      "Iteration 22151, loss = 0.00815478\n",
      "Iteration 22152, loss = 0.00815447\n",
      "Iteration 22153, loss = 0.00815415\n",
      "Iteration 22154, loss = 0.00815384\n",
      "Iteration 22155, loss = 0.00815352\n",
      "Iteration 22156, loss = 0.00815321\n",
      "Iteration 22157, loss = 0.00815289\n",
      "Iteration 22158, loss = 0.00815258\n",
      "Iteration 22159, loss = 0.00815227\n",
      "Iteration 22160, loss = 0.00815195\n",
      "Iteration 22161, loss = 0.00815164\n",
      "Iteration 22162, loss = 0.00815132\n",
      "Iteration 22163, loss = 0.00815101\n",
      "Iteration 22164, loss = 0.00815070\n",
      "Iteration 22165, loss = 0.00815038\n",
      "Iteration 22166, loss = 0.00815007\n",
      "Iteration 22167, loss = 0.00814975\n",
      "Iteration 22168, loss = 0.00814944\n",
      "Iteration 22169, loss = 0.00814913\n",
      "Iteration 22170, loss = 0.00814881\n",
      "Iteration 22171, loss = 0.00814850\n",
      "Iteration 22172, loss = 0.00814818\n",
      "Iteration 22173, loss = 0.00814787\n",
      "Iteration 22174, loss = 0.00814756\n",
      "Iteration 22175, loss = 0.00814724\n",
      "Iteration 22176, loss = 0.00814693\n",
      "Iteration 22177, loss = 0.00814662\n",
      "Iteration 22178, loss = 0.00814630\n",
      "Iteration 22179, loss = 0.00814599\n",
      "Iteration 22180, loss = 0.00814568\n",
      "Iteration 22181, loss = 0.00814536\n",
      "Iteration 22182, loss = 0.00814505\n",
      "Iteration 22183, loss = 0.00814474\n",
      "Iteration 22184, loss = 0.00814442\n",
      "Iteration 22185, loss = 0.00814411\n",
      "Iteration 22186, loss = 0.00814380\n",
      "Iteration 22187, loss = 0.00814348\n",
      "Iteration 22188, loss = 0.00814317\n",
      "Iteration 22189, loss = 0.00814286\n",
      "Iteration 22190, loss = 0.00814254\n",
      "Iteration 22191, loss = 0.00814223\n",
      "Iteration 22192, loss = 0.00814192\n",
      "Iteration 22193, loss = 0.00814160\n",
      "Iteration 22194, loss = 0.00814129\n",
      "Iteration 22195, loss = 0.00814098\n",
      "Iteration 22196, loss = 0.00814067\n",
      "Iteration 22197, loss = 0.00814035\n",
      "Iteration 22198, loss = 0.00814004\n",
      "Iteration 22199, loss = 0.00813973\n",
      "Iteration 22200, loss = 0.00813941\n",
      "Iteration 22201, loss = 0.00813910\n",
      "Iteration 22202, loss = 0.00813879\n",
      "Iteration 22203, loss = 0.00813848\n",
      "Iteration 22204, loss = 0.00813816\n",
      "Iteration 22205, loss = 0.00813785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22206, loss = 0.00813754\n",
      "Iteration 22207, loss = 0.00813723\n",
      "Iteration 22208, loss = 0.00813691\n",
      "Iteration 22209, loss = 0.00813660\n",
      "Iteration 22210, loss = 0.00813629\n",
      "Iteration 22211, loss = 0.00813598\n",
      "Iteration 22212, loss = 0.00813567\n",
      "Iteration 22213, loss = 0.00813535\n",
      "Iteration 22214, loss = 0.00813504\n",
      "Iteration 22215, loss = 0.00813473\n",
      "Iteration 22216, loss = 0.00813442\n",
      "Iteration 22217, loss = 0.00813411\n",
      "Iteration 22218, loss = 0.00813379\n",
      "Iteration 22219, loss = 0.00813348\n",
      "Iteration 22220, loss = 0.00813317\n",
      "Iteration 22221, loss = 0.00813286\n",
      "Iteration 22222, loss = 0.00813255\n",
      "Iteration 22223, loss = 0.00813223\n",
      "Iteration 22224, loss = 0.00813192\n",
      "Iteration 22225, loss = 0.00813161\n",
      "Iteration 22226, loss = 0.00813130\n",
      "Iteration 22227, loss = 0.00813099\n",
      "Iteration 22228, loss = 0.00813068\n",
      "Iteration 22229, loss = 0.00813036\n",
      "Iteration 22230, loss = 0.00813005\n",
      "Iteration 22231, loss = 0.00812974\n",
      "Iteration 22232, loss = 0.00812943\n",
      "Iteration 22233, loss = 0.00812912\n",
      "Iteration 22234, loss = 0.00812881\n",
      "Iteration 22235, loss = 0.00812850\n",
      "Iteration 22236, loss = 0.00812818\n",
      "Iteration 22237, loss = 0.00812787\n",
      "Iteration 22238, loss = 0.00812756\n",
      "Iteration 22239, loss = 0.00812725\n",
      "Iteration 22240, loss = 0.00812694\n",
      "Iteration 22241, loss = 0.00812663\n",
      "Iteration 22242, loss = 0.00812632\n",
      "Iteration 22243, loss = 0.00812601\n",
      "Iteration 22244, loss = 0.00812570\n",
      "Iteration 22245, loss = 0.00812539\n",
      "Iteration 22246, loss = 0.00812507\n",
      "Iteration 22247, loss = 0.00812476\n",
      "Iteration 22248, loss = 0.00812445\n",
      "Iteration 22249, loss = 0.00812414\n",
      "Iteration 22250, loss = 0.00812383\n",
      "Iteration 22251, loss = 0.00812352\n",
      "Iteration 22252, loss = 0.00812321\n",
      "Iteration 22253, loss = 0.00812290\n",
      "Iteration 22254, loss = 0.00812259\n",
      "Iteration 22255, loss = 0.00812228\n",
      "Iteration 22256, loss = 0.00812197\n",
      "Iteration 22257, loss = 0.00812166\n",
      "Iteration 22258, loss = 0.00812135\n",
      "Iteration 22259, loss = 0.00812104\n",
      "Iteration 22260, loss = 0.00812073\n",
      "Iteration 22261, loss = 0.00812042\n",
      "Iteration 22262, loss = 0.00812011\n",
      "Iteration 22263, loss = 0.00811980\n",
      "Iteration 22264, loss = 0.00811949\n",
      "Iteration 22265, loss = 0.00811918\n",
      "Iteration 22266, loss = 0.00811887\n",
      "Iteration 22267, loss = 0.00811855\n",
      "Iteration 22268, loss = 0.00811824\n",
      "Iteration 22269, loss = 0.00811793\n",
      "Iteration 22270, loss = 0.00811762\n",
      "Iteration 22271, loss = 0.00811732\n",
      "Iteration 22272, loss = 0.00811701\n",
      "Iteration 22273, loss = 0.00811670\n",
      "Iteration 22274, loss = 0.00811639\n",
      "Iteration 22275, loss = 0.00811608\n",
      "Iteration 22276, loss = 0.00811577\n",
      "Iteration 22277, loss = 0.00811546\n",
      "Iteration 22278, loss = 0.00811515\n",
      "Iteration 22279, loss = 0.00811484\n",
      "Iteration 22280, loss = 0.00811453\n",
      "Iteration 22281, loss = 0.00811422\n",
      "Iteration 22282, loss = 0.00811391\n",
      "Iteration 22283, loss = 0.00811360\n",
      "Iteration 22284, loss = 0.00811329\n",
      "Iteration 22285, loss = 0.00811298\n",
      "Iteration 22286, loss = 0.00811267\n",
      "Iteration 22287, loss = 0.00811236\n",
      "Iteration 22288, loss = 0.00811205\n",
      "Iteration 22289, loss = 0.00811174\n",
      "Iteration 22290, loss = 0.00811143\n",
      "Iteration 22291, loss = 0.00811112\n",
      "Iteration 22292, loss = 0.00811082\n",
      "Iteration 22293, loss = 0.00811051\n",
      "Iteration 22294, loss = 0.00811020\n",
      "Iteration 22295, loss = 0.00810989\n",
      "Iteration 22296, loss = 0.00810958\n",
      "Iteration 22297, loss = 0.00810927\n",
      "Iteration 22298, loss = 0.00810896\n",
      "Iteration 22299, loss = 0.00810865\n",
      "Iteration 22300, loss = 0.00810834\n",
      "Iteration 22301, loss = 0.00810804\n",
      "Iteration 22302, loss = 0.00810773\n",
      "Iteration 22303, loss = 0.00810742\n",
      "Iteration 22304, loss = 0.00810711\n",
      "Iteration 22305, loss = 0.00810680\n",
      "Iteration 22306, loss = 0.00810649\n",
      "Iteration 22307, loss = 0.00810618\n",
      "Iteration 22308, loss = 0.00810588\n",
      "Iteration 22309, loss = 0.00810557\n",
      "Iteration 22310, loss = 0.00810526\n",
      "Iteration 22311, loss = 0.00810495\n",
      "Iteration 22312, loss = 0.00810464\n",
      "Iteration 22313, loss = 0.00810433\n",
      "Iteration 22314, loss = 0.00810403\n",
      "Iteration 22315, loss = 0.00810372\n",
      "Iteration 22316, loss = 0.00810341\n",
      "Iteration 22317, loss = 0.00810310\n",
      "Iteration 22318, loss = 0.00810279\n",
      "Iteration 22319, loss = 0.00810248\n",
      "Iteration 22320, loss = 0.00810218\n",
      "Iteration 22321, loss = 0.00810187\n",
      "Iteration 22322, loss = 0.00810156\n",
      "Iteration 22323, loss = 0.00810125\n",
      "Iteration 22324, loss = 0.00810094\n",
      "Iteration 22325, loss = 0.00810064\n",
      "Iteration 22326, loss = 0.00810033\n",
      "Iteration 22327, loss = 0.00810002\n",
      "Iteration 22328, loss = 0.00809971\n",
      "Iteration 22329, loss = 0.00809941\n",
      "Iteration 22330, loss = 0.00809910\n",
      "Iteration 22331, loss = 0.00809879\n",
      "Iteration 22332, loss = 0.00809848\n",
      "Iteration 22333, loss = 0.00809818\n",
      "Iteration 22334, loss = 0.00809787\n",
      "Iteration 22335, loss = 0.00809756\n",
      "Iteration 22336, loss = 0.00809725\n",
      "Iteration 22337, loss = 0.00809695\n",
      "Iteration 22338, loss = 0.00809664\n",
      "Iteration 22339, loss = 0.00809633\n",
      "Iteration 22340, loss = 0.00809602\n",
      "Iteration 22341, loss = 0.00809572\n",
      "Iteration 22342, loss = 0.00809541\n",
      "Iteration 22343, loss = 0.00809510\n",
      "Iteration 22344, loss = 0.00809480\n",
      "Iteration 22345, loss = 0.00809449\n",
      "Iteration 22346, loss = 0.00809418\n",
      "Iteration 22347, loss = 0.00809387\n",
      "Iteration 22348, loss = 0.00809357\n",
      "Iteration 22349, loss = 0.00809326\n",
      "Iteration 22350, loss = 0.00809295\n",
      "Iteration 22351, loss = 0.00809265\n",
      "Iteration 22352, loss = 0.00809234\n",
      "Iteration 22353, loss = 0.00809203\n",
      "Iteration 22354, loss = 0.00809173\n",
      "Iteration 22355, loss = 0.00809142\n",
      "Iteration 22356, loss = 0.00809111\n",
      "Iteration 22357, loss = 0.00809081\n",
      "Iteration 22358, loss = 0.00809050\n",
      "Iteration 22359, loss = 0.00809019\n",
      "Iteration 22360, loss = 0.00808989\n",
      "Iteration 22361, loss = 0.00808958\n",
      "Iteration 22362, loss = 0.00808927\n",
      "Iteration 22363, loss = 0.00808897\n",
      "Iteration 22364, loss = 0.00808866\n",
      "Iteration 22365, loss = 0.00808835\n",
      "Iteration 22366, loss = 0.00808805\n",
      "Iteration 22367, loss = 0.00808774\n",
      "Iteration 22368, loss = 0.00808744\n",
      "Iteration 22369, loss = 0.00808713\n",
      "Iteration 22370, loss = 0.00808682\n",
      "Iteration 22371, loss = 0.00808652\n",
      "Iteration 22372, loss = 0.00808621\n",
      "Iteration 22373, loss = 0.00808591\n",
      "Iteration 22374, loss = 0.00808560\n",
      "Iteration 22375, loss = 0.00808529\n",
      "Iteration 22376, loss = 0.00808499\n",
      "Iteration 22377, loss = 0.00808468\n",
      "Iteration 22378, loss = 0.00808438\n",
      "Iteration 22379, loss = 0.00808407\n",
      "Iteration 22380, loss = 0.00808376\n",
      "Iteration 22381, loss = 0.00808346\n",
      "Iteration 22382, loss = 0.00808315\n",
      "Iteration 22383, loss = 0.00808285\n",
      "Iteration 22384, loss = 0.00808254\n",
      "Iteration 22385, loss = 0.00808224\n",
      "Iteration 22386, loss = 0.00808193\n",
      "Iteration 22387, loss = 0.00808163\n",
      "Iteration 22388, loss = 0.00808132\n",
      "Iteration 22389, loss = 0.00808101\n",
      "Iteration 22390, loss = 0.00808071\n",
      "Iteration 22391, loss = 0.00808040\n",
      "Iteration 22392, loss = 0.00808010\n",
      "Iteration 22393, loss = 0.00807979\n",
      "Iteration 22394, loss = 0.00807949\n",
      "Iteration 22395, loss = 0.00807918\n",
      "Iteration 22396, loss = 0.00807888\n",
      "Iteration 22397, loss = 0.00807857\n",
      "Iteration 22398, loss = 0.00807827\n",
      "Iteration 22399, loss = 0.00807796\n",
      "Iteration 22400, loss = 0.00807766\n",
      "Iteration 22401, loss = 0.00807735\n",
      "Iteration 22402, loss = 0.00807705\n",
      "Iteration 22403, loss = 0.00807674\n",
      "Iteration 22404, loss = 0.00807644\n",
      "Iteration 22405, loss = 0.00807613\n",
      "Iteration 22406, loss = 0.00807583\n",
      "Iteration 22407, loss = 0.00807552\n",
      "Iteration 22408, loss = 0.00807522\n",
      "Iteration 22409, loss = 0.00807491\n",
      "Iteration 22410, loss = 0.00807461\n",
      "Iteration 22411, loss = 0.00807430\n",
      "Iteration 22412, loss = 0.00807400\n",
      "Iteration 22413, loss = 0.00807370\n",
      "Iteration 22414, loss = 0.00807339\n",
      "Iteration 22415, loss = 0.00807309\n",
      "Iteration 22416, loss = 0.00807278\n",
      "Iteration 22417, loss = 0.00807248\n",
      "Iteration 22418, loss = 0.00807217\n",
      "Iteration 22419, loss = 0.00807187\n",
      "Iteration 22420, loss = 0.00807157\n",
      "Iteration 22421, loss = 0.00807126\n",
      "Iteration 22422, loss = 0.00807096\n",
      "Iteration 22423, loss = 0.00807065\n",
      "Iteration 22424, loss = 0.00807035\n",
      "Iteration 22425, loss = 0.00807004\n",
      "Iteration 22426, loss = 0.00806974\n",
      "Iteration 22427, loss = 0.00806944\n",
      "Iteration 22428, loss = 0.00806913\n",
      "Iteration 22429, loss = 0.00806883\n",
      "Iteration 22430, loss = 0.00806852\n",
      "Iteration 22431, loss = 0.00806822\n",
      "Iteration 22432, loss = 0.00806792\n",
      "Iteration 22433, loss = 0.00806761\n",
      "Iteration 22434, loss = 0.00806731\n",
      "Iteration 22435, loss = 0.00806701\n",
      "Iteration 22436, loss = 0.00806670\n",
      "Iteration 22437, loss = 0.00806640\n",
      "Iteration 22438, loss = 0.00806610\n",
      "Iteration 22439, loss = 0.00806579\n",
      "Iteration 22440, loss = 0.00806549\n",
      "Iteration 22441, loss = 0.00806518\n",
      "Iteration 22442, loss = 0.00806488\n",
      "Iteration 22443, loss = 0.00806458\n",
      "Iteration 22444, loss = 0.00806427\n",
      "Iteration 22445, loss = 0.00806397\n",
      "Iteration 22446, loss = 0.00806367\n",
      "Iteration 22447, loss = 0.00806336\n",
      "Iteration 22448, loss = 0.00806306\n",
      "Iteration 22449, loss = 0.00806276\n",
      "Iteration 22450, loss = 0.00806246\n",
      "Iteration 22451, loss = 0.00806215\n",
      "Iteration 22452, loss = 0.00806185\n",
      "Iteration 22453, loss = 0.00806155\n",
      "Iteration 22454, loss = 0.00806124\n",
      "Iteration 22455, loss = 0.00806094\n",
      "Iteration 22456, loss = 0.00806064\n",
      "Iteration 22457, loss = 0.00806033\n",
      "Iteration 22458, loss = 0.00806003\n",
      "Iteration 22459, loss = 0.00805973\n",
      "Iteration 22460, loss = 0.00805943\n",
      "Iteration 22461, loss = 0.00805912\n",
      "Iteration 22462, loss = 0.00805882\n",
      "Iteration 22463, loss = 0.00805852\n",
      "Iteration 22464, loss = 0.00805822\n",
      "Iteration 22465, loss = 0.00805791\n",
      "Iteration 22466, loss = 0.00805761\n",
      "Iteration 22467, loss = 0.00805731\n",
      "Iteration 22468, loss = 0.00805701\n",
      "Iteration 22469, loss = 0.00805670\n",
      "Iteration 22470, loss = 0.00805640\n",
      "Iteration 22471, loss = 0.00805610\n",
      "Iteration 22472, loss = 0.00805580\n",
      "Iteration 22473, loss = 0.00805549\n",
      "Iteration 22474, loss = 0.00805519\n",
      "Iteration 22475, loss = 0.00805489\n",
      "Iteration 22476, loss = 0.00805459\n",
      "Iteration 22477, loss = 0.00805429\n",
      "Iteration 22478, loss = 0.00805398\n",
      "Iteration 22479, loss = 0.00805368\n",
      "Iteration 22480, loss = 0.00805338\n",
      "Iteration 22481, loss = 0.00805308\n",
      "Iteration 22482, loss = 0.00805278\n",
      "Iteration 22483, loss = 0.00805247\n",
      "Iteration 22484, loss = 0.00805217\n",
      "Iteration 22485, loss = 0.00805187\n",
      "Iteration 22486, loss = 0.00805157\n",
      "Iteration 22487, loss = 0.00805127\n",
      "Iteration 22488, loss = 0.00805096\n",
      "Iteration 22489, loss = 0.00805066\n",
      "Iteration 22490, loss = 0.00805036\n",
      "Iteration 22491, loss = 0.00805006\n",
      "Iteration 22492, loss = 0.00804976\n",
      "Iteration 22493, loss = 0.00804946\n",
      "Iteration 22494, loss = 0.00804915\n",
      "Iteration 22495, loss = 0.00804885\n",
      "Iteration 22496, loss = 0.00804855\n",
      "Iteration 22497, loss = 0.00804825\n",
      "Iteration 22498, loss = 0.00804795\n",
      "Iteration 22499, loss = 0.00804765\n",
      "Iteration 22500, loss = 0.00804735\n",
      "Iteration 22501, loss = 0.00804705\n",
      "Iteration 22502, loss = 0.00804674\n",
      "Iteration 22503, loss = 0.00804644\n",
      "Iteration 22504, loss = 0.00804614\n",
      "Iteration 22505, loss = 0.00804584\n",
      "Iteration 22506, loss = 0.00804554\n",
      "Iteration 22507, loss = 0.00804524\n",
      "Iteration 22508, loss = 0.00804494\n",
      "Iteration 22509, loss = 0.00804464\n",
      "Iteration 22510, loss = 0.00804434\n",
      "Iteration 22511, loss = 0.00804404\n",
      "Iteration 22512, loss = 0.00804373\n",
      "Iteration 22513, loss = 0.00804343\n",
      "Iteration 22514, loss = 0.00804313\n",
      "Iteration 22515, loss = 0.00804283\n",
      "Iteration 22516, loss = 0.00804253\n",
      "Iteration 22517, loss = 0.00804223\n",
      "Iteration 22518, loss = 0.00804193\n",
      "Iteration 22519, loss = 0.00804163\n",
      "Iteration 22520, loss = 0.00804133\n",
      "Iteration 22521, loss = 0.00804103\n",
      "Iteration 22522, loss = 0.00804073\n",
      "Iteration 22523, loss = 0.00804043\n",
      "Iteration 22524, loss = 0.00804013\n",
      "Iteration 22525, loss = 0.00803983\n",
      "Iteration 22526, loss = 0.00803953\n",
      "Iteration 22527, loss = 0.00803923\n",
      "Iteration 22528, loss = 0.00803893\n",
      "Iteration 22529, loss = 0.00803863\n",
      "Iteration 22530, loss = 0.00803833\n",
      "Iteration 22531, loss = 0.00803803\n",
      "Iteration 22532, loss = 0.00803773\n",
      "Iteration 22533, loss = 0.00803743\n",
      "Iteration 22534, loss = 0.00803713\n",
      "Iteration 22535, loss = 0.00803683\n",
      "Iteration 22536, loss = 0.00803653\n",
      "Iteration 22537, loss = 0.00803623\n",
      "Iteration 22538, loss = 0.00803593\n",
      "Iteration 22539, loss = 0.00803563\n",
      "Iteration 22540, loss = 0.00803533\n",
      "Iteration 22541, loss = 0.00803503\n",
      "Iteration 22542, loss = 0.00803473\n",
      "Iteration 22543, loss = 0.00803443\n",
      "Iteration 22544, loss = 0.00803413\n",
      "Iteration 22545, loss = 0.00803383\n",
      "Iteration 22546, loss = 0.00803353\n",
      "Iteration 22547, loss = 0.00803323\n",
      "Iteration 22548, loss = 0.00803293\n",
      "Iteration 22549, loss = 0.00803263\n",
      "Iteration 22550, loss = 0.00803233\n",
      "Iteration 22551, loss = 0.00803203\n",
      "Iteration 22552, loss = 0.00803173\n",
      "Iteration 22553, loss = 0.00803143\n",
      "Iteration 22554, loss = 0.00803113\n",
      "Iteration 22555, loss = 0.00803083\n",
      "Iteration 22556, loss = 0.00803054\n",
      "Iteration 22557, loss = 0.00803024\n",
      "Iteration 22558, loss = 0.00802994\n",
      "Iteration 22559, loss = 0.00802964\n",
      "Iteration 22560, loss = 0.00802934\n",
      "Iteration 22561, loss = 0.00802904\n",
      "Iteration 22562, loss = 0.00802874\n",
      "Iteration 22563, loss = 0.00802844\n",
      "Iteration 22564, loss = 0.00802814\n",
      "Iteration 22565, loss = 0.00802784\n",
      "Iteration 22566, loss = 0.00802755\n",
      "Iteration 22567, loss = 0.00802725\n",
      "Iteration 22568, loss = 0.00802695\n",
      "Iteration 22569, loss = 0.00802665\n",
      "Iteration 22570, loss = 0.00802635\n",
      "Iteration 22571, loss = 0.00802605\n",
      "Iteration 22572, loss = 0.00802575\n",
      "Iteration 22573, loss = 0.00802545\n",
      "Iteration 22574, loss = 0.00802516\n",
      "Iteration 22575, loss = 0.00802486\n",
      "Iteration 22576, loss = 0.00802456\n",
      "Iteration 22577, loss = 0.00802426\n",
      "Iteration 22578, loss = 0.00802396\n",
      "Iteration 22579, loss = 0.00802366\n",
      "Iteration 22580, loss = 0.00802337\n",
      "Iteration 22581, loss = 0.00802307\n",
      "Iteration 22582, loss = 0.00802277\n",
      "Iteration 22583, loss = 0.00802247\n",
      "Iteration 22584, loss = 0.00802217\n",
      "Iteration 22585, loss = 0.00802187\n",
      "Iteration 22586, loss = 0.00802158\n",
      "Iteration 22587, loss = 0.00802128\n",
      "Iteration 22588, loss = 0.00802098\n",
      "Iteration 22589, loss = 0.00802068\n",
      "Iteration 22590, loss = 0.00802038\n",
      "Iteration 22591, loss = 0.00802009\n",
      "Iteration 22592, loss = 0.00801979\n",
      "Iteration 22593, loss = 0.00801949\n",
      "Iteration 22594, loss = 0.00801919\n",
      "Iteration 22595, loss = 0.00801890\n",
      "Iteration 22596, loss = 0.00801860\n",
      "Iteration 22597, loss = 0.00801830\n",
      "Iteration 22598, loss = 0.00801800\n",
      "Iteration 22599, loss = 0.00801770\n",
      "Iteration 22600, loss = 0.00801741\n",
      "Iteration 22601, loss = 0.00801711\n",
      "Iteration 22602, loss = 0.00801681\n",
      "Iteration 22603, loss = 0.00801651\n",
      "Iteration 22604, loss = 0.00801622\n",
      "Iteration 22605, loss = 0.00801592\n",
      "Iteration 22606, loss = 0.00801562\n",
      "Iteration 22607, loss = 0.00801532\n",
      "Iteration 22608, loss = 0.00801503\n",
      "Iteration 22609, loss = 0.00801473\n",
      "Iteration 22610, loss = 0.00801443\n",
      "Iteration 22611, loss = 0.00801414\n",
      "Iteration 22612, loss = 0.00801384\n",
      "Iteration 22613, loss = 0.00801354\n",
      "Iteration 22614, loss = 0.00801324\n",
      "Iteration 22615, loss = 0.00801295\n",
      "Iteration 22616, loss = 0.00801265\n",
      "Iteration 22617, loss = 0.00801235\n",
      "Iteration 22618, loss = 0.00801206\n",
      "Iteration 22619, loss = 0.00801176\n",
      "Iteration 22620, loss = 0.00801146\n",
      "Iteration 22621, loss = 0.00801117\n",
      "Iteration 22622, loss = 0.00801087\n",
      "Iteration 22623, loss = 0.00801057\n",
      "Iteration 22624, loss = 0.00801028\n",
      "Iteration 22625, loss = 0.00800998\n",
      "Iteration 22626, loss = 0.00800968\n",
      "Iteration 22627, loss = 0.00800939\n",
      "Iteration 22628, loss = 0.00800909\n",
      "Iteration 22629, loss = 0.00800879\n",
      "Iteration 22630, loss = 0.00800850\n",
      "Iteration 22631, loss = 0.00800820\n",
      "Iteration 22632, loss = 0.00800790\n",
      "Iteration 22633, loss = 0.00800761\n",
      "Iteration 22634, loss = 0.00800731\n",
      "Iteration 22635, loss = 0.00800701\n",
      "Iteration 22636, loss = 0.00800672\n",
      "Iteration 22637, loss = 0.00800642\n",
      "Iteration 22638, loss = 0.00800613\n",
      "Iteration 22639, loss = 0.00800583\n",
      "Iteration 22640, loss = 0.00800553\n",
      "Iteration 22641, loss = 0.00800524\n",
      "Iteration 22642, loss = 0.00800494\n",
      "Iteration 22643, loss = 0.00800464\n",
      "Iteration 22644, loss = 0.00800435\n",
      "Iteration 22645, loss = 0.00800405\n",
      "Iteration 22646, loss = 0.00800376\n",
      "Iteration 22647, loss = 0.00800346\n",
      "Iteration 22648, loss = 0.00800317\n",
      "Iteration 22649, loss = 0.00800287\n",
      "Iteration 22650, loss = 0.00800257\n",
      "Iteration 22651, loss = 0.00800228\n",
      "Iteration 22652, loss = 0.00800198\n",
      "Iteration 22653, loss = 0.00800169\n",
      "Iteration 22654, loss = 0.00800139\n",
      "Iteration 22655, loss = 0.00800110\n",
      "Iteration 22656, loss = 0.00800080\n",
      "Iteration 22657, loss = 0.00800050\n",
      "Iteration 22658, loss = 0.00800021\n",
      "Iteration 22659, loss = 0.00799991\n",
      "Iteration 22660, loss = 0.00799962\n",
      "Iteration 22661, loss = 0.00799932\n",
      "Iteration 22662, loss = 0.00799903\n",
      "Iteration 22663, loss = 0.00799873\n",
      "Iteration 22664, loss = 0.00799844\n",
      "Iteration 22665, loss = 0.00799814\n",
      "Iteration 22666, loss = 0.00799785\n",
      "Iteration 22667, loss = 0.00799755\n",
      "Iteration 22668, loss = 0.00799726\n",
      "Iteration 22669, loss = 0.00799696\n",
      "Iteration 22670, loss = 0.00799667\n",
      "Iteration 22671, loss = 0.00799637\n",
      "Iteration 22672, loss = 0.00799608\n",
      "Iteration 22673, loss = 0.00799578\n",
      "Iteration 22674, loss = 0.00799549\n",
      "Iteration 22675, loss = 0.00799519\n",
      "Iteration 22676, loss = 0.00799490\n",
      "Iteration 22677, loss = 0.00799460\n",
      "Iteration 22678, loss = 0.00799431\n",
      "Iteration 22679, loss = 0.00799401\n",
      "Iteration 22680, loss = 0.00799372\n",
      "Iteration 22681, loss = 0.00799342\n",
      "Iteration 22682, loss = 0.00799313\n",
      "Iteration 22683, loss = 0.00799283\n",
      "Iteration 22684, loss = 0.00799254\n",
      "Iteration 22685, loss = 0.00799224\n",
      "Iteration 22686, loss = 0.00799195\n",
      "Iteration 22687, loss = 0.00799166\n",
      "Iteration 22688, loss = 0.00799136\n",
      "Iteration 22689, loss = 0.00799107\n",
      "Iteration 22690, loss = 0.00799077\n",
      "Iteration 22691, loss = 0.00799048\n",
      "Iteration 22692, loss = 0.00799018\n",
      "Iteration 22693, loss = 0.00798989\n",
      "Iteration 22694, loss = 0.00798960\n",
      "Iteration 22695, loss = 0.00798930\n",
      "Iteration 22696, loss = 0.00798901\n",
      "Iteration 22697, loss = 0.00798871\n",
      "Iteration 22698, loss = 0.00798842\n",
      "Iteration 22699, loss = 0.00798812\n",
      "Iteration 22700, loss = 0.00798783\n",
      "Iteration 22701, loss = 0.00798754\n",
      "Iteration 22702, loss = 0.00798724\n",
      "Iteration 22703, loss = 0.00798695\n",
      "Iteration 22704, loss = 0.00798666\n",
      "Iteration 22705, loss = 0.00798636\n",
      "Iteration 22706, loss = 0.00798607\n",
      "Iteration 22707, loss = 0.00798577\n",
      "Iteration 22708, loss = 0.00798548\n",
      "Iteration 22709, loss = 0.00798519\n",
      "Iteration 22710, loss = 0.00798489\n",
      "Iteration 22711, loss = 0.00798460\n",
      "Iteration 22712, loss = 0.00798431\n",
      "Iteration 22713, loss = 0.00798401\n",
      "Iteration 22714, loss = 0.00798372\n",
      "Iteration 22715, loss = 0.00798343\n",
      "Iteration 22716, loss = 0.00798313\n",
      "Iteration 22717, loss = 0.00798284\n",
      "Iteration 22718, loss = 0.00798255\n",
      "Iteration 22719, loss = 0.00798225\n",
      "Iteration 22720, loss = 0.00798196\n",
      "Iteration 22721, loss = 0.00798167\n",
      "Iteration 22722, loss = 0.00798137\n",
      "Iteration 22723, loss = 0.00798108\n",
      "Iteration 22724, loss = 0.00798079\n",
      "Iteration 22725, loss = 0.00798049\n",
      "Iteration 22726, loss = 0.00798020\n",
      "Iteration 22727, loss = 0.00797991\n",
      "Iteration 22728, loss = 0.00797961\n",
      "Iteration 22729, loss = 0.00797932\n",
      "Iteration 22730, loss = 0.00797903\n",
      "Iteration 22731, loss = 0.00797874\n",
      "Iteration 22732, loss = 0.00797844\n",
      "Iteration 22733, loss = 0.00797815\n",
      "Iteration 22734, loss = 0.00797786\n",
      "Iteration 22735, loss = 0.00797756\n",
      "Iteration 22736, loss = 0.00797727\n",
      "Iteration 22737, loss = 0.00797698\n",
      "Iteration 22738, loss = 0.00797669\n",
      "Iteration 22739, loss = 0.00797639\n",
      "Iteration 22740, loss = 0.00797610\n",
      "Iteration 22741, loss = 0.00797581\n",
      "Iteration 22742, loss = 0.00797552\n",
      "Iteration 22743, loss = 0.00797522\n",
      "Iteration 22744, loss = 0.00797493\n",
      "Iteration 22745, loss = 0.00797464\n",
      "Iteration 22746, loss = 0.00797435\n",
      "Iteration 22747, loss = 0.00797405\n",
      "Iteration 22748, loss = 0.00797376\n",
      "Iteration 22749, loss = 0.00797347\n",
      "Iteration 22750, loss = 0.00797318\n",
      "Iteration 22751, loss = 0.00797289\n",
      "Iteration 22752, loss = 0.00797259\n",
      "Iteration 22753, loss = 0.00797230\n",
      "Iteration 22754, loss = 0.00797201\n",
      "Iteration 22755, loss = 0.00797172\n",
      "Iteration 22756, loss = 0.00797143\n",
      "Iteration 22757, loss = 0.00797113\n",
      "Iteration 22758, loss = 0.00797084\n",
      "Iteration 22759, loss = 0.00797055\n",
      "Iteration 22760, loss = 0.00797026\n",
      "Iteration 22761, loss = 0.00796997\n",
      "Iteration 22762, loss = 0.00796967\n",
      "Iteration 22763, loss = 0.00796938\n",
      "Iteration 22764, loss = 0.00796909\n",
      "Iteration 22765, loss = 0.00796880\n",
      "Iteration 22766, loss = 0.00796851\n",
      "Iteration 22767, loss = 0.00796822\n",
      "Iteration 22768, loss = 0.00796792\n",
      "Iteration 22769, loss = 0.00796763\n",
      "Iteration 22770, loss = 0.00796734\n",
      "Iteration 22771, loss = 0.00796705\n",
      "Iteration 22772, loss = 0.00796676\n",
      "Iteration 22773, loss = 0.00796647\n",
      "Iteration 22774, loss = 0.00796618\n",
      "Iteration 22775, loss = 0.00796588\n",
      "Iteration 22776, loss = 0.00796559\n",
      "Iteration 22777, loss = 0.00796530\n",
      "Iteration 22778, loss = 0.00796501\n",
      "Iteration 22779, loss = 0.00796472\n",
      "Iteration 22780, loss = 0.00796443\n",
      "Iteration 22781, loss = 0.00796414\n",
      "Iteration 22782, loss = 0.00796385\n",
      "Iteration 22783, loss = 0.00796355\n",
      "Iteration 22784, loss = 0.00796326\n",
      "Iteration 22785, loss = 0.00796297\n",
      "Iteration 22786, loss = 0.00796268\n",
      "Iteration 22787, loss = 0.00796239\n",
      "Iteration 22788, loss = 0.00796210\n",
      "Iteration 22789, loss = 0.00796181\n",
      "Iteration 22790, loss = 0.00796152\n",
      "Iteration 22791, loss = 0.00796123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22792, loss = 0.00796094\n",
      "Iteration 22793, loss = 0.00796065\n",
      "Iteration 22794, loss = 0.00796036\n",
      "Iteration 22795, loss = 0.00796007\n",
      "Iteration 22796, loss = 0.00795977\n",
      "Iteration 22797, loss = 0.00795948\n",
      "Iteration 22798, loss = 0.00795919\n",
      "Iteration 22799, loss = 0.00795890\n",
      "Iteration 22800, loss = 0.00795861\n",
      "Iteration 22801, loss = 0.00795832\n",
      "Iteration 22802, loss = 0.00795803\n",
      "Iteration 22803, loss = 0.00795774\n",
      "Iteration 22804, loss = 0.00795745\n",
      "Iteration 22805, loss = 0.00795716\n",
      "Iteration 22806, loss = 0.00795687\n",
      "Iteration 22807, loss = 0.00795658\n",
      "Iteration 22808, loss = 0.00795629\n",
      "Iteration 22809, loss = 0.00795600\n",
      "Iteration 22810, loss = 0.00795571\n",
      "Iteration 22811, loss = 0.00795542\n",
      "Iteration 22812, loss = 0.00795513\n",
      "Iteration 22813, loss = 0.00795484\n",
      "Iteration 22814, loss = 0.00795455\n",
      "Iteration 22815, loss = 0.00795426\n",
      "Iteration 22816, loss = 0.00795397\n",
      "Iteration 22817, loss = 0.00795368\n",
      "Iteration 22818, loss = 0.00795339\n",
      "Iteration 22819, loss = 0.00795310\n",
      "Iteration 22820, loss = 0.00795281\n",
      "Iteration 22821, loss = 0.00795252\n",
      "Iteration 22822, loss = 0.00795223\n",
      "Iteration 22823, loss = 0.00795194\n",
      "Iteration 22824, loss = 0.00795165\n",
      "Iteration 22825, loss = 0.00795136\n",
      "Iteration 22826, loss = 0.00795107\n",
      "Iteration 22827, loss = 0.00795078\n",
      "Iteration 22828, loss = 0.00795050\n",
      "Iteration 22829, loss = 0.00795021\n",
      "Iteration 22830, loss = 0.00794992\n",
      "Iteration 22831, loss = 0.00794963\n",
      "Iteration 22832, loss = 0.00794934\n",
      "Iteration 22833, loss = 0.00794905\n",
      "Iteration 22834, loss = 0.00794876\n",
      "Iteration 22835, loss = 0.00794847\n",
      "Iteration 22836, loss = 0.00794818\n",
      "Iteration 22837, loss = 0.00794789\n",
      "Iteration 22838, loss = 0.00794760\n",
      "Iteration 22839, loss = 0.00794731\n",
      "Iteration 22840, loss = 0.00794702\n",
      "Iteration 22841, loss = 0.00794674\n",
      "Iteration 22842, loss = 0.00794645\n",
      "Iteration 22843, loss = 0.00794616\n",
      "Iteration 22844, loss = 0.00794587\n",
      "Iteration 22845, loss = 0.00794558\n",
      "Iteration 22846, loss = 0.00794529\n",
      "Iteration 22847, loss = 0.00794500\n",
      "Iteration 22848, loss = 0.00794471\n",
      "Iteration 22849, loss = 0.00794443\n",
      "Iteration 22850, loss = 0.00794414\n",
      "Iteration 22851, loss = 0.00794385\n",
      "Iteration 22852, loss = 0.00794356\n",
      "Iteration 22853, loss = 0.00794327\n",
      "Iteration 22854, loss = 0.00794298\n",
      "Iteration 22855, loss = 0.00794269\n",
      "Iteration 22856, loss = 0.00794241\n",
      "Iteration 22857, loss = 0.00794212\n",
      "Iteration 22858, loss = 0.00794183\n",
      "Iteration 22859, loss = 0.00794154\n",
      "Iteration 22860, loss = 0.00794125\n",
      "Iteration 22861, loss = 0.00794096\n",
      "Iteration 22862, loss = 0.00794068\n",
      "Iteration 22863, loss = 0.00794039\n",
      "Iteration 22864, loss = 0.00794010\n",
      "Iteration 22865, loss = 0.00793981\n",
      "Iteration 22866, loss = 0.00793952\n",
      "Iteration 22867, loss = 0.00793923\n",
      "Iteration 22868, loss = 0.00793895\n",
      "Iteration 22869, loss = 0.00793866\n",
      "Iteration 22870, loss = 0.00793837\n",
      "Iteration 22871, loss = 0.00793808\n",
      "Iteration 22872, loss = 0.00793779\n",
      "Iteration 22873, loss = 0.00793751\n",
      "Iteration 22874, loss = 0.00793722\n",
      "Iteration 22875, loss = 0.00793693\n",
      "Iteration 22876, loss = 0.00793664\n",
      "Iteration 22877, loss = 0.00793636\n",
      "Iteration 22878, loss = 0.00793607\n",
      "Iteration 22879, loss = 0.00793578\n",
      "Iteration 22880, loss = 0.00793549\n",
      "Iteration 22881, loss = 0.00793520\n",
      "Iteration 22882, loss = 0.00793492\n",
      "Iteration 22883, loss = 0.00793463\n",
      "Iteration 22884, loss = 0.00793434\n",
      "Iteration 22885, loss = 0.00793405\n",
      "Iteration 22886, loss = 0.00793377\n",
      "Iteration 22887, loss = 0.00793348\n",
      "Iteration 22888, loss = 0.00793319\n",
      "Iteration 22889, loss = 0.00793291\n",
      "Iteration 22890, loss = 0.00793262\n",
      "Iteration 22891, loss = 0.00793233\n",
      "Iteration 22892, loss = 0.00793204\n",
      "Iteration 22893, loss = 0.00793176\n",
      "Iteration 22894, loss = 0.00793147\n",
      "Iteration 22895, loss = 0.00793118\n",
      "Iteration 22896, loss = 0.00793090\n",
      "Iteration 22897, loss = 0.00793061\n",
      "Iteration 22898, loss = 0.00793032\n",
      "Iteration 22899, loss = 0.00793003\n",
      "Iteration 22900, loss = 0.00792975\n",
      "Iteration 22901, loss = 0.00792946\n",
      "Iteration 22902, loss = 0.00792917\n",
      "Iteration 22903, loss = 0.00792889\n",
      "Iteration 22904, loss = 0.00792860\n",
      "Iteration 22905, loss = 0.00792831\n",
      "Iteration 22906, loss = 0.00792803\n",
      "Iteration 22907, loss = 0.00792774\n",
      "Iteration 22908, loss = 0.00792745\n",
      "Iteration 22909, loss = 0.00792717\n",
      "Iteration 22910, loss = 0.00792688\n",
      "Iteration 22911, loss = 0.00792659\n",
      "Iteration 22912, loss = 0.00792631\n",
      "Iteration 22913, loss = 0.00792602\n",
      "Iteration 22914, loss = 0.00792573\n",
      "Iteration 22915, loss = 0.00792545\n",
      "Iteration 22916, loss = 0.00792516\n",
      "Iteration 22917, loss = 0.00792487\n",
      "Iteration 22918, loss = 0.00792459\n",
      "Iteration 22919, loss = 0.00792430\n",
      "Iteration 22920, loss = 0.00792402\n",
      "Iteration 22921, loss = 0.00792373\n",
      "Iteration 22922, loss = 0.00792344\n",
      "Iteration 22923, loss = 0.00792316\n",
      "Iteration 22924, loss = 0.00792287\n",
      "Iteration 22925, loss = 0.00792258\n",
      "Iteration 22926, loss = 0.00792230\n",
      "Iteration 22927, loss = 0.00792201\n",
      "Iteration 22928, loss = 0.00792173\n",
      "Iteration 22929, loss = 0.00792144\n",
      "Iteration 22930, loss = 0.00792116\n",
      "Iteration 22931, loss = 0.00792087\n",
      "Iteration 22932, loss = 0.00792058\n",
      "Iteration 22933, loss = 0.00792030\n",
      "Iteration 22934, loss = 0.00792001\n",
      "Iteration 22935, loss = 0.00791973\n",
      "Iteration 22936, loss = 0.00791944\n",
      "Iteration 22937, loss = 0.00791915\n",
      "Iteration 22938, loss = 0.00791887\n",
      "Iteration 22939, loss = 0.00791858\n",
      "Iteration 22940, loss = 0.00791830\n",
      "Iteration 22941, loss = 0.00791801\n",
      "Iteration 22942, loss = 0.00791773\n",
      "Iteration 22943, loss = 0.00791744\n",
      "Iteration 22944, loss = 0.00791716\n",
      "Iteration 22945, loss = 0.00791687\n",
      "Iteration 22946, loss = 0.00791659\n",
      "Iteration 22947, loss = 0.00791630\n",
      "Iteration 22948, loss = 0.00791601\n",
      "Iteration 22949, loss = 0.00791573\n",
      "Iteration 22950, loss = 0.00791544\n",
      "Iteration 22951, loss = 0.00791516\n",
      "Iteration 22952, loss = 0.00791487\n",
      "Iteration 22953, loss = 0.00791459\n",
      "Iteration 22954, loss = 0.00791430\n",
      "Iteration 22955, loss = 0.00791402\n",
      "Iteration 22956, loss = 0.00791373\n",
      "Iteration 22957, loss = 0.00791345\n",
      "Iteration 22958, loss = 0.00791316\n",
      "Iteration 22959, loss = 0.00791288\n",
      "Iteration 22960, loss = 0.00791259\n",
      "Iteration 22961, loss = 0.00791231\n",
      "Iteration 22962, loss = 0.00791202\n",
      "Iteration 22963, loss = 0.00791174\n",
      "Iteration 22964, loss = 0.00791146\n",
      "Iteration 22965, loss = 0.00791117\n",
      "Iteration 22966, loss = 0.00791089\n",
      "Iteration 22967, loss = 0.00791060\n",
      "Iteration 22968, loss = 0.00791032\n",
      "Iteration 22969, loss = 0.00791003\n",
      "Iteration 22970, loss = 0.00790975\n",
      "Iteration 22971, loss = 0.00790946\n",
      "Iteration 22972, loss = 0.00790918\n",
      "Iteration 22973, loss = 0.00790889\n",
      "Iteration 22974, loss = 0.00790861\n",
      "Iteration 22975, loss = 0.00790833\n",
      "Iteration 22976, loss = 0.00790804\n",
      "Iteration 22977, loss = 0.00790776\n",
      "Iteration 22978, loss = 0.00790747\n",
      "Iteration 22979, loss = 0.00790719\n",
      "Iteration 22980, loss = 0.00790690\n",
      "Iteration 22981, loss = 0.00790662\n",
      "Iteration 22982, loss = 0.00790634\n",
      "Iteration 22983, loss = 0.00790605\n",
      "Iteration 22984, loss = 0.00790577\n",
      "Iteration 22985, loss = 0.00790548\n",
      "Iteration 22986, loss = 0.00790520\n",
      "Iteration 22987, loss = 0.00790492\n",
      "Iteration 22988, loss = 0.00790463\n",
      "Iteration 22989, loss = 0.00790435\n",
      "Iteration 22990, loss = 0.00790406\n",
      "Iteration 22991, loss = 0.00790378\n",
      "Iteration 22992, loss = 0.00790350\n",
      "Iteration 22993, loss = 0.00790321\n",
      "Iteration 22994, loss = 0.00790293\n",
      "Iteration 22995, loss = 0.00790265\n",
      "Iteration 22996, loss = 0.00790236\n",
      "Iteration 22997, loss = 0.00790208\n",
      "Iteration 22998, loss = 0.00790179\n",
      "Iteration 22999, loss = 0.00790151\n",
      "Iteration 23000, loss = 0.00790123\n",
      "Iteration 23001, loss = 0.00790094\n",
      "Iteration 23002, loss = 0.00790066\n",
      "Iteration 23003, loss = 0.00790038\n",
      "Iteration 23004, loss = 0.00790009\n",
      "Iteration 23005, loss = 0.00789981\n",
      "Iteration 23006, loss = 0.00789953\n",
      "Iteration 23007, loss = 0.00789924\n",
      "Iteration 23008, loss = 0.00789896\n",
      "Iteration 23009, loss = 0.00789868\n",
      "Iteration 23010, loss = 0.00789839\n",
      "Iteration 23011, loss = 0.00789811\n",
      "Iteration 23012, loss = 0.00789783\n",
      "Iteration 23013, loss = 0.00789755\n",
      "Iteration 23014, loss = 0.00789726\n",
      "Iteration 23015, loss = 0.00789698\n",
      "Iteration 23016, loss = 0.00789670\n",
      "Iteration 23017, loss = 0.00789641\n",
      "Iteration 23018, loss = 0.00789613\n",
      "Iteration 23019, loss = 0.00789585\n",
      "Iteration 23020, loss = 0.00789556\n",
      "Iteration 23021, loss = 0.00789528\n",
      "Iteration 23022, loss = 0.00789500\n",
      "Iteration 23023, loss = 0.00789472\n",
      "Iteration 23024, loss = 0.00789443\n",
      "Iteration 23025, loss = 0.00789415\n",
      "Iteration 23026, loss = 0.00789387\n",
      "Iteration 23027, loss = 0.00789359\n",
      "Iteration 23028, loss = 0.00789330\n",
      "Iteration 23029, loss = 0.00789302\n",
      "Iteration 23030, loss = 0.00789274\n",
      "Iteration 23031, loss = 0.00789246\n",
      "Iteration 23032, loss = 0.00789217\n",
      "Iteration 23033, loss = 0.00789189\n",
      "Iteration 23034, loss = 0.00789161\n",
      "Iteration 23035, loss = 0.00789133\n",
      "Iteration 23036, loss = 0.00789104\n",
      "Iteration 23037, loss = 0.00789076\n",
      "Iteration 23038, loss = 0.00789048\n",
      "Iteration 23039, loss = 0.00789020\n",
      "Iteration 23040, loss = 0.00788992\n",
      "Iteration 23041, loss = 0.00788963\n",
      "Iteration 23042, loss = 0.00788935\n",
      "Iteration 23043, loss = 0.00788907\n",
      "Iteration 23044, loss = 0.00788879\n",
      "Iteration 23045, loss = 0.00788851\n",
      "Iteration 23046, loss = 0.00788822\n",
      "Iteration 23047, loss = 0.00788794\n",
      "Iteration 23048, loss = 0.00788766\n",
      "Iteration 23049, loss = 0.00788738\n",
      "Iteration 23050, loss = 0.00788710\n",
      "Iteration 23051, loss = 0.00788681\n",
      "Iteration 23052, loss = 0.00788653\n",
      "Iteration 23053, loss = 0.00788625\n",
      "Iteration 23054, loss = 0.00788597\n",
      "Iteration 23055, loss = 0.00788569\n",
      "Iteration 23056, loss = 0.00788541\n",
      "Iteration 23057, loss = 0.00788512\n",
      "Iteration 23058, loss = 0.00788484\n",
      "Iteration 23059, loss = 0.00788456\n",
      "Iteration 23060, loss = 0.00788428\n",
      "Iteration 23061, loss = 0.00788400\n",
      "Iteration 23062, loss = 0.00788372\n",
      "Iteration 23063, loss = 0.00788344\n",
      "Iteration 23064, loss = 0.00788315\n",
      "Iteration 23065, loss = 0.00788287\n",
      "Iteration 23066, loss = 0.00788259\n",
      "Iteration 23067, loss = 0.00788231\n",
      "Iteration 23068, loss = 0.00788203\n",
      "Iteration 23069, loss = 0.00788175\n",
      "Iteration 23070, loss = 0.00788147\n",
      "Iteration 23071, loss = 0.00788119\n",
      "Iteration 23072, loss = 0.00788090\n",
      "Iteration 23073, loss = 0.00788062\n",
      "Iteration 23074, loss = 0.00788034\n",
      "Iteration 23075, loss = 0.00788006\n",
      "Iteration 23076, loss = 0.00787978\n",
      "Iteration 23077, loss = 0.00787950\n",
      "Iteration 23078, loss = 0.00787922\n",
      "Iteration 23079, loss = 0.00787894\n",
      "Iteration 23080, loss = 0.00787866\n",
      "Iteration 23081, loss = 0.00787838\n",
      "Iteration 23082, loss = 0.00787810\n",
      "Iteration 23083, loss = 0.00787782\n",
      "Iteration 23084, loss = 0.00787753\n",
      "Iteration 23085, loss = 0.00787725\n",
      "Iteration 23086, loss = 0.00787697\n",
      "Iteration 23087, loss = 0.00787669\n",
      "Iteration 23088, loss = 0.00787641\n",
      "Iteration 23089, loss = 0.00787613\n",
      "Iteration 23090, loss = 0.00787585\n",
      "Iteration 23091, loss = 0.00787557\n",
      "Iteration 23092, loss = 0.00787529\n",
      "Iteration 23093, loss = 0.00787501\n",
      "Iteration 23094, loss = 0.00787473\n",
      "Iteration 23095, loss = 0.00787445\n",
      "Iteration 23096, loss = 0.00787417\n",
      "Iteration 23097, loss = 0.00787389\n",
      "Iteration 23098, loss = 0.00787361\n",
      "Iteration 23099, loss = 0.00787333\n",
      "Iteration 23100, loss = 0.00787305\n",
      "Iteration 23101, loss = 0.00787277\n",
      "Iteration 23102, loss = 0.00787249\n",
      "Iteration 23103, loss = 0.00787221\n",
      "Iteration 23104, loss = 0.00787193\n",
      "Iteration 23105, loss = 0.00787165\n",
      "Iteration 23106, loss = 0.00787137\n",
      "Iteration 23107, loss = 0.00787109\n",
      "Iteration 23108, loss = 0.00787081\n",
      "Iteration 23109, loss = 0.00787053\n",
      "Iteration 23110, loss = 0.00787025\n",
      "Iteration 23111, loss = 0.00786997\n",
      "Iteration 23112, loss = 0.00786969\n",
      "Iteration 23113, loss = 0.00786941\n",
      "Iteration 23114, loss = 0.00786913\n",
      "Iteration 23115, loss = 0.00786885\n",
      "Iteration 23116, loss = 0.00786857\n",
      "Iteration 23117, loss = 0.00786829\n",
      "Iteration 23118, loss = 0.00786801\n",
      "Iteration 23119, loss = 0.00786773\n",
      "Iteration 23120, loss = 0.00786745\n",
      "Iteration 23121, loss = 0.00786717\n",
      "Iteration 23122, loss = 0.00786689\n",
      "Iteration 23123, loss = 0.00786661\n",
      "Iteration 23124, loss = 0.00786634\n",
      "Iteration 23125, loss = 0.00786606\n",
      "Iteration 23126, loss = 0.00786578\n",
      "Iteration 23127, loss = 0.00786550\n",
      "Iteration 23128, loss = 0.00786522\n",
      "Iteration 23129, loss = 0.00786494\n",
      "Iteration 23130, loss = 0.00786466\n",
      "Iteration 23131, loss = 0.00786438\n",
      "Iteration 23132, loss = 0.00786410\n",
      "Iteration 23133, loss = 0.00786382\n",
      "Iteration 23134, loss = 0.00786354\n",
      "Iteration 23135, loss = 0.00786327\n",
      "Iteration 23136, loss = 0.00786299\n",
      "Iteration 23137, loss = 0.00786271\n",
      "Iteration 23138, loss = 0.00786243\n",
      "Iteration 23139, loss = 0.00786215\n",
      "Iteration 23140, loss = 0.00786187\n",
      "Iteration 23141, loss = 0.00786159\n",
      "Iteration 23142, loss = 0.00786131\n",
      "Iteration 23143, loss = 0.00786103\n",
      "Iteration 23144, loss = 0.00786076\n",
      "Iteration 23145, loss = 0.00786048\n",
      "Iteration 23146, loss = 0.00786020\n",
      "Iteration 23147, loss = 0.00785992\n",
      "Iteration 23148, loss = 0.00785964\n",
      "Iteration 23149, loss = 0.00785936\n",
      "Iteration 23150, loss = 0.00785908\n",
      "Iteration 23151, loss = 0.00785881\n",
      "Iteration 23152, loss = 0.00785853\n",
      "Iteration 23153, loss = 0.00785825\n",
      "Iteration 23154, loss = 0.00785797\n",
      "Iteration 23155, loss = 0.00785769\n",
      "Iteration 23156, loss = 0.00785741\n",
      "Iteration 23157, loss = 0.00785714\n",
      "Iteration 23158, loss = 0.00785686\n",
      "Iteration 23159, loss = 0.00785658\n",
      "Iteration 23160, loss = 0.00785630\n",
      "Iteration 23161, loss = 0.00785602\n",
      "Iteration 23162, loss = 0.00785575\n",
      "Iteration 23163, loss = 0.00785547\n",
      "Iteration 23164, loss = 0.00785519\n",
      "Iteration 23165, loss = 0.00785491\n",
      "Iteration 23166, loss = 0.00785463\n",
      "Iteration 23167, loss = 0.00785436\n",
      "Iteration 23168, loss = 0.00785408\n",
      "Iteration 23169, loss = 0.00785380\n",
      "Iteration 23170, loss = 0.00785352\n",
      "Iteration 23171, loss = 0.00785325\n",
      "Iteration 23172, loss = 0.00785297\n",
      "Iteration 23173, loss = 0.00785269\n",
      "Iteration 23174, loss = 0.00785241\n",
      "Iteration 23175, loss = 0.00785213\n",
      "Iteration 23176, loss = 0.00785186\n",
      "Iteration 23177, loss = 0.00785158\n",
      "Iteration 23178, loss = 0.00785130\n",
      "Iteration 23179, loss = 0.00785102\n",
      "Iteration 23180, loss = 0.00785075\n",
      "Iteration 23181, loss = 0.00785047\n",
      "Iteration 23182, loss = 0.00785019\n",
      "Iteration 23183, loss = 0.00784991\n",
      "Iteration 23184, loss = 0.00784964\n",
      "Iteration 23185, loss = 0.00784936\n",
      "Iteration 23186, loss = 0.00784908\n",
      "Iteration 23187, loss = 0.00784881\n",
      "Iteration 23188, loss = 0.00784853\n",
      "Iteration 23189, loss = 0.00784825\n",
      "Iteration 23190, loss = 0.00784797\n",
      "Iteration 23191, loss = 0.00784770\n",
      "Iteration 23192, loss = 0.00784742\n",
      "Iteration 23193, loss = 0.00784714\n",
      "Iteration 23194, loss = 0.00784687\n",
      "Iteration 23195, loss = 0.00784659\n",
      "Iteration 23196, loss = 0.00784631\n",
      "Iteration 23197, loss = 0.00784603\n",
      "Iteration 23198, loss = 0.00784576\n",
      "Iteration 23199, loss = 0.00784548\n",
      "Iteration 23200, loss = 0.00784520\n",
      "Iteration 23201, loss = 0.00784493\n",
      "Iteration 23202, loss = 0.00784465\n",
      "Iteration 23203, loss = 0.00784437\n",
      "Iteration 23204, loss = 0.00784410\n",
      "Iteration 23205, loss = 0.00784382\n",
      "Iteration 23206, loss = 0.00784354\n",
      "Iteration 23207, loss = 0.00784327\n",
      "Iteration 23208, loss = 0.00784299\n",
      "Iteration 23209, loss = 0.00784271\n",
      "Iteration 23210, loss = 0.00784244\n",
      "Iteration 23211, loss = 0.00784216\n",
      "Iteration 23212, loss = 0.00784189\n",
      "Iteration 23213, loss = 0.00784161\n",
      "Iteration 23214, loss = 0.00784133\n",
      "Iteration 23215, loss = 0.00784106\n",
      "Iteration 23216, loss = 0.00784078\n",
      "Iteration 23217, loss = 0.00784050\n",
      "Iteration 23218, loss = 0.00784023\n",
      "Iteration 23219, loss = 0.00783995\n",
      "Iteration 23220, loss = 0.00783968\n",
      "Iteration 23221, loss = 0.00783940\n",
      "Iteration 23222, loss = 0.00783912\n",
      "Iteration 23223, loss = 0.00783885\n",
      "Iteration 23224, loss = 0.00783857\n",
      "Iteration 23225, loss = 0.00783830\n",
      "Iteration 23226, loss = 0.00783802\n",
      "Iteration 23227, loss = 0.00783774\n",
      "Iteration 23228, loss = 0.00783747\n",
      "Iteration 23229, loss = 0.00783719\n",
      "Iteration 23230, loss = 0.00783692\n",
      "Iteration 23231, loss = 0.00783664\n",
      "Iteration 23232, loss = 0.00783636\n",
      "Iteration 23233, loss = 0.00783609\n",
      "Iteration 23234, loss = 0.00783581\n",
      "Iteration 23235, loss = 0.00783554\n",
      "Iteration 23236, loss = 0.00783526\n",
      "Iteration 23237, loss = 0.00783499\n",
      "Iteration 23238, loss = 0.00783471\n",
      "Iteration 23239, loss = 0.00783443\n",
      "Iteration 23240, loss = 0.00783416\n",
      "Iteration 23241, loss = 0.00783388\n",
      "Iteration 23242, loss = 0.00783361\n",
      "Iteration 23243, loss = 0.00783333\n",
      "Iteration 23244, loss = 0.00783306\n",
      "Iteration 23245, loss = 0.00783278\n",
      "Iteration 23246, loss = 0.00783251\n",
      "Iteration 23247, loss = 0.00783223\n",
      "Iteration 23248, loss = 0.00783196\n",
      "Iteration 23249, loss = 0.00783168\n",
      "Iteration 23250, loss = 0.00783141\n",
      "Iteration 23251, loss = 0.00783113\n",
      "Iteration 23252, loss = 0.00783086\n",
      "Iteration 23253, loss = 0.00783058\n",
      "Iteration 23254, loss = 0.00783031\n",
      "Iteration 23255, loss = 0.00783003\n",
      "Iteration 23256, loss = 0.00782976\n",
      "Iteration 23257, loss = 0.00782948\n",
      "Iteration 23258, loss = 0.00782921\n",
      "Iteration 23259, loss = 0.00782893\n",
      "Iteration 23260, loss = 0.00782866\n",
      "Iteration 23261, loss = 0.00782838\n",
      "Iteration 23262, loss = 0.00782811\n",
      "Iteration 23263, loss = 0.00782783\n",
      "Iteration 23264, loss = 0.00782756\n",
      "Iteration 23265, loss = 0.00782728\n",
      "Iteration 23266, loss = 0.00782701\n",
      "Iteration 23267, loss = 0.00782673\n",
      "Iteration 23268, loss = 0.00782646\n",
      "Iteration 23269, loss = 0.00782618\n",
      "Iteration 23270, loss = 0.00782591\n",
      "Iteration 23271, loss = 0.00782564\n",
      "Iteration 23272, loss = 0.00782536\n",
      "Iteration 23273, loss = 0.00782509\n",
      "Iteration 23274, loss = 0.00782481\n",
      "Iteration 23275, loss = 0.00782454\n",
      "Iteration 23276, loss = 0.00782426\n",
      "Iteration 23277, loss = 0.00782399\n",
      "Iteration 23278, loss = 0.00782372\n",
      "Iteration 23279, loss = 0.00782344\n",
      "Iteration 23280, loss = 0.00782317\n",
      "Iteration 23281, loss = 0.00782289\n",
      "Iteration 23282, loss = 0.00782262\n",
      "Iteration 23283, loss = 0.00782234\n",
      "Iteration 23284, loss = 0.00782207\n",
      "Iteration 23285, loss = 0.00782180\n",
      "Iteration 23286, loss = 0.00782152\n",
      "Iteration 23287, loss = 0.00782125\n",
      "Iteration 23288, loss = 0.00782097\n",
      "Iteration 23289, loss = 0.00782070\n",
      "Iteration 23290, loss = 0.00782043\n",
      "Iteration 23291, loss = 0.00782015\n",
      "Iteration 23292, loss = 0.00781988\n",
      "Iteration 23293, loss = 0.00781961\n",
      "Iteration 23294, loss = 0.00781933\n",
      "Iteration 23295, loss = 0.00781906\n",
      "Iteration 23296, loss = 0.00781878\n",
      "Iteration 23297, loss = 0.00781851\n",
      "Iteration 23298, loss = 0.00781824\n",
      "Iteration 23299, loss = 0.00781796\n",
      "Iteration 23300, loss = 0.00781769\n",
      "Iteration 23301, loss = 0.00781742\n",
      "Iteration 23302, loss = 0.00781714\n",
      "Iteration 23303, loss = 0.00781687\n",
      "Iteration 23304, loss = 0.00781660\n",
      "Iteration 23305, loss = 0.00781632\n",
      "Iteration 23306, loss = 0.00781605\n",
      "Iteration 23307, loss = 0.00781578\n",
      "Iteration 23308, loss = 0.00781550\n",
      "Iteration 23309, loss = 0.00781523\n",
      "Iteration 23310, loss = 0.00781496\n",
      "Iteration 23311, loss = 0.00781468\n",
      "Iteration 23312, loss = 0.00781441\n",
      "Iteration 23313, loss = 0.00781414\n",
      "Iteration 23314, loss = 0.00781386\n",
      "Iteration 23315, loss = 0.00781359\n",
      "Iteration 23316, loss = 0.00781332\n",
      "Iteration 23317, loss = 0.00781304\n",
      "Iteration 23318, loss = 0.00781277\n",
      "Iteration 23319, loss = 0.00781250\n",
      "Iteration 23320, loss = 0.00781223\n",
      "Iteration 23321, loss = 0.00781195\n",
      "Iteration 23322, loss = 0.00781168\n",
      "Iteration 23323, loss = 0.00781141\n",
      "Iteration 23324, loss = 0.00781113\n",
      "Iteration 23325, loss = 0.00781086\n",
      "Iteration 23326, loss = 0.00781059\n",
      "Iteration 23327, loss = 0.00781032\n",
      "Iteration 23328, loss = 0.00781004\n",
      "Iteration 23329, loss = 0.00780977\n",
      "Iteration 23330, loss = 0.00780950\n",
      "Iteration 23331, loss = 0.00780923\n",
      "Iteration 23332, loss = 0.00780895\n",
      "Iteration 23333, loss = 0.00780868\n",
      "Iteration 23334, loss = 0.00780841\n",
      "Iteration 23335, loss = 0.00780814\n",
      "Iteration 23336, loss = 0.00780786\n",
      "Iteration 23337, loss = 0.00780759\n",
      "Iteration 23338, loss = 0.00780732\n",
      "Iteration 23339, loss = 0.00780705\n",
      "Iteration 23340, loss = 0.00780678\n",
      "Iteration 23341, loss = 0.00780650\n",
      "Iteration 23342, loss = 0.00780623\n",
      "Iteration 23343, loss = 0.00780596\n",
      "Iteration 23344, loss = 0.00780569\n",
      "Iteration 23345, loss = 0.00780541\n",
      "Iteration 23346, loss = 0.00780514\n",
      "Iteration 23347, loss = 0.00780487\n",
      "Iteration 23348, loss = 0.00780460\n",
      "Iteration 23349, loss = 0.00780433\n",
      "Iteration 23350, loss = 0.00780405\n",
      "Iteration 23351, loss = 0.00780378\n",
      "Iteration 23352, loss = 0.00780351\n",
      "Iteration 23353, loss = 0.00780324\n",
      "Iteration 23354, loss = 0.00780297\n",
      "Iteration 23355, loss = 0.00780270\n",
      "Iteration 23356, loss = 0.00780242\n",
      "Iteration 23357, loss = 0.00780215\n",
      "Iteration 23358, loss = 0.00780188\n",
      "Iteration 23359, loss = 0.00780161\n",
      "Iteration 23360, loss = 0.00780134\n",
      "Iteration 23361, loss = 0.00780107\n",
      "Iteration 23362, loss = 0.00780079\n",
      "Iteration 23363, loss = 0.00780052\n",
      "Iteration 23364, loss = 0.00780025\n",
      "Iteration 23365, loss = 0.00779998\n",
      "Iteration 23366, loss = 0.00779971\n",
      "Iteration 23367, loss = 0.00779944\n",
      "Iteration 23368, loss = 0.00779917\n",
      "Iteration 23369, loss = 0.00779889\n",
      "Iteration 23370, loss = 0.00779862\n",
      "Iteration 23371, loss = 0.00779835\n",
      "Iteration 23372, loss = 0.00779808\n",
      "Iteration 23373, loss = 0.00779781\n",
      "Iteration 23374, loss = 0.00779754\n",
      "Iteration 23375, loss = 0.00779727\n",
      "Iteration 23376, loss = 0.00779700\n",
      "Iteration 23377, loss = 0.00779672\n",
      "Iteration 23378, loss = 0.00779645\n",
      "Iteration 23379, loss = 0.00779618\n",
      "Iteration 23380, loss = 0.00779591\n",
      "Iteration 23381, loss = 0.00779564\n",
      "Iteration 23382, loss = 0.00779537\n",
      "Iteration 23383, loss = 0.00779510\n",
      "Iteration 23384, loss = 0.00779483\n",
      "Iteration 23385, loss = 0.00779456\n",
      "Iteration 23386, loss = 0.00779429\n",
      "Iteration 23387, loss = 0.00779402\n",
      "Iteration 23388, loss = 0.00779375\n",
      "Iteration 23389, loss = 0.00779347\n",
      "Iteration 23390, loss = 0.00779320\n",
      "Iteration 23391, loss = 0.00779293\n",
      "Iteration 23392, loss = 0.00779266\n",
      "Iteration 23393, loss = 0.00779239\n",
      "Iteration 23394, loss = 0.00779212\n",
      "Iteration 23395, loss = 0.00779185\n",
      "Iteration 23396, loss = 0.00779158\n",
      "Iteration 23397, loss = 0.00779131\n",
      "Iteration 23398, loss = 0.00779104\n",
      "Iteration 23399, loss = 0.00779077\n",
      "Iteration 23400, loss = 0.00779050\n",
      "Iteration 23401, loss = 0.00779023\n",
      "Iteration 23402, loss = 0.00778996\n",
      "Iteration 23403, loss = 0.00778969\n",
      "Iteration 23404, loss = 0.00778942\n",
      "Iteration 23405, loss = 0.00778915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23406, loss = 0.00778888\n",
      "Iteration 23407, loss = 0.00778861\n",
      "Iteration 23408, loss = 0.00778834\n",
      "Iteration 23409, loss = 0.00778807\n",
      "Iteration 23410, loss = 0.00778780\n",
      "Iteration 23411, loss = 0.00778753\n",
      "Iteration 23412, loss = 0.00778726\n",
      "Iteration 23413, loss = 0.00778699\n",
      "Iteration 23414, loss = 0.00778672\n",
      "Iteration 23415, loss = 0.00778645\n",
      "Iteration 23416, loss = 0.00778618\n",
      "Iteration 23417, loss = 0.00778591\n",
      "Iteration 23418, loss = 0.00778564\n",
      "Iteration 23419, loss = 0.00778537\n",
      "Iteration 23420, loss = 0.00778510\n",
      "Iteration 23421, loss = 0.00778483\n",
      "Iteration 23422, loss = 0.00778456\n",
      "Iteration 23423, loss = 0.00778429\n",
      "Iteration 23424, loss = 0.00778402\n",
      "Iteration 23425, loss = 0.00778375\n",
      "Iteration 23426, loss = 0.00778348\n",
      "Iteration 23427, loss = 0.00778321\n",
      "Iteration 23428, loss = 0.00778294\n",
      "Iteration 23429, loss = 0.00778267\n",
      "Iteration 23430, loss = 0.00778240\n",
      "Iteration 23431, loss = 0.00778214\n",
      "Iteration 23432, loss = 0.00778187\n",
      "Iteration 23433, loss = 0.00778160\n",
      "Iteration 23434, loss = 0.00778133\n",
      "Iteration 23435, loss = 0.00778106\n",
      "Iteration 23436, loss = 0.00778079\n",
      "Iteration 23437, loss = 0.00778052\n",
      "Iteration 23438, loss = 0.00778025\n",
      "Iteration 23439, loss = 0.00777998\n",
      "Iteration 23440, loss = 0.00777971\n",
      "Iteration 23441, loss = 0.00777944\n",
      "Iteration 23442, loss = 0.00777917\n",
      "Iteration 23443, loss = 0.00777891\n",
      "Iteration 23444, loss = 0.00777864\n",
      "Iteration 23445, loss = 0.00777837\n",
      "Iteration 23446, loss = 0.00777810\n",
      "Iteration 23447, loss = 0.00777783\n",
      "Iteration 23448, loss = 0.00777756\n",
      "Iteration 23449, loss = 0.00777729\n",
      "Iteration 23450, loss = 0.00777702\n",
      "Iteration 23451, loss = 0.00777676\n",
      "Iteration 23452, loss = 0.00777649\n",
      "Iteration 23453, loss = 0.00777622\n",
      "Iteration 23454, loss = 0.00777595\n",
      "Iteration 23455, loss = 0.00777568\n",
      "Iteration 23456, loss = 0.00777541\n",
      "Iteration 23457, loss = 0.00777514\n",
      "Iteration 23458, loss = 0.00777488\n",
      "Iteration 23459, loss = 0.00777461\n",
      "Iteration 23460, loss = 0.00777434\n",
      "Iteration 23461, loss = 0.00777407\n",
      "Iteration 23462, loss = 0.00777380\n",
      "Iteration 23463, loss = 0.00777353\n",
      "Iteration 23464, loss = 0.00777327\n",
      "Iteration 23465, loss = 0.00777300\n",
      "Iteration 23466, loss = 0.00777273\n",
      "Iteration 23467, loss = 0.00777246\n",
      "Iteration 23468, loss = 0.00777219\n",
      "Iteration 23469, loss = 0.00777192\n",
      "Iteration 23470, loss = 0.00777166\n",
      "Iteration 23471, loss = 0.00777139\n",
      "Iteration 23472, loss = 0.00777112\n",
      "Iteration 23473, loss = 0.00777085\n",
      "Iteration 23474, loss = 0.00777058\n",
      "Iteration 23475, loss = 0.00777032\n",
      "Iteration 23476, loss = 0.00777005\n",
      "Iteration 23477, loss = 0.00776978\n",
      "Iteration 23478, loss = 0.00776951\n",
      "Iteration 23479, loss = 0.00776924\n",
      "Iteration 23480, loss = 0.00776898\n",
      "Iteration 23481, loss = 0.00776871\n",
      "Iteration 23482, loss = 0.00776844\n",
      "Iteration 23483, loss = 0.00776817\n",
      "Iteration 23484, loss = 0.00776791\n",
      "Iteration 23485, loss = 0.00776764\n",
      "Iteration 23486, loss = 0.00776737\n",
      "Iteration 23487, loss = 0.00776710\n",
      "Iteration 23488, loss = 0.00776684\n",
      "Iteration 23489, loss = 0.00776657\n",
      "Iteration 23490, loss = 0.00776630\n",
      "Iteration 23491, loss = 0.00776603\n",
      "Iteration 23492, loss = 0.00776577\n",
      "Iteration 23493, loss = 0.00776550\n",
      "Iteration 23494, loss = 0.00776523\n",
      "Iteration 23495, loss = 0.00776496\n",
      "Iteration 23496, loss = 0.00776470\n",
      "Iteration 23497, loss = 0.00776443\n",
      "Iteration 23498, loss = 0.00776416\n",
      "Iteration 23499, loss = 0.00776390\n",
      "Iteration 23500, loss = 0.00776363\n",
      "Iteration 23501, loss = 0.00776336\n",
      "Iteration 23502, loss = 0.00776309\n",
      "Iteration 23503, loss = 0.00776283\n",
      "Iteration 23504, loss = 0.00776256\n",
      "Iteration 23505, loss = 0.00776229\n",
      "Iteration 23506, loss = 0.00776203\n",
      "Iteration 23507, loss = 0.00776176\n",
      "Iteration 23508, loss = 0.00776149\n",
      "Iteration 23509, loss = 0.00776123\n",
      "Iteration 23510, loss = 0.00776096\n",
      "Iteration 23511, loss = 0.00776069\n",
      "Iteration 23512, loss = 0.00776042\n",
      "Iteration 23513, loss = 0.00776016\n",
      "Iteration 23514, loss = 0.00775989\n",
      "Iteration 23515, loss = 0.00775962\n",
      "Iteration 23516, loss = 0.00775936\n",
      "Iteration 23517, loss = 0.00775909\n",
      "Iteration 23518, loss = 0.00775882\n",
      "Iteration 23519, loss = 0.00775856\n",
      "Iteration 23520, loss = 0.00775829\n",
      "Iteration 23521, loss = 0.00775803\n",
      "Iteration 23522, loss = 0.00775776\n",
      "Iteration 23523, loss = 0.00775749\n",
      "Iteration 23524, loss = 0.00775723\n",
      "Iteration 23525, loss = 0.00775696\n",
      "Iteration 23526, loss = 0.00775669\n",
      "Iteration 23527, loss = 0.00775643\n",
      "Iteration 23528, loss = 0.00775616\n",
      "Iteration 23529, loss = 0.00775589\n",
      "Iteration 23530, loss = 0.00775563\n",
      "Iteration 23531, loss = 0.00775536\n",
      "Iteration 23532, loss = 0.00775510\n",
      "Iteration 23533, loss = 0.00775483\n",
      "Iteration 23534, loss = 0.00775456\n",
      "Iteration 23535, loss = 0.00775430\n",
      "Iteration 23536, loss = 0.00775403\n",
      "Iteration 23537, loss = 0.00775377\n",
      "Iteration 23538, loss = 0.00775350\n",
      "Iteration 23539, loss = 0.00775323\n",
      "Iteration 23540, loss = 0.00775297\n",
      "Iteration 23541, loss = 0.00775270\n",
      "Iteration 23542, loss = 0.00775244\n",
      "Iteration 23543, loss = 0.00775217\n",
      "Iteration 23544, loss = 0.00775190\n",
      "Iteration 23545, loss = 0.00775164\n",
      "Iteration 23546, loss = 0.00775137\n",
      "Iteration 23547, loss = 0.00775111\n",
      "Iteration 23548, loss = 0.00775084\n",
      "Iteration 23549, loss = 0.00775058\n",
      "Iteration 23550, loss = 0.00775031\n",
      "Iteration 23551, loss = 0.00775004\n",
      "Iteration 23552, loss = 0.00774978\n",
      "Iteration 23553, loss = 0.00774951\n",
      "Iteration 23554, loss = 0.00774925\n",
      "Iteration 23555, loss = 0.00774898\n",
      "Iteration 23556, loss = 0.00774872\n",
      "Iteration 23557, loss = 0.00774845\n",
      "Iteration 23558, loss = 0.00774819\n",
      "Iteration 23559, loss = 0.00774792\n",
      "Iteration 23560, loss = 0.00774766\n",
      "Iteration 23561, loss = 0.00774739\n",
      "Iteration 23562, loss = 0.00774713\n",
      "Iteration 23563, loss = 0.00774686\n",
      "Iteration 23564, loss = 0.00774660\n",
      "Iteration 23565, loss = 0.00774633\n",
      "Iteration 23566, loss = 0.00774607\n",
      "Iteration 23567, loss = 0.00774580\n",
      "Iteration 23568, loss = 0.00774553\n",
      "Iteration 23569, loss = 0.00774527\n",
      "Iteration 23570, loss = 0.00774501\n",
      "Iteration 23571, loss = 0.00774474\n",
      "Iteration 23572, loss = 0.00774448\n",
      "Iteration 23573, loss = 0.00774421\n",
      "Iteration 23574, loss = 0.00774395\n",
      "Iteration 23575, loss = 0.00774368\n",
      "Iteration 23576, loss = 0.00774342\n",
      "Iteration 23577, loss = 0.00774315\n",
      "Iteration 23578, loss = 0.00774289\n",
      "Iteration 23579, loss = 0.00774262\n",
      "Iteration 23580, loss = 0.00774236\n",
      "Iteration 23581, loss = 0.00774209\n",
      "Iteration 23582, loss = 0.00774183\n",
      "Iteration 23583, loss = 0.00774156\n",
      "Iteration 23584, loss = 0.00774130\n",
      "Iteration 23585, loss = 0.00774103\n",
      "Iteration 23586, loss = 0.00774077\n",
      "Iteration 23587, loss = 0.00774051\n",
      "Iteration 23588, loss = 0.00774024\n",
      "Iteration 23589, loss = 0.00773998\n",
      "Iteration 23590, loss = 0.00773971\n",
      "Iteration 23591, loss = 0.00773945\n",
      "Iteration 23592, loss = 0.00773918\n",
      "Iteration 23593, loss = 0.00773892\n",
      "Iteration 23594, loss = 0.00773866\n",
      "Iteration 23595, loss = 0.00773839\n",
      "Iteration 23596, loss = 0.00773813\n",
      "Iteration 23597, loss = 0.00773786\n",
      "Iteration 23598, loss = 0.00773760\n",
      "Iteration 23599, loss = 0.00773733\n",
      "Iteration 23600, loss = 0.00773707\n",
      "Iteration 23601, loss = 0.00773681\n",
      "Iteration 23602, loss = 0.00773654\n",
      "Iteration 23603, loss = 0.00773628\n",
      "Iteration 23604, loss = 0.00773601\n",
      "Iteration 23605, loss = 0.00773575\n",
      "Iteration 23606, loss = 0.00773549\n",
      "Iteration 23607, loss = 0.00773522\n",
      "Iteration 23608, loss = 0.00773496\n",
      "Iteration 23609, loss = 0.00773470\n",
      "Iteration 23610, loss = 0.00773443\n",
      "Iteration 23611, loss = 0.00773417\n",
      "Iteration 23612, loss = 0.00773390\n",
      "Iteration 23613, loss = 0.00773364\n",
      "Iteration 23614, loss = 0.00773338\n",
      "Iteration 23615, loss = 0.00773311\n",
      "Iteration 23616, loss = 0.00773285\n",
      "Iteration 23617, loss = 0.00773259\n",
      "Iteration 23618, loss = 0.00773232\n",
      "Iteration 23619, loss = 0.00773206\n",
      "Iteration 23620, loss = 0.00773180\n",
      "Iteration 23621, loss = 0.00773153\n",
      "Iteration 23622, loss = 0.00773127\n",
      "Iteration 23623, loss = 0.00773101\n",
      "Iteration 23624, loss = 0.00773074\n",
      "Iteration 23625, loss = 0.00773048\n",
      "Iteration 23626, loss = 0.00773022\n",
      "Iteration 23627, loss = 0.00772995\n",
      "Iteration 23628, loss = 0.00772969\n",
      "Iteration 23629, loss = 0.00772943\n",
      "Iteration 23630, loss = 0.00772916\n",
      "Iteration 23631, loss = 0.00772890\n",
      "Iteration 23632, loss = 0.00772864\n",
      "Iteration 23633, loss = 0.00772837\n",
      "Iteration 23634, loss = 0.00772811\n",
      "Iteration 23635, loss = 0.00772785\n",
      "Iteration 23636, loss = 0.00772759\n",
      "Iteration 23637, loss = 0.00772732\n",
      "Iteration 23638, loss = 0.00772706\n",
      "Iteration 23639, loss = 0.00772680\n",
      "Iteration 23640, loss = 0.00772653\n",
      "Iteration 23641, loss = 0.00772627\n",
      "Iteration 23642, loss = 0.00772601\n",
      "Iteration 23643, loss = 0.00772575\n",
      "Iteration 23644, loss = 0.00772548\n",
      "Iteration 23645, loss = 0.00772522\n",
      "Iteration 23646, loss = 0.00772496\n",
      "Iteration 23647, loss = 0.00772470\n",
      "Iteration 23648, loss = 0.00772443\n",
      "Iteration 23649, loss = 0.00772417\n",
      "Iteration 23650, loss = 0.00772391\n",
      "Iteration 23651, loss = 0.00772365\n",
      "Iteration 23652, loss = 0.00772338\n",
      "Iteration 23653, loss = 0.00772312\n",
      "Iteration 23654, loss = 0.00772286\n",
      "Iteration 23655, loss = 0.00772260\n",
      "Iteration 23656, loss = 0.00772233\n",
      "Iteration 23657, loss = 0.00772207\n",
      "Iteration 23658, loss = 0.00772181\n",
      "Iteration 23659, loss = 0.00772155\n",
      "Iteration 23660, loss = 0.00772128\n",
      "Iteration 23661, loss = 0.00772102\n",
      "Iteration 23662, loss = 0.00772076\n",
      "Iteration 23663, loss = 0.00772050\n",
      "Iteration 23664, loss = 0.00772024\n",
      "Iteration 23665, loss = 0.00771997\n",
      "Iteration 23666, loss = 0.00771971\n",
      "Iteration 23667, loss = 0.00771945\n",
      "Iteration 23668, loss = 0.00771919\n",
      "Iteration 23669, loss = 0.00771893\n",
      "Iteration 23670, loss = 0.00771866\n",
      "Iteration 23671, loss = 0.00771840\n",
      "Iteration 23672, loss = 0.00771814\n",
      "Iteration 23673, loss = 0.00771788\n",
      "Iteration 23674, loss = 0.00771762\n",
      "Iteration 23675, loss = 0.00771735\n",
      "Iteration 23676, loss = 0.00771709\n",
      "Iteration 23677, loss = 0.00771683\n",
      "Iteration 23678, loss = 0.00771657\n",
      "Iteration 23679, loss = 0.00771631\n",
      "Iteration 23680, loss = 0.00771605\n",
      "Iteration 23681, loss = 0.00771579\n",
      "Iteration 23682, loss = 0.00771552\n",
      "Iteration 23683, loss = 0.00771526\n",
      "Iteration 23684, loss = 0.00771500\n",
      "Iteration 23685, loss = 0.00771474\n",
      "Iteration 23686, loss = 0.00771448\n",
      "Iteration 23687, loss = 0.00771422\n",
      "Iteration 23688, loss = 0.00771396\n",
      "Iteration 23689, loss = 0.00771369\n",
      "Iteration 23690, loss = 0.00771343\n",
      "Iteration 23691, loss = 0.00771317\n",
      "Iteration 23692, loss = 0.00771291\n",
      "Iteration 23693, loss = 0.00771265\n",
      "Iteration 23694, loss = 0.00771239\n",
      "Iteration 23695, loss = 0.00771213\n",
      "Iteration 23696, loss = 0.00771187\n",
      "Iteration 23697, loss = 0.00771160\n",
      "Iteration 23698, loss = 0.00771134\n",
      "Iteration 23699, loss = 0.00771108\n",
      "Iteration 23700, loss = 0.00771082\n",
      "Iteration 23701, loss = 0.00771056\n",
      "Iteration 23702, loss = 0.00771030\n",
      "Iteration 23703, loss = 0.00771004\n",
      "Iteration 23704, loss = 0.00770978\n",
      "Iteration 23705, loss = 0.00770952\n",
      "Iteration 23706, loss = 0.00770926\n",
      "Iteration 23707, loss = 0.00770900\n",
      "Iteration 23708, loss = 0.00770873\n",
      "Iteration 23709, loss = 0.00770847\n",
      "Iteration 23710, loss = 0.00770821\n",
      "Iteration 23711, loss = 0.00770795\n",
      "Iteration 23712, loss = 0.00770769\n",
      "Iteration 23713, loss = 0.00770743\n",
      "Iteration 23714, loss = 0.00770717\n",
      "Iteration 23715, loss = 0.00770691\n",
      "Iteration 23716, loss = 0.00770665\n",
      "Iteration 23717, loss = 0.00770639\n",
      "Iteration 23718, loss = 0.00770613\n",
      "Iteration 23719, loss = 0.00770587\n",
      "Iteration 23720, loss = 0.00770561\n",
      "Iteration 23721, loss = 0.00770535\n",
      "Iteration 23722, loss = 0.00770509\n",
      "Iteration 23723, loss = 0.00770483\n",
      "Iteration 23724, loss = 0.00770457\n",
      "Iteration 23725, loss = 0.00770431\n",
      "Iteration 23726, loss = 0.00770405\n",
      "Iteration 23727, loss = 0.00770379\n",
      "Iteration 23728, loss = 0.00770353\n",
      "Iteration 23729, loss = 0.00770327\n",
      "Iteration 23730, loss = 0.00770301\n",
      "Iteration 23731, loss = 0.00770275\n",
      "Iteration 23732, loss = 0.00770249\n",
      "Iteration 23733, loss = 0.00770223\n",
      "Iteration 23734, loss = 0.00770197\n",
      "Iteration 23735, loss = 0.00770171\n",
      "Iteration 23736, loss = 0.00770145\n",
      "Iteration 23737, loss = 0.00770119\n",
      "Iteration 23738, loss = 0.00770093\n",
      "Iteration 23739, loss = 0.00770067\n",
      "Iteration 23740, loss = 0.00770041\n",
      "Iteration 23741, loss = 0.00770015\n",
      "Iteration 23742, loss = 0.00769989\n",
      "Iteration 23743, loss = 0.00769963\n",
      "Iteration 23744, loss = 0.00769937\n",
      "Iteration 23745, loss = 0.00769911\n",
      "Iteration 23746, loss = 0.00769885\n",
      "Iteration 23747, loss = 0.00769859\n",
      "Iteration 23748, loss = 0.00769833\n",
      "Iteration 23749, loss = 0.00769807\n",
      "Iteration 23750, loss = 0.00769781\n",
      "Iteration 23751, loss = 0.00769755\n",
      "Iteration 23752, loss = 0.00769729\n",
      "Iteration 23753, loss = 0.00769703\n",
      "Iteration 23754, loss = 0.00769677\n",
      "Iteration 23755, loss = 0.00769651\n",
      "Iteration 23756, loss = 0.00769625\n",
      "Iteration 23757, loss = 0.00769600\n",
      "Iteration 23758, loss = 0.00769574\n",
      "Iteration 23759, loss = 0.00769548\n",
      "Iteration 23760, loss = 0.00769522\n",
      "Iteration 23761, loss = 0.00769496\n",
      "Iteration 23762, loss = 0.00769470\n",
      "Iteration 23763, loss = 0.00769444\n",
      "Iteration 23764, loss = 0.00769418\n",
      "Iteration 23765, loss = 0.00769392\n",
      "Iteration 23766, loss = 0.00769366\n",
      "Iteration 23767, loss = 0.00769340\n",
      "Iteration 23768, loss = 0.00769315\n",
      "Iteration 23769, loss = 0.00769289\n",
      "Iteration 23770, loss = 0.00769263\n",
      "Iteration 23771, loss = 0.00769237\n",
      "Iteration 23772, loss = 0.00769211\n",
      "Iteration 23773, loss = 0.00769185\n",
      "Iteration 23774, loss = 0.00769159\n",
      "Iteration 23775, loss = 0.00769133\n",
      "Iteration 23776, loss = 0.00769108\n",
      "Iteration 23777, loss = 0.00769082\n",
      "Iteration 23778, loss = 0.00769056\n",
      "Iteration 23779, loss = 0.00769030\n",
      "Iteration 23780, loss = 0.00769004\n",
      "Iteration 23781, loss = 0.00768978\n",
      "Iteration 23782, loss = 0.00768952\n",
      "Iteration 23783, loss = 0.00768927\n",
      "Iteration 23784, loss = 0.00768901\n",
      "Iteration 23785, loss = 0.00768875\n",
      "Iteration 23786, loss = 0.00768849\n",
      "Iteration 23787, loss = 0.00768823\n",
      "Iteration 23788, loss = 0.00768797\n",
      "Iteration 23789, loss = 0.00768772\n",
      "Iteration 23790, loss = 0.00768746\n",
      "Iteration 23791, loss = 0.00768720\n",
      "Iteration 23792, loss = 0.00768694\n",
      "Iteration 23793, loss = 0.00768668\n",
      "Iteration 23794, loss = 0.00768642\n",
      "Iteration 23795, loss = 0.00768617\n",
      "Iteration 23796, loss = 0.00768591\n",
      "Iteration 23797, loss = 0.00768565\n",
      "Iteration 23798, loss = 0.00768539\n",
      "Iteration 23799, loss = 0.00768513\n",
      "Iteration 23800, loss = 0.00768488\n",
      "Iteration 23801, loss = 0.00768462\n",
      "Iteration 23802, loss = 0.00768436\n",
      "Iteration 23803, loss = 0.00768410\n",
      "Iteration 23804, loss = 0.00768384\n",
      "Iteration 23805, loss = 0.00768359\n",
      "Iteration 23806, loss = 0.00768333\n",
      "Iteration 23807, loss = 0.00768307\n",
      "Iteration 23808, loss = 0.00768281\n",
      "Iteration 23809, loss = 0.00768256\n",
      "Iteration 23810, loss = 0.00768230\n",
      "Iteration 23811, loss = 0.00768204\n",
      "Iteration 23812, loss = 0.00768178\n",
      "Iteration 23813, loss = 0.00768153\n",
      "Iteration 23814, loss = 0.00768127\n",
      "Iteration 23815, loss = 0.00768101\n",
      "Iteration 23816, loss = 0.00768075\n",
      "Iteration 23817, loss = 0.00768050\n",
      "Iteration 23818, loss = 0.00768024\n",
      "Iteration 23819, loss = 0.00767998\n",
      "Iteration 23820, loss = 0.00767972\n",
      "Iteration 23821, loss = 0.00767947\n",
      "Iteration 23822, loss = 0.00767921\n",
      "Iteration 23823, loss = 0.00767895\n",
      "Iteration 23824, loss = 0.00767869\n",
      "Iteration 23825, loss = 0.00767844\n",
      "Iteration 23826, loss = 0.00767818\n",
      "Iteration 23827, loss = 0.00767792\n",
      "Iteration 23828, loss = 0.00767767\n",
      "Iteration 23829, loss = 0.00767741\n",
      "Iteration 23830, loss = 0.00767715\n",
      "Iteration 23831, loss = 0.00767689\n",
      "Iteration 23832, loss = 0.00767664\n",
      "Iteration 23833, loss = 0.00767638\n",
      "Iteration 23834, loss = 0.00767612\n",
      "Iteration 23835, loss = 0.00767587\n",
      "Iteration 23836, loss = 0.00767561\n",
      "Iteration 23837, loss = 0.00767535\n",
      "Iteration 23838, loss = 0.00767510\n",
      "Iteration 23839, loss = 0.00767484\n",
      "Iteration 23840, loss = 0.00767458\n",
      "Iteration 23841, loss = 0.00767433\n",
      "Iteration 23842, loss = 0.00767407\n",
      "Iteration 23843, loss = 0.00767381\n",
      "Iteration 23844, loss = 0.00767356\n",
      "Iteration 23845, loss = 0.00767330\n",
      "Iteration 23846, loss = 0.00767304\n",
      "Iteration 23847, loss = 0.00767279\n",
      "Iteration 23848, loss = 0.00767253\n",
      "Iteration 23849, loss = 0.00767227\n",
      "Iteration 23850, loss = 0.00767202\n",
      "Iteration 23851, loss = 0.00767176\n",
      "Iteration 23852, loss = 0.00767150\n",
      "Iteration 23853, loss = 0.00767125\n",
      "Iteration 23854, loss = 0.00767099\n",
      "Iteration 23855, loss = 0.00767073\n",
      "Iteration 23856, loss = 0.00767048\n",
      "Iteration 23857, loss = 0.00767022\n",
      "Iteration 23858, loss = 0.00766997\n",
      "Iteration 23859, loss = 0.00766971\n",
      "Iteration 23860, loss = 0.00766945\n",
      "Iteration 23861, loss = 0.00766920\n",
      "Iteration 23862, loss = 0.00766894\n",
      "Iteration 23863, loss = 0.00766868\n",
      "Iteration 23864, loss = 0.00766843\n",
      "Iteration 23865, loss = 0.00766817\n",
      "Iteration 23866, loss = 0.00766792\n",
      "Iteration 23867, loss = 0.00766766\n",
      "Iteration 23868, loss = 0.00766740\n",
      "Iteration 23869, loss = 0.00766715\n",
      "Iteration 23870, loss = 0.00766689\n",
      "Iteration 23871, loss = 0.00766664\n",
      "Iteration 23872, loss = 0.00766638\n",
      "Iteration 23873, loss = 0.00766613\n",
      "Iteration 23874, loss = 0.00766587\n",
      "Iteration 23875, loss = 0.00766561\n",
      "Iteration 23876, loss = 0.00766536\n",
      "Iteration 23877, loss = 0.00766510\n",
      "Iteration 23878, loss = 0.00766485\n",
      "Iteration 23879, loss = 0.00766459\n",
      "Iteration 23880, loss = 0.00766434\n",
      "Iteration 23881, loss = 0.00766408\n",
      "Iteration 23882, loss = 0.00766382\n",
      "Iteration 23883, loss = 0.00766357\n",
      "Iteration 23884, loss = 0.00766331\n",
      "Iteration 23885, loss = 0.00766306\n",
      "Iteration 23886, loss = 0.00766280\n",
      "Iteration 23887, loss = 0.00766255\n",
      "Iteration 23888, loss = 0.00766229\n",
      "Iteration 23889, loss = 0.00766204\n",
      "Iteration 23890, loss = 0.00766178\n",
      "Iteration 23891, loss = 0.00766153\n",
      "Iteration 23892, loss = 0.00766127\n",
      "Iteration 23893, loss = 0.00766102\n",
      "Iteration 23894, loss = 0.00766076\n",
      "Iteration 23895, loss = 0.00766051\n",
      "Iteration 23896, loss = 0.00766025\n",
      "Iteration 23897, loss = 0.00766000\n",
      "Iteration 23898, loss = 0.00765974\n",
      "Iteration 23899, loss = 0.00765949\n",
      "Iteration 23900, loss = 0.00765923\n",
      "Iteration 23901, loss = 0.00765898\n",
      "Iteration 23902, loss = 0.00765872\n",
      "Iteration 23903, loss = 0.00765847\n",
      "Iteration 23904, loss = 0.00765821\n",
      "Iteration 23905, loss = 0.00765796\n",
      "Iteration 23906, loss = 0.00765770\n",
      "Iteration 23907, loss = 0.00765745\n",
      "Iteration 23908, loss = 0.00765719\n",
      "Iteration 23909, loss = 0.00765694\n",
      "Iteration 23910, loss = 0.00765668\n",
      "Iteration 23911, loss = 0.00765643\n",
      "Iteration 23912, loss = 0.00765617\n",
      "Iteration 23913, loss = 0.00765592\n",
      "Iteration 23914, loss = 0.00765566\n",
      "Iteration 23915, loss = 0.00765541\n",
      "Iteration 23916, loss = 0.00765515\n",
      "Iteration 23917, loss = 0.00765490\n",
      "Iteration 23918, loss = 0.00765465\n",
      "Iteration 23919, loss = 0.00765439\n",
      "Iteration 23920, loss = 0.00765414\n",
      "Iteration 23921, loss = 0.00765388\n",
      "Iteration 23922, loss = 0.00765363\n",
      "Iteration 23923, loss = 0.00765337\n",
      "Iteration 23924, loss = 0.00765312\n",
      "Iteration 23925, loss = 0.00765286\n",
      "Iteration 23926, loss = 0.00765261\n",
      "Iteration 23927, loss = 0.00765236\n",
      "Iteration 23928, loss = 0.00765210\n",
      "Iteration 23929, loss = 0.00765185\n",
      "Iteration 23930, loss = 0.00765159\n",
      "Iteration 23931, loss = 0.00765134\n",
      "Iteration 23932, loss = 0.00765109\n",
      "Iteration 23933, loss = 0.00765083\n",
      "Iteration 23934, loss = 0.00765058\n",
      "Iteration 23935, loss = 0.00765032\n",
      "Iteration 23936, loss = 0.00765007\n",
      "Iteration 23937, loss = 0.00764982\n",
      "Iteration 23938, loss = 0.00764956\n",
      "Iteration 23939, loss = 0.00764931\n",
      "Iteration 23940, loss = 0.00764905\n",
      "Iteration 23941, loss = 0.00764880\n",
      "Iteration 23942, loss = 0.00764855\n",
      "Iteration 23943, loss = 0.00764829\n",
      "Iteration 23944, loss = 0.00764804\n",
      "Iteration 23945, loss = 0.00764779\n",
      "Iteration 23946, loss = 0.00764753\n",
      "Iteration 23947, loss = 0.00764728\n",
      "Iteration 23948, loss = 0.00764702\n",
      "Iteration 23949, loss = 0.00764677\n",
      "Iteration 23950, loss = 0.00764652\n",
      "Iteration 23951, loss = 0.00764626\n",
      "Iteration 23952, loss = 0.00764601\n",
      "Iteration 23953, loss = 0.00764576\n",
      "Iteration 23954, loss = 0.00764550\n",
      "Iteration 23955, loss = 0.00764525\n",
      "Iteration 23956, loss = 0.00764500\n",
      "Iteration 23957, loss = 0.00764474\n",
      "Iteration 23958, loss = 0.00764449\n",
      "Iteration 23959, loss = 0.00764424\n",
      "Iteration 23960, loss = 0.00764398\n",
      "Iteration 23961, loss = 0.00764373\n",
      "Iteration 23962, loss = 0.00764348\n",
      "Iteration 23963, loss = 0.00764322\n",
      "Iteration 23964, loss = 0.00764297\n",
      "Iteration 23965, loss = 0.00764272\n",
      "Iteration 23966, loss = 0.00764247\n",
      "Iteration 23967, loss = 0.00764221\n",
      "Iteration 23968, loss = 0.00764196\n",
      "Iteration 23969, loss = 0.00764171\n",
      "Iteration 23970, loss = 0.00764145\n",
      "Iteration 23971, loss = 0.00764120\n",
      "Iteration 23972, loss = 0.00764095\n",
      "Iteration 23973, loss = 0.00764069\n",
      "Iteration 23974, loss = 0.00764044\n",
      "Iteration 23975, loss = 0.00764019\n",
      "Iteration 23976, loss = 0.00763994\n",
      "Iteration 23977, loss = 0.00763968\n",
      "Iteration 23978, loss = 0.00763943\n",
      "Iteration 23979, loss = 0.00763918\n",
      "Iteration 23980, loss = 0.00763893\n",
      "Iteration 23981, loss = 0.00763867\n",
      "Iteration 23982, loss = 0.00763842\n",
      "Iteration 23983, loss = 0.00763817\n",
      "Iteration 23984, loss = 0.00763791\n",
      "Iteration 23985, loss = 0.00763766\n",
      "Iteration 23986, loss = 0.00763741\n",
      "Iteration 23987, loss = 0.00763716\n",
      "Iteration 23988, loss = 0.00763691\n",
      "Iteration 23989, loss = 0.00763665\n",
      "Iteration 23990, loss = 0.00763640\n",
      "Iteration 23991, loss = 0.00763615\n",
      "Iteration 23992, loss = 0.00763590\n",
      "Iteration 23993, loss = 0.00763564\n",
      "Iteration 23994, loss = 0.00763539\n",
      "Iteration 23995, loss = 0.00763514\n",
      "Iteration 23996, loss = 0.00763489\n",
      "Iteration 23997, loss = 0.00763463\n",
      "Iteration 23998, loss = 0.00763438\n",
      "Iteration 23999, loss = 0.00763413\n",
      "Iteration 24000, loss = 0.00763388\n",
      "Iteration 24001, loss = 0.00763363\n",
      "Iteration 24002, loss = 0.00763337\n",
      "Iteration 24003, loss = 0.00763312\n",
      "Iteration 24004, loss = 0.00763287\n",
      "Iteration 24005, loss = 0.00763262\n",
      "Iteration 24006, loss = 0.00763237\n",
      "Iteration 24007, loss = 0.00763211\n",
      "Iteration 24008, loss = 0.00763186\n",
      "Iteration 24009, loss = 0.00763161\n",
      "Iteration 24010, loss = 0.00763136\n",
      "Iteration 24011, loss = 0.00763111\n",
      "Iteration 24012, loss = 0.00763086\n",
      "Iteration 24013, loss = 0.00763060\n",
      "Iteration 24014, loss = 0.00763035\n",
      "Iteration 24015, loss = 0.00763010\n",
      "Iteration 24016, loss = 0.00762985\n",
      "Iteration 24017, loss = 0.00762960\n",
      "Iteration 24018, loss = 0.00762935\n",
      "Iteration 24019, loss = 0.00762909\n",
      "Iteration 24020, loss = 0.00762884\n",
      "Iteration 24021, loss = 0.00762859\n",
      "Iteration 24022, loss = 0.00762834\n",
      "Iteration 24023, loss = 0.00762809\n",
      "Iteration 24024, loss = 0.00762784\n",
      "Iteration 24025, loss = 0.00762759\n",
      "Iteration 24026, loss = 0.00762733\n",
      "Iteration 24027, loss = 0.00762708\n",
      "Iteration 24028, loss = 0.00762683\n",
      "Iteration 24029, loss = 0.00762658\n",
      "Iteration 24030, loss = 0.00762633\n",
      "Iteration 24031, loss = 0.00762608\n",
      "Iteration 24032, loss = 0.00762583\n",
      "Iteration 24033, loss = 0.00762558\n",
      "Iteration 24034, loss = 0.00762532\n",
      "Iteration 24035, loss = 0.00762507\n",
      "Iteration 24036, loss = 0.00762482\n",
      "Iteration 24037, loss = 0.00762457\n",
      "Iteration 24038, loss = 0.00762432\n",
      "Iteration 24039, loss = 0.00762407\n",
      "Iteration 24040, loss = 0.00762382\n",
      "Iteration 24041, loss = 0.00762357\n",
      "Iteration 24042, loss = 0.00762332\n",
      "Iteration 24043, loss = 0.00762307\n",
      "Iteration 24044, loss = 0.00762282\n",
      "Iteration 24045, loss = 0.00762256\n",
      "Iteration 24046, loss = 0.00762231\n",
      "Iteration 24047, loss = 0.00762206\n",
      "Iteration 24048, loss = 0.00762181\n",
      "Iteration 24049, loss = 0.00762156\n",
      "Iteration 24050, loss = 0.00762131\n",
      "Iteration 24051, loss = 0.00762106\n",
      "Iteration 24052, loss = 0.00762081\n",
      "Iteration 24053, loss = 0.00762056\n",
      "Iteration 24054, loss = 0.00762031\n",
      "Iteration 24055, loss = 0.00762006\n",
      "Iteration 24056, loss = 0.00761981\n",
      "Iteration 24057, loss = 0.00761956\n",
      "Iteration 24058, loss = 0.00761931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24059, loss = 0.00761906\n",
      "Iteration 24060, loss = 0.00761881\n",
      "Iteration 24061, loss = 0.00761856\n",
      "Iteration 24062, loss = 0.00761831\n",
      "Iteration 24063, loss = 0.00761806\n",
      "Iteration 24064, loss = 0.00761781\n",
      "Iteration 24065, loss = 0.00761756\n",
      "Iteration 24066, loss = 0.00761730\n",
      "Iteration 24067, loss = 0.00761705\n",
      "Iteration 24068, loss = 0.00761680\n",
      "Iteration 24069, loss = 0.00761655\n",
      "Iteration 24070, loss = 0.00761630\n",
      "Iteration 24071, loss = 0.00761605\n",
      "Iteration 24072, loss = 0.00761580\n",
      "Iteration 24073, loss = 0.00761555\n",
      "Iteration 24074, loss = 0.00761530\n",
      "Iteration 24075, loss = 0.00761505\n",
      "Iteration 24076, loss = 0.00761480\n",
      "Iteration 24077, loss = 0.00761455\n",
      "Iteration 24078, loss = 0.00761431\n",
      "Iteration 24079, loss = 0.00761406\n",
      "Iteration 24080, loss = 0.00761381\n",
      "Iteration 24081, loss = 0.00761356\n",
      "Iteration 24082, loss = 0.00761331\n",
      "Iteration 24083, loss = 0.00761306\n",
      "Iteration 24084, loss = 0.00761281\n",
      "Iteration 24085, loss = 0.00761256\n",
      "Iteration 24086, loss = 0.00761231\n",
      "Iteration 24087, loss = 0.00761206\n",
      "Iteration 24088, loss = 0.00761181\n",
      "Iteration 24089, loss = 0.00761156\n",
      "Iteration 24090, loss = 0.00761131\n",
      "Iteration 24091, loss = 0.00761106\n",
      "Iteration 24092, loss = 0.00761081\n",
      "Iteration 24093, loss = 0.00761056\n",
      "Iteration 24094, loss = 0.00761031\n",
      "Iteration 24095, loss = 0.00761006\n",
      "Iteration 24096, loss = 0.00760981\n",
      "Iteration 24097, loss = 0.00760956\n",
      "Iteration 24098, loss = 0.00760931\n",
      "Iteration 24099, loss = 0.00760907\n",
      "Iteration 24100, loss = 0.00760882\n",
      "Iteration 24101, loss = 0.00760857\n",
      "Iteration 24102, loss = 0.00760832\n",
      "Iteration 24103, loss = 0.00760807\n",
      "Iteration 24104, loss = 0.00760782\n",
      "Iteration 24105, loss = 0.00760757\n",
      "Iteration 24106, loss = 0.00760732\n",
      "Iteration 24107, loss = 0.00760707\n",
      "Iteration 24108, loss = 0.00760682\n",
      "Iteration 24109, loss = 0.00760657\n",
      "Iteration 24110, loss = 0.00760633\n",
      "Iteration 24111, loss = 0.00760608\n",
      "Iteration 24112, loss = 0.00760583\n",
      "Iteration 24113, loss = 0.00760558\n",
      "Iteration 24114, loss = 0.00760533\n",
      "Iteration 24115, loss = 0.00760508\n",
      "Iteration 24116, loss = 0.00760483\n",
      "Iteration 24117, loss = 0.00760458\n",
      "Iteration 24118, loss = 0.00760434\n",
      "Iteration 24119, loss = 0.00760409\n",
      "Iteration 24120, loss = 0.00760384\n",
      "Iteration 24121, loss = 0.00760359\n",
      "Iteration 24122, loss = 0.00760334\n",
      "Iteration 24123, loss = 0.00760309\n",
      "Iteration 24124, loss = 0.00760284\n",
      "Iteration 24125, loss = 0.00760260\n",
      "Iteration 24126, loss = 0.00760235\n",
      "Iteration 24127, loss = 0.00760210\n",
      "Iteration 24128, loss = 0.00760185\n",
      "Iteration 24129, loss = 0.00760160\n",
      "Iteration 24130, loss = 0.00760135\n",
      "Iteration 24131, loss = 0.00760110\n",
      "Iteration 24132, loss = 0.00760086\n",
      "Iteration 24133, loss = 0.00760061\n",
      "Iteration 24134, loss = 0.00760036\n",
      "Iteration 24135, loss = 0.00760011\n",
      "Iteration 24136, loss = 0.00759986\n",
      "Iteration 24137, loss = 0.00759962\n",
      "Iteration 24138, loss = 0.00759937\n",
      "Iteration 24139, loss = 0.00759912\n",
      "Iteration 24140, loss = 0.00759887\n",
      "Iteration 24141, loss = 0.00759862\n",
      "Iteration 24142, loss = 0.00759838\n",
      "Iteration 24143, loss = 0.00759813\n",
      "Iteration 24144, loss = 0.00759788\n",
      "Iteration 24145, loss = 0.00759763\n",
      "Iteration 24146, loss = 0.00759738\n",
      "Iteration 24147, loss = 0.00759714\n",
      "Iteration 24148, loss = 0.00759689\n",
      "Iteration 24149, loss = 0.00759664\n",
      "Iteration 24150, loss = 0.00759639\n",
      "Iteration 24151, loss = 0.00759614\n",
      "Iteration 24152, loss = 0.00759590\n",
      "Iteration 24153, loss = 0.00759565\n",
      "Iteration 24154, loss = 0.00759540\n",
      "Iteration 24155, loss = 0.00759515\n",
      "Iteration 24156, loss = 0.00759491\n",
      "Iteration 24157, loss = 0.00759466\n",
      "Iteration 24158, loss = 0.00759441\n",
      "Iteration 24159, loss = 0.00759416\n",
      "Iteration 24160, loss = 0.00759392\n",
      "Iteration 24161, loss = 0.00759367\n",
      "Iteration 24162, loss = 0.00759342\n",
      "Iteration 24163, loss = 0.00759317\n",
      "Iteration 24164, loss = 0.00759293\n",
      "Iteration 24165, loss = 0.00759268\n",
      "Iteration 24166, loss = 0.00759243\n",
      "Iteration 24167, loss = 0.00759218\n",
      "Iteration 24168, loss = 0.00759194\n",
      "Iteration 24169, loss = 0.00759169\n",
      "Iteration 24170, loss = 0.00759144\n",
      "Iteration 24171, loss = 0.00759120\n",
      "Iteration 24172, loss = 0.00759095\n",
      "Iteration 24173, loss = 0.00759070\n",
      "Iteration 24174, loss = 0.00759045\n",
      "Iteration 24175, loss = 0.00759021\n",
      "Iteration 24176, loss = 0.00758996\n",
      "Iteration 24177, loss = 0.00758971\n",
      "Iteration 24178, loss = 0.00758947\n",
      "Iteration 24179, loss = 0.00758922\n",
      "Iteration 24180, loss = 0.00758897\n",
      "Iteration 24181, loss = 0.00758872\n",
      "Iteration 24182, loss = 0.00758848\n",
      "Iteration 24183, loss = 0.00758823\n",
      "Iteration 24184, loss = 0.00758798\n",
      "Iteration 24185, loss = 0.00758774\n",
      "Iteration 24186, loss = 0.00758749\n",
      "Iteration 24187, loss = 0.00758724\n",
      "Iteration 24188, loss = 0.00758700\n",
      "Iteration 24189, loss = 0.00758675\n",
      "Iteration 24190, loss = 0.00758650\n",
      "Iteration 24191, loss = 0.00758626\n",
      "Iteration 24192, loss = 0.00758601\n",
      "Iteration 24193, loss = 0.00758576\n",
      "Iteration 24194, loss = 0.00758552\n",
      "Iteration 24195, loss = 0.00758527\n",
      "Iteration 24196, loss = 0.00758502\n",
      "Iteration 24197, loss = 0.00758478\n",
      "Iteration 24198, loss = 0.00758453\n",
      "Iteration 24199, loss = 0.00758429\n",
      "Iteration 24200, loss = 0.00758404\n",
      "Iteration 24201, loss = 0.00758379\n",
      "Iteration 24202, loss = 0.00758355\n",
      "Iteration 24203, loss = 0.00758330\n",
      "Iteration 24204, loss = 0.00758305\n",
      "Iteration 24205, loss = 0.00758281\n",
      "Iteration 24206, loss = 0.00758256\n",
      "Iteration 24207, loss = 0.00758231\n",
      "Iteration 24208, loss = 0.00758207\n",
      "Iteration 24209, loss = 0.00758182\n",
      "Iteration 24210, loss = 0.00758158\n",
      "Iteration 24211, loss = 0.00758133\n",
      "Iteration 24212, loss = 0.00758108\n",
      "Iteration 24213, loss = 0.00758084\n",
      "Iteration 24214, loss = 0.00758059\n",
      "Iteration 24215, loss = 0.00758035\n",
      "Iteration 24216, loss = 0.00758010\n",
      "Iteration 24217, loss = 0.00757985\n",
      "Iteration 24218, loss = 0.00757961\n",
      "Iteration 24219, loss = 0.00757936\n",
      "Iteration 24220, loss = 0.00757912\n",
      "Iteration 24221, loss = 0.00757887\n",
      "Iteration 24222, loss = 0.00757863\n",
      "Iteration 24223, loss = 0.00757838\n",
      "Iteration 24224, loss = 0.00757813\n",
      "Iteration 24225, loss = 0.00757789\n",
      "Iteration 24226, loss = 0.00757764\n",
      "Iteration 24227, loss = 0.00757740\n",
      "Iteration 24228, loss = 0.00757715\n",
      "Iteration 24229, loss = 0.00757691\n",
      "Iteration 24230, loss = 0.00757666\n",
      "Iteration 24231, loss = 0.00757642\n",
      "Iteration 24232, loss = 0.00757617\n",
      "Iteration 24233, loss = 0.00757592\n",
      "Iteration 24234, loss = 0.00757568\n",
      "Iteration 24235, loss = 0.00757543\n",
      "Iteration 24236, loss = 0.00757519\n",
      "Iteration 24237, loss = 0.00757494\n",
      "Iteration 24238, loss = 0.00757470\n",
      "Iteration 24239, loss = 0.00757445\n",
      "Iteration 24240, loss = 0.00757421\n",
      "Iteration 24241, loss = 0.00757396\n",
      "Iteration 24242, loss = 0.00757372\n",
      "Iteration 24243, loss = 0.00757347\n",
      "Iteration 24244, loss = 0.00757323\n",
      "Iteration 24245, loss = 0.00757298\n",
      "Iteration 24246, loss = 0.00757274\n",
      "Iteration 24247, loss = 0.00757249\n",
      "Iteration 24248, loss = 0.00757225\n",
      "Iteration 24249, loss = 0.00757200\n",
      "Iteration 24250, loss = 0.00757176\n",
      "Iteration 24251, loss = 0.00757151\n",
      "Iteration 24252, loss = 0.00757127\n",
      "Iteration 24253, loss = 0.00757102\n",
      "Iteration 24254, loss = 0.00757078\n",
      "Iteration 24255, loss = 0.00757053\n",
      "Iteration 24256, loss = 0.00757029\n",
      "Iteration 24257, loss = 0.00757004\n",
      "Iteration 24258, loss = 0.00756980\n",
      "Iteration 24259, loss = 0.00756955\n",
      "Iteration 24260, loss = 0.00756931\n",
      "Iteration 24261, loss = 0.00756906\n",
      "Iteration 24262, loss = 0.00756882\n",
      "Iteration 24263, loss = 0.00756857\n",
      "Iteration 24264, loss = 0.00756833\n",
      "Iteration 24265, loss = 0.00756808\n",
      "Iteration 24266, loss = 0.00756784\n",
      "Iteration 24267, loss = 0.00756760\n",
      "Iteration 24268, loss = 0.00756735\n",
      "Iteration 24269, loss = 0.00756711\n",
      "Iteration 24270, loss = 0.00756686\n",
      "Iteration 24271, loss = 0.00756662\n",
      "Iteration 24272, loss = 0.00756637\n",
      "Iteration 24273, loss = 0.00756613\n",
      "Iteration 24274, loss = 0.00756588\n",
      "Iteration 24275, loss = 0.00756564\n",
      "Iteration 24276, loss = 0.00756540\n",
      "Iteration 24277, loss = 0.00756515\n",
      "Iteration 24278, loss = 0.00756491\n",
      "Iteration 24279, loss = 0.00756466\n",
      "Iteration 24280, loss = 0.00756442\n",
      "Iteration 24281, loss = 0.00756418\n",
      "Iteration 24282, loss = 0.00756393\n",
      "Iteration 24283, loss = 0.00756369\n",
      "Iteration 24284, loss = 0.00756344\n",
      "Iteration 24285, loss = 0.00756320\n",
      "Iteration 24286, loss = 0.00756296\n",
      "Iteration 24287, loss = 0.00756271\n",
      "Iteration 24288, loss = 0.00756247\n",
      "Iteration 24289, loss = 0.00756222\n",
      "Iteration 24290, loss = 0.00756198\n",
      "Iteration 24291, loss = 0.00756174\n",
      "Iteration 24292, loss = 0.00756149\n",
      "Iteration 24293, loss = 0.00756125\n",
      "Iteration 24294, loss = 0.00756100\n",
      "Iteration 24295, loss = 0.00756076\n",
      "Iteration 24296, loss = 0.00756052\n",
      "Iteration 24297, loss = 0.00756027\n",
      "Iteration 24298, loss = 0.00756003\n",
      "Iteration 24299, loss = 0.00755979\n",
      "Iteration 24300, loss = 0.00755954\n",
      "Iteration 24301, loss = 0.00755930\n",
      "Iteration 24302, loss = 0.00755906\n",
      "Iteration 24303, loss = 0.00755881\n",
      "Iteration 24304, loss = 0.00755857\n",
      "Iteration 24305, loss = 0.00755832\n",
      "Iteration 24306, loss = 0.00755808\n",
      "Iteration 24307, loss = 0.00755784\n",
      "Iteration 24308, loss = 0.00755759\n",
      "Iteration 24309, loss = 0.00755735\n",
      "Iteration 24310, loss = 0.00755711\n",
      "Iteration 24311, loss = 0.00755686\n",
      "Iteration 24312, loss = 0.00755662\n",
      "Iteration 24313, loss = 0.00755638\n",
      "Iteration 24314, loss = 0.00755613\n",
      "Iteration 24315, loss = 0.00755589\n",
      "Iteration 24316, loss = 0.00755565\n",
      "Iteration 24317, loss = 0.00755541\n",
      "Iteration 24318, loss = 0.00755516\n",
      "Iteration 24319, loss = 0.00755492\n",
      "Iteration 24320, loss = 0.00755468\n",
      "Iteration 24321, loss = 0.00755443\n",
      "Iteration 24322, loss = 0.00755419\n",
      "Iteration 24323, loss = 0.00755395\n",
      "Iteration 24324, loss = 0.00755370\n",
      "Iteration 24325, loss = 0.00755346\n",
      "Iteration 24326, loss = 0.00755322\n",
      "Iteration 24327, loss = 0.00755298\n",
      "Iteration 24328, loss = 0.00755273\n",
      "Iteration 24329, loss = 0.00755249\n",
      "Iteration 24330, loss = 0.00755225\n",
      "Iteration 24331, loss = 0.00755200\n",
      "Iteration 24332, loss = 0.00755176\n",
      "Iteration 24333, loss = 0.00755152\n",
      "Iteration 24334, loss = 0.00755128\n",
      "Iteration 24335, loss = 0.00755103\n",
      "Iteration 24336, loss = 0.00755079\n",
      "Iteration 24337, loss = 0.00755055\n",
      "Iteration 24338, loss = 0.00755031\n",
      "Iteration 24339, loss = 0.00755006\n",
      "Iteration 24340, loss = 0.00754982\n",
      "Iteration 24341, loss = 0.00754958\n",
      "Iteration 24342, loss = 0.00754934\n",
      "Iteration 24343, loss = 0.00754909\n",
      "Iteration 24344, loss = 0.00754885\n",
      "Iteration 24345, loss = 0.00754861\n",
      "Iteration 24346, loss = 0.00754837\n",
      "Iteration 24347, loss = 0.00754812\n",
      "Iteration 24348, loss = 0.00754788\n",
      "Iteration 24349, loss = 0.00754764\n",
      "Iteration 24350, loss = 0.00754740\n",
      "Iteration 24351, loss = 0.00754716\n",
      "Iteration 24352, loss = 0.00754691\n",
      "Iteration 24353, loss = 0.00754667\n",
      "Iteration 24354, loss = 0.00754643\n",
      "Iteration 24355, loss = 0.00754619\n",
      "Iteration 24356, loss = 0.00754595\n",
      "Iteration 24357, loss = 0.00754570\n",
      "Iteration 24358, loss = 0.00754546\n",
      "Iteration 24359, loss = 0.00754522\n",
      "Iteration 24360, loss = 0.00754498\n",
      "Iteration 24361, loss = 0.00754474\n",
      "Iteration 24362, loss = 0.00754449\n",
      "Iteration 24363, loss = 0.00754425\n",
      "Iteration 24364, loss = 0.00754401\n",
      "Iteration 24365, loss = 0.00754377\n",
      "Iteration 24366, loss = 0.00754353\n",
      "Iteration 24367, loss = 0.00754328\n",
      "Iteration 24368, loss = 0.00754304\n",
      "Iteration 24369, loss = 0.00754280\n",
      "Iteration 24370, loss = 0.00754256\n",
      "Iteration 24371, loss = 0.00754232\n",
      "Iteration 24372, loss = 0.00754208\n",
      "Iteration 24373, loss = 0.00754183\n",
      "Iteration 24374, loss = 0.00754159\n",
      "Iteration 24375, loss = 0.00754135\n",
      "Iteration 24376, loss = 0.00754111\n",
      "Iteration 24377, loss = 0.00754087\n",
      "Iteration 24378, loss = 0.00754063\n",
      "Iteration 24379, loss = 0.00754039\n",
      "Iteration 24380, loss = 0.00754014\n",
      "Iteration 24381, loss = 0.00753990\n",
      "Iteration 24382, loss = 0.00753966\n",
      "Iteration 24383, loss = 0.00753942\n",
      "Iteration 24384, loss = 0.00753918\n",
      "Iteration 24385, loss = 0.00753894\n",
      "Iteration 24386, loss = 0.00753870\n",
      "Iteration 24387, loss = 0.00753846\n",
      "Iteration 24388, loss = 0.00753821\n",
      "Iteration 24389, loss = 0.00753797\n",
      "Iteration 24390, loss = 0.00753773\n",
      "Iteration 24391, loss = 0.00753749\n",
      "Iteration 24392, loss = 0.00753725\n",
      "Iteration 24393, loss = 0.00753701\n",
      "Iteration 24394, loss = 0.00753677\n",
      "Iteration 24395, loss = 0.00753653\n",
      "Iteration 24396, loss = 0.00753629\n",
      "Iteration 24397, loss = 0.00753604\n",
      "Iteration 24398, loss = 0.00753580\n",
      "Iteration 24399, loss = 0.00753556\n",
      "Iteration 24400, loss = 0.00753532\n",
      "Iteration 24401, loss = 0.00753508\n",
      "Iteration 24402, loss = 0.00753484\n",
      "Iteration 24403, loss = 0.00753460\n",
      "Iteration 24404, loss = 0.00753436\n",
      "Iteration 24405, loss = 0.00753412\n",
      "Iteration 24406, loss = 0.00753388\n",
      "Iteration 24407, loss = 0.00753364\n",
      "Iteration 24408, loss = 0.00753340\n",
      "Iteration 24409, loss = 0.00753316\n",
      "Iteration 24410, loss = 0.00753292\n",
      "Iteration 24411, loss = 0.00753267\n",
      "Iteration 24412, loss = 0.00753243\n",
      "Iteration 24413, loss = 0.00753219\n",
      "Iteration 24414, loss = 0.00753195\n",
      "Iteration 24415, loss = 0.00753171\n",
      "Iteration 24416, loss = 0.00753147\n",
      "Iteration 24417, loss = 0.00753123\n",
      "Iteration 24418, loss = 0.00753099\n",
      "Iteration 24419, loss = 0.00753075\n",
      "Iteration 24420, loss = 0.00753051\n",
      "Iteration 24421, loss = 0.00753027\n",
      "Iteration 24422, loss = 0.00753003\n",
      "Iteration 24423, loss = 0.00752979\n",
      "Iteration 24424, loss = 0.00752955\n",
      "Iteration 24425, loss = 0.00752931\n",
      "Iteration 24426, loss = 0.00752907\n",
      "Iteration 24427, loss = 0.00752883\n",
      "Iteration 24428, loss = 0.00752859\n",
      "Iteration 24429, loss = 0.00752835\n",
      "Iteration 24430, loss = 0.00752811\n",
      "Iteration 24431, loss = 0.00752787\n",
      "Iteration 24432, loss = 0.00752763\n",
      "Iteration 24433, loss = 0.00752739\n",
      "Iteration 24434, loss = 0.00752715\n",
      "Iteration 24435, loss = 0.00752691\n",
      "Iteration 24436, loss = 0.00752667\n",
      "Iteration 24437, loss = 0.00752643\n",
      "Iteration 24438, loss = 0.00752619\n",
      "Iteration 24439, loss = 0.00752595\n",
      "Iteration 24440, loss = 0.00752571\n",
      "Iteration 24441, loss = 0.00752547\n",
      "Iteration 24442, loss = 0.00752523\n",
      "Iteration 24443, loss = 0.00752499\n",
      "Iteration 24444, loss = 0.00752475\n",
      "Iteration 24445, loss = 0.00752451\n",
      "Iteration 24446, loss = 0.00752427\n",
      "Iteration 24447, loss = 0.00752403\n",
      "Iteration 24448, loss = 0.00752379\n",
      "Iteration 24449, loss = 0.00752355\n",
      "Iteration 24450, loss = 0.00752331\n",
      "Iteration 24451, loss = 0.00752307\n",
      "Iteration 24452, loss = 0.00752284\n",
      "Iteration 24453, loss = 0.00752260\n",
      "Iteration 24454, loss = 0.00752236\n",
      "Iteration 24455, loss = 0.00752212\n",
      "Iteration 24456, loss = 0.00752188\n",
      "Iteration 24457, loss = 0.00752164\n",
      "Iteration 24458, loss = 0.00752140\n",
      "Iteration 24459, loss = 0.00752116\n",
      "Iteration 24460, loss = 0.00752092\n",
      "Iteration 24461, loss = 0.00752068\n",
      "Iteration 24462, loss = 0.00752044\n",
      "Iteration 24463, loss = 0.00752020\n",
      "Iteration 24464, loss = 0.00751996\n",
      "Iteration 24465, loss = 0.00751973\n",
      "Iteration 24466, loss = 0.00751949\n",
      "Iteration 24467, loss = 0.00751925\n",
      "Iteration 24468, loss = 0.00751901\n",
      "Iteration 24469, loss = 0.00751877\n",
      "Iteration 24470, loss = 0.00751853\n",
      "Iteration 24471, loss = 0.00751829\n",
      "Iteration 24472, loss = 0.00751805\n",
      "Iteration 24473, loss = 0.00751781\n",
      "Iteration 24474, loss = 0.00751757\n",
      "Iteration 24475, loss = 0.00751734\n",
      "Iteration 24476, loss = 0.00751710\n",
      "Iteration 24477, loss = 0.00751686\n",
      "Iteration 24478, loss = 0.00751662\n",
      "Iteration 24479, loss = 0.00751638\n",
      "Iteration 24480, loss = 0.00751614\n",
      "Iteration 24481, loss = 0.00751590\n",
      "Iteration 24482, loss = 0.00751566\n",
      "Iteration 24483, loss = 0.00751543\n",
      "Iteration 24484, loss = 0.00751519\n",
      "Iteration 24485, loss = 0.00751495\n",
      "Iteration 24486, loss = 0.00751471\n",
      "Iteration 24487, loss = 0.00751447\n",
      "Iteration 24488, loss = 0.00751423\n",
      "Iteration 24489, loss = 0.00751400\n",
      "Iteration 24490, loss = 0.00751376\n",
      "Iteration 24491, loss = 0.00751352\n",
      "Iteration 24492, loss = 0.00751328\n",
      "Iteration 24493, loss = 0.00751304\n",
      "Iteration 24494, loss = 0.00751280\n",
      "Iteration 24495, loss = 0.00751256\n",
      "Iteration 24496, loss = 0.00751233\n",
      "Iteration 24497, loss = 0.00751209\n",
      "Iteration 24498, loss = 0.00751185\n",
      "Iteration 24499, loss = 0.00751161\n",
      "Iteration 24500, loss = 0.00751137\n",
      "Iteration 24501, loss = 0.00751114\n",
      "Iteration 24502, loss = 0.00751090\n",
      "Iteration 24503, loss = 0.00751066\n",
      "Iteration 24504, loss = 0.00751042\n",
      "Iteration 24505, loss = 0.00751018\n",
      "Iteration 24506, loss = 0.00750995\n",
      "Iteration 24507, loss = 0.00750971\n",
      "Iteration 24508, loss = 0.00750947\n",
      "Iteration 24509, loss = 0.00750923\n",
      "Iteration 24510, loss = 0.00750899\n",
      "Iteration 24511, loss = 0.00750876\n",
      "Iteration 24512, loss = 0.00750852\n",
      "Iteration 24513, loss = 0.00750828\n",
      "Iteration 24514, loss = 0.00750804\n",
      "Iteration 24515, loss = 0.00750780\n",
      "Iteration 24516, loss = 0.00750757\n",
      "Iteration 24517, loss = 0.00750733\n",
      "Iteration 24518, loss = 0.00750709\n",
      "Iteration 24519, loss = 0.00750685\n",
      "Iteration 24520, loss = 0.00750662\n",
      "Iteration 24521, loss = 0.00750638\n",
      "Iteration 24522, loss = 0.00750614\n",
      "Iteration 24523, loss = 0.00750590\n",
      "Iteration 24524, loss = 0.00750567\n",
      "Iteration 24525, loss = 0.00750543\n",
      "Iteration 24526, loss = 0.00750519\n",
      "Iteration 24527, loss = 0.00750495\n",
      "Iteration 24528, loss = 0.00750472\n",
      "Iteration 24529, loss = 0.00750448\n",
      "Iteration 24530, loss = 0.00750424\n",
      "Iteration 24531, loss = 0.00750400\n",
      "Iteration 24532, loss = 0.00750377\n",
      "Iteration 24533, loss = 0.00750353\n",
      "Iteration 24534, loss = 0.00750329\n",
      "Iteration 24535, loss = 0.00750306\n",
      "Iteration 24536, loss = 0.00750282\n",
      "Iteration 24537, loss = 0.00750258\n",
      "Iteration 24538, loss = 0.00750234\n",
      "Iteration 24539, loss = 0.00750211\n",
      "Iteration 24540, loss = 0.00750187\n",
      "Iteration 24541, loss = 0.00750163\n",
      "Iteration 24542, loss = 0.00750140\n",
      "Iteration 24543, loss = 0.00750116\n",
      "Iteration 24544, loss = 0.00750092\n",
      "Iteration 24545, loss = 0.00750068\n",
      "Iteration 24546, loss = 0.00750045\n",
      "Iteration 24547, loss = 0.00750021\n",
      "Iteration 24548, loss = 0.00749997\n",
      "Iteration 24549, loss = 0.00749974\n",
      "Iteration 24550, loss = 0.00749950\n",
      "Iteration 24551, loss = 0.00749926\n",
      "Iteration 24552, loss = 0.00749903\n",
      "Iteration 24553, loss = 0.00749879\n",
      "Iteration 24554, loss = 0.00749855\n",
      "Iteration 24555, loss = 0.00749832\n",
      "Iteration 24556, loss = 0.00749808\n",
      "Iteration 24557, loss = 0.00749784\n",
      "Iteration 24558, loss = 0.00749761\n",
      "Iteration 24559, loss = 0.00749737\n",
      "Iteration 24560, loss = 0.00749713\n",
      "Iteration 24561, loss = 0.00749690\n",
      "Iteration 24562, loss = 0.00749666\n",
      "Iteration 24563, loss = 0.00749642\n",
      "Iteration 24564, loss = 0.00749619\n",
      "Iteration 24565, loss = 0.00749595\n",
      "Iteration 24566, loss = 0.00749572\n",
      "Iteration 24567, loss = 0.00749548\n",
      "Iteration 24568, loss = 0.00749524\n",
      "Iteration 24569, loss = 0.00749501\n",
      "Iteration 24570, loss = 0.00749477\n",
      "Iteration 24571, loss = 0.00749453\n",
      "Iteration 24572, loss = 0.00749430\n",
      "Iteration 24573, loss = 0.00749406\n",
      "Iteration 24574, loss = 0.00749382\n",
      "Iteration 24575, loss = 0.00749359\n",
      "Iteration 24576, loss = 0.00749335\n",
      "Iteration 24577, loss = 0.00749312\n",
      "Iteration 24578, loss = 0.00749288\n",
      "Iteration 24579, loss = 0.00749264\n",
      "Iteration 24580, loss = 0.00749241\n",
      "Iteration 24581, loss = 0.00749217\n",
      "Iteration 24582, loss = 0.00749194\n",
      "Iteration 24583, loss = 0.00749170\n",
      "Iteration 24584, loss = 0.00749146\n",
      "Iteration 24585, loss = 0.00749123\n",
      "Iteration 24586, loss = 0.00749099\n",
      "Iteration 24587, loss = 0.00749076\n",
      "Iteration 24588, loss = 0.00749052\n",
      "Iteration 24589, loss = 0.00749029\n",
      "Iteration 24590, loss = 0.00749005\n",
      "Iteration 24591, loss = 0.00748981\n",
      "Iteration 24592, loss = 0.00748958\n",
      "Iteration 24593, loss = 0.00748934\n",
      "Iteration 24594, loss = 0.00748911\n",
      "Iteration 24595, loss = 0.00748887\n",
      "Iteration 24596, loss = 0.00748864\n",
      "Iteration 24597, loss = 0.00748840\n",
      "Iteration 24598, loss = 0.00748816\n",
      "Iteration 24599, loss = 0.00748793\n",
      "Iteration 24600, loss = 0.00748769\n",
      "Iteration 24601, loss = 0.00748746\n",
      "Iteration 24602, loss = 0.00748722\n",
      "Iteration 24603, loss = 0.00748699\n",
      "Iteration 24604, loss = 0.00748675\n",
      "Iteration 24605, loss = 0.00748652\n",
      "Iteration 24606, loss = 0.00748628\n",
      "Iteration 24607, loss = 0.00748605\n",
      "Iteration 24608, loss = 0.00748581\n",
      "Iteration 24609, loss = 0.00748558\n",
      "Iteration 24610, loss = 0.00748534\n",
      "Iteration 24611, loss = 0.00748510\n",
      "Iteration 24612, loss = 0.00748487\n",
      "Iteration 24613, loss = 0.00748463\n",
      "Iteration 24614, loss = 0.00748440\n",
      "Iteration 24615, loss = 0.00748416\n",
      "Iteration 24616, loss = 0.00748393\n",
      "Iteration 24617, loss = 0.00748369\n",
      "Iteration 24618, loss = 0.00748346\n",
      "Iteration 24619, loss = 0.00748322\n",
      "Iteration 24620, loss = 0.00748299\n",
      "Iteration 24621, loss = 0.00748275\n",
      "Iteration 24622, loss = 0.00748252\n",
      "Iteration 24623, loss = 0.00748228\n",
      "Iteration 24624, loss = 0.00748205\n",
      "Iteration 24625, loss = 0.00748181\n",
      "Iteration 24626, loss = 0.00748158\n",
      "Iteration 24627, loss = 0.00748135\n",
      "Iteration 24628, loss = 0.00748111\n",
      "Iteration 24629, loss = 0.00748088\n",
      "Iteration 24630, loss = 0.00748064\n",
      "Iteration 24631, loss = 0.00748041\n",
      "Iteration 24632, loss = 0.00748017\n",
      "Iteration 24633, loss = 0.00747994\n",
      "Iteration 24634, loss = 0.00747970\n",
      "Iteration 24635, loss = 0.00747947\n",
      "Iteration 24636, loss = 0.00747923\n",
      "Iteration 24637, loss = 0.00747900\n",
      "Iteration 24638, loss = 0.00747876\n",
      "Iteration 24639, loss = 0.00747853\n",
      "Iteration 24640, loss = 0.00747830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24641, loss = 0.00747806\n",
      "Iteration 24642, loss = 0.00747783\n",
      "Iteration 24643, loss = 0.00747759\n",
      "Iteration 24644, loss = 0.00747736\n",
      "Iteration 24645, loss = 0.00747712\n",
      "Iteration 24646, loss = 0.00747689\n",
      "Iteration 24647, loss = 0.00747666\n",
      "Iteration 24648, loss = 0.00747642\n",
      "Iteration 24649, loss = 0.00747619\n",
      "Iteration 24650, loss = 0.00747595\n",
      "Iteration 24651, loss = 0.00747572\n",
      "Iteration 24652, loss = 0.00747548\n",
      "Iteration 24653, loss = 0.00747525\n",
      "Iteration 24654, loss = 0.00747502\n",
      "Iteration 24655, loss = 0.00747478\n",
      "Iteration 24656, loss = 0.00747455\n",
      "Iteration 24657, loss = 0.00747431\n",
      "Iteration 24658, loss = 0.00747408\n",
      "Iteration 24659, loss = 0.00747385\n",
      "Iteration 24660, loss = 0.00747361\n",
      "Iteration 24661, loss = 0.00747338\n",
      "Iteration 24662, loss = 0.00747314\n",
      "Iteration 24663, loss = 0.00747291\n",
      "Iteration 24664, loss = 0.00747268\n",
      "Iteration 24665, loss = 0.00747244\n",
      "Iteration 24666, loss = 0.00747221\n",
      "Iteration 24667, loss = 0.00747198\n",
      "Iteration 24668, loss = 0.00747174\n",
      "Iteration 24669, loss = 0.00747151\n",
      "Iteration 24670, loss = 0.00747127\n",
      "Iteration 24671, loss = 0.00747104\n",
      "Iteration 24672, loss = 0.00747081\n",
      "Iteration 24673, loss = 0.00747057\n",
      "Iteration 24674, loss = 0.00747034\n",
      "Iteration 24675, loss = 0.00747011\n",
      "Iteration 24676, loss = 0.00746987\n",
      "Iteration 24677, loss = 0.00746964\n",
      "Iteration 24678, loss = 0.00746941\n",
      "Iteration 24679, loss = 0.00746917\n",
      "Iteration 24680, loss = 0.00746894\n",
      "Iteration 24681, loss = 0.00746871\n",
      "Iteration 24682, loss = 0.00746847\n",
      "Iteration 24683, loss = 0.00746824\n",
      "Iteration 24684, loss = 0.00746801\n",
      "Iteration 24685, loss = 0.00746777\n",
      "Iteration 24686, loss = 0.00746754\n",
      "Iteration 24687, loss = 0.00746731\n",
      "Iteration 24688, loss = 0.00746707\n",
      "Iteration 24689, loss = 0.00746684\n",
      "Iteration 24690, loss = 0.00746661\n",
      "Iteration 24691, loss = 0.00746637\n",
      "Iteration 24692, loss = 0.00746614\n",
      "Iteration 24693, loss = 0.00746591\n",
      "Iteration 24694, loss = 0.00746567\n",
      "Iteration 24695, loss = 0.00746544\n",
      "Iteration 24696, loss = 0.00746521\n",
      "Iteration 24697, loss = 0.00746498\n",
      "Iteration 24698, loss = 0.00746474\n",
      "Iteration 24699, loss = 0.00746451\n",
      "Iteration 24700, loss = 0.00746428\n",
      "Iteration 24701, loss = 0.00746404\n",
      "Iteration 24702, loss = 0.00746381\n",
      "Iteration 24703, loss = 0.00746358\n",
      "Iteration 24704, loss = 0.00746335\n",
      "Iteration 24705, loss = 0.00746311\n",
      "Iteration 24706, loss = 0.00746288\n",
      "Iteration 24707, loss = 0.00746265\n",
      "Iteration 24708, loss = 0.00746241\n",
      "Iteration 24709, loss = 0.00746218\n",
      "Iteration 24710, loss = 0.00746195\n",
      "Iteration 24711, loss = 0.00746172\n",
      "Iteration 24712, loss = 0.00746148\n",
      "Iteration 24713, loss = 0.00746125\n",
      "Iteration 24714, loss = 0.00746102\n",
      "Iteration 24715, loss = 0.00746079\n",
      "Iteration 24716, loss = 0.00746055\n",
      "Iteration 24717, loss = 0.00746032\n",
      "Iteration 24718, loss = 0.00746009\n",
      "Iteration 24719, loss = 0.00745986\n",
      "Iteration 24720, loss = 0.00745962\n",
      "Iteration 24721, loss = 0.00745939\n",
      "Iteration 24722, loss = 0.00745916\n",
      "Iteration 24723, loss = 0.00745893\n",
      "Iteration 24724, loss = 0.00745870\n",
      "Iteration 24725, loss = 0.00745846\n",
      "Iteration 24726, loss = 0.00745823\n",
      "Iteration 24727, loss = 0.00745800\n",
      "Iteration 24728, loss = 0.00745777\n",
      "Iteration 24729, loss = 0.00745753\n",
      "Iteration 24730, loss = 0.00745730\n",
      "Iteration 24731, loss = 0.00745707\n",
      "Iteration 24732, loss = 0.00745684\n",
      "Iteration 24733, loss = 0.00745661\n",
      "Iteration 24734, loss = 0.00745637\n",
      "Iteration 24735, loss = 0.00745614\n",
      "Iteration 24736, loss = 0.00745591\n",
      "Iteration 24737, loss = 0.00745568\n",
      "Iteration 24738, loss = 0.00745545\n",
      "Iteration 24739, loss = 0.00745522\n",
      "Iteration 24740, loss = 0.00745498\n",
      "Iteration 24741, loss = 0.00745475\n",
      "Iteration 24742, loss = 0.00745452\n",
      "Iteration 24743, loss = 0.00745429\n",
      "Iteration 24744, loss = 0.00745406\n",
      "Iteration 24745, loss = 0.00745382\n",
      "Iteration 24746, loss = 0.00745359\n",
      "Iteration 24747, loss = 0.00745336\n",
      "Iteration 24748, loss = 0.00745313\n",
      "Iteration 24749, loss = 0.00745290\n",
      "Iteration 24750, loss = 0.00745267\n",
      "Iteration 24751, loss = 0.00745243\n",
      "Iteration 24752, loss = 0.00745220\n",
      "Iteration 24753, loss = 0.00745197\n",
      "Iteration 24754, loss = 0.00745174\n",
      "Iteration 24755, loss = 0.00745151\n",
      "Iteration 24756, loss = 0.00745128\n",
      "Iteration 24757, loss = 0.00745105\n",
      "Iteration 24758, loss = 0.00745081\n",
      "Iteration 24759, loss = 0.00745058\n",
      "Iteration 24760, loss = 0.00745035\n",
      "Iteration 24761, loss = 0.00745012\n",
      "Iteration 24762, loss = 0.00744989\n",
      "Iteration 24763, loss = 0.00744966\n",
      "Iteration 24764, loss = 0.00744943\n",
      "Iteration 24765, loss = 0.00744920\n",
      "Iteration 24766, loss = 0.00744896\n",
      "Iteration 24767, loss = 0.00744873\n",
      "Iteration 24768, loss = 0.00744850\n",
      "Iteration 24769, loss = 0.00744827\n",
      "Iteration 24770, loss = 0.00744804\n",
      "Iteration 24771, loss = 0.00744781\n",
      "Iteration 24772, loss = 0.00744758\n",
      "Iteration 24773, loss = 0.00744735\n",
      "Iteration 24774, loss = 0.00744712\n",
      "Iteration 24775, loss = 0.00744689\n",
      "Iteration 24776, loss = 0.00744665\n",
      "Iteration 24777, loss = 0.00744642\n",
      "Iteration 24778, loss = 0.00744619\n",
      "Iteration 24779, loss = 0.00744596\n",
      "Iteration 24780, loss = 0.00744573\n",
      "Iteration 24781, loss = 0.00744550\n",
      "Iteration 24782, loss = 0.00744527\n",
      "Iteration 24783, loss = 0.00744504\n",
      "Iteration 24784, loss = 0.00744481\n",
      "Iteration 24785, loss = 0.00744458\n",
      "Iteration 24786, loss = 0.00744435\n",
      "Iteration 24787, loss = 0.00744412\n",
      "Iteration 24788, loss = 0.00744389\n",
      "Iteration 24789, loss = 0.00744365\n",
      "Iteration 24790, loss = 0.00744342\n",
      "Iteration 24791, loss = 0.00744319\n",
      "Iteration 24792, loss = 0.00744296\n",
      "Iteration 24793, loss = 0.00744273\n",
      "Iteration 24794, loss = 0.00744250\n",
      "Iteration 24795, loss = 0.00744227\n",
      "Iteration 24796, loss = 0.00744204\n",
      "Iteration 24797, loss = 0.00744181\n",
      "Iteration 24798, loss = 0.00744158\n",
      "Iteration 24799, loss = 0.00744135\n",
      "Iteration 24800, loss = 0.00744112\n",
      "Iteration 24801, loss = 0.00744089\n",
      "Iteration 24802, loss = 0.00744066\n",
      "Iteration 24803, loss = 0.00744043\n",
      "Iteration 24804, loss = 0.00744020\n",
      "Iteration 24805, loss = 0.00743997\n",
      "Iteration 24806, loss = 0.00743974\n",
      "Iteration 24807, loss = 0.00743951\n",
      "Iteration 24808, loss = 0.00743928\n",
      "Iteration 24809, loss = 0.00743905\n",
      "Iteration 24810, loss = 0.00743882\n",
      "Iteration 24811, loss = 0.00743859\n",
      "Iteration 24812, loss = 0.00743836\n",
      "Iteration 24813, loss = 0.00743813\n",
      "Iteration 24814, loss = 0.00743790\n",
      "Iteration 24815, loss = 0.00743767\n",
      "Iteration 24816, loss = 0.00743744\n",
      "Iteration 24817, loss = 0.00743721\n",
      "Iteration 24818, loss = 0.00743698\n",
      "Iteration 24819, loss = 0.00743675\n",
      "Iteration 24820, loss = 0.00743652\n",
      "Iteration 24821, loss = 0.00743629\n",
      "Iteration 24822, loss = 0.00743606\n",
      "Iteration 24823, loss = 0.00743583\n",
      "Iteration 24824, loss = 0.00743560\n",
      "Iteration 24825, loss = 0.00743537\n",
      "Iteration 24826, loss = 0.00743514\n",
      "Iteration 24827, loss = 0.00743491\n",
      "Iteration 24828, loss = 0.00743468\n",
      "Iteration 24829, loss = 0.00743445\n",
      "Iteration 24830, loss = 0.00743422\n",
      "Iteration 24831, loss = 0.00743399\n",
      "Iteration 24832, loss = 0.00743376\n",
      "Iteration 24833, loss = 0.00743353\n",
      "Iteration 24834, loss = 0.00743331\n",
      "Iteration 24835, loss = 0.00743308\n",
      "Iteration 24836, loss = 0.00743285\n",
      "Iteration 24837, loss = 0.00743262\n",
      "Iteration 24838, loss = 0.00743239\n",
      "Iteration 24839, loss = 0.00743216\n",
      "Iteration 24840, loss = 0.00743193\n",
      "Iteration 24841, loss = 0.00743170\n",
      "Iteration 24842, loss = 0.00743147\n",
      "Iteration 24843, loss = 0.00743124\n",
      "Iteration 24844, loss = 0.00743101\n",
      "Iteration 24845, loss = 0.00743078\n",
      "Iteration 24846, loss = 0.00743055\n",
      "Iteration 24847, loss = 0.00743032\n",
      "Iteration 24848, loss = 0.00743010\n",
      "Iteration 24849, loss = 0.00742987\n",
      "Iteration 24850, loss = 0.00742964\n",
      "Iteration 24851, loss = 0.00742941\n",
      "Iteration 24852, loss = 0.00742918\n",
      "Iteration 24853, loss = 0.00742895\n",
      "Iteration 24854, loss = 0.00742872\n",
      "Iteration 24855, loss = 0.00742849\n",
      "Iteration 24856, loss = 0.00742826\n",
      "Iteration 24857, loss = 0.00742804\n",
      "Iteration 24858, loss = 0.00742781\n",
      "Iteration 24859, loss = 0.00742758\n",
      "Iteration 24860, loss = 0.00742735\n",
      "Iteration 24861, loss = 0.00742712\n",
      "Iteration 24862, loss = 0.00742689\n",
      "Iteration 24863, loss = 0.00742666\n",
      "Iteration 24864, loss = 0.00742643\n",
      "Iteration 24865, loss = 0.00742621\n",
      "Iteration 24866, loss = 0.00742598\n",
      "Iteration 24867, loss = 0.00742575\n",
      "Iteration 24868, loss = 0.00742552\n",
      "Iteration 24869, loss = 0.00742529\n",
      "Iteration 24870, loss = 0.00742506\n",
      "Iteration 24871, loss = 0.00742483\n",
      "Iteration 24872, loss = 0.00742461\n",
      "Iteration 24873, loss = 0.00742438\n",
      "Iteration 24874, loss = 0.00742415\n",
      "Iteration 24875, loss = 0.00742392\n",
      "Iteration 24876, loss = 0.00742369\n",
      "Iteration 24877, loss = 0.00742346\n",
      "Iteration 24878, loss = 0.00742324\n",
      "Iteration 24879, loss = 0.00742301\n",
      "Iteration 24880, loss = 0.00742278\n",
      "Iteration 24881, loss = 0.00742255\n",
      "Iteration 24882, loss = 0.00742232\n",
      "Iteration 24883, loss = 0.00742209\n",
      "Iteration 24884, loss = 0.00742187\n",
      "Iteration 24885, loss = 0.00742164\n",
      "Iteration 24886, loss = 0.00742141\n",
      "Iteration 24887, loss = 0.00742118\n",
      "Iteration 24888, loss = 0.00742095\n",
      "Iteration 24889, loss = 0.00742073\n",
      "Iteration 24890, loss = 0.00742050\n",
      "Iteration 24891, loss = 0.00742027\n",
      "Iteration 24892, loss = 0.00742004\n",
      "Iteration 24893, loss = 0.00741981\n",
      "Iteration 24894, loss = 0.00741959\n",
      "Iteration 24895, loss = 0.00741936\n",
      "Iteration 24896, loss = 0.00741913\n",
      "Iteration 24897, loss = 0.00741890\n",
      "Iteration 24898, loss = 0.00741867\n",
      "Iteration 24899, loss = 0.00741845\n",
      "Iteration 24900, loss = 0.00741822\n",
      "Iteration 24901, loss = 0.00741799\n",
      "Iteration 24902, loss = 0.00741776\n",
      "Iteration 24903, loss = 0.00741754\n",
      "Iteration 24904, loss = 0.00741731\n",
      "Iteration 24905, loss = 0.00741708\n",
      "Iteration 24906, loss = 0.00741685\n",
      "Iteration 24907, loss = 0.00741663\n",
      "Iteration 24908, loss = 0.00741640\n",
      "Iteration 24909, loss = 0.00741617\n",
      "Iteration 24910, loss = 0.00741594\n",
      "Iteration 24911, loss = 0.00741572\n",
      "Iteration 24912, loss = 0.00741549\n",
      "Iteration 24913, loss = 0.00741526\n",
      "Iteration 24914, loss = 0.00741503\n",
      "Iteration 24915, loss = 0.00741481\n",
      "Iteration 24916, loss = 0.00741458\n",
      "Iteration 24917, loss = 0.00741435\n",
      "Iteration 24918, loss = 0.00741412\n",
      "Iteration 24919, loss = 0.00741390\n",
      "Iteration 24920, loss = 0.00741367\n",
      "Iteration 24921, loss = 0.00741344\n",
      "Iteration 24922, loss = 0.00741321\n",
      "Iteration 24923, loss = 0.00741299\n",
      "Iteration 24924, loss = 0.00741276\n",
      "Iteration 24925, loss = 0.00741253\n",
      "Iteration 24926, loss = 0.00741231\n",
      "Iteration 24927, loss = 0.00741208\n",
      "Iteration 24928, loss = 0.00741185\n",
      "Iteration 24929, loss = 0.00741162\n",
      "Iteration 24930, loss = 0.00741140\n",
      "Iteration 24931, loss = 0.00741117\n",
      "Iteration 24932, loss = 0.00741094\n",
      "Iteration 24933, loss = 0.00741072\n",
      "Iteration 24934, loss = 0.00741049\n",
      "Iteration 24935, loss = 0.00741026\n",
      "Iteration 24936, loss = 0.00741004\n",
      "Iteration 24937, loss = 0.00740981\n",
      "Iteration 24938, loss = 0.00740958\n",
      "Iteration 24939, loss = 0.00740936\n",
      "Iteration 24940, loss = 0.00740913\n",
      "Iteration 24941, loss = 0.00740890\n",
      "Iteration 24942, loss = 0.00740868\n",
      "Iteration 24943, loss = 0.00740845\n",
      "Iteration 24944, loss = 0.00740822\n",
      "Iteration 24945, loss = 0.00740800\n",
      "Iteration 24946, loss = 0.00740777\n",
      "Iteration 24947, loss = 0.00740754\n",
      "Iteration 24948, loss = 0.00740732\n",
      "Iteration 24949, loss = 0.00740709\n",
      "Iteration 24950, loss = 0.00740686\n",
      "Iteration 24951, loss = 0.00740664\n",
      "Iteration 24952, loss = 0.00740641\n",
      "Iteration 24953, loss = 0.00740618\n",
      "Iteration 24954, loss = 0.00740596\n",
      "Iteration 24955, loss = 0.00740573\n",
      "Iteration 24956, loss = 0.00740550\n",
      "Iteration 24957, loss = 0.00740528\n",
      "Iteration 24958, loss = 0.00740505\n",
      "Iteration 24959, loss = 0.00740483\n",
      "Iteration 24960, loss = 0.00740460\n",
      "Iteration 24961, loss = 0.00740437\n",
      "Iteration 24962, loss = 0.00740415\n",
      "Iteration 24963, loss = 0.00740392\n",
      "Iteration 24964, loss = 0.00740369\n",
      "Iteration 24965, loss = 0.00740347\n",
      "Iteration 24966, loss = 0.00740324\n",
      "Iteration 24967, loss = 0.00740302\n",
      "Iteration 24968, loss = 0.00740279\n",
      "Iteration 24969, loss = 0.00740256\n",
      "Iteration 24970, loss = 0.00740234\n",
      "Iteration 24971, loss = 0.00740211\n",
      "Iteration 24972, loss = 0.00740189\n",
      "Iteration 24973, loss = 0.00740166\n",
      "Iteration 24974, loss = 0.00740143\n",
      "Iteration 24975, loss = 0.00740121\n",
      "Iteration 24976, loss = 0.00740098\n",
      "Iteration 24977, loss = 0.00740076\n",
      "Iteration 24978, loss = 0.00740053\n",
      "Iteration 24979, loss = 0.00740031\n",
      "Iteration 24980, loss = 0.00740008\n",
      "Iteration 24981, loss = 0.00739985\n",
      "Iteration 24982, loss = 0.00739963\n",
      "Iteration 24983, loss = 0.00739940\n",
      "Iteration 24984, loss = 0.00739918\n",
      "Iteration 24985, loss = 0.00739895\n",
      "Iteration 24986, loss = 0.00739873\n",
      "Iteration 24987, loss = 0.00739850\n",
      "Iteration 24988, loss = 0.00739827\n",
      "Iteration 24989, loss = 0.00739805\n",
      "Iteration 24990, loss = 0.00739782\n",
      "Iteration 24991, loss = 0.00739760\n",
      "Iteration 24992, loss = 0.00739737\n",
      "Iteration 24993, loss = 0.00739715\n",
      "Iteration 24994, loss = 0.00739692\n",
      "Iteration 24995, loss = 0.00739670\n",
      "Iteration 24996, loss = 0.00739647\n",
      "Iteration 24997, loss = 0.00739625\n",
      "Iteration 24998, loss = 0.00739602\n",
      "Iteration 24999, loss = 0.00739579\n",
      "Iteration 25000, loss = 0.00739557\n",
      "Iteration 25001, loss = 0.00739534\n",
      "Iteration 25002, loss = 0.00739512\n",
      "Iteration 25003, loss = 0.00739489\n",
      "Iteration 25004, loss = 0.00739467\n",
      "Iteration 25005, loss = 0.00739444\n",
      "Iteration 25006, loss = 0.00739422\n",
      "Iteration 25007, loss = 0.00739399\n",
      "Iteration 25008, loss = 0.00739377\n",
      "Iteration 25009, loss = 0.00739354\n",
      "Iteration 25010, loss = 0.00739332\n",
      "Iteration 25011, loss = 0.00739309\n",
      "Iteration 25012, loss = 0.00739287\n",
      "Iteration 25013, loss = 0.00739264\n",
      "Iteration 25014, loss = 0.00739242\n",
      "Iteration 25015, loss = 0.00739219\n",
      "Iteration 25016, loss = 0.00739197\n",
      "Iteration 25017, loss = 0.00739174\n",
      "Iteration 25018, loss = 0.00739152\n",
      "Iteration 25019, loss = 0.00739129\n",
      "Iteration 25020, loss = 0.00739107\n",
      "Iteration 25021, loss = 0.00739084\n",
      "Iteration 25022, loss = 0.00739062\n",
      "Iteration 25023, loss = 0.00739040\n",
      "Iteration 25024, loss = 0.00739017\n",
      "Iteration 25025, loss = 0.00738995\n",
      "Iteration 25026, loss = 0.00738972\n",
      "Iteration 25027, loss = 0.00738950\n",
      "Iteration 25028, loss = 0.00738927\n",
      "Iteration 25029, loss = 0.00738905\n",
      "Iteration 25030, loss = 0.00738882\n",
      "Iteration 25031, loss = 0.00738860\n",
      "Iteration 25032, loss = 0.00738837\n",
      "Iteration 25033, loss = 0.00738815\n",
      "Iteration 25034, loss = 0.00738793\n",
      "Iteration 25035, loss = 0.00738770\n",
      "Iteration 25036, loss = 0.00738748\n",
      "Iteration 25037, loss = 0.00738725\n",
      "Iteration 25038, loss = 0.00738703\n",
      "Iteration 25039, loss = 0.00738680\n",
      "Iteration 25040, loss = 0.00738658\n",
      "Iteration 25041, loss = 0.00738636\n",
      "Iteration 25042, loss = 0.00738613\n",
      "Iteration 25043, loss = 0.00738591\n",
      "Iteration 25044, loss = 0.00738568\n",
      "Iteration 25045, loss = 0.00738546\n",
      "Iteration 25046, loss = 0.00738523\n",
      "Iteration 25047, loss = 0.00738501\n",
      "Iteration 25048, loss = 0.00738479\n",
      "Iteration 25049, loss = 0.00738456\n",
      "Iteration 25050, loss = 0.00738434\n",
      "Iteration 25051, loss = 0.00738411\n",
      "Iteration 25052, loss = 0.00738389\n",
      "Iteration 25053, loss = 0.00738367\n",
      "Iteration 25054, loss = 0.00738344\n",
      "Iteration 25055, loss = 0.00738322\n",
      "Iteration 25056, loss = 0.00738299\n",
      "Iteration 25057, loss = 0.00738277\n",
      "Iteration 25058, loss = 0.00738255\n",
      "Iteration 25059, loss = 0.00738232\n",
      "Iteration 25060, loss = 0.00738210\n",
      "Iteration 25061, loss = 0.00738188\n",
      "Iteration 25062, loss = 0.00738165\n",
      "Iteration 25063, loss = 0.00738143\n",
      "Iteration 25064, loss = 0.00738120\n",
      "Iteration 25065, loss = 0.00738098\n",
      "Iteration 25066, loss = 0.00738076\n",
      "Iteration 25067, loss = 0.00738053\n",
      "Iteration 25068, loss = 0.00738031\n",
      "Iteration 25069, loss = 0.00738009\n",
      "Iteration 25070, loss = 0.00737986\n",
      "Iteration 25071, loss = 0.00737964\n",
      "Iteration 25072, loss = 0.00737942\n",
      "Iteration 25073, loss = 0.00737919\n",
      "Iteration 25074, loss = 0.00737897\n",
      "Iteration 25075, loss = 0.00737875\n",
      "Iteration 25076, loss = 0.00737852\n",
      "Iteration 25077, loss = 0.00737830\n",
      "Iteration 25078, loss = 0.00737808\n",
      "Iteration 25079, loss = 0.00737785\n",
      "Iteration 25080, loss = 0.00737763\n",
      "Iteration 25081, loss = 0.00737741\n",
      "Iteration 25082, loss = 0.00737718\n",
      "Iteration 25083, loss = 0.00737696\n",
      "Iteration 25084, loss = 0.00737674\n",
      "Iteration 25085, loss = 0.00737651\n",
      "Iteration 25086, loss = 0.00737629\n",
      "Iteration 25087, loss = 0.00737607\n",
      "Iteration 25088, loss = 0.00737584\n",
      "Iteration 25089, loss = 0.00737562\n",
      "Iteration 25090, loss = 0.00737540\n",
      "Iteration 25091, loss = 0.00737517\n",
      "Iteration 25092, loss = 0.00737495\n",
      "Iteration 25093, loss = 0.00737473\n",
      "Iteration 25094, loss = 0.00737451\n",
      "Iteration 25095, loss = 0.00737428\n",
      "Iteration 25096, loss = 0.00737406\n",
      "Iteration 25097, loss = 0.00737384\n",
      "Iteration 25098, loss = 0.00737361\n",
      "Iteration 25099, loss = 0.00737339\n",
      "Iteration 25100, loss = 0.00737317\n",
      "Iteration 25101, loss = 0.00737295\n",
      "Iteration 25102, loss = 0.00737272\n",
      "Iteration 25103, loss = 0.00737250\n",
      "Iteration 25104, loss = 0.00737228\n",
      "Iteration 25105, loss = 0.00737205\n",
      "Iteration 25106, loss = 0.00737183\n",
      "Iteration 25107, loss = 0.00737161\n",
      "Iteration 25108, loss = 0.00737139\n",
      "Iteration 25109, loss = 0.00737116\n",
      "Iteration 25110, loss = 0.00737094\n",
      "Iteration 25111, loss = 0.00737072\n",
      "Iteration 25112, loss = 0.00737050\n",
      "Iteration 25113, loss = 0.00737027\n",
      "Iteration 25114, loss = 0.00737005\n",
      "Iteration 25115, loss = 0.00736983\n",
      "Iteration 25116, loss = 0.00736961\n",
      "Iteration 25117, loss = 0.00736938\n",
      "Iteration 25118, loss = 0.00736916\n",
      "Iteration 25119, loss = 0.00736894\n",
      "Iteration 25120, loss = 0.00736872\n",
      "Iteration 25121, loss = 0.00736850\n",
      "Iteration 25122, loss = 0.00736827\n",
      "Iteration 25123, loss = 0.00736805\n",
      "Iteration 25124, loss = 0.00736783\n",
      "Iteration 25125, loss = 0.00736761\n",
      "Iteration 25126, loss = 0.00736738\n",
      "Iteration 25127, loss = 0.00736716\n",
      "Iteration 25128, loss = 0.00736694\n",
      "Iteration 25129, loss = 0.00736672\n",
      "Iteration 25130, loss = 0.00736650\n",
      "Iteration 25131, loss = 0.00736627\n",
      "Iteration 25132, loss = 0.00736605\n",
      "Iteration 25133, loss = 0.00736583\n",
      "Iteration 25134, loss = 0.00736561\n",
      "Iteration 25135, loss = 0.00736539\n",
      "Iteration 25136, loss = 0.00736516\n",
      "Iteration 25137, loss = 0.00736494\n",
      "Iteration 25138, loss = 0.00736472\n",
      "Iteration 25139, loss = 0.00736450\n",
      "Iteration 25140, loss = 0.00736428\n",
      "Iteration 25141, loss = 0.00736405\n",
      "Iteration 25142, loss = 0.00736383\n",
      "Iteration 25143, loss = 0.00736361\n",
      "Iteration 25144, loss = 0.00736339\n",
      "Iteration 25145, loss = 0.00736317\n",
      "Iteration 25146, loss = 0.00736295\n",
      "Iteration 25147, loss = 0.00736272\n",
      "Iteration 25148, loss = 0.00736250\n",
      "Iteration 25149, loss = 0.00736228\n",
      "Iteration 25150, loss = 0.00736206\n",
      "Iteration 25151, loss = 0.00736184\n",
      "Iteration 25152, loss = 0.00736162\n",
      "Iteration 25153, loss = 0.00736140\n",
      "Iteration 25154, loss = 0.00736117\n",
      "Iteration 25155, loss = 0.00736095\n",
      "Iteration 25156, loss = 0.00736073\n",
      "Iteration 25157, loss = 0.00736051\n",
      "Iteration 25158, loss = 0.00736029\n",
      "Iteration 25159, loss = 0.00736007\n",
      "Iteration 25160, loss = 0.00735985\n",
      "Iteration 25161, loss = 0.00735962\n",
      "Iteration 25162, loss = 0.00735940\n",
      "Iteration 25163, loss = 0.00735918\n",
      "Iteration 25164, loss = 0.00735896\n",
      "Iteration 25165, loss = 0.00735874\n",
      "Iteration 25166, loss = 0.00735852\n",
      "Iteration 25167, loss = 0.00735830\n",
      "Iteration 25168, loss = 0.00735808\n",
      "Iteration 25169, loss = 0.00735785\n",
      "Iteration 25170, loss = 0.00735763\n",
      "Iteration 25171, loss = 0.00735741\n",
      "Iteration 25172, loss = 0.00735719\n",
      "Iteration 25173, loss = 0.00735697\n",
      "Iteration 25174, loss = 0.00735675\n",
      "Iteration 25175, loss = 0.00735653\n",
      "Iteration 25176, loss = 0.00735631\n",
      "Iteration 25177, loss = 0.00735609\n",
      "Iteration 25178, loss = 0.00735587\n",
      "Iteration 25179, loss = 0.00735564\n",
      "Iteration 25180, loss = 0.00735542\n",
      "Iteration 25181, loss = 0.00735520\n",
      "Iteration 25182, loss = 0.00735498\n",
      "Iteration 25183, loss = 0.00735476\n",
      "Iteration 25184, loss = 0.00735454\n",
      "Iteration 25185, loss = 0.00735432\n",
      "Iteration 25186, loss = 0.00735410\n",
      "Iteration 25187, loss = 0.00735388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25188, loss = 0.00735366\n",
      "Iteration 25189, loss = 0.00735344\n",
      "Iteration 25190, loss = 0.00735322\n",
      "Iteration 25191, loss = 0.00735300\n",
      "Iteration 25192, loss = 0.00735278\n",
      "Iteration 25193, loss = 0.00735255\n",
      "Iteration 25194, loss = 0.00735233\n",
      "Iteration 25195, loss = 0.00735211\n",
      "Iteration 25196, loss = 0.00735189\n",
      "Iteration 25197, loss = 0.00735167\n",
      "Iteration 25198, loss = 0.00735145\n",
      "Iteration 25199, loss = 0.00735123\n",
      "Iteration 25200, loss = 0.00735101\n",
      "Iteration 25201, loss = 0.00735079\n",
      "Iteration 25202, loss = 0.00735057\n",
      "Iteration 25203, loss = 0.00735035\n",
      "Iteration 25204, loss = 0.00735013\n",
      "Iteration 25205, loss = 0.00734991\n",
      "Iteration 25206, loss = 0.00734969\n",
      "Iteration 25207, loss = 0.00734947\n",
      "Iteration 25208, loss = 0.00734925\n",
      "Iteration 25209, loss = 0.00734903\n",
      "Iteration 25210, loss = 0.00734881\n",
      "Iteration 25211, loss = 0.00734859\n",
      "Iteration 25212, loss = 0.00734837\n",
      "Iteration 25213, loss = 0.00734815\n",
      "Iteration 25214, loss = 0.00734793\n",
      "Iteration 25215, loss = 0.00734771\n",
      "Iteration 25216, loss = 0.00734749\n",
      "Iteration 25217, loss = 0.00734727\n",
      "Iteration 25218, loss = 0.00734705\n",
      "Iteration 25219, loss = 0.00734683\n",
      "Iteration 25220, loss = 0.00734661\n",
      "Iteration 25221, loss = 0.00734639\n",
      "Iteration 25222, loss = 0.00734617\n",
      "Iteration 25223, loss = 0.00734595\n",
      "Iteration 25224, loss = 0.00734573\n",
      "Iteration 25225, loss = 0.00734551\n",
      "Iteration 25226, loss = 0.00734529\n",
      "Iteration 25227, loss = 0.00734507\n",
      "Iteration 25228, loss = 0.00734485\n",
      "Iteration 25229, loss = 0.00734463\n",
      "Iteration 25230, loss = 0.00734441\n",
      "Iteration 25231, loss = 0.00734419\n",
      "Iteration 25232, loss = 0.00734397\n",
      "Iteration 25233, loss = 0.00734375\n",
      "Iteration 25234, loss = 0.00734353\n",
      "Iteration 25235, loss = 0.00734331\n",
      "Iteration 25236, loss = 0.00734309\n",
      "Iteration 25237, loss = 0.00734287\n",
      "Iteration 25238, loss = 0.00734266\n",
      "Iteration 25239, loss = 0.00734244\n",
      "Iteration 25240, loss = 0.00734222\n",
      "Iteration 25241, loss = 0.00734200\n",
      "Iteration 25242, loss = 0.00734178\n",
      "Iteration 25243, loss = 0.00734156\n",
      "Iteration 25244, loss = 0.00734134\n",
      "Iteration 25245, loss = 0.00734112\n",
      "Iteration 25246, loss = 0.00734090\n",
      "Iteration 25247, loss = 0.00734068\n",
      "Iteration 25248, loss = 0.00734046\n",
      "Iteration 25249, loss = 0.00734024\n",
      "Iteration 25250, loss = 0.00734002\n",
      "Iteration 25251, loss = 0.00733980\n",
      "Iteration 25252, loss = 0.00733959\n",
      "Iteration 25253, loss = 0.00733937\n",
      "Iteration 25254, loss = 0.00733915\n",
      "Iteration 25255, loss = 0.00733893\n",
      "Iteration 25256, loss = 0.00733871\n",
      "Iteration 25257, loss = 0.00733849\n",
      "Iteration 25258, loss = 0.00733827\n",
      "Iteration 25259, loss = 0.00733805\n",
      "Iteration 25260, loss = 0.00733783\n",
      "Iteration 25261, loss = 0.00733761\n",
      "Iteration 25262, loss = 0.00733740\n",
      "Iteration 25263, loss = 0.00733718\n",
      "Iteration 25264, loss = 0.00733696\n",
      "Iteration 25265, loss = 0.00733674\n",
      "Iteration 25266, loss = 0.00733652\n",
      "Iteration 25267, loss = 0.00733630\n",
      "Iteration 25268, loss = 0.00733608\n",
      "Iteration 25269, loss = 0.00733586\n",
      "Iteration 25270, loss = 0.00733565\n",
      "Iteration 25271, loss = 0.00733543\n",
      "Iteration 25272, loss = 0.00733521\n",
      "Iteration 25273, loss = 0.00733499\n",
      "Iteration 25274, loss = 0.00733477\n",
      "Iteration 25275, loss = 0.00733455\n",
      "Iteration 25276, loss = 0.00733433\n",
      "Iteration 25277, loss = 0.00733412\n",
      "Iteration 25278, loss = 0.00733390\n",
      "Iteration 25279, loss = 0.00733368\n",
      "Iteration 25280, loss = 0.00733346\n",
      "Iteration 25281, loss = 0.00733324\n",
      "Iteration 25282, loss = 0.00733302\n",
      "Iteration 25283, loss = 0.00733280\n",
      "Iteration 25284, loss = 0.00733259\n",
      "Iteration 25285, loss = 0.00733237\n",
      "Iteration 25286, loss = 0.00733215\n",
      "Iteration 25287, loss = 0.00733193\n",
      "Iteration 25288, loss = 0.00733171\n",
      "Iteration 25289, loss = 0.00733149\n",
      "Iteration 25290, loss = 0.00733128\n",
      "Iteration 25291, loss = 0.00733106\n",
      "Iteration 25292, loss = 0.00733084\n",
      "Iteration 25293, loss = 0.00733062\n",
      "Iteration 25294, loss = 0.00733040\n",
      "Iteration 25295, loss = 0.00733019\n",
      "Iteration 25296, loss = 0.00732997\n",
      "Iteration 25297, loss = 0.00732975\n",
      "Iteration 25298, loss = 0.00732953\n",
      "Iteration 25299, loss = 0.00732931\n",
      "Iteration 25300, loss = 0.00732910\n",
      "Iteration 25301, loss = 0.00732888\n",
      "Iteration 25302, loss = 0.00732866\n",
      "Iteration 25303, loss = 0.00732844\n",
      "Iteration 25304, loss = 0.00732822\n",
      "Iteration 25305, loss = 0.00732801\n",
      "Iteration 25306, loss = 0.00732779\n",
      "Iteration 25307, loss = 0.00732757\n",
      "Iteration 25308, loss = 0.00732735\n",
      "Iteration 25309, loss = 0.00732713\n",
      "Iteration 25310, loss = 0.00732692\n",
      "Iteration 25311, loss = 0.00732670\n",
      "Iteration 25312, loss = 0.00732648\n",
      "Iteration 25313, loss = 0.00732626\n",
      "Iteration 25314, loss = 0.00732605\n",
      "Iteration 25315, loss = 0.00732583\n",
      "Iteration 25316, loss = 0.00732561\n",
      "Iteration 25317, loss = 0.00732539\n",
      "Iteration 25318, loss = 0.00732518\n",
      "Iteration 25319, loss = 0.00732496\n",
      "Iteration 25320, loss = 0.00732474\n",
      "Iteration 25321, loss = 0.00732452\n",
      "Iteration 25322, loss = 0.00732431\n",
      "Iteration 25323, loss = 0.00732409\n",
      "Iteration 25324, loss = 0.00732387\n",
      "Iteration 25325, loss = 0.00732365\n",
      "Iteration 25326, loss = 0.00732344\n",
      "Iteration 25327, loss = 0.00732322\n",
      "Iteration 25328, loss = 0.00732300\n",
      "Iteration 25329, loss = 0.00732278\n",
      "Iteration 25330, loss = 0.00732257\n",
      "Iteration 25331, loss = 0.00732235\n",
      "Iteration 25332, loss = 0.00732213\n",
      "Iteration 25333, loss = 0.00732192\n",
      "Iteration 25334, loss = 0.00732170\n",
      "Iteration 25335, loss = 0.00732148\n",
      "Iteration 25336, loss = 0.00732126\n",
      "Iteration 25337, loss = 0.00732105\n",
      "Iteration 25338, loss = 0.00732083\n",
      "Iteration 25339, loss = 0.00732061\n",
      "Iteration 25340, loss = 0.00732040\n",
      "Iteration 25341, loss = 0.00732018\n",
      "Iteration 25342, loss = 0.00731996\n",
      "Iteration 25343, loss = 0.00731974\n",
      "Iteration 25344, loss = 0.00731953\n",
      "Iteration 25345, loss = 0.00731931\n",
      "Iteration 25346, loss = 0.00731909\n",
      "Iteration 25347, loss = 0.00731888\n",
      "Iteration 25348, loss = 0.00731866\n",
      "Iteration 25349, loss = 0.00731844\n",
      "Iteration 25350, loss = 0.00731823\n",
      "Iteration 25351, loss = 0.00731801\n",
      "Iteration 25352, loss = 0.00731779\n",
      "Iteration 25353, loss = 0.00731758\n",
      "Iteration 25354, loss = 0.00731736\n",
      "Iteration 25355, loss = 0.00731714\n",
      "Iteration 25356, loss = 0.00731693\n",
      "Iteration 25357, loss = 0.00731671\n",
      "Iteration 25358, loss = 0.00731649\n",
      "Iteration 25359, loss = 0.00731628\n",
      "Iteration 25360, loss = 0.00731606\n",
      "Iteration 25361, loss = 0.00731584\n",
      "Iteration 25362, loss = 0.00731563\n",
      "Iteration 25363, loss = 0.00731541\n",
      "Iteration 25364, loss = 0.00731519\n",
      "Iteration 25365, loss = 0.00731498\n",
      "Iteration 25366, loss = 0.00731476\n",
      "Iteration 25367, loss = 0.00731454\n",
      "Iteration 25368, loss = 0.00731433\n",
      "Iteration 25369, loss = 0.00731411\n",
      "Iteration 25370, loss = 0.00731390\n",
      "Iteration 25371, loss = 0.00731368\n",
      "Iteration 25372, loss = 0.00731346\n",
      "Iteration 25373, loss = 0.00731325\n",
      "Iteration 25374, loss = 0.00731303\n",
      "Iteration 25375, loss = 0.00731281\n",
      "Iteration 25376, loss = 0.00731260\n",
      "Iteration 25377, loss = 0.00731238\n",
      "Iteration 25378, loss = 0.00731217\n",
      "Iteration 25379, loss = 0.00731195\n",
      "Iteration 25380, loss = 0.00731173\n",
      "Iteration 25381, loss = 0.00731152\n",
      "Iteration 25382, loss = 0.00731130\n",
      "Iteration 25383, loss = 0.00731109\n",
      "Iteration 25384, loss = 0.00731087\n",
      "Iteration 25385, loss = 0.00731065\n",
      "Iteration 25386, loss = 0.00731044\n",
      "Iteration 25387, loss = 0.00731022\n",
      "Iteration 25388, loss = 0.00731001\n",
      "Iteration 25389, loss = 0.00730979\n",
      "Iteration 25390, loss = 0.00730957\n",
      "Iteration 25391, loss = 0.00730936\n",
      "Iteration 25392, loss = 0.00730914\n",
      "Iteration 25393, loss = 0.00730893\n",
      "Iteration 25394, loss = 0.00730871\n",
      "Iteration 25395, loss = 0.00730850\n",
      "Iteration 25396, loss = 0.00730828\n",
      "Iteration 25397, loss = 0.00730806\n",
      "Iteration 25398, loss = 0.00730785\n",
      "Iteration 25399, loss = 0.00730763\n",
      "Iteration 25400, loss = 0.00730742\n",
      "Iteration 25401, loss = 0.00730720\n",
      "Iteration 25402, loss = 0.00730699\n",
      "Iteration 25403, loss = 0.00730677\n",
      "Iteration 25404, loss = 0.00730655\n",
      "Iteration 25405, loss = 0.00730634\n",
      "Iteration 25406, loss = 0.00730612\n",
      "Iteration 25407, loss = 0.00730591\n",
      "Iteration 25408, loss = 0.00730569\n",
      "Iteration 25409, loss = 0.00730548\n",
      "Iteration 25410, loss = 0.00730526\n",
      "Iteration 25411, loss = 0.00730505\n",
      "Iteration 25412, loss = 0.00730483\n",
      "Iteration 25413, loss = 0.00730462\n",
      "Iteration 25414, loss = 0.00730440\n",
      "Iteration 25415, loss = 0.00730419\n",
      "Iteration 25416, loss = 0.00730397\n",
      "Iteration 25417, loss = 0.00730375\n",
      "Iteration 25418, loss = 0.00730354\n",
      "Iteration 25419, loss = 0.00730332\n",
      "Iteration 25420, loss = 0.00730311\n",
      "Iteration 25421, loss = 0.00730289\n",
      "Iteration 25422, loss = 0.00730268\n",
      "Iteration 25423, loss = 0.00730246\n",
      "Iteration 25424, loss = 0.00730225\n",
      "Iteration 25425, loss = 0.00730203\n",
      "Iteration 25426, loss = 0.00730182\n",
      "Iteration 25427, loss = 0.00730160\n",
      "Iteration 25428, loss = 0.00730139\n",
      "Iteration 25429, loss = 0.00730117\n",
      "Iteration 25430, loss = 0.00730096\n",
      "Iteration 25431, loss = 0.00730074\n",
      "Iteration 25432, loss = 0.00730053\n",
      "Iteration 25433, loss = 0.00730031\n",
      "Iteration 25434, loss = 0.00730010\n",
      "Iteration 25435, loss = 0.00729988\n",
      "Iteration 25436, loss = 0.00729967\n",
      "Iteration 25437, loss = 0.00729946\n",
      "Iteration 25438, loss = 0.00729924\n",
      "Iteration 25439, loss = 0.00729903\n",
      "Iteration 25440, loss = 0.00729881\n",
      "Iteration 25441, loss = 0.00729860\n",
      "Iteration 25442, loss = 0.00729838\n",
      "Iteration 25443, loss = 0.00729817\n",
      "Iteration 25444, loss = 0.00729795\n",
      "Iteration 25445, loss = 0.00729774\n",
      "Iteration 25446, loss = 0.00729752\n",
      "Iteration 25447, loss = 0.00729731\n",
      "Iteration 25448, loss = 0.00729709\n",
      "Iteration 25449, loss = 0.00729688\n",
      "Iteration 25450, loss = 0.00729667\n",
      "Iteration 25451, loss = 0.00729645\n",
      "Iteration 25452, loss = 0.00729624\n",
      "Iteration 25453, loss = 0.00729602\n",
      "Iteration 25454, loss = 0.00729581\n",
      "Iteration 25455, loss = 0.00729559\n",
      "Iteration 25456, loss = 0.00729538\n",
      "Iteration 25457, loss = 0.00729516\n",
      "Iteration 25458, loss = 0.00729495\n",
      "Iteration 25459, loss = 0.00729474\n",
      "Iteration 25460, loss = 0.00729452\n",
      "Iteration 25461, loss = 0.00729431\n",
      "Iteration 25462, loss = 0.00729409\n",
      "Iteration 25463, loss = 0.00729388\n",
      "Iteration 25464, loss = 0.00729367\n",
      "Iteration 25465, loss = 0.00729345\n",
      "Iteration 25466, loss = 0.00729324\n",
      "Iteration 25467, loss = 0.00729302\n",
      "Iteration 25468, loss = 0.00729281\n",
      "Iteration 25469, loss = 0.00729259\n",
      "Iteration 25470, loss = 0.00729238\n",
      "Iteration 25471, loss = 0.00729217\n",
      "Iteration 25472, loss = 0.00729195\n",
      "Iteration 25473, loss = 0.00729174\n",
      "Iteration 25474, loss = 0.00729153\n",
      "Iteration 25475, loss = 0.00729131\n",
      "Iteration 25476, loss = 0.00729110\n",
      "Iteration 25477, loss = 0.00729088\n",
      "Iteration 25478, loss = 0.00729067\n",
      "Iteration 25479, loss = 0.00729046\n",
      "Iteration 25480, loss = 0.00729024\n",
      "Iteration 25481, loss = 0.00729003\n",
      "Iteration 25482, loss = 0.00728981\n",
      "Iteration 25483, loss = 0.00728960\n",
      "Iteration 25484, loss = 0.00728939\n",
      "Iteration 25485, loss = 0.00728917\n",
      "Iteration 25486, loss = 0.00728896\n",
      "Iteration 25487, loss = 0.00728875\n",
      "Iteration 25488, loss = 0.00728853\n",
      "Iteration 25489, loss = 0.00728832\n",
      "Iteration 25490, loss = 0.00728811\n",
      "Iteration 25491, loss = 0.00728789\n",
      "Iteration 25492, loss = 0.00728768\n",
      "Iteration 25493, loss = 0.00728747\n",
      "Iteration 25494, loss = 0.00728725\n",
      "Iteration 25495, loss = 0.00728704\n",
      "Iteration 25496, loss = 0.00728682\n",
      "Iteration 25497, loss = 0.00728661\n",
      "Iteration 25498, loss = 0.00728640\n",
      "Iteration 25499, loss = 0.00728618\n",
      "Iteration 25500, loss = 0.00728597\n",
      "Iteration 25501, loss = 0.00728576\n",
      "Iteration 25502, loss = 0.00728555\n",
      "Iteration 25503, loss = 0.00728533\n",
      "Iteration 25504, loss = 0.00728512\n",
      "Iteration 25505, loss = 0.00728491\n",
      "Iteration 25506, loss = 0.00728469\n",
      "Iteration 25507, loss = 0.00728448\n",
      "Iteration 25508, loss = 0.00728427\n",
      "Iteration 25509, loss = 0.00728405\n",
      "Iteration 25510, loss = 0.00728384\n",
      "Iteration 25511, loss = 0.00728363\n",
      "Iteration 25512, loss = 0.00728341\n",
      "Iteration 25513, loss = 0.00728320\n",
      "Iteration 25514, loss = 0.00728299\n",
      "Iteration 25515, loss = 0.00728277\n",
      "Iteration 25516, loss = 0.00728256\n",
      "Iteration 25517, loss = 0.00728235\n",
      "Iteration 25518, loss = 0.00728214\n",
      "Iteration 25519, loss = 0.00728192\n",
      "Iteration 25520, loss = 0.00728171\n",
      "Iteration 25521, loss = 0.00728150\n",
      "Iteration 25522, loss = 0.00728128\n",
      "Iteration 25523, loss = 0.00728107\n",
      "Iteration 25524, loss = 0.00728086\n",
      "Iteration 25525, loss = 0.00728065\n",
      "Iteration 25526, loss = 0.00728043\n",
      "Iteration 25527, loss = 0.00728022\n",
      "Iteration 25528, loss = 0.00728001\n",
      "Iteration 25529, loss = 0.00727980\n",
      "Iteration 25530, loss = 0.00727958\n",
      "Iteration 25531, loss = 0.00727937\n",
      "Iteration 25532, loss = 0.00727916\n",
      "Iteration 25533, loss = 0.00727895\n",
      "Iteration 25534, loss = 0.00727873\n",
      "Iteration 25535, loss = 0.00727852\n",
      "Iteration 25536, loss = 0.00727831\n",
      "Iteration 25537, loss = 0.00727810\n",
      "Iteration 25538, loss = 0.00727788\n",
      "Iteration 25539, loss = 0.00727767\n",
      "Iteration 25540, loss = 0.00727746\n",
      "Iteration 25541, loss = 0.00727725\n",
      "Iteration 25542, loss = 0.00727703\n",
      "Iteration 25543, loss = 0.00727682\n",
      "Iteration 25544, loss = 0.00727661\n",
      "Iteration 25545, loss = 0.00727640\n",
      "Iteration 25546, loss = 0.00727618\n",
      "Iteration 25547, loss = 0.00727597\n",
      "Iteration 25548, loss = 0.00727576\n",
      "Iteration 25549, loss = 0.00727555\n",
      "Iteration 25550, loss = 0.00727534\n",
      "Iteration 25551, loss = 0.00727512\n",
      "Iteration 25552, loss = 0.00727491\n",
      "Iteration 25553, loss = 0.00727470\n",
      "Iteration 25554, loss = 0.00727449\n",
      "Iteration 25555, loss = 0.00727428\n",
      "Iteration 25556, loss = 0.00727406\n",
      "Iteration 25557, loss = 0.00727385\n",
      "Iteration 25558, loss = 0.00727364\n",
      "Iteration 25559, loss = 0.00727343\n",
      "Iteration 25560, loss = 0.00727322\n",
      "Iteration 25561, loss = 0.00727300\n",
      "Iteration 25562, loss = 0.00727279\n",
      "Iteration 25563, loss = 0.00727258\n",
      "Iteration 25564, loss = 0.00727237\n",
      "Iteration 25565, loss = 0.00727216\n",
      "Iteration 25566, loss = 0.00727194\n",
      "Iteration 25567, loss = 0.00727173\n",
      "Iteration 25568, loss = 0.00727152\n",
      "Iteration 25569, loss = 0.00727131\n",
      "Iteration 25570, loss = 0.00727110\n",
      "Iteration 25571, loss = 0.00727089\n",
      "Iteration 25572, loss = 0.00727067\n",
      "Iteration 25573, loss = 0.00727046\n",
      "Iteration 25574, loss = 0.00727025\n",
      "Iteration 25575, loss = 0.00727004\n",
      "Iteration 25576, loss = 0.00726983\n",
      "Iteration 25577, loss = 0.00726962\n",
      "Iteration 25578, loss = 0.00726940\n",
      "Iteration 25579, loss = 0.00726919\n",
      "Iteration 25580, loss = 0.00726898\n",
      "Iteration 25581, loss = 0.00726877\n",
      "Iteration 25582, loss = 0.00726856\n",
      "Iteration 25583, loss = 0.00726835\n",
      "Iteration 25584, loss = 0.00726814\n",
      "Iteration 25585, loss = 0.00726792\n",
      "Iteration 25586, loss = 0.00726771\n",
      "Iteration 25587, loss = 0.00726750\n",
      "Iteration 25588, loss = 0.00726729\n",
      "Iteration 25589, loss = 0.00726708\n",
      "Iteration 25590, loss = 0.00726687\n",
      "Iteration 25591, loss = 0.00726666\n",
      "Iteration 25592, loss = 0.00726645\n",
      "Iteration 25593, loss = 0.00726623\n",
      "Iteration 25594, loss = 0.00726602\n",
      "Iteration 25595, loss = 0.00726581\n",
      "Iteration 25596, loss = 0.00726560\n",
      "Iteration 25597, loss = 0.00726539\n",
      "Iteration 25598, loss = 0.00726518\n",
      "Iteration 25599, loss = 0.00726497\n",
      "Iteration 25600, loss = 0.00726476\n",
      "Iteration 25601, loss = 0.00726455\n",
      "Iteration 25602, loss = 0.00726433\n",
      "Iteration 25603, loss = 0.00726412\n",
      "Iteration 25604, loss = 0.00726391\n",
      "Iteration 25605, loss = 0.00726370\n",
      "Iteration 25606, loss = 0.00726349\n",
      "Iteration 25607, loss = 0.00726328\n",
      "Iteration 25608, loss = 0.00726307\n",
      "Iteration 25609, loss = 0.00726286\n",
      "Iteration 25610, loss = 0.00726265\n",
      "Iteration 25611, loss = 0.00726244\n",
      "Iteration 25612, loss = 0.00726223\n",
      "Iteration 25613, loss = 0.00726202\n",
      "Iteration 25614, loss = 0.00726180\n",
      "Iteration 25615, loss = 0.00726159\n",
      "Iteration 25616, loss = 0.00726138\n",
      "Iteration 25617, loss = 0.00726117\n",
      "Iteration 25618, loss = 0.00726096\n",
      "Iteration 25619, loss = 0.00726075\n",
      "Iteration 25620, loss = 0.00726054\n",
      "Iteration 25621, loss = 0.00726033\n",
      "Iteration 25622, loss = 0.00726012\n",
      "Iteration 25623, loss = 0.00725991\n",
      "Iteration 25624, loss = 0.00725970\n",
      "Iteration 25625, loss = 0.00725949\n",
      "Iteration 25626, loss = 0.00725928\n",
      "Iteration 25627, loss = 0.00725907\n",
      "Iteration 25628, loss = 0.00725886\n",
      "Iteration 25629, loss = 0.00725865\n",
      "Iteration 25630, loss = 0.00725844\n",
      "Iteration 25631, loss = 0.00725823\n",
      "Iteration 25632, loss = 0.00725802\n",
      "Iteration 25633, loss = 0.00725781\n",
      "Iteration 25634, loss = 0.00725760\n",
      "Iteration 25635, loss = 0.00725739\n",
      "Iteration 25636, loss = 0.00725717\n",
      "Iteration 25637, loss = 0.00725696\n",
      "Iteration 25638, loss = 0.00725675\n",
      "Iteration 25639, loss = 0.00725654\n",
      "Iteration 25640, loss = 0.00725633\n",
      "Iteration 25641, loss = 0.00725612\n",
      "Iteration 25642, loss = 0.00725591\n",
      "Iteration 25643, loss = 0.00725570\n",
      "Iteration 25644, loss = 0.00725549\n",
      "Iteration 25645, loss = 0.00725528\n",
      "Iteration 25646, loss = 0.00725507\n",
      "Iteration 25647, loss = 0.00725486\n",
      "Iteration 25648, loss = 0.00725465\n",
      "Iteration 25649, loss = 0.00725444\n",
      "Iteration 25650, loss = 0.00725423\n",
      "Iteration 25651, loss = 0.00725402\n",
      "Iteration 25652, loss = 0.00725381\n",
      "Iteration 25653, loss = 0.00725360\n",
      "Iteration 25654, loss = 0.00725339\n",
      "Iteration 25655, loss = 0.00725319\n",
      "Iteration 25656, loss = 0.00725298\n",
      "Iteration 25657, loss = 0.00725277\n",
      "Iteration 25658, loss = 0.00725256\n",
      "Iteration 25659, loss = 0.00725235\n",
      "Iteration 25660, loss = 0.00725214\n",
      "Iteration 25661, loss = 0.00725193\n",
      "Iteration 25662, loss = 0.00725172\n",
      "Iteration 25663, loss = 0.00725151\n",
      "Iteration 25664, loss = 0.00725130\n",
      "Iteration 25665, loss = 0.00725109\n",
      "Iteration 25666, loss = 0.00725088\n",
      "Iteration 25667, loss = 0.00725067\n",
      "Iteration 25668, loss = 0.00725046\n",
      "Iteration 25669, loss = 0.00725025\n",
      "Iteration 25670, loss = 0.00725004\n",
      "Iteration 25671, loss = 0.00724983\n",
      "Iteration 25672, loss = 0.00724962\n",
      "Iteration 25673, loss = 0.00724941\n",
      "Iteration 25674, loss = 0.00724920\n",
      "Iteration 25675, loss = 0.00724899\n",
      "Iteration 25676, loss = 0.00724878\n",
      "Iteration 25677, loss = 0.00724858\n",
      "Iteration 25678, loss = 0.00724837\n",
      "Iteration 25679, loss = 0.00724816\n",
      "Iteration 25680, loss = 0.00724795\n",
      "Iteration 25681, loss = 0.00724774\n",
      "Iteration 25682, loss = 0.00724753\n",
      "Iteration 25683, loss = 0.00724732\n",
      "Iteration 25684, loss = 0.00724711\n",
      "Iteration 25685, loss = 0.00724690\n",
      "Iteration 25686, loss = 0.00724669\n",
      "Iteration 25687, loss = 0.00724648\n",
      "Iteration 25688, loss = 0.00724627\n",
      "Iteration 25689, loss = 0.00724607\n",
      "Iteration 25690, loss = 0.00724586\n",
      "Iteration 25691, loss = 0.00724565\n",
      "Iteration 25692, loss = 0.00724544\n",
      "Iteration 25693, loss = 0.00724523\n",
      "Iteration 25694, loss = 0.00724502\n",
      "Iteration 25695, loss = 0.00724481\n",
      "Iteration 25696, loss = 0.00724460\n",
      "Iteration 25697, loss = 0.00724439\n",
      "Iteration 25698, loss = 0.00724419\n",
      "Iteration 25699, loss = 0.00724398\n",
      "Iteration 25700, loss = 0.00724377\n",
      "Iteration 25701, loss = 0.00724356\n",
      "Iteration 25702, loss = 0.00724335\n",
      "Iteration 25703, loss = 0.00724314\n",
      "Iteration 25704, loss = 0.00724293\n",
      "Iteration 25705, loss = 0.00724272\n",
      "Iteration 25706, loss = 0.00724252\n",
      "Iteration 25707, loss = 0.00724231\n",
      "Iteration 25708, loss = 0.00724210\n",
      "Iteration 25709, loss = 0.00724189\n",
      "Iteration 25710, loss = 0.00724168\n",
      "Iteration 25711, loss = 0.00724147\n",
      "Iteration 25712, loss = 0.00724126\n",
      "Iteration 25713, loss = 0.00724106\n",
      "Iteration 25714, loss = 0.00724085\n",
      "Iteration 25715, loss = 0.00724064\n",
      "Iteration 25716, loss = 0.00724043\n",
      "Iteration 25717, loss = 0.00724022\n",
      "Iteration 25718, loss = 0.00724001\n",
      "Iteration 25719, loss = 0.00723981\n",
      "Iteration 25720, loss = 0.00723960\n",
      "Iteration 25721, loss = 0.00723939\n",
      "Iteration 25722, loss = 0.00723918\n",
      "Iteration 25723, loss = 0.00723897\n",
      "Iteration 25724, loss = 0.00723876\n",
      "Iteration 25725, loss = 0.00723856\n",
      "Iteration 25726, loss = 0.00723835\n",
      "Iteration 25727, loss = 0.00723814\n",
      "Iteration 25728, loss = 0.00723793\n",
      "Iteration 25729, loss = 0.00723772\n",
      "Iteration 25730, loss = 0.00723752\n",
      "Iteration 25731, loss = 0.00723731\n",
      "Iteration 25732, loss = 0.00723710\n",
      "Iteration 25733, loss = 0.00723689\n",
      "Iteration 25734, loss = 0.00723668\n",
      "Iteration 25735, loss = 0.00723648\n",
      "Iteration 25736, loss = 0.00723627\n",
      "Iteration 25737, loss = 0.00723606\n",
      "Iteration 25738, loss = 0.00723585\n",
      "Iteration 25739, loss = 0.00723564\n",
      "Iteration 25740, loss = 0.00723544\n",
      "Iteration 25741, loss = 0.00723523\n",
      "Iteration 25742, loss = 0.00723502\n",
      "Iteration 25743, loss = 0.00723481\n",
      "Iteration 25744, loss = 0.00723460\n",
      "Iteration 25745, loss = 0.00723440\n",
      "Iteration 25746, loss = 0.00723419\n",
      "Iteration 25747, loss = 0.00723398\n",
      "Iteration 25748, loss = 0.00723377\n",
      "Iteration 25749, loss = 0.00723357\n",
      "Iteration 25750, loss = 0.00723336\n",
      "Iteration 25751, loss = 0.00723315\n",
      "Iteration 25752, loss = 0.00723294\n",
      "Iteration 25753, loss = 0.00723274\n",
      "Iteration 25754, loss = 0.00723253\n",
      "Iteration 25755, loss = 0.00723232\n",
      "Iteration 25756, loss = 0.00723211\n",
      "Iteration 25757, loss = 0.00723191\n",
      "Iteration 25758, loss = 0.00723170\n",
      "Iteration 25759, loss = 0.00723149\n",
      "Iteration 25760, loss = 0.00723128\n",
      "Iteration 25761, loss = 0.00723108\n",
      "Iteration 25762, loss = 0.00723087\n",
      "Iteration 25763, loss = 0.00723066\n",
      "Iteration 25764, loss = 0.00723045\n",
      "Iteration 25765, loss = 0.00723025\n",
      "Iteration 25766, loss = 0.00723004\n",
      "Iteration 25767, loss = 0.00722983\n",
      "Iteration 25768, loss = 0.00722962\n",
      "Iteration 25769, loss = 0.00722942\n",
      "Iteration 25770, loss = 0.00722921\n",
      "Iteration 25771, loss = 0.00722900\n",
      "Iteration 25772, loss = 0.00722880\n",
      "Iteration 25773, loss = 0.00722859\n",
      "Iteration 25774, loss = 0.00722838\n",
      "Iteration 25775, loss = 0.00722817\n",
      "Iteration 25776, loss = 0.00722797\n",
      "Iteration 25777, loss = 0.00722776\n",
      "Iteration 25778, loss = 0.00722755\n",
      "Iteration 25779, loss = 0.00722735\n",
      "Iteration 25780, loss = 0.00722714\n",
      "Iteration 25781, loss = 0.00722693\n",
      "Iteration 25782, loss = 0.00722672\n",
      "Iteration 25783, loss = 0.00722652\n",
      "Iteration 25784, loss = 0.00722631\n",
      "Iteration 25785, loss = 0.00722610\n",
      "Iteration 25786, loss = 0.00722590\n",
      "Iteration 25787, loss = 0.00722569\n",
      "Iteration 25788, loss = 0.00722548\n",
      "Iteration 25789, loss = 0.00722528\n",
      "Iteration 25790, loss = 0.00722507\n",
      "Iteration 25791, loss = 0.00722486\n",
      "Iteration 25792, loss = 0.00722466\n",
      "Iteration 25793, loss = 0.00722445\n",
      "Iteration 25794, loss = 0.00722424\n",
      "Iteration 25795, loss = 0.00722404\n",
      "Iteration 25796, loss = 0.00722383\n",
      "Iteration 25797, loss = 0.00722362\n",
      "Iteration 25798, loss = 0.00722342\n",
      "Iteration 25799, loss = 0.00722321\n",
      "Iteration 25800, loss = 0.00722300\n",
      "Iteration 25801, loss = 0.00722280\n",
      "Iteration 25802, loss = 0.00722259\n",
      "Iteration 25803, loss = 0.00722238\n",
      "Iteration 25804, loss = 0.00722218\n",
      "Iteration 25805, loss = 0.00722197\n",
      "Iteration 25806, loss = 0.00722176\n",
      "Iteration 25807, loss = 0.00722156\n",
      "Iteration 25808, loss = 0.00722135\n",
      "Iteration 25809, loss = 0.00722115\n",
      "Iteration 25810, loss = 0.00722094\n",
      "Iteration 25811, loss = 0.00722073\n",
      "Iteration 25812, loss = 0.00722053\n",
      "Iteration 25813, loss = 0.00722032\n",
      "Iteration 25814, loss = 0.00722011\n",
      "Iteration 25815, loss = 0.00721991\n",
      "Iteration 25816, loss = 0.00721970\n",
      "Iteration 25817, loss = 0.00721950\n",
      "Iteration 25818, loss = 0.00721929\n",
      "Iteration 25819, loss = 0.00721908\n",
      "Iteration 25820, loss = 0.00721888\n",
      "Iteration 25821, loss = 0.00721867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25822, loss = 0.00721846\n",
      "Iteration 25823, loss = 0.00721826\n",
      "Iteration 25824, loss = 0.00721805\n",
      "Iteration 25825, loss = 0.00721785\n",
      "Iteration 25826, loss = 0.00721764\n",
      "Iteration 25827, loss = 0.00721743\n",
      "Iteration 25828, loss = 0.00721723\n",
      "Iteration 25829, loss = 0.00721702\n",
      "Iteration 25830, loss = 0.00721682\n",
      "Iteration 25831, loss = 0.00721661\n",
      "Iteration 25832, loss = 0.00721641\n",
      "Iteration 25833, loss = 0.00721620\n",
      "Iteration 25834, loss = 0.00721599\n",
      "Iteration 25835, loss = 0.00721579\n",
      "Iteration 25836, loss = 0.00721558\n",
      "Iteration 25837, loss = 0.00721538\n",
      "Iteration 25838, loss = 0.00721517\n",
      "Iteration 25839, loss = 0.00721497\n",
      "Iteration 25840, loss = 0.00721476\n",
      "Iteration 25841, loss = 0.00721455\n",
      "Iteration 25842, loss = 0.00721435\n",
      "Iteration 25843, loss = 0.00721414\n",
      "Iteration 25844, loss = 0.00721394\n",
      "Iteration 25845, loss = 0.00721373\n",
      "Iteration 25846, loss = 0.00721353\n",
      "Iteration 25847, loss = 0.00721332\n",
      "Iteration 25848, loss = 0.00721311\n",
      "Iteration 25849, loss = 0.00721291\n",
      "Iteration 25850, loss = 0.00721270\n",
      "Iteration 25851, loss = 0.00721250\n",
      "Iteration 25852, loss = 0.00721229\n",
      "Iteration 25853, loss = 0.00721209\n",
      "Iteration 25854, loss = 0.00721188\n",
      "Iteration 25855, loss = 0.00721168\n",
      "Iteration 25856, loss = 0.00721147\n",
      "Iteration 25857, loss = 0.00721127\n",
      "Iteration 25858, loss = 0.00721106\n",
      "Iteration 25859, loss = 0.00721086\n",
      "Iteration 25860, loss = 0.00721065\n",
      "Iteration 25861, loss = 0.00721045\n",
      "Iteration 25862, loss = 0.00721024\n",
      "Iteration 25863, loss = 0.00721004\n",
      "Iteration 25864, loss = 0.00720983\n",
      "Iteration 25865, loss = 0.00720962\n",
      "Iteration 25866, loss = 0.00720942\n",
      "Iteration 25867, loss = 0.00720921\n",
      "Iteration 25868, loss = 0.00720901\n",
      "Iteration 25869, loss = 0.00720880\n",
      "Iteration 25870, loss = 0.00720860\n",
      "Iteration 25871, loss = 0.00720839\n",
      "Iteration 25872, loss = 0.00720819\n",
      "Iteration 25873, loss = 0.00720798\n",
      "Iteration 25874, loss = 0.00720778\n",
      "Iteration 25875, loss = 0.00720757\n",
      "Iteration 25876, loss = 0.00720737\n",
      "Iteration 25877, loss = 0.00720716\n",
      "Iteration 25878, loss = 0.00720696\n",
      "Iteration 25879, loss = 0.00720676\n",
      "Iteration 25880, loss = 0.00720655\n",
      "Iteration 25881, loss = 0.00720635\n",
      "Iteration 25882, loss = 0.00720614\n",
      "Iteration 25883, loss = 0.00720594\n",
      "Iteration 25884, loss = 0.00720573\n",
      "Iteration 25885, loss = 0.00720553\n",
      "Iteration 25886, loss = 0.00720532\n",
      "Iteration 25887, loss = 0.00720512\n",
      "Iteration 25888, loss = 0.00720491\n",
      "Iteration 25889, loss = 0.00720471\n",
      "Iteration 25890, loss = 0.00720450\n",
      "Iteration 25891, loss = 0.00720430\n",
      "Iteration 25892, loss = 0.00720409\n",
      "Iteration 25893, loss = 0.00720389\n",
      "Iteration 25894, loss = 0.00720369\n",
      "Iteration 25895, loss = 0.00720348\n",
      "Iteration 25896, loss = 0.00720328\n",
      "Iteration 25897, loss = 0.00720307\n",
      "Iteration 25898, loss = 0.00720287\n",
      "Iteration 25899, loss = 0.00720266\n",
      "Iteration 25900, loss = 0.00720246\n",
      "Iteration 25901, loss = 0.00720225\n",
      "Iteration 25902, loss = 0.00720205\n",
      "Iteration 25903, loss = 0.00720185\n",
      "Iteration 25904, loss = 0.00720164\n",
      "Iteration 25905, loss = 0.00720144\n",
      "Iteration 25906, loss = 0.00720123\n",
      "Iteration 25907, loss = 0.00720103\n",
      "Iteration 25908, loss = 0.00720082\n",
      "Iteration 25909, loss = 0.00720062\n",
      "Iteration 25910, loss = 0.00720042\n",
      "Iteration 25911, loss = 0.00720021\n",
      "Iteration 25912, loss = 0.00720001\n",
      "Iteration 25913, loss = 0.00719980\n",
      "Iteration 25914, loss = 0.00719960\n",
      "Iteration 25915, loss = 0.00719940\n",
      "Iteration 25916, loss = 0.00719919\n",
      "Iteration 25917, loss = 0.00719899\n",
      "Iteration 25918, loss = 0.00719878\n",
      "Iteration 25919, loss = 0.00719858\n",
      "Iteration 25920, loss = 0.00719838\n",
      "Iteration 25921, loss = 0.00719817\n",
      "Iteration 25922, loss = 0.00719797\n",
      "Iteration 25923, loss = 0.00719776\n",
      "Iteration 25924, loss = 0.00719756\n",
      "Iteration 25925, loss = 0.00719736\n",
      "Iteration 25926, loss = 0.00719715\n",
      "Iteration 25927, loss = 0.00719695\n",
      "Iteration 25928, loss = 0.00719675\n",
      "Iteration 25929, loss = 0.00719654\n",
      "Iteration 25930, loss = 0.00719634\n",
      "Iteration 25931, loss = 0.00719613\n",
      "Iteration 25932, loss = 0.00719593\n",
      "Iteration 25933, loss = 0.00719573\n",
      "Iteration 25934, loss = 0.00719552\n",
      "Iteration 25935, loss = 0.00719532\n",
      "Iteration 25936, loss = 0.00719512\n",
      "Iteration 25937, loss = 0.00719491\n",
      "Iteration 25938, loss = 0.00719471\n",
      "Iteration 25939, loss = 0.00719451\n",
      "Iteration 25940, loss = 0.00719430\n",
      "Iteration 25941, loss = 0.00719410\n",
      "Iteration 25942, loss = 0.00719390\n",
      "Iteration 25943, loss = 0.00719369\n",
      "Iteration 25944, loss = 0.00719349\n",
      "Iteration 25945, loss = 0.00719328\n",
      "Iteration 25946, loss = 0.00719308\n",
      "Iteration 25947, loss = 0.00719288\n",
      "Iteration 25948, loss = 0.00719267\n",
      "Iteration 25949, loss = 0.00719247\n",
      "Iteration 25950, loss = 0.00719227\n",
      "Iteration 25951, loss = 0.00719207\n",
      "Iteration 25952, loss = 0.00719186\n",
      "Iteration 25953, loss = 0.00719166\n",
      "Iteration 25954, loss = 0.00719146\n",
      "Iteration 25955, loss = 0.00719125\n",
      "Iteration 25956, loss = 0.00719105\n",
      "Iteration 25957, loss = 0.00719085\n",
      "Iteration 25958, loss = 0.00719064\n",
      "Iteration 25959, loss = 0.00719044\n",
      "Iteration 25960, loss = 0.00719024\n",
      "Iteration 25961, loss = 0.00719003\n",
      "Iteration 25962, loss = 0.00718983\n",
      "Iteration 25963, loss = 0.00718963\n",
      "Iteration 25964, loss = 0.00718942\n",
      "Iteration 25965, loss = 0.00718922\n",
      "Iteration 25966, loss = 0.00718902\n",
      "Iteration 25967, loss = 0.00718882\n",
      "Iteration 25968, loss = 0.00718861\n",
      "Iteration 25969, loss = 0.00718841\n",
      "Iteration 25970, loss = 0.00718821\n",
      "Iteration 25971, loss = 0.00718800\n",
      "Iteration 25972, loss = 0.00718780\n",
      "Iteration 25973, loss = 0.00718760\n",
      "Iteration 25974, loss = 0.00718740\n",
      "Iteration 25975, loss = 0.00718719\n",
      "Iteration 25976, loss = 0.00718699\n",
      "Iteration 25977, loss = 0.00718679\n",
      "Iteration 25978, loss = 0.00718659\n",
      "Iteration 25979, loss = 0.00718638\n",
      "Iteration 25980, loss = 0.00718618\n",
      "Iteration 25981, loss = 0.00718598\n",
      "Iteration 25982, loss = 0.00718577\n",
      "Iteration 25983, loss = 0.00718557\n",
      "Iteration 25984, loss = 0.00718537\n",
      "Iteration 25985, loss = 0.00718517\n",
      "Iteration 25986, loss = 0.00718496\n",
      "Iteration 25987, loss = 0.00718476\n",
      "Iteration 25988, loss = 0.00718456\n",
      "Iteration 25989, loss = 0.00718436\n",
      "Iteration 25990, loss = 0.00718415\n",
      "Iteration 25991, loss = 0.00718395\n",
      "Iteration 25992, loss = 0.00718375\n",
      "Iteration 25993, loss = 0.00718355\n",
      "Iteration 25994, loss = 0.00718335\n",
      "Iteration 25995, loss = 0.00718314\n",
      "Iteration 25996, loss = 0.00718294\n",
      "Iteration 25997, loss = 0.00718274\n",
      "Iteration 25998, loss = 0.00718254\n",
      "Iteration 25999, loss = 0.00718233\n",
      "Iteration 26000, loss = 0.00718213\n",
      "Iteration 26001, loss = 0.00718193\n",
      "Iteration 26002, loss = 0.00718173\n",
      "Iteration 26003, loss = 0.00718153\n",
      "Iteration 26004, loss = 0.00718132\n",
      "Iteration 26005, loss = 0.00718112\n",
      "Iteration 26006, loss = 0.00718092\n",
      "Iteration 26007, loss = 0.00718072\n",
      "Iteration 26008, loss = 0.00718051\n",
      "Iteration 26009, loss = 0.00718031\n",
      "Iteration 26010, loss = 0.00718011\n",
      "Iteration 26011, loss = 0.00717991\n",
      "Iteration 26012, loss = 0.00717971\n",
      "Iteration 26013, loss = 0.00717950\n",
      "Iteration 26014, loss = 0.00717930\n",
      "Iteration 26015, loss = 0.00717910\n",
      "Iteration 26016, loss = 0.00717890\n",
      "Iteration 26017, loss = 0.00717870\n",
      "Iteration 26018, loss = 0.00717850\n",
      "Iteration 26019, loss = 0.00717829\n",
      "Iteration 26020, loss = 0.00717809\n",
      "Iteration 26021, loss = 0.00717789\n",
      "Iteration 26022, loss = 0.00717769\n",
      "Iteration 26023, loss = 0.00717749\n",
      "Iteration 26024, loss = 0.00717728\n",
      "Iteration 26025, loss = 0.00717708\n",
      "Iteration 26026, loss = 0.00717688\n",
      "Iteration 26027, loss = 0.00717668\n",
      "Iteration 26028, loss = 0.00717648\n",
      "Iteration 26029, loss = 0.00717628\n",
      "Iteration 26030, loss = 0.00717608\n",
      "Iteration 26031, loss = 0.00717587\n",
      "Iteration 26032, loss = 0.00717567\n",
      "Iteration 26033, loss = 0.00717547\n",
      "Iteration 26034, loss = 0.00717527\n",
      "Iteration 26035, loss = 0.00717507\n",
      "Iteration 26036, loss = 0.00717487\n",
      "Iteration 26037, loss = 0.00717466\n",
      "Iteration 26038, loss = 0.00717446\n",
      "Iteration 26039, loss = 0.00717426\n",
      "Iteration 26040, loss = 0.00717406\n",
      "Iteration 26041, loss = 0.00717386\n",
      "Iteration 26042, loss = 0.00717366\n",
      "Iteration 26043, loss = 0.00717346\n",
      "Iteration 26044, loss = 0.00717326\n",
      "Iteration 26045, loss = 0.00717305\n",
      "Iteration 26046, loss = 0.00717285\n",
      "Iteration 26047, loss = 0.00717265\n",
      "Iteration 26048, loss = 0.00717245\n",
      "Iteration 26049, loss = 0.00717225\n",
      "Iteration 26050, loss = 0.00717205\n",
      "Iteration 26051, loss = 0.00717185\n",
      "Iteration 26052, loss = 0.00717165\n",
      "Iteration 26053, loss = 0.00717144\n",
      "Iteration 26054, loss = 0.00717124\n",
      "Iteration 26055, loss = 0.00717104\n",
      "Iteration 26056, loss = 0.00717084\n",
      "Iteration 26057, loss = 0.00717064\n",
      "Iteration 26058, loss = 0.00717044\n",
      "Iteration 26059, loss = 0.00717024\n",
      "Iteration 26060, loss = 0.00717004\n",
      "Iteration 26061, loss = 0.00716984\n",
      "Iteration 26062, loss = 0.00716964\n",
      "Iteration 26063, loss = 0.00716944\n",
      "Iteration 26064, loss = 0.00716923\n",
      "Iteration 26065, loss = 0.00716903\n",
      "Iteration 26066, loss = 0.00716883\n",
      "Iteration 26067, loss = 0.00716863\n",
      "Iteration 26068, loss = 0.00716843\n",
      "Iteration 26069, loss = 0.00716823\n",
      "Iteration 26070, loss = 0.00716803\n",
      "Iteration 26071, loss = 0.00716783\n",
      "Iteration 26072, loss = 0.00716763\n",
      "Iteration 26073, loss = 0.00716743\n",
      "Iteration 26074, loss = 0.00716723\n",
      "Iteration 26075, loss = 0.00716703\n",
      "Iteration 26076, loss = 0.00716683\n",
      "Iteration 26077, loss = 0.00716663\n",
      "Iteration 26078, loss = 0.00716642\n",
      "Iteration 26079, loss = 0.00716622\n",
      "Iteration 26080, loss = 0.00716602\n",
      "Iteration 26081, loss = 0.00716582\n",
      "Iteration 26082, loss = 0.00716562\n",
      "Iteration 26083, loss = 0.00716542\n",
      "Iteration 26084, loss = 0.00716522\n",
      "Iteration 26085, loss = 0.00716502\n",
      "Iteration 26086, loss = 0.00716482\n",
      "Iteration 26087, loss = 0.00716462\n",
      "Iteration 26088, loss = 0.00716442\n",
      "Iteration 26089, loss = 0.00716422\n",
      "Iteration 26090, loss = 0.00716402\n",
      "Iteration 26091, loss = 0.00716382\n",
      "Iteration 26092, loss = 0.00716362\n",
      "Iteration 26093, loss = 0.00716342\n",
      "Iteration 26094, loss = 0.00716322\n",
      "Iteration 26095, loss = 0.00716302\n",
      "Iteration 26096, loss = 0.00716282\n",
      "Iteration 26097, loss = 0.00716262\n",
      "Iteration 26098, loss = 0.00716242\n",
      "Iteration 26099, loss = 0.00716222\n",
      "Iteration 26100, loss = 0.00716202\n",
      "Iteration 26101, loss = 0.00716182\n",
      "Iteration 26102, loss = 0.00716162\n",
      "Iteration 26103, loss = 0.00716142\n",
      "Iteration 26104, loss = 0.00716122\n",
      "Iteration 26105, loss = 0.00716102\n",
      "Iteration 26106, loss = 0.00716082\n",
      "Iteration 26107, loss = 0.00716062\n",
      "Iteration 26108, loss = 0.00716042\n",
      "Iteration 26109, loss = 0.00716022\n",
      "Iteration 26110, loss = 0.00716002\n",
      "Iteration 26111, loss = 0.00715982\n",
      "Iteration 26112, loss = 0.00715962\n",
      "Iteration 26113, loss = 0.00715942\n",
      "Iteration 26114, loss = 0.00715922\n",
      "Iteration 26115, loss = 0.00715902\n",
      "Iteration 26116, loss = 0.00715882\n",
      "Iteration 26117, loss = 0.00715862\n",
      "Iteration 26118, loss = 0.00715842\n",
      "Iteration 26119, loss = 0.00715822\n",
      "Iteration 26120, loss = 0.00715802\n",
      "Iteration 26121, loss = 0.00715782\n",
      "Iteration 26122, loss = 0.00715762\n",
      "Iteration 26123, loss = 0.00715742\n",
      "Iteration 26124, loss = 0.00715722\n",
      "Iteration 26125, loss = 0.00715702\n",
      "Iteration 26126, loss = 0.00715682\n",
      "Iteration 26127, loss = 0.00715662\n",
      "Iteration 26128, loss = 0.00715642\n",
      "Iteration 26129, loss = 0.00715622\n",
      "Iteration 26130, loss = 0.00715603\n",
      "Iteration 26131, loss = 0.00715583\n",
      "Iteration 26132, loss = 0.00715563\n",
      "Iteration 26133, loss = 0.00715543\n",
      "Iteration 26134, loss = 0.00715523\n",
      "Iteration 26135, loss = 0.00715503\n",
      "Iteration 26136, loss = 0.00715483\n",
      "Iteration 26137, loss = 0.00715463\n",
      "Iteration 26138, loss = 0.00715443\n",
      "Iteration 26139, loss = 0.00715423\n",
      "Iteration 26140, loss = 0.00715403\n",
      "Iteration 26141, loss = 0.00715383\n",
      "Iteration 26142, loss = 0.00715363\n",
      "Iteration 26143, loss = 0.00715343\n",
      "Iteration 26144, loss = 0.00715323\n",
      "Iteration 26145, loss = 0.00715304\n",
      "Iteration 26146, loss = 0.00715284\n",
      "Iteration 26147, loss = 0.00715264\n",
      "Iteration 26148, loss = 0.00715244\n",
      "Iteration 26149, loss = 0.00715224\n",
      "Iteration 26150, loss = 0.00715204\n",
      "Iteration 26151, loss = 0.00715184\n",
      "Iteration 26152, loss = 0.00715164\n",
      "Iteration 26153, loss = 0.00715144\n",
      "Iteration 26154, loss = 0.00715124\n",
      "Iteration 26155, loss = 0.00715105\n",
      "Iteration 26156, loss = 0.00715085\n",
      "Iteration 26157, loss = 0.00715065\n",
      "Iteration 26158, loss = 0.00715045\n",
      "Iteration 26159, loss = 0.00715025\n",
      "Iteration 26160, loss = 0.00715005\n",
      "Iteration 26161, loss = 0.00714985\n",
      "Iteration 26162, loss = 0.00714965\n",
      "Iteration 26163, loss = 0.00714945\n",
      "Iteration 26164, loss = 0.00714926\n",
      "Iteration 26165, loss = 0.00714906\n",
      "Iteration 26166, loss = 0.00714886\n",
      "Iteration 26167, loss = 0.00714866\n",
      "Iteration 26168, loss = 0.00714846\n",
      "Iteration 26169, loss = 0.00714826\n",
      "Iteration 26170, loss = 0.00714806\n",
      "Iteration 26171, loss = 0.00714787\n",
      "Iteration 26172, loss = 0.00714767\n",
      "Iteration 26173, loss = 0.00714747\n",
      "Iteration 26174, loss = 0.00714727\n",
      "Iteration 26175, loss = 0.00714707\n",
      "Iteration 26176, loss = 0.00714687\n",
      "Iteration 26177, loss = 0.00714667\n",
      "Iteration 26178, loss = 0.00714648\n",
      "Iteration 26179, loss = 0.00714628\n",
      "Iteration 26180, loss = 0.00714608\n",
      "Iteration 26181, loss = 0.00714588\n",
      "Iteration 26182, loss = 0.00714568\n",
      "Iteration 26183, loss = 0.00714548\n",
      "Iteration 26184, loss = 0.00714529\n",
      "Iteration 26185, loss = 0.00714509\n",
      "Iteration 26186, loss = 0.00714489\n",
      "Iteration 26187, loss = 0.00714469\n",
      "Iteration 26188, loss = 0.00714449\n",
      "Iteration 26189, loss = 0.00714429\n",
      "Iteration 26190, loss = 0.00714410\n",
      "Iteration 26191, loss = 0.00714390\n",
      "Iteration 26192, loss = 0.00714370\n",
      "Iteration 26193, loss = 0.00714350\n",
      "Iteration 26194, loss = 0.00714330\n",
      "Iteration 26195, loss = 0.00714311\n",
      "Iteration 26196, loss = 0.00714291\n",
      "Iteration 26197, loss = 0.00714271\n",
      "Iteration 26198, loss = 0.00714251\n",
      "Iteration 26199, loss = 0.00714231\n",
      "Iteration 26200, loss = 0.00714212\n",
      "Iteration 26201, loss = 0.00714192\n",
      "Iteration 26202, loss = 0.00714172\n",
      "Iteration 26203, loss = 0.00714152\n",
      "Iteration 26204, loss = 0.00714132\n",
      "Iteration 26205, loss = 0.00714113\n",
      "Iteration 26206, loss = 0.00714093\n",
      "Iteration 26207, loss = 0.00714073\n",
      "Iteration 26208, loss = 0.00714053\n",
      "Iteration 26209, loss = 0.00714033\n",
      "Iteration 26210, loss = 0.00714014\n",
      "Iteration 26211, loss = 0.00713994\n",
      "Iteration 26212, loss = 0.00713974\n",
      "Iteration 26213, loss = 0.00713954\n",
      "Iteration 26214, loss = 0.00713935\n",
      "Iteration 26215, loss = 0.00713915\n",
      "Iteration 26216, loss = 0.00713895\n",
      "Iteration 26217, loss = 0.00713875\n",
      "Iteration 26218, loss = 0.00713855\n",
      "Iteration 26219, loss = 0.00713836\n",
      "Iteration 26220, loss = 0.00713816\n",
      "Iteration 26221, loss = 0.00713796\n",
      "Iteration 26222, loss = 0.00713776\n",
      "Iteration 26223, loss = 0.00713757\n",
      "Iteration 26224, loss = 0.00713737\n",
      "Iteration 26225, loss = 0.00713717\n",
      "Iteration 26226, loss = 0.00713697\n",
      "Iteration 26227, loss = 0.00713678\n",
      "Iteration 26228, loss = 0.00713658\n",
      "Iteration 26229, loss = 0.00713638\n",
      "Iteration 26230, loss = 0.00713619\n",
      "Iteration 26231, loss = 0.00713599\n",
      "Iteration 26232, loss = 0.00713579\n",
      "Iteration 26233, loss = 0.00713559\n",
      "Iteration 26234, loss = 0.00713540\n",
      "Iteration 26235, loss = 0.00713520\n",
      "Iteration 26236, loss = 0.00713500\n",
      "Iteration 26237, loss = 0.00713480\n",
      "Iteration 26238, loss = 0.00713461\n",
      "Iteration 26239, loss = 0.00713441\n",
      "Iteration 26240, loss = 0.00713421\n",
      "Iteration 26241, loss = 0.00713402\n",
      "Iteration 26242, loss = 0.00713382\n",
      "Iteration 26243, loss = 0.00713362\n",
      "Iteration 26244, loss = 0.00713342\n",
      "Iteration 26245, loss = 0.00713323\n",
      "Iteration 26246, loss = 0.00713303\n",
      "Iteration 26247, loss = 0.00713283\n",
      "Iteration 26248, loss = 0.00713264\n",
      "Iteration 26249, loss = 0.00713244\n",
      "Iteration 26250, loss = 0.00713224\n",
      "Iteration 26251, loss = 0.00713205\n",
      "Iteration 26252, loss = 0.00713185\n",
      "Iteration 26253, loss = 0.00713165\n",
      "Iteration 26254, loss = 0.00713145\n",
      "Iteration 26255, loss = 0.00713126\n",
      "Iteration 26256, loss = 0.00713106\n",
      "Iteration 26257, loss = 0.00713086\n",
      "Iteration 26258, loss = 0.00713067\n",
      "Iteration 26259, loss = 0.00713047\n",
      "Iteration 26260, loss = 0.00713027\n",
      "Iteration 26261, loss = 0.00713008\n",
      "Iteration 26262, loss = 0.00712988\n",
      "Iteration 26263, loss = 0.00712968\n",
      "Iteration 26264, loss = 0.00712949\n",
      "Iteration 26265, loss = 0.00712929\n",
      "Iteration 26266, loss = 0.00712909\n",
      "Iteration 26267, loss = 0.00712890\n",
      "Iteration 26268, loss = 0.00712870\n",
      "Iteration 26269, loss = 0.00712850\n",
      "Iteration 26270, loss = 0.00712831\n",
      "Iteration 26271, loss = 0.00712811\n",
      "Iteration 26272, loss = 0.00712791\n",
      "Iteration 26273, loss = 0.00712772\n",
      "Iteration 26274, loss = 0.00712752\n",
      "Iteration 26275, loss = 0.00712732\n",
      "Iteration 26276, loss = 0.00712713\n",
      "Iteration 26277, loss = 0.00712693\n",
      "Iteration 26278, loss = 0.00712674\n",
      "Iteration 26279, loss = 0.00712654\n",
      "Iteration 26280, loss = 0.00712634\n",
      "Iteration 26281, loss = 0.00712615\n",
      "Iteration 26282, loss = 0.00712595\n",
      "Iteration 26283, loss = 0.00712575\n",
      "Iteration 26284, loss = 0.00712556\n",
      "Iteration 26285, loss = 0.00712536\n",
      "Iteration 26286, loss = 0.00712517\n",
      "Iteration 26287, loss = 0.00712497\n",
      "Iteration 26288, loss = 0.00712477\n",
      "Iteration 26289, loss = 0.00712458\n",
      "Iteration 26290, loss = 0.00712438\n",
      "Iteration 26291, loss = 0.00712418\n",
      "Iteration 26292, loss = 0.00712399\n",
      "Iteration 26293, loss = 0.00712379\n",
      "Iteration 26294, loss = 0.00712360\n",
      "Iteration 26295, loss = 0.00712340\n",
      "Iteration 26296, loss = 0.00712320\n",
      "Iteration 26297, loss = 0.00712301\n",
      "Iteration 26298, loss = 0.00712281\n",
      "Iteration 26299, loss = 0.00712262\n",
      "Iteration 26300, loss = 0.00712242\n",
      "Iteration 26301, loss = 0.00712222\n",
      "Iteration 26302, loss = 0.00712203\n",
      "Iteration 26303, loss = 0.00712183\n",
      "Iteration 26304, loss = 0.00712164\n",
      "Iteration 26305, loss = 0.00712144\n",
      "Iteration 26306, loss = 0.00712125\n",
      "Iteration 26307, loss = 0.00712105\n",
      "Iteration 26308, loss = 0.00712085\n",
      "Iteration 26309, loss = 0.00712066\n",
      "Iteration 26310, loss = 0.00712046\n",
      "Iteration 26311, loss = 0.00712027\n",
      "Iteration 26312, loss = 0.00712007\n",
      "Iteration 26313, loss = 0.00711988\n",
      "Iteration 26314, loss = 0.00711968\n",
      "Iteration 26315, loss = 0.00711948\n",
      "Iteration 26316, loss = 0.00711929\n",
      "Iteration 26317, loss = 0.00711909\n",
      "Iteration 26318, loss = 0.00711890\n",
      "Iteration 26319, loss = 0.00711870\n",
      "Iteration 26320, loss = 0.00711851\n",
      "Iteration 26321, loss = 0.00711831\n",
      "Iteration 26322, loss = 0.00711812\n",
      "Iteration 26323, loss = 0.00711792\n",
      "Iteration 26324, loss = 0.00711772\n",
      "Iteration 26325, loss = 0.00711753\n",
      "Iteration 26326, loss = 0.00711733\n",
      "Iteration 26327, loss = 0.00711714\n",
      "Iteration 26328, loss = 0.00711694\n",
      "Iteration 26329, loss = 0.00711675\n",
      "Iteration 26330, loss = 0.00711655\n",
      "Iteration 26331, loss = 0.00711636\n",
      "Iteration 26332, loss = 0.00711616\n",
      "Iteration 26333, loss = 0.00711597\n",
      "Iteration 26334, loss = 0.00711577\n",
      "Iteration 26335, loss = 0.00711558\n",
      "Iteration 26336, loss = 0.00711538\n",
      "Iteration 26337, loss = 0.00711519\n",
      "Iteration 26338, loss = 0.00711499\n",
      "Iteration 26339, loss = 0.00711479\n",
      "Iteration 26340, loss = 0.00711460\n",
      "Iteration 26341, loss = 0.00711440\n",
      "Iteration 26342, loss = 0.00711421\n",
      "Iteration 26343, loss = 0.00711401\n",
      "Iteration 26344, loss = 0.00711382\n",
      "Iteration 26345, loss = 0.00711362\n",
      "Iteration 26346, loss = 0.00711343\n",
      "Iteration 26347, loss = 0.00711323\n",
      "Iteration 26348, loss = 0.00711304\n",
      "Iteration 26349, loss = 0.00711284\n",
      "Iteration 26350, loss = 0.00711265\n",
      "Iteration 26351, loss = 0.00711245\n",
      "Iteration 26352, loss = 0.00711226\n",
      "Iteration 26353, loss = 0.00711207\n",
      "Iteration 26354, loss = 0.00711187\n",
      "Iteration 26355, loss = 0.00711168\n",
      "Iteration 26356, loss = 0.00711148\n",
      "Iteration 26357, loss = 0.00711129\n",
      "Iteration 26358, loss = 0.00711109\n",
      "Iteration 26359, loss = 0.00711090\n",
      "Iteration 26360, loss = 0.00711070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26361, loss = 0.00711051\n",
      "Iteration 26362, loss = 0.00711031\n",
      "Iteration 26363, loss = 0.00711012\n",
      "Iteration 26364, loss = 0.00710992\n",
      "Iteration 26365, loss = 0.00710973\n",
      "Iteration 26366, loss = 0.00710953\n",
      "Iteration 26367, loss = 0.00710934\n",
      "Iteration 26368, loss = 0.00710914\n",
      "Iteration 26369, loss = 0.00710895\n",
      "Iteration 26370, loss = 0.00710876\n",
      "Iteration 26371, loss = 0.00710856\n",
      "Iteration 26372, loss = 0.00710837\n",
      "Iteration 26373, loss = 0.00710817\n",
      "Iteration 26374, loss = 0.00710798\n",
      "Iteration 26375, loss = 0.00710778\n",
      "Iteration 26376, loss = 0.00710759\n",
      "Iteration 26377, loss = 0.00710739\n",
      "Iteration 26378, loss = 0.00710720\n",
      "Iteration 26379, loss = 0.00710701\n",
      "Iteration 26380, loss = 0.00710681\n",
      "Iteration 26381, loss = 0.00710662\n",
      "Iteration 26382, loss = 0.00710642\n",
      "Iteration 26383, loss = 0.00710623\n",
      "Iteration 26384, loss = 0.00710604\n",
      "Iteration 26385, loss = 0.00710584\n",
      "Iteration 26386, loss = 0.00710565\n",
      "Iteration 26387, loss = 0.00710545\n",
      "Iteration 26388, loss = 0.00710526\n",
      "Iteration 26389, loss = 0.00710506\n",
      "Iteration 26390, loss = 0.00710487\n",
      "Iteration 26391, loss = 0.00710468\n",
      "Iteration 26392, loss = 0.00710448\n",
      "Iteration 26393, loss = 0.00710429\n",
      "Iteration 26394, loss = 0.00710409\n",
      "Iteration 26395, loss = 0.00710390\n",
      "Iteration 26396, loss = 0.00710371\n",
      "Iteration 26397, loss = 0.00710351\n",
      "Iteration 26398, loss = 0.00710332\n",
      "Iteration 26399, loss = 0.00710312\n",
      "Iteration 26400, loss = 0.00710293\n",
      "Iteration 26401, loss = 0.00710274\n",
      "Iteration 26402, loss = 0.00710254\n",
      "Iteration 26403, loss = 0.00710235\n",
      "Iteration 26404, loss = 0.00710215\n",
      "Iteration 26405, loss = 0.00710196\n",
      "Iteration 26406, loss = 0.00710177\n",
      "Iteration 26407, loss = 0.00710157\n",
      "Iteration 26408, loss = 0.00710138\n",
      "Iteration 26409, loss = 0.00710119\n",
      "Iteration 26410, loss = 0.00710099\n",
      "Iteration 26411, loss = 0.00710080\n",
      "Iteration 26412, loss = 0.00710061\n",
      "Iteration 26413, loss = 0.00710041\n",
      "Iteration 26414, loss = 0.00710022\n",
      "Iteration 26415, loss = 0.00710002\n",
      "Iteration 26416, loss = 0.00709983\n",
      "Iteration 26417, loss = 0.00709964\n",
      "Iteration 26418, loss = 0.00709944\n",
      "Iteration 26419, loss = 0.00709925\n",
      "Iteration 26420, loss = 0.00709906\n",
      "Iteration 26421, loss = 0.00709886\n",
      "Iteration 26422, loss = 0.00709867\n",
      "Iteration 26423, loss = 0.00709848\n",
      "Iteration 26424, loss = 0.00709828\n",
      "Iteration 26425, loss = 0.00709809\n",
      "Iteration 26426, loss = 0.00709790\n",
      "Iteration 26427, loss = 0.00709770\n",
      "Iteration 26428, loss = 0.00709751\n",
      "Iteration 26429, loss = 0.00709732\n",
      "Iteration 26430, loss = 0.00709712\n",
      "Iteration 26431, loss = 0.00709693\n",
      "Iteration 26432, loss = 0.00709674\n",
      "Iteration 26433, loss = 0.00709654\n",
      "Iteration 26434, loss = 0.00709635\n",
      "Iteration 26435, loss = 0.00709616\n",
      "Iteration 26436, loss = 0.00709596\n",
      "Iteration 26437, loss = 0.00709577\n",
      "Iteration 26438, loss = 0.00709558\n",
      "Iteration 26439, loss = 0.00709538\n",
      "Iteration 26440, loss = 0.00709519\n",
      "Iteration 26441, loss = 0.00709500\n",
      "Iteration 26442, loss = 0.00709480\n",
      "Iteration 26443, loss = 0.00709461\n",
      "Iteration 26444, loss = 0.00709442\n",
      "Iteration 26445, loss = 0.00709423\n",
      "Iteration 26446, loss = 0.00709403\n",
      "Iteration 26447, loss = 0.00709384\n",
      "Iteration 26448, loss = 0.00709365\n",
      "Iteration 26449, loss = 0.00709345\n",
      "Iteration 26450, loss = 0.00709326\n",
      "Iteration 26451, loss = 0.00709307\n",
      "Iteration 26452, loss = 0.00709288\n",
      "Iteration 26453, loss = 0.00709268\n",
      "Iteration 26454, loss = 0.00709249\n",
      "Iteration 26455, loss = 0.00709230\n",
      "Iteration 26456, loss = 0.00709210\n",
      "Iteration 26457, loss = 0.00709191\n",
      "Iteration 26458, loss = 0.00709172\n",
      "Iteration 26459, loss = 0.00709153\n",
      "Iteration 26460, loss = 0.00709133\n",
      "Iteration 26461, loss = 0.00709114\n",
      "Iteration 26462, loss = 0.00709095\n",
      "Iteration 26463, loss = 0.00709076\n",
      "Iteration 26464, loss = 0.00709056\n",
      "Iteration 26465, loss = 0.00709037\n",
      "Iteration 26466, loss = 0.00709018\n",
      "Iteration 26467, loss = 0.00708999\n",
      "Iteration 26468, loss = 0.00708979\n",
      "Iteration 26469, loss = 0.00708960\n",
      "Iteration 26470, loss = 0.00708941\n",
      "Iteration 26471, loss = 0.00708922\n",
      "Iteration 26472, loss = 0.00708902\n",
      "Iteration 26473, loss = 0.00708883\n",
      "Iteration 26474, loss = 0.00708864\n",
      "Iteration 26475, loss = 0.00708845\n",
      "Iteration 26476, loss = 0.00708825\n",
      "Iteration 26477, loss = 0.00708806\n",
      "Iteration 26478, loss = 0.00708787\n",
      "Iteration 26479, loss = 0.00708768\n",
      "Iteration 26480, loss = 0.00708748\n",
      "Iteration 26481, loss = 0.00708729\n",
      "Iteration 26482, loss = 0.00708710\n",
      "Iteration 26483, loss = 0.00708691\n",
      "Iteration 26484, loss = 0.00708671\n",
      "Iteration 26485, loss = 0.00708652\n",
      "Iteration 26486, loss = 0.00708633\n",
      "Iteration 26487, loss = 0.00708614\n",
      "Iteration 26488, loss = 0.00708595\n",
      "Iteration 26489, loss = 0.00708575\n",
      "Iteration 26490, loss = 0.00708556\n",
      "Iteration 26491, loss = 0.00708537\n",
      "Iteration 26492, loss = 0.00708518\n",
      "Iteration 26493, loss = 0.00708499\n",
      "Iteration 26494, loss = 0.00708479\n",
      "Iteration 26495, loss = 0.00708460\n",
      "Iteration 26496, loss = 0.00708441\n",
      "Iteration 26497, loss = 0.00708422\n",
      "Iteration 26498, loss = 0.00708403\n",
      "Iteration 26499, loss = 0.00708383\n",
      "Iteration 26500, loss = 0.00708364\n",
      "Iteration 26501, loss = 0.00708345\n",
      "Iteration 26502, loss = 0.00708326\n",
      "Iteration 26503, loss = 0.00708307\n",
      "Iteration 26504, loss = 0.00708287\n",
      "Iteration 26505, loss = 0.00708268\n",
      "Iteration 26506, loss = 0.00708249\n",
      "Iteration 26507, loss = 0.00708230\n",
      "Iteration 26508, loss = 0.00708211\n",
      "Iteration 26509, loss = 0.00708192\n",
      "Iteration 26510, loss = 0.00708172\n",
      "Iteration 26511, loss = 0.00708153\n",
      "Iteration 26512, loss = 0.00708134\n",
      "Iteration 26513, loss = 0.00708115\n",
      "Iteration 26514, loss = 0.00708096\n",
      "Iteration 26515, loss = 0.00708077\n",
      "Iteration 26516, loss = 0.00708057\n",
      "Iteration 26517, loss = 0.00708038\n",
      "Iteration 26518, loss = 0.00708019\n",
      "Iteration 26519, loss = 0.00708000\n",
      "Iteration 26520, loss = 0.00707981\n",
      "Iteration 26521, loss = 0.00707962\n",
      "Iteration 26522, loss = 0.00707943\n",
      "Iteration 26523, loss = 0.00707923\n",
      "Iteration 26524, loss = 0.00707904\n",
      "Iteration 26525, loss = 0.00707885\n",
      "Iteration 26526, loss = 0.00707866\n",
      "Iteration 26527, loss = 0.00707847\n",
      "Iteration 26528, loss = 0.00707828\n",
      "Iteration 26529, loss = 0.00707809\n",
      "Iteration 26530, loss = 0.00707789\n",
      "Iteration 26531, loss = 0.00707770\n",
      "Iteration 26532, loss = 0.00707751\n",
      "Iteration 26533, loss = 0.00707732\n",
      "Iteration 26534, loss = 0.00707713\n",
      "Iteration 26535, loss = 0.00707694\n",
      "Iteration 26536, loss = 0.00707675\n",
      "Iteration 26537, loss = 0.00707656\n",
      "Iteration 26538, loss = 0.00707636\n",
      "Iteration 26539, loss = 0.00707617\n",
      "Iteration 26540, loss = 0.00707598\n",
      "Iteration 26541, loss = 0.00707579\n",
      "Iteration 26542, loss = 0.00707560\n",
      "Iteration 26543, loss = 0.00707541\n",
      "Iteration 26544, loss = 0.00707522\n",
      "Iteration 26545, loss = 0.00707503\n",
      "Iteration 26546, loss = 0.00707484\n",
      "Iteration 26547, loss = 0.00707465\n",
      "Iteration 26548, loss = 0.00707445\n",
      "Iteration 26549, loss = 0.00707426\n",
      "Iteration 26550, loss = 0.00707407\n",
      "Iteration 26551, loss = 0.00707388\n",
      "Iteration 26552, loss = 0.00707369\n",
      "Iteration 26553, loss = 0.00707350\n",
      "Iteration 26554, loss = 0.00707331\n",
      "Iteration 26555, loss = 0.00707312\n",
      "Iteration 26556, loss = 0.00707293\n",
      "Iteration 26557, loss = 0.00707274\n",
      "Iteration 26558, loss = 0.00707255\n",
      "Iteration 26559, loss = 0.00707236\n",
      "Iteration 26560, loss = 0.00707216\n",
      "Iteration 26561, loss = 0.00707197\n",
      "Iteration 26562, loss = 0.00707178\n",
      "Iteration 26563, loss = 0.00707159\n",
      "Iteration 26564, loss = 0.00707140\n",
      "Iteration 26565, loss = 0.00707121\n",
      "Iteration 26566, loss = 0.00707102\n",
      "Iteration 26567, loss = 0.00707083\n",
      "Iteration 26568, loss = 0.00707064\n",
      "Iteration 26569, loss = 0.00707045\n",
      "Iteration 26570, loss = 0.00707026\n",
      "Iteration 26571, loss = 0.00707007\n",
      "Iteration 26572, loss = 0.00706988\n",
      "Iteration 26573, loss = 0.00706969\n",
      "Iteration 26574, loss = 0.00706950\n",
      "Iteration 26575, loss = 0.00706931\n",
      "Iteration 26576, loss = 0.00706912\n",
      "Iteration 26577, loss = 0.00706893\n",
      "Iteration 26578, loss = 0.00706874\n",
      "Iteration 26579, loss = 0.00706855\n",
      "Iteration 26580, loss = 0.00706836\n",
      "Iteration 26581, loss = 0.00706816\n",
      "Iteration 26582, loss = 0.00706797\n",
      "Iteration 26583, loss = 0.00706778\n",
      "Iteration 26584, loss = 0.00706759\n",
      "Iteration 26585, loss = 0.00706740\n",
      "Iteration 26586, loss = 0.00706721\n",
      "Iteration 26587, loss = 0.00706702\n",
      "Iteration 26588, loss = 0.00706683\n",
      "Iteration 26589, loss = 0.00706664\n",
      "Iteration 26590, loss = 0.00706645\n",
      "Iteration 26591, loss = 0.00706626\n",
      "Iteration 26592, loss = 0.00706607\n",
      "Iteration 26593, loss = 0.00706588\n",
      "Iteration 26594, loss = 0.00706569\n",
      "Iteration 26595, loss = 0.00706550\n",
      "Iteration 26596, loss = 0.00706531\n",
      "Iteration 26597, loss = 0.00706512\n",
      "Iteration 26598, loss = 0.00706493\n",
      "Iteration 26599, loss = 0.00706474\n",
      "Iteration 26600, loss = 0.00706455\n",
      "Iteration 26601, loss = 0.00706436\n",
      "Iteration 26602, loss = 0.00706417\n",
      "Iteration 26603, loss = 0.00706398\n",
      "Iteration 26604, loss = 0.00706379\n",
      "Iteration 26605, loss = 0.00706360\n",
      "Iteration 26606, loss = 0.00706341\n",
      "Iteration 26607, loss = 0.00706322\n",
      "Iteration 26608, loss = 0.00706303\n",
      "Iteration 26609, loss = 0.00706284\n",
      "Iteration 26610, loss = 0.00706265\n",
      "Iteration 26611, loss = 0.00706247\n",
      "Iteration 26612, loss = 0.00706228\n",
      "Iteration 26613, loss = 0.00706209\n",
      "Iteration 26614, loss = 0.00706190\n",
      "Iteration 26615, loss = 0.00706171\n",
      "Iteration 26616, loss = 0.00706152\n",
      "Iteration 26617, loss = 0.00706133\n",
      "Iteration 26618, loss = 0.00706114\n",
      "Iteration 26619, loss = 0.00706095\n",
      "Iteration 26620, loss = 0.00706076\n",
      "Iteration 26621, loss = 0.00706057\n",
      "Iteration 26622, loss = 0.00706038\n",
      "Iteration 26623, loss = 0.00706019\n",
      "Iteration 26624, loss = 0.00706000\n",
      "Iteration 26625, loss = 0.00705981\n",
      "Iteration 26626, loss = 0.00705962\n",
      "Iteration 26627, loss = 0.00705943\n",
      "Iteration 26628, loss = 0.00705924\n",
      "Iteration 26629, loss = 0.00705905\n",
      "Iteration 26630, loss = 0.00705886\n",
      "Iteration 26631, loss = 0.00705868\n",
      "Iteration 26632, loss = 0.00705849\n",
      "Iteration 26633, loss = 0.00705830\n",
      "Iteration 26634, loss = 0.00705811\n",
      "Iteration 26635, loss = 0.00705792\n",
      "Iteration 26636, loss = 0.00705773\n",
      "Iteration 26637, loss = 0.00705754\n",
      "Iteration 26638, loss = 0.00705735\n",
      "Iteration 26639, loss = 0.00705716\n",
      "Iteration 26640, loss = 0.00705697\n",
      "Iteration 26641, loss = 0.00705678\n",
      "Iteration 26642, loss = 0.00705659\n",
      "Iteration 26643, loss = 0.00705641\n",
      "Iteration 26644, loss = 0.00705622\n",
      "Iteration 26645, loss = 0.00705603\n",
      "Iteration 26646, loss = 0.00705584\n",
      "Iteration 26647, loss = 0.00705565\n",
      "Iteration 26648, loss = 0.00705546\n",
      "Iteration 26649, loss = 0.00705527\n",
      "Iteration 26650, loss = 0.00705508\n",
      "Iteration 26651, loss = 0.00705489\n",
      "Iteration 26652, loss = 0.00705470\n",
      "Iteration 26653, loss = 0.00705452\n",
      "Iteration 26654, loss = 0.00705433\n",
      "Iteration 26655, loss = 0.00705414\n",
      "Iteration 26656, loss = 0.00705395\n",
      "Iteration 26657, loss = 0.00705376\n",
      "Iteration 26658, loss = 0.00705357\n",
      "Iteration 26659, loss = 0.00705338\n",
      "Iteration 26660, loss = 0.00705319\n",
      "Iteration 26661, loss = 0.00705301\n",
      "Iteration 26662, loss = 0.00705282\n",
      "Iteration 26663, loss = 0.00705263\n",
      "Iteration 26664, loss = 0.00705244\n",
      "Iteration 26665, loss = 0.00705225\n",
      "Iteration 26666, loss = 0.00705206\n",
      "Iteration 26667, loss = 0.00705187\n",
      "Iteration 26668, loss = 0.00705168\n",
      "Iteration 26669, loss = 0.00705150\n",
      "Iteration 26670, loss = 0.00705131\n",
      "Iteration 26671, loss = 0.00705112\n",
      "Iteration 26672, loss = 0.00705093\n",
      "Iteration 26673, loss = 0.00705074\n",
      "Iteration 26674, loss = 0.00705055\n",
      "Iteration 26675, loss = 0.00705037\n",
      "Iteration 26676, loss = 0.00705018\n",
      "Iteration 26677, loss = 0.00704999\n",
      "Iteration 26678, loss = 0.00704980\n",
      "Iteration 26679, loss = 0.00704961\n",
      "Iteration 26680, loss = 0.00704942\n",
      "Iteration 26681, loss = 0.00704923\n",
      "Iteration 26682, loss = 0.00704905\n",
      "Iteration 26683, loss = 0.00704886\n",
      "Iteration 26684, loss = 0.00704867\n",
      "Iteration 26685, loss = 0.00704848\n",
      "Iteration 26686, loss = 0.00704829\n",
      "Iteration 26687, loss = 0.00704811\n",
      "Iteration 26688, loss = 0.00704792\n",
      "Iteration 26689, loss = 0.00704773\n",
      "Iteration 26690, loss = 0.00704754\n",
      "Iteration 26691, loss = 0.00704735\n",
      "Iteration 26692, loss = 0.00704716\n",
      "Iteration 26693, loss = 0.00704698\n",
      "Iteration 26694, loss = 0.00704679\n",
      "Iteration 26695, loss = 0.00704660\n",
      "Iteration 26696, loss = 0.00704641\n",
      "Iteration 26697, loss = 0.00704622\n",
      "Iteration 26698, loss = 0.00704604\n",
      "Iteration 26699, loss = 0.00704585\n",
      "Iteration 26700, loss = 0.00704566\n",
      "Iteration 26701, loss = 0.00704547\n",
      "Iteration 26702, loss = 0.00704528\n",
      "Iteration 26703, loss = 0.00704510\n",
      "Iteration 26704, loss = 0.00704491\n",
      "Iteration 26705, loss = 0.00704472\n",
      "Iteration 26706, loss = 0.00704453\n",
      "Iteration 26707, loss = 0.00704435\n",
      "Iteration 26708, loss = 0.00704416\n",
      "Iteration 26709, loss = 0.00704397\n",
      "Iteration 26710, loss = 0.00704378\n",
      "Iteration 26711, loss = 0.00704359\n",
      "Iteration 26712, loss = 0.00704341\n",
      "Iteration 26713, loss = 0.00704322\n",
      "Iteration 26714, loss = 0.00704303\n",
      "Iteration 26715, loss = 0.00704284\n",
      "Iteration 26716, loss = 0.00704266\n",
      "Iteration 26717, loss = 0.00704247\n",
      "Iteration 26718, loss = 0.00704228\n",
      "Iteration 26719, loss = 0.00704209\n",
      "Iteration 26720, loss = 0.00704191\n",
      "Iteration 26721, loss = 0.00704172\n",
      "Iteration 26722, loss = 0.00704153\n",
      "Iteration 26723, loss = 0.00704134\n",
      "Iteration 26724, loss = 0.00704115\n",
      "Iteration 26725, loss = 0.00704097\n",
      "Iteration 26726, loss = 0.00704078\n",
      "Iteration 26727, loss = 0.00704059\n",
      "Iteration 26728, loss = 0.00704041\n",
      "Iteration 26729, loss = 0.00704022\n",
      "Iteration 26730, loss = 0.00704003\n",
      "Iteration 26731, loss = 0.00703984\n",
      "Iteration 26732, loss = 0.00703966\n",
      "Iteration 26733, loss = 0.00703947\n",
      "Iteration 26734, loss = 0.00703928\n",
      "Iteration 26735, loss = 0.00703909\n",
      "Iteration 26736, loss = 0.00703891\n",
      "Iteration 26737, loss = 0.00703872\n",
      "Iteration 26738, loss = 0.00703853\n",
      "Iteration 26739, loss = 0.00703834\n",
      "Iteration 26740, loss = 0.00703816\n",
      "Iteration 26741, loss = 0.00703797\n",
      "Iteration 26742, loss = 0.00703778\n",
      "Iteration 26743, loss = 0.00703760\n",
      "Iteration 26744, loss = 0.00703741\n",
      "Iteration 26745, loss = 0.00703722\n",
      "Iteration 26746, loss = 0.00703703\n",
      "Iteration 26747, loss = 0.00703685\n",
      "Iteration 26748, loss = 0.00703666\n",
      "Iteration 26749, loss = 0.00703647\n",
      "Iteration 26750, loss = 0.00703629\n",
      "Iteration 26751, loss = 0.00703610\n",
      "Iteration 26752, loss = 0.00703591\n",
      "Iteration 26753, loss = 0.00703573\n",
      "Iteration 26754, loss = 0.00703554\n",
      "Iteration 26755, loss = 0.00703535\n",
      "Iteration 26756, loss = 0.00703517\n",
      "Iteration 26757, loss = 0.00703498\n",
      "Iteration 26758, loss = 0.00703479\n",
      "Iteration 26759, loss = 0.00703460\n",
      "Iteration 26760, loss = 0.00703442\n",
      "Iteration 26761, loss = 0.00703423\n",
      "Iteration 26762, loss = 0.00703404\n",
      "Iteration 26763, loss = 0.00703386\n",
      "Iteration 26764, loss = 0.00703367\n",
      "Iteration 26765, loss = 0.00703348\n",
      "Iteration 26766, loss = 0.00703330\n",
      "Iteration 26767, loss = 0.00703311\n",
      "Iteration 26768, loss = 0.00703292\n",
      "Iteration 26769, loss = 0.00703274\n",
      "Iteration 26770, loss = 0.00703255\n",
      "Iteration 26771, loss = 0.00703236\n",
      "Iteration 26772, loss = 0.00703218\n",
      "Iteration 26773, loss = 0.00703199\n",
      "Iteration 26774, loss = 0.00703180\n",
      "Iteration 26775, loss = 0.00703162\n",
      "Iteration 26776, loss = 0.00703143\n",
      "Iteration 26777, loss = 0.00703124\n",
      "Iteration 26778, loss = 0.00703106\n",
      "Iteration 26779, loss = 0.00703087\n",
      "Iteration 26780, loss = 0.00703069\n",
      "Iteration 26781, loss = 0.00703050\n",
      "Iteration 26782, loss = 0.00703031\n",
      "Iteration 26783, loss = 0.00703013\n",
      "Iteration 26784, loss = 0.00702994\n",
      "Iteration 26785, loss = 0.00702975\n",
      "Iteration 26786, loss = 0.00702957\n",
      "Iteration 26787, loss = 0.00702938\n",
      "Iteration 26788, loss = 0.00702919\n",
      "Iteration 26789, loss = 0.00702901\n",
      "Iteration 26790, loss = 0.00702882\n",
      "Iteration 26791, loss = 0.00702864\n",
      "Iteration 26792, loss = 0.00702845\n",
      "Iteration 26793, loss = 0.00702826\n",
      "Iteration 26794, loss = 0.00702808\n",
      "Iteration 26795, loss = 0.00702789\n",
      "Iteration 26796, loss = 0.00702771\n",
      "Iteration 26797, loss = 0.00702752\n",
      "Iteration 26798, loss = 0.00702733\n",
      "Iteration 26799, loss = 0.00702715\n",
      "Iteration 26800, loss = 0.00702696\n",
      "Iteration 26801, loss = 0.00702677\n",
      "Iteration 26802, loss = 0.00702659\n",
      "Iteration 26803, loss = 0.00702640\n",
      "Iteration 26804, loss = 0.00702622\n",
      "Iteration 26805, loss = 0.00702603\n",
      "Iteration 26806, loss = 0.00702585\n",
      "Iteration 26807, loss = 0.00702566\n",
      "Iteration 26808, loss = 0.00702547\n",
      "Iteration 26809, loss = 0.00702529\n",
      "Iteration 26810, loss = 0.00702510\n",
      "Iteration 26811, loss = 0.00702492\n",
      "Iteration 26812, loss = 0.00702473\n",
      "Iteration 26813, loss = 0.00702454\n",
      "Iteration 26814, loss = 0.00702436\n",
      "Iteration 26815, loss = 0.00702417\n",
      "Iteration 26816, loss = 0.00702399\n",
      "Iteration 26817, loss = 0.00702380\n",
      "Iteration 26818, loss = 0.00702362\n",
      "Iteration 26819, loss = 0.00702343\n",
      "Iteration 26820, loss = 0.00702324\n",
      "Iteration 26821, loss = 0.00702306\n",
      "Iteration 26822, loss = 0.00702287\n",
      "Iteration 26823, loss = 0.00702269\n",
      "Iteration 26824, loss = 0.00702250\n",
      "Iteration 26825, loss = 0.00702232\n",
      "Iteration 26826, loss = 0.00702213\n",
      "Iteration 26827, loss = 0.00702195\n",
      "Iteration 26828, loss = 0.00702176\n",
      "Iteration 26829, loss = 0.00702157\n",
      "Iteration 26830, loss = 0.00702139\n",
      "Iteration 26831, loss = 0.00702120\n",
      "Iteration 26832, loss = 0.00702102\n",
      "Iteration 26833, loss = 0.00702083\n",
      "Iteration 26834, loss = 0.00702065\n",
      "Iteration 26835, loss = 0.00702046\n",
      "Iteration 26836, loss = 0.00702028\n",
      "Iteration 26837, loss = 0.00702009\n",
      "Iteration 26838, loss = 0.00701991\n",
      "Iteration 26839, loss = 0.00701972\n",
      "Iteration 26840, loss = 0.00701953\n",
      "Iteration 26841, loss = 0.00701935\n",
      "Iteration 26842, loss = 0.00701916\n",
      "Iteration 26843, loss = 0.00701898\n",
      "Iteration 26844, loss = 0.00701879\n",
      "Iteration 26845, loss = 0.00701861\n",
      "Iteration 26846, loss = 0.00701842\n",
      "Iteration 26847, loss = 0.00701824\n",
      "Iteration 26848, loss = 0.00701805\n",
      "Iteration 26849, loss = 0.00701787\n",
      "Iteration 26850, loss = 0.00701768\n",
      "Iteration 26851, loss = 0.00701750\n",
      "Iteration 26852, loss = 0.00701731\n",
      "Iteration 26853, loss = 0.00701713\n",
      "Iteration 26854, loss = 0.00701694\n",
      "Iteration 26855, loss = 0.00701676\n",
      "Iteration 26856, loss = 0.00701657\n",
      "Iteration 26857, loss = 0.00701639\n",
      "Iteration 26858, loss = 0.00701620\n",
      "Iteration 26859, loss = 0.00701602\n",
      "Iteration 26860, loss = 0.00701583\n",
      "Iteration 26861, loss = 0.00701565\n",
      "Iteration 26862, loss = 0.00701546\n",
      "Iteration 26863, loss = 0.00701528\n",
      "Iteration 26864, loss = 0.00701509\n",
      "Iteration 26865, loss = 0.00701491\n",
      "Iteration 26866, loss = 0.00701472\n",
      "Iteration 26867, loss = 0.00701454\n",
      "Iteration 26868, loss = 0.00701435\n",
      "Iteration 26869, loss = 0.00701417\n",
      "Iteration 26870, loss = 0.00701399\n",
      "Iteration 26871, loss = 0.00701380\n",
      "Iteration 26872, loss = 0.00701362\n",
      "Iteration 26873, loss = 0.00701343\n",
      "Iteration 26874, loss = 0.00701325\n",
      "Iteration 26875, loss = 0.00701306\n",
      "Iteration 26876, loss = 0.00701288\n",
      "Iteration 26877, loss = 0.00701269\n",
      "Iteration 26878, loss = 0.00701251\n",
      "Iteration 26879, loss = 0.00701232\n",
      "Iteration 26880, loss = 0.00701214\n",
      "Iteration 26881, loss = 0.00701195\n",
      "Iteration 26882, loss = 0.00701177\n",
      "Iteration 26883, loss = 0.00701159\n",
      "Iteration 26884, loss = 0.00701140\n",
      "Iteration 26885, loss = 0.00701122\n",
      "Iteration 26886, loss = 0.00701103\n",
      "Iteration 26887, loss = 0.00701085\n",
      "Iteration 26888, loss = 0.00701066\n",
      "Iteration 26889, loss = 0.00701048\n",
      "Iteration 26890, loss = 0.00701029\n",
      "Iteration 26891, loss = 0.00701011\n",
      "Iteration 26892, loss = 0.00700993\n",
      "Iteration 26893, loss = 0.00700974\n",
      "Iteration 26894, loss = 0.00700956\n",
      "Iteration 26895, loss = 0.00700937\n",
      "Iteration 26896, loss = 0.00700919\n",
      "Iteration 26897, loss = 0.00700901\n",
      "Iteration 26898, loss = 0.00700882\n",
      "Iteration 26899, loss = 0.00700864\n",
      "Iteration 26900, loss = 0.00700845\n",
      "Iteration 26901, loss = 0.00700827\n",
      "Iteration 26902, loss = 0.00700808\n",
      "Iteration 26903, loss = 0.00700790\n",
      "Iteration 26904, loss = 0.00700772\n",
      "Iteration 26905, loss = 0.00700753\n",
      "Iteration 26906, loss = 0.00700735\n",
      "Iteration 26907, loss = 0.00700716\n",
      "Iteration 26908, loss = 0.00700698\n",
      "Iteration 26909, loss = 0.00700680\n",
      "Iteration 26910, loss = 0.00700661\n",
      "Iteration 26911, loss = 0.00700643\n",
      "Iteration 26912, loss = 0.00700624\n",
      "Iteration 26913, loss = 0.00700606\n",
      "Iteration 26914, loss = 0.00700588\n",
      "Iteration 26915, loss = 0.00700569\n",
      "Iteration 26916, loss = 0.00700551\n",
      "Iteration 26917, loss = 0.00700532\n",
      "Iteration 26918, loss = 0.00700514\n",
      "Iteration 26919, loss = 0.00700496\n",
      "Iteration 26920, loss = 0.00700477\n",
      "Iteration 26921, loss = 0.00700459\n",
      "Iteration 26922, loss = 0.00700441\n",
      "Iteration 26923, loss = 0.00700422\n",
      "Iteration 26924, loss = 0.00700404\n",
      "Iteration 26925, loss = 0.00700385\n",
      "Iteration 26926, loss = 0.00700367\n",
      "Iteration 26927, loss = 0.00700349\n",
      "Iteration 26928, loss = 0.00700330\n",
      "Iteration 26929, loss = 0.00700312\n",
      "Iteration 26930, loss = 0.00700294\n",
      "Iteration 26931, loss = 0.00700275\n",
      "Iteration 26932, loss = 0.00700257\n",
      "Iteration 26933, loss = 0.00700239\n",
      "Iteration 26934, loss = 0.00700220\n",
      "Iteration 26935, loss = 0.00700202\n",
      "Iteration 26936, loss = 0.00700184\n",
      "Iteration 26937, loss = 0.00700165\n",
      "Iteration 26938, loss = 0.00700147\n",
      "Iteration 26939, loss = 0.00700129\n",
      "Iteration 26940, loss = 0.00700110\n",
      "Iteration 26941, loss = 0.00700092\n",
      "Iteration 26942, loss = 0.00700074\n",
      "Iteration 26943, loss = 0.00700055\n",
      "Iteration 26944, loss = 0.00700037\n",
      "Iteration 26945, loss = 0.00700019\n",
      "Iteration 26946, loss = 0.00700000\n",
      "Iteration 26947, loss = 0.00699982\n",
      "Iteration 26948, loss = 0.00699964\n",
      "Iteration 26949, loss = 0.00699945\n",
      "Iteration 26950, loss = 0.00699927\n",
      "Iteration 26951, loss = 0.00699909\n",
      "Iteration 26952, loss = 0.00699890\n",
      "Iteration 26953, loss = 0.00699872\n",
      "Iteration 26954, loss = 0.00699854\n",
      "Iteration 26955, loss = 0.00699835\n",
      "Iteration 26956, loss = 0.00699817\n",
      "Iteration 26957, loss = 0.00699799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26958, loss = 0.00699780\n",
      "Iteration 26959, loss = 0.00699762\n",
      "Iteration 26960, loss = 0.00699744\n",
      "Iteration 26961, loss = 0.00699725\n",
      "Iteration 26962, loss = 0.00699707\n",
      "Iteration 26963, loss = 0.00699689\n",
      "Iteration 26964, loss = 0.00699671\n",
      "Iteration 26965, loss = 0.00699652\n",
      "Iteration 26966, loss = 0.00699634\n",
      "Iteration 26967, loss = 0.00699616\n",
      "Iteration 26968, loss = 0.00699597\n",
      "Iteration 26969, loss = 0.00699579\n",
      "Iteration 26970, loss = 0.00699561\n",
      "Iteration 26971, loss = 0.00699543\n",
      "Iteration 26972, loss = 0.00699524\n",
      "Iteration 26973, loss = 0.00699506\n",
      "Iteration 26974, loss = 0.00699488\n",
      "Iteration 26975, loss = 0.00699469\n",
      "Iteration 26976, loss = 0.00699451\n",
      "Iteration 26977, loss = 0.00699433\n",
      "Iteration 26978, loss = 0.00699415\n",
      "Iteration 26979, loss = 0.00699396\n",
      "Iteration 26980, loss = 0.00699378\n",
      "Iteration 26981, loss = 0.00699360\n",
      "Iteration 26982, loss = 0.00699342\n",
      "Iteration 26983, loss = 0.00699323\n",
      "Iteration 26984, loss = 0.00699305\n",
      "Iteration 26985, loss = 0.00699287\n",
      "Iteration 26986, loss = 0.00699269\n",
      "Iteration 26987, loss = 0.00699250\n",
      "Iteration 26988, loss = 0.00699232\n",
      "Iteration 26989, loss = 0.00699214\n",
      "Iteration 26990, loss = 0.00699196\n",
      "Iteration 26991, loss = 0.00699177\n",
      "Iteration 26992, loss = 0.00699159\n",
      "Iteration 26993, loss = 0.00699141\n",
      "Iteration 26994, loss = 0.00699123\n",
      "Iteration 26995, loss = 0.00699104\n",
      "Iteration 26996, loss = 0.00699086\n",
      "Iteration 26997, loss = 0.00699068\n",
      "Iteration 26998, loss = 0.00699050\n",
      "Iteration 26999, loss = 0.00699031\n",
      "Iteration 27000, loss = 0.00699013\n",
      "Iteration 27001, loss = 0.00698995\n",
      "Iteration 27002, loss = 0.00698977\n",
      "Iteration 27003, loss = 0.00698959\n",
      "Iteration 27004, loss = 0.00698940\n",
      "Iteration 27005, loss = 0.00698922\n",
      "Iteration 27006, loss = 0.00698904\n",
      "Iteration 27007, loss = 0.00698886\n",
      "Iteration 27008, loss = 0.00698867\n",
      "Iteration 27009, loss = 0.00698849\n",
      "Iteration 27010, loss = 0.00698831\n",
      "Iteration 27011, loss = 0.00698813\n",
      "Iteration 27012, loss = 0.00698795\n",
      "Iteration 27013, loss = 0.00698776\n",
      "Iteration 27014, loss = 0.00698758\n",
      "Iteration 27015, loss = 0.00698740\n",
      "Iteration 27016, loss = 0.00698722\n",
      "Iteration 27017, loss = 0.00698704\n",
      "Iteration 27018, loss = 0.00698685\n",
      "Iteration 27019, loss = 0.00698667\n",
      "Iteration 27020, loss = 0.00698649\n",
      "Iteration 27021, loss = 0.00698631\n",
      "Iteration 27022, loss = 0.00698613\n",
      "Iteration 27023, loss = 0.00698594\n",
      "Iteration 27024, loss = 0.00698576\n",
      "Iteration 27025, loss = 0.00698558\n",
      "Iteration 27026, loss = 0.00698540\n",
      "Iteration 27027, loss = 0.00698522\n",
      "Iteration 27028, loss = 0.00698504\n",
      "Iteration 27029, loss = 0.00698485\n",
      "Iteration 27030, loss = 0.00698467\n",
      "Iteration 27031, loss = 0.00698449\n",
      "Iteration 27032, loss = 0.00698431\n",
      "Iteration 27033, loss = 0.00698413\n",
      "Iteration 27034, loss = 0.00698395\n",
      "Iteration 27035, loss = 0.00698376\n",
      "Iteration 27036, loss = 0.00698358\n",
      "Iteration 27037, loss = 0.00698340\n",
      "Iteration 27038, loss = 0.00698322\n",
      "Iteration 27039, loss = 0.00698304\n",
      "Iteration 27040, loss = 0.00698286\n",
      "Iteration 27041, loss = 0.00698267\n",
      "Iteration 27042, loss = 0.00698249\n",
      "Iteration 27043, loss = 0.00698231\n",
      "Iteration 27044, loss = 0.00698213\n",
      "Iteration 27045, loss = 0.00698195\n",
      "Iteration 27046, loss = 0.00698177\n",
      "Iteration 27047, loss = 0.00698159\n",
      "Iteration 27048, loss = 0.00698140\n",
      "Iteration 27049, loss = 0.00698122\n",
      "Iteration 27050, loss = 0.00698104\n",
      "Iteration 27051, loss = 0.00698086\n",
      "Iteration 27052, loss = 0.00698068\n",
      "Iteration 27053, loss = 0.00698050\n",
      "Iteration 27054, loss = 0.00698032\n",
      "Iteration 27055, loss = 0.00698014\n",
      "Iteration 27056, loss = 0.00697995\n",
      "Iteration 27057, loss = 0.00697977\n",
      "Iteration 27058, loss = 0.00697959\n",
      "Iteration 27059, loss = 0.00697941\n",
      "Iteration 27060, loss = 0.00697923\n",
      "Iteration 27061, loss = 0.00697905\n",
      "Iteration 27062, loss = 0.00697887\n",
      "Iteration 27063, loss = 0.00697869\n",
      "Iteration 27064, loss = 0.00697850\n",
      "Iteration 27065, loss = 0.00697832\n",
      "Iteration 27066, loss = 0.00697814\n",
      "Iteration 27067, loss = 0.00697796\n",
      "Iteration 27068, loss = 0.00697778\n",
      "Iteration 27069, loss = 0.00697760\n",
      "Iteration 27070, loss = 0.00697742\n",
      "Iteration 27071, loss = 0.00697724\n",
      "Iteration 27072, loss = 0.00697706\n",
      "Iteration 27073, loss = 0.00697688\n",
      "Iteration 27074, loss = 0.00697669\n",
      "Iteration 27075, loss = 0.00697651\n",
      "Iteration 27076, loss = 0.00697633\n",
      "Iteration 27077, loss = 0.00697615\n",
      "Iteration 27078, loss = 0.00697597\n",
      "Iteration 27079, loss = 0.00697579\n",
      "Iteration 27080, loss = 0.00697561\n",
      "Iteration 27081, loss = 0.00697543\n",
      "Iteration 27082, loss = 0.00697525\n",
      "Iteration 27083, loss = 0.00697507\n",
      "Iteration 27084, loss = 0.00697489\n",
      "Iteration 27085, loss = 0.00697471\n",
      "Iteration 27086, loss = 0.00697453\n",
      "Iteration 27087, loss = 0.00697434\n",
      "Iteration 27088, loss = 0.00697416\n",
      "Iteration 27089, loss = 0.00697398\n",
      "Iteration 27090, loss = 0.00697380\n",
      "Iteration 27091, loss = 0.00697362\n",
      "Iteration 27092, loss = 0.00697344\n",
      "Iteration 27093, loss = 0.00697326\n",
      "Iteration 27094, loss = 0.00697308\n",
      "Iteration 27095, loss = 0.00697290\n",
      "Iteration 27096, loss = 0.00697272\n",
      "Iteration 27097, loss = 0.00697254\n",
      "Iteration 27098, loss = 0.00697236\n",
      "Iteration 27099, loss = 0.00697218\n",
      "Iteration 27100, loss = 0.00697200\n",
      "Iteration 27101, loss = 0.00697182\n",
      "Iteration 27102, loss = 0.00697164\n",
      "Iteration 27103, loss = 0.00697146\n",
      "Iteration 27104, loss = 0.00697128\n",
      "Iteration 27105, loss = 0.00697110\n",
      "Iteration 27106, loss = 0.00697092\n",
      "Iteration 27107, loss = 0.00697073\n",
      "Iteration 27108, loss = 0.00697055\n",
      "Iteration 27109, loss = 0.00697037\n",
      "Iteration 27110, loss = 0.00697019\n",
      "Iteration 27111, loss = 0.00697001\n",
      "Iteration 27112, loss = 0.00696983\n",
      "Iteration 27113, loss = 0.00696965\n",
      "Iteration 27114, loss = 0.00696947\n",
      "Iteration 27115, loss = 0.00696929\n",
      "Iteration 27116, loss = 0.00696911\n",
      "Iteration 27117, loss = 0.00696893\n",
      "Iteration 27118, loss = 0.00696875\n",
      "Iteration 27119, loss = 0.00696857\n",
      "Iteration 27120, loss = 0.00696839\n",
      "Iteration 27121, loss = 0.00696821\n",
      "Iteration 27122, loss = 0.00696803\n",
      "Iteration 27123, loss = 0.00696785\n",
      "Iteration 27124, loss = 0.00696767\n",
      "Iteration 27125, loss = 0.00696749\n",
      "Iteration 27126, loss = 0.00696731\n",
      "Iteration 27127, loss = 0.00696713\n",
      "Iteration 27128, loss = 0.00696695\n",
      "Iteration 27129, loss = 0.00696677\n",
      "Iteration 27130, loss = 0.00696659\n",
      "Iteration 27131, loss = 0.00696641\n",
      "Iteration 27132, loss = 0.00696623\n",
      "Iteration 27133, loss = 0.00696605\n",
      "Iteration 27134, loss = 0.00696587\n",
      "Iteration 27135, loss = 0.00696569\n",
      "Iteration 27136, loss = 0.00696551\n",
      "Iteration 27137, loss = 0.00696533\n",
      "Iteration 27138, loss = 0.00696515\n",
      "Iteration 27139, loss = 0.00696497\n",
      "Iteration 27140, loss = 0.00696479\n",
      "Iteration 27141, loss = 0.00696462\n",
      "Iteration 27142, loss = 0.00696444\n",
      "Iteration 27143, loss = 0.00696426\n",
      "Iteration 27144, loss = 0.00696408\n",
      "Iteration 27145, loss = 0.00696390\n",
      "Iteration 27146, loss = 0.00696372\n",
      "Iteration 27147, loss = 0.00696354\n",
      "Iteration 27148, loss = 0.00696336\n",
      "Iteration 27149, loss = 0.00696318\n",
      "Iteration 27150, loss = 0.00696300\n",
      "Iteration 27151, loss = 0.00696282\n",
      "Iteration 27152, loss = 0.00696264\n",
      "Iteration 27153, loss = 0.00696246\n",
      "Iteration 27154, loss = 0.00696228\n",
      "Iteration 27155, loss = 0.00696210\n",
      "Iteration 27156, loss = 0.00696192\n",
      "Iteration 27157, loss = 0.00696174\n",
      "Iteration 27158, loss = 0.00696156\n",
      "Iteration 27159, loss = 0.00696138\n",
      "Iteration 27160, loss = 0.00696120\n",
      "Iteration 27161, loss = 0.00696103\n",
      "Iteration 27162, loss = 0.00696085\n",
      "Iteration 27163, loss = 0.00696067\n",
      "Iteration 27164, loss = 0.00696049\n",
      "Iteration 27165, loss = 0.00696031\n",
      "Iteration 27166, loss = 0.00696013\n",
      "Iteration 27167, loss = 0.00695995\n",
      "Iteration 27168, loss = 0.00695977\n",
      "Iteration 27169, loss = 0.00695959\n",
      "Iteration 27170, loss = 0.00695941\n",
      "Iteration 27171, loss = 0.00695923\n",
      "Iteration 27172, loss = 0.00695905\n",
      "Iteration 27173, loss = 0.00695887\n",
      "Iteration 27174, loss = 0.00695870\n",
      "Iteration 27175, loss = 0.00695852\n",
      "Iteration 27176, loss = 0.00695834\n",
      "Iteration 27177, loss = 0.00695816\n",
      "Iteration 27178, loss = 0.00695798\n",
      "Iteration 27179, loss = 0.00695780\n",
      "Iteration 27180, loss = 0.00695762\n",
      "Iteration 27181, loss = 0.00695744\n",
      "Iteration 27182, loss = 0.00695726\n",
      "Iteration 27183, loss = 0.00695708\n",
      "Iteration 27184, loss = 0.00695691\n",
      "Iteration 27185, loss = 0.00695673\n",
      "Iteration 27186, loss = 0.00695655\n",
      "Iteration 27187, loss = 0.00695637\n",
      "Iteration 27188, loss = 0.00695619\n",
      "Iteration 27189, loss = 0.00695601\n",
      "Iteration 27190, loss = 0.00695583\n",
      "Iteration 27191, loss = 0.00695565\n",
      "Iteration 27192, loss = 0.00695548\n",
      "Iteration 27193, loss = 0.00695530\n",
      "Iteration 27194, loss = 0.00695512\n",
      "Iteration 27195, loss = 0.00695494\n",
      "Iteration 27196, loss = 0.00695476\n",
      "Iteration 27197, loss = 0.00695458\n",
      "Iteration 27198, loss = 0.00695440\n",
      "Iteration 27199, loss = 0.00695422\n",
      "Iteration 27200, loss = 0.00695405\n",
      "Iteration 27201, loss = 0.00695387\n",
      "Iteration 27202, loss = 0.00695369\n",
      "Iteration 27203, loss = 0.00695351\n",
      "Iteration 27204, loss = 0.00695333\n",
      "Iteration 27205, loss = 0.00695315\n",
      "Iteration 27206, loss = 0.00695297\n",
      "Iteration 27207, loss = 0.00695280\n",
      "Iteration 27208, loss = 0.00695262\n",
      "Iteration 27209, loss = 0.00695244\n",
      "Iteration 27210, loss = 0.00695226\n",
      "Iteration 27211, loss = 0.00695208\n",
      "Iteration 27212, loss = 0.00695190\n",
      "Iteration 27213, loss = 0.00695173\n",
      "Iteration 27214, loss = 0.00695155\n",
      "Iteration 27215, loss = 0.00695137\n",
      "Iteration 27216, loss = 0.00695119\n",
      "Iteration 27217, loss = 0.00695101\n",
      "Iteration 27218, loss = 0.00695083\n",
      "Iteration 27219, loss = 0.00695066\n",
      "Iteration 27220, loss = 0.00695048\n",
      "Iteration 27221, loss = 0.00695030\n",
      "Iteration 27222, loss = 0.00695012\n",
      "Iteration 27223, loss = 0.00694994\n",
      "Iteration 27224, loss = 0.00694976\n",
      "Iteration 27225, loss = 0.00694959\n",
      "Iteration 27226, loss = 0.00694941\n",
      "Iteration 27227, loss = 0.00694923\n",
      "Iteration 27228, loss = 0.00694905\n",
      "Iteration 27229, loss = 0.00694887\n",
      "Iteration 27230, loss = 0.00694870\n",
      "Iteration 27231, loss = 0.00694852\n",
      "Iteration 27232, loss = 0.00694834\n",
      "Iteration 27233, loss = 0.00694816\n",
      "Iteration 27234, loss = 0.00694798\n",
      "Iteration 27235, loss = 0.00694781\n",
      "Iteration 27236, loss = 0.00694763\n",
      "Iteration 27237, loss = 0.00694745\n",
      "Iteration 27238, loss = 0.00694727\n",
      "Iteration 27239, loss = 0.00694709\n",
      "Iteration 27240, loss = 0.00694692\n",
      "Iteration 27241, loss = 0.00694674\n",
      "Iteration 27242, loss = 0.00694656\n",
      "Iteration 27243, loss = 0.00694638\n",
      "Iteration 27244, loss = 0.00694620\n",
      "Iteration 27245, loss = 0.00694603\n",
      "Iteration 27246, loss = 0.00694585\n",
      "Iteration 27247, loss = 0.00694567\n",
      "Iteration 27248, loss = 0.00694549\n",
      "Iteration 27249, loss = 0.00694532\n",
      "Iteration 27250, loss = 0.00694514\n",
      "Iteration 27251, loss = 0.00694496\n",
      "Iteration 27252, loss = 0.00694478\n",
      "Iteration 27253, loss = 0.00694460\n",
      "Iteration 27254, loss = 0.00694443\n",
      "Iteration 27255, loss = 0.00694425\n",
      "Iteration 27256, loss = 0.00694407\n",
      "Iteration 27257, loss = 0.00694389\n",
      "Iteration 27258, loss = 0.00694372\n",
      "Iteration 27259, loss = 0.00694354\n",
      "Iteration 27260, loss = 0.00694336\n",
      "Iteration 27261, loss = 0.00694318\n",
      "Iteration 27262, loss = 0.00694301\n",
      "Iteration 27263, loss = 0.00694283\n",
      "Iteration 27264, loss = 0.00694265\n",
      "Iteration 27265, loss = 0.00694247\n",
      "Iteration 27266, loss = 0.00694230\n",
      "Iteration 27267, loss = 0.00694212\n",
      "Iteration 27268, loss = 0.00694194\n",
      "Iteration 27269, loss = 0.00694176\n",
      "Iteration 27270, loss = 0.00694159\n",
      "Iteration 27271, loss = 0.00694141\n",
      "Iteration 27272, loss = 0.00694123\n",
      "Iteration 27273, loss = 0.00694105\n",
      "Iteration 27274, loss = 0.00694088\n",
      "Iteration 27275, loss = 0.00694070\n",
      "Iteration 27276, loss = 0.00694052\n",
      "Iteration 27277, loss = 0.00694035\n",
      "Iteration 27278, loss = 0.00694017\n",
      "Iteration 27279, loss = 0.00693999\n",
      "Iteration 27280, loss = 0.00693981\n",
      "Iteration 27281, loss = 0.00693964\n",
      "Iteration 27282, loss = 0.00693946\n",
      "Iteration 27283, loss = 0.00693928\n",
      "Iteration 27284, loss = 0.00693911\n",
      "Iteration 27285, loss = 0.00693893\n",
      "Iteration 27286, loss = 0.00693875\n",
      "Iteration 27287, loss = 0.00693857\n",
      "Iteration 27288, loss = 0.00693840\n",
      "Iteration 27289, loss = 0.00693822\n",
      "Iteration 27290, loss = 0.00693804\n",
      "Iteration 27291, loss = 0.00693787\n",
      "Iteration 27292, loss = 0.00693769\n",
      "Iteration 27293, loss = 0.00693751\n",
      "Iteration 27294, loss = 0.00693734\n",
      "Iteration 27295, loss = 0.00693716\n",
      "Iteration 27296, loss = 0.00693698\n",
      "Iteration 27297, loss = 0.00693680\n",
      "Iteration 27298, loss = 0.00693663\n",
      "Iteration 27299, loss = 0.00693645\n",
      "Iteration 27300, loss = 0.00693627\n",
      "Iteration 27301, loss = 0.00693610\n",
      "Iteration 27302, loss = 0.00693592\n",
      "Iteration 27303, loss = 0.00693574\n",
      "Iteration 27304, loss = 0.00693557\n",
      "Iteration 27305, loss = 0.00693539\n",
      "Iteration 27306, loss = 0.00693521\n",
      "Iteration 27307, loss = 0.00693504\n",
      "Iteration 27308, loss = 0.00693486\n",
      "Iteration 27309, loss = 0.00693468\n",
      "Iteration 27310, loss = 0.00693451\n",
      "Iteration 27311, loss = 0.00693433\n",
      "Iteration 27312, loss = 0.00693415\n",
      "Iteration 27313, loss = 0.00693398\n",
      "Iteration 27314, loss = 0.00693380\n",
      "Iteration 27315, loss = 0.00693362\n",
      "Iteration 27316, loss = 0.00693345\n",
      "Iteration 27317, loss = 0.00693327\n",
      "Iteration 27318, loss = 0.00693309\n",
      "Iteration 27319, loss = 0.00693292\n",
      "Iteration 27320, loss = 0.00693274\n",
      "Iteration 27321, loss = 0.00693257\n",
      "Iteration 27322, loss = 0.00693239\n",
      "Iteration 27323, loss = 0.00693221\n",
      "Iteration 27324, loss = 0.00693204\n",
      "Iteration 27325, loss = 0.00693186\n",
      "Iteration 27326, loss = 0.00693168\n",
      "Iteration 27327, loss = 0.00693151\n",
      "Iteration 27328, loss = 0.00693133\n",
      "Iteration 27329, loss = 0.00693115\n",
      "Iteration 27330, loss = 0.00693098\n",
      "Iteration 27331, loss = 0.00693080\n",
      "Iteration 27332, loss = 0.00693063\n",
      "Iteration 27333, loss = 0.00693045\n",
      "Iteration 27334, loss = 0.00693027\n",
      "Iteration 27335, loss = 0.00693010\n",
      "Iteration 27336, loss = 0.00692992\n",
      "Iteration 27337, loss = 0.00692974\n",
      "Iteration 27338, loss = 0.00692957\n",
      "Iteration 27339, loss = 0.00692939\n",
      "Iteration 27340, loss = 0.00692922\n",
      "Iteration 27341, loss = 0.00692904\n",
      "Iteration 27342, loss = 0.00692886\n",
      "Iteration 27343, loss = 0.00692869\n",
      "Iteration 27344, loss = 0.00692851\n",
      "Iteration 27345, loss = 0.00692834\n",
      "Iteration 27346, loss = 0.00692816\n",
      "Iteration 27347, loss = 0.00692798\n",
      "Iteration 27348, loss = 0.00692781\n",
      "Iteration 27349, loss = 0.00692763\n",
      "Iteration 27350, loss = 0.00692746\n",
      "Iteration 27351, loss = 0.00692728\n",
      "Iteration 27352, loss = 0.00692710\n",
      "Iteration 27353, loss = 0.00692693\n",
      "Iteration 27354, loss = 0.00692675\n",
      "Iteration 27355, loss = 0.00692658\n",
      "Iteration 27356, loss = 0.00692640\n",
      "Iteration 27357, loss = 0.00692622\n",
      "Iteration 27358, loss = 0.00692605\n",
      "Iteration 27359, loss = 0.00692587\n",
      "Iteration 27360, loss = 0.00692570\n",
      "Iteration 27361, loss = 0.00692552\n",
      "Iteration 27362, loss = 0.00692535\n",
      "Iteration 27363, loss = 0.00692517\n",
      "Iteration 27364, loss = 0.00692499\n",
      "Iteration 27365, loss = 0.00692482\n",
      "Iteration 27366, loss = 0.00692464\n",
      "Iteration 27367, loss = 0.00692447\n",
      "Iteration 27368, loss = 0.00692429\n",
      "Iteration 27369, loss = 0.00692412\n",
      "Iteration 27370, loss = 0.00692394\n",
      "Iteration 27371, loss = 0.00692376\n",
      "Iteration 27372, loss = 0.00692359\n",
      "Iteration 27373, loss = 0.00692341\n",
      "Iteration 27374, loss = 0.00692324\n",
      "Iteration 27375, loss = 0.00692306\n",
      "Iteration 27376, loss = 0.00692289\n",
      "Iteration 27377, loss = 0.00692271\n",
      "Iteration 27378, loss = 0.00692254\n",
      "Iteration 27379, loss = 0.00692236\n",
      "Iteration 27380, loss = 0.00692219\n",
      "Iteration 27381, loss = 0.00692201\n",
      "Iteration 27382, loss = 0.00692183\n",
      "Iteration 27383, loss = 0.00692166\n",
      "Iteration 27384, loss = 0.00692148\n",
      "Iteration 27385, loss = 0.00692131\n",
      "Iteration 27386, loss = 0.00692113\n",
      "Iteration 27387, loss = 0.00692096\n",
      "Iteration 27388, loss = 0.00692078\n",
      "Iteration 27389, loss = 0.00692061\n",
      "Iteration 27390, loss = 0.00692043\n",
      "Iteration 27391, loss = 0.00692026\n",
      "Iteration 27392, loss = 0.00692008\n",
      "Iteration 27393, loss = 0.00691991\n",
      "Iteration 27394, loss = 0.00691973\n",
      "Iteration 27395, loss = 0.00691956\n",
      "Iteration 27396, loss = 0.00691938\n",
      "Iteration 27397, loss = 0.00691921\n",
      "Iteration 27398, loss = 0.00691903\n",
      "Iteration 27399, loss = 0.00691886\n",
      "Iteration 27400, loss = 0.00691868\n",
      "Iteration 27401, loss = 0.00691851\n",
      "Iteration 27402, loss = 0.00691833\n",
      "Iteration 27403, loss = 0.00691816\n",
      "Iteration 27404, loss = 0.00691798\n",
      "Iteration 27405, loss = 0.00691781\n",
      "Iteration 27406, loss = 0.00691763\n",
      "Iteration 27407, loss = 0.00691746\n",
      "Iteration 27408, loss = 0.00691728\n",
      "Iteration 27409, loss = 0.00691711\n",
      "Iteration 27410, loss = 0.00691693\n",
      "Iteration 27411, loss = 0.00691676\n",
      "Iteration 27412, loss = 0.00691658\n",
      "Iteration 27413, loss = 0.00691641\n",
      "Iteration 27414, loss = 0.00691623\n",
      "Iteration 27415, loss = 0.00691606\n",
      "Iteration 27416, loss = 0.00691588\n",
      "Iteration 27417, loss = 0.00691571\n",
      "Iteration 27418, loss = 0.00691553\n",
      "Iteration 27419, loss = 0.00691536\n",
      "Iteration 27420, loss = 0.00691518\n",
      "Iteration 27421, loss = 0.00691501\n",
      "Iteration 27422, loss = 0.00691483\n",
      "Iteration 27423, loss = 0.00691466\n",
      "Iteration 27424, loss = 0.00691448\n",
      "Iteration 27425, loss = 0.00691431\n",
      "Iteration 27426, loss = 0.00691414\n",
      "Iteration 27427, loss = 0.00691396\n",
      "Iteration 27428, loss = 0.00691379\n",
      "Iteration 27429, loss = 0.00691361\n",
      "Iteration 27430, loss = 0.00691344\n",
      "Iteration 27431, loss = 0.00691326\n",
      "Iteration 27432, loss = 0.00691309\n",
      "Iteration 27433, loss = 0.00691291\n",
      "Iteration 27434, loss = 0.00691274\n",
      "Iteration 27435, loss = 0.00691256\n",
      "Iteration 27436, loss = 0.00691239\n",
      "Iteration 27437, loss = 0.00691222\n",
      "Iteration 27438, loss = 0.00691204\n",
      "Iteration 27439, loss = 0.00691187\n",
      "Iteration 27440, loss = 0.00691169\n",
      "Iteration 27441, loss = 0.00691152\n",
      "Iteration 27442, loss = 0.00691134\n",
      "Iteration 27443, loss = 0.00691117\n",
      "Iteration 27444, loss = 0.00691100\n",
      "Iteration 27445, loss = 0.00691082\n",
      "Iteration 27446, loss = 0.00691065\n",
      "Iteration 27447, loss = 0.00691047\n",
      "Iteration 27448, loss = 0.00691030\n",
      "Iteration 27449, loss = 0.00691012\n",
      "Iteration 27450, loss = 0.00690995\n",
      "Iteration 27451, loss = 0.00690978\n",
      "Iteration 27452, loss = 0.00690960\n",
      "Iteration 27453, loss = 0.00690943\n",
      "Iteration 27454, loss = 0.00690925\n",
      "Iteration 27455, loss = 0.00690908\n",
      "Iteration 27456, loss = 0.00690891\n",
      "Iteration 27457, loss = 0.00690873\n",
      "Iteration 27458, loss = 0.00690856\n",
      "Iteration 27459, loss = 0.00690838\n",
      "Iteration 27460, loss = 0.00690821\n",
      "Iteration 27461, loss = 0.00690804\n",
      "Iteration 27462, loss = 0.00690786\n",
      "Iteration 27463, loss = 0.00690769\n",
      "Iteration 27464, loss = 0.00690751\n",
      "Iteration 27465, loss = 0.00690734\n",
      "Iteration 27466, loss = 0.00690717\n",
      "Iteration 27467, loss = 0.00690699\n",
      "Iteration 27468, loss = 0.00690682\n",
      "Iteration 27469, loss = 0.00690664\n",
      "Iteration 27470, loss = 0.00690647\n",
      "Iteration 27471, loss = 0.00690630\n",
      "Iteration 27472, loss = 0.00690612\n",
      "Iteration 27473, loss = 0.00690595\n",
      "Iteration 27474, loss = 0.00690578\n",
      "Iteration 27475, loss = 0.00690560\n",
      "Iteration 27476, loss = 0.00690543\n",
      "Iteration 27477, loss = 0.00690525\n",
      "Iteration 27478, loss = 0.00690508\n",
      "Iteration 27479, loss = 0.00690491\n",
      "Iteration 27480, loss = 0.00690473\n",
      "Iteration 27481, loss = 0.00690456\n",
      "Iteration 27482, loss = 0.00690439\n",
      "Iteration 27483, loss = 0.00690421\n",
      "Iteration 27484, loss = 0.00690404\n",
      "Iteration 27485, loss = 0.00690386\n",
      "Iteration 27486, loss = 0.00690369\n",
      "Iteration 27487, loss = 0.00690352\n",
      "Iteration 27488, loss = 0.00690334\n",
      "Iteration 27489, loss = 0.00690317\n",
      "Iteration 27490, loss = 0.00690300\n",
      "Iteration 27491, loss = 0.00690282\n",
      "Iteration 27492, loss = 0.00690265\n",
      "Iteration 27493, loss = 0.00690248\n",
      "Iteration 27494, loss = 0.00690230\n",
      "Iteration 27495, loss = 0.00690213\n",
      "Iteration 27496, loss = 0.00690196\n",
      "Iteration 27497, loss = 0.00690178\n",
      "Iteration 27498, loss = 0.00690161\n",
      "Iteration 27499, loss = 0.00690144\n",
      "Iteration 27500, loss = 0.00690126\n",
      "Iteration 27501, loss = 0.00690109\n",
      "Iteration 27502, loss = 0.00690092\n",
      "Iteration 27503, loss = 0.00690074\n",
      "Iteration 27504, loss = 0.00690057\n",
      "Iteration 27505, loss = 0.00690040\n",
      "Iteration 27506, loss = 0.00690022\n",
      "Iteration 27507, loss = 0.00690005\n",
      "Iteration 27508, loss = 0.00689988\n",
      "Iteration 27509, loss = 0.00689970\n",
      "Iteration 27510, loss = 0.00689953\n",
      "Iteration 27511, loss = 0.00689936\n",
      "Iteration 27512, loss = 0.00689918\n",
      "Iteration 27513, loss = 0.00689901\n",
      "Iteration 27514, loss = 0.00689884\n",
      "Iteration 27515, loss = 0.00689867\n",
      "Iteration 27516, loss = 0.00689849\n",
      "Iteration 27517, loss = 0.00689832\n",
      "Iteration 27518, loss = 0.00689815\n",
      "Iteration 27519, loss = 0.00689797\n",
      "Iteration 27520, loss = 0.00689780\n",
      "Iteration 27521, loss = 0.00689763\n",
      "Iteration 27522, loss = 0.00689745\n",
      "Iteration 27523, loss = 0.00689728\n",
      "Iteration 27524, loss = 0.00689711\n",
      "Iteration 27525, loss = 0.00689694\n",
      "Iteration 27526, loss = 0.00689676\n",
      "Iteration 27527, loss = 0.00689659\n",
      "Iteration 27528, loss = 0.00689642\n",
      "Iteration 27529, loss = 0.00689624\n",
      "Iteration 27530, loss = 0.00689607\n",
      "Iteration 27531, loss = 0.00689590\n",
      "Iteration 27532, loss = 0.00689573\n",
      "Iteration 27533, loss = 0.00689555\n",
      "Iteration 27534, loss = 0.00689538\n",
      "Iteration 27535, loss = 0.00689521\n",
      "Iteration 27536, loss = 0.00689504\n",
      "Iteration 27537, loss = 0.00689486\n",
      "Iteration 27538, loss = 0.00689469\n",
      "Iteration 27539, loss = 0.00689452\n",
      "Iteration 27540, loss = 0.00689434\n",
      "Iteration 27541, loss = 0.00689417\n",
      "Iteration 27542, loss = 0.00689400\n",
      "Iteration 27543, loss = 0.00689383\n",
      "Iteration 27544, loss = 0.00689365\n",
      "Iteration 27545, loss = 0.00689348\n",
      "Iteration 27546, loss = 0.00689331\n",
      "Iteration 27547, loss = 0.00689314\n",
      "Iteration 27548, loss = 0.00689296\n",
      "Iteration 27549, loss = 0.00689279\n",
      "Iteration 27550, loss = 0.00689262\n",
      "Iteration 27551, loss = 0.00689245\n",
      "Iteration 27552, loss = 0.00689227\n",
      "Iteration 27553, loss = 0.00689210\n",
      "Iteration 27554, loss = 0.00689193\n",
      "Iteration 27555, loss = 0.00689176\n",
      "Iteration 27556, loss = 0.00689158\n",
      "Iteration 27557, loss = 0.00689141\n",
      "Iteration 27558, loss = 0.00689124\n",
      "Iteration 27559, loss = 0.00689107\n",
      "Iteration 27560, loss = 0.00689090\n",
      "Iteration 27561, loss = 0.00689072\n",
      "Iteration 27562, loss = 0.00689055\n",
      "Iteration 27563, loss = 0.00689038\n",
      "Iteration 27564, loss = 0.00689021\n",
      "Iteration 27565, loss = 0.00689003\n",
      "Iteration 27566, loss = 0.00688986\n",
      "Iteration 27567, loss = 0.00688969\n",
      "Iteration 27568, loss = 0.00688952\n",
      "Iteration 27569, loss = 0.00688935\n",
      "Iteration 27570, loss = 0.00688917\n",
      "Iteration 27571, loss = 0.00688900\n",
      "Iteration 27572, loss = 0.00688883\n",
      "Iteration 27573, loss = 0.00688866\n",
      "Iteration 27574, loss = 0.00688849\n",
      "Iteration 27575, loss = 0.00688831\n",
      "Iteration 27576, loss = 0.00688814\n",
      "Iteration 27577, loss = 0.00688797\n",
      "Iteration 27578, loss = 0.00688780\n",
      "Iteration 27579, loss = 0.00688763\n",
      "Iteration 27580, loss = 0.00688745\n",
      "Iteration 27581, loss = 0.00688728\n",
      "Iteration 27582, loss = 0.00688711\n",
      "Iteration 27583, loss = 0.00688694\n",
      "Iteration 27584, loss = 0.00688677\n",
      "Iteration 27585, loss = 0.00688659\n",
      "Iteration 27586, loss = 0.00688642\n",
      "Iteration 27587, loss = 0.00688625\n",
      "Iteration 27588, loss = 0.00688608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27589, loss = 0.00688591\n",
      "Iteration 27590, loss = 0.00688574\n",
      "Iteration 27591, loss = 0.00688556\n",
      "Iteration 27592, loss = 0.00688539\n",
      "Iteration 27593, loss = 0.00688522\n",
      "Iteration 27594, loss = 0.00688505\n",
      "Iteration 27595, loss = 0.00688488\n",
      "Iteration 27596, loss = 0.00688471\n",
      "Iteration 27597, loss = 0.00688453\n",
      "Iteration 27598, loss = 0.00688436\n",
      "Iteration 27599, loss = 0.00688419\n",
      "Iteration 27600, loss = 0.00688402\n",
      "Iteration 27601, loss = 0.00688385\n",
      "Iteration 27602, loss = 0.00688368\n",
      "Iteration 27603, loss = 0.00688350\n",
      "Iteration 27604, loss = 0.00688333\n",
      "Iteration 27605, loss = 0.00688316\n",
      "Iteration 27606, loss = 0.00688299\n",
      "Iteration 27607, loss = 0.00688282\n",
      "Iteration 27608, loss = 0.00688265\n",
      "Iteration 27609, loss = 0.00688247\n",
      "Iteration 27610, loss = 0.00688230\n",
      "Iteration 27611, loss = 0.00688213\n",
      "Iteration 27612, loss = 0.00688196\n",
      "Iteration 27613, loss = 0.00688179\n",
      "Iteration 27614, loss = 0.00688162\n",
      "Iteration 27615, loss = 0.00688145\n",
      "Iteration 27616, loss = 0.00688128\n",
      "Iteration 27617, loss = 0.00688110\n",
      "Iteration 27618, loss = 0.00688093\n",
      "Iteration 27619, loss = 0.00688076\n",
      "Iteration 27620, loss = 0.00688059\n",
      "Iteration 27621, loss = 0.00688042\n",
      "Iteration 27622, loss = 0.00688025\n",
      "Iteration 27623, loss = 0.00688008\n",
      "Iteration 27624, loss = 0.00687991\n",
      "Iteration 27625, loss = 0.00687973\n",
      "Iteration 27626, loss = 0.00687956\n",
      "Iteration 27627, loss = 0.00687939\n",
      "Iteration 27628, loss = 0.00687922\n",
      "Iteration 27629, loss = 0.00687905\n",
      "Iteration 27630, loss = 0.00687888\n",
      "Iteration 27631, loss = 0.00687871\n",
      "Iteration 27632, loss = 0.00687854\n",
      "Iteration 27633, loss = 0.00687837\n",
      "Iteration 27634, loss = 0.00687819\n",
      "Iteration 27635, loss = 0.00687802\n",
      "Iteration 27636, loss = 0.00687785\n",
      "Iteration 27637, loss = 0.00687768\n",
      "Iteration 27638, loss = 0.00687751\n",
      "Iteration 27639, loss = 0.00687734\n",
      "Iteration 27640, loss = 0.00687717\n",
      "Iteration 27641, loss = 0.00687700\n",
      "Iteration 27642, loss = 0.00687683\n",
      "Iteration 27643, loss = 0.00687666\n",
      "Iteration 27644, loss = 0.00687649\n",
      "Iteration 27645, loss = 0.00687631\n",
      "Iteration 27646, loss = 0.00687614\n",
      "Iteration 27647, loss = 0.00687597\n",
      "Iteration 27648, loss = 0.00687580\n",
      "Iteration 27649, loss = 0.00687563\n",
      "Iteration 27650, loss = 0.00687546\n",
      "Iteration 27651, loss = 0.00687529\n",
      "Iteration 27652, loss = 0.00687512\n",
      "Iteration 27653, loss = 0.00687495\n",
      "Iteration 27654, loss = 0.00687478\n",
      "Iteration 27655, loss = 0.00687461\n",
      "Iteration 27656, loss = 0.00687444\n",
      "Iteration 27657, loss = 0.00687427\n",
      "Iteration 27658, loss = 0.00687410\n",
      "Iteration 27659, loss = 0.00687392\n",
      "Iteration 27660, loss = 0.00687375\n",
      "Iteration 27661, loss = 0.00687358\n",
      "Iteration 27662, loss = 0.00687341\n",
      "Iteration 27663, loss = 0.00687324\n",
      "Iteration 27664, loss = 0.00687307\n",
      "Iteration 27665, loss = 0.00687290\n",
      "Iteration 27666, loss = 0.00687273\n",
      "Iteration 27667, loss = 0.00687256\n",
      "Iteration 27668, loss = 0.00687239\n",
      "Iteration 27669, loss = 0.00687222\n",
      "Iteration 27670, loss = 0.00687205\n",
      "Iteration 27671, loss = 0.00687188\n",
      "Iteration 27672, loss = 0.00687171\n",
      "Iteration 27673, loss = 0.00687154\n",
      "Iteration 27674, loss = 0.00687137\n",
      "Iteration 27675, loss = 0.00687120\n",
      "Iteration 27676, loss = 0.00687103\n",
      "Iteration 27677, loss = 0.00687086\n",
      "Iteration 27678, loss = 0.00687069\n",
      "Iteration 27679, loss = 0.00687052\n",
      "Iteration 27680, loss = 0.00687035\n",
      "Iteration 27681, loss = 0.00687018\n",
      "Iteration 27682, loss = 0.00687001\n",
      "Iteration 27683, loss = 0.00686984\n",
      "Iteration 27684, loss = 0.00686967\n",
      "Iteration 27685, loss = 0.00686949\n",
      "Iteration 27686, loss = 0.00686932\n",
      "Iteration 27687, loss = 0.00686915\n",
      "Iteration 27688, loss = 0.00686898\n",
      "Iteration 27689, loss = 0.00686881\n",
      "Iteration 27690, loss = 0.00686864\n",
      "Iteration 27691, loss = 0.00686847\n",
      "Iteration 27692, loss = 0.00686830\n",
      "Iteration 27693, loss = 0.00686813\n",
      "Iteration 27694, loss = 0.00686796\n",
      "Iteration 27695, loss = 0.00686779\n",
      "Iteration 27696, loss = 0.00686762\n",
      "Iteration 27697, loss = 0.00686745\n",
      "Iteration 27698, loss = 0.00686728\n",
      "Iteration 27699, loss = 0.00686711\n",
      "Iteration 27700, loss = 0.00686694\n",
      "Iteration 27701, loss = 0.00686677\n",
      "Iteration 27702, loss = 0.00686660\n",
      "Iteration 27703, loss = 0.00686644\n",
      "Iteration 27704, loss = 0.00686627\n",
      "Iteration 27705, loss = 0.00686610\n",
      "Iteration 27706, loss = 0.00686593\n",
      "Iteration 27707, loss = 0.00686576\n",
      "Iteration 27708, loss = 0.00686559\n",
      "Iteration 27709, loss = 0.00686542\n",
      "Iteration 27710, loss = 0.00686525\n",
      "Iteration 27711, loss = 0.00686508\n",
      "Iteration 27712, loss = 0.00686491\n",
      "Iteration 27713, loss = 0.00686474\n",
      "Iteration 27714, loss = 0.00686457\n",
      "Iteration 27715, loss = 0.00686440\n",
      "Iteration 27716, loss = 0.00686423\n",
      "Iteration 27717, loss = 0.00686406\n",
      "Iteration 27718, loss = 0.00686389\n",
      "Iteration 27719, loss = 0.00686372\n",
      "Iteration 27720, loss = 0.00686355\n",
      "Iteration 27721, loss = 0.00686338\n",
      "Iteration 27722, loss = 0.00686321\n",
      "Iteration 27723, loss = 0.00686304\n",
      "Iteration 27724, loss = 0.00686287\n",
      "Iteration 27725, loss = 0.00686270\n",
      "Iteration 27726, loss = 0.00686253\n",
      "Iteration 27727, loss = 0.00686236\n",
      "Iteration 27728, loss = 0.00686219\n",
      "Iteration 27729, loss = 0.00686202\n",
      "Iteration 27730, loss = 0.00686186\n",
      "Iteration 27731, loss = 0.00686169\n",
      "Iteration 27732, loss = 0.00686152\n",
      "Iteration 27733, loss = 0.00686135\n",
      "Iteration 27734, loss = 0.00686118\n",
      "Iteration 27735, loss = 0.00686101\n",
      "Iteration 27736, loss = 0.00686084\n",
      "Iteration 27737, loss = 0.00686067\n",
      "Iteration 27738, loss = 0.00686050\n",
      "Iteration 27739, loss = 0.00686033\n",
      "Iteration 27740, loss = 0.00686016\n",
      "Iteration 27741, loss = 0.00685999\n",
      "Iteration 27742, loss = 0.00685982\n",
      "Iteration 27743, loss = 0.00685965\n",
      "Iteration 27744, loss = 0.00685949\n",
      "Iteration 27745, loss = 0.00685932\n",
      "Iteration 27746, loss = 0.00685915\n",
      "Iteration 27747, loss = 0.00685898\n",
      "Iteration 27748, loss = 0.00685881\n",
      "Iteration 27749, loss = 0.00685864\n",
      "Iteration 27750, loss = 0.00685847\n",
      "Iteration 27751, loss = 0.00685830\n",
      "Iteration 27752, loss = 0.00685813\n",
      "Iteration 27753, loss = 0.00685796\n",
      "Iteration 27754, loss = 0.00685780\n",
      "Iteration 27755, loss = 0.00685763\n",
      "Iteration 27756, loss = 0.00685746\n",
      "Iteration 27757, loss = 0.00685729\n",
      "Iteration 27758, loss = 0.00685712\n",
      "Iteration 27759, loss = 0.00685695\n",
      "Iteration 27760, loss = 0.00685678\n",
      "Iteration 27761, loss = 0.00685661\n",
      "Iteration 27762, loss = 0.00685644\n",
      "Iteration 27763, loss = 0.00685627\n",
      "Iteration 27764, loss = 0.00685611\n",
      "Iteration 27765, loss = 0.00685594\n",
      "Iteration 27766, loss = 0.00685577\n",
      "Iteration 27767, loss = 0.00685560\n",
      "Iteration 27768, loss = 0.00685543\n",
      "Iteration 27769, loss = 0.00685526\n",
      "Iteration 27770, loss = 0.00685509\n",
      "Iteration 27771, loss = 0.00685492\n",
      "Iteration 27772, loss = 0.00685476\n",
      "Iteration 27773, loss = 0.00685459\n",
      "Iteration 27774, loss = 0.00685442\n",
      "Iteration 27775, loss = 0.00685425\n",
      "Iteration 27776, loss = 0.00685408\n",
      "Iteration 27777, loss = 0.00685391\n",
      "Iteration 27778, loss = 0.00685374\n",
      "Iteration 27779, loss = 0.00685358\n",
      "Iteration 27780, loss = 0.00685341\n",
      "Iteration 27781, loss = 0.00685324\n",
      "Iteration 27782, loss = 0.00685307\n",
      "Iteration 27783, loss = 0.00685290\n",
      "Iteration 27784, loss = 0.00685273\n",
      "Iteration 27785, loss = 0.00685256\n",
      "Iteration 27786, loss = 0.00685240\n",
      "Iteration 27787, loss = 0.00685223\n",
      "Iteration 27788, loss = 0.00685206\n",
      "Iteration 27789, loss = 0.00685189\n",
      "Iteration 27790, loss = 0.00685172\n",
      "Iteration 27791, loss = 0.00685155\n",
      "Iteration 27792, loss = 0.00685139\n",
      "Iteration 27793, loss = 0.00685122\n",
      "Iteration 27794, loss = 0.00685105\n",
      "Iteration 27795, loss = 0.00685088\n",
      "Iteration 27796, loss = 0.00685071\n",
      "Iteration 27797, loss = 0.00685054\n",
      "Iteration 27798, loss = 0.00685038\n",
      "Iteration 27799, loss = 0.00685021\n",
      "Iteration 27800, loss = 0.00685004\n",
      "Iteration 27801, loss = 0.00684987\n",
      "Iteration 27802, loss = 0.00684970\n",
      "Iteration 27803, loss = 0.00684953\n",
      "Iteration 27804, loss = 0.00684937\n",
      "Iteration 27805, loss = 0.00684920\n",
      "Iteration 27806, loss = 0.00684903\n",
      "Iteration 27807, loss = 0.00684886\n",
      "Iteration 27808, loss = 0.00684869\n",
      "Iteration 27809, loss = 0.00684853\n",
      "Iteration 27810, loss = 0.00684836\n",
      "Iteration 27811, loss = 0.00684819\n",
      "Iteration 27812, loss = 0.00684802\n",
      "Iteration 27813, loss = 0.00684785\n",
      "Iteration 27814, loss = 0.00684769\n",
      "Iteration 27815, loss = 0.00684752\n",
      "Iteration 27816, loss = 0.00684735\n",
      "Iteration 27817, loss = 0.00684718\n",
      "Iteration 27818, loss = 0.00684701\n",
      "Iteration 27819, loss = 0.00684685\n",
      "Iteration 27820, loss = 0.00684668\n",
      "Iteration 27821, loss = 0.00684651\n",
      "Iteration 27822, loss = 0.00684634\n",
      "Iteration 27823, loss = 0.00684617\n",
      "Iteration 27824, loss = 0.00684601\n",
      "Iteration 27825, loss = 0.00684584\n",
      "Iteration 27826, loss = 0.00684567\n",
      "Iteration 27827, loss = 0.00684550\n",
      "Iteration 27828, loss = 0.00684534\n",
      "Iteration 27829, loss = 0.00684517\n",
      "Iteration 27830, loss = 0.00684500\n",
      "Iteration 27831, loss = 0.00684483\n",
      "Iteration 27832, loss = 0.00684466\n",
      "Iteration 27833, loss = 0.00684450\n",
      "Iteration 27834, loss = 0.00684433\n",
      "Iteration 27835, loss = 0.00684416\n",
      "Iteration 27836, loss = 0.00684399\n",
      "Iteration 27837, loss = 0.00684383\n",
      "Iteration 27838, loss = 0.00684366\n",
      "Iteration 27839, loss = 0.00684349\n",
      "Iteration 27840, loss = 0.00684332\n",
      "Iteration 27841, loss = 0.00684316\n",
      "Iteration 27842, loss = 0.00684299\n",
      "Iteration 27843, loss = 0.00684282\n",
      "Iteration 27844, loss = 0.00684265\n",
      "Iteration 27845, loss = 0.00684249\n",
      "Iteration 27846, loss = 0.00684232\n",
      "Iteration 27847, loss = 0.00684215\n",
      "Iteration 27848, loss = 0.00684198\n",
      "Iteration 27849, loss = 0.00684182\n",
      "Iteration 27850, loss = 0.00684165\n",
      "Iteration 27851, loss = 0.00684148\n",
      "Iteration 27852, loss = 0.00684131\n",
      "Iteration 27853, loss = 0.00684115\n",
      "Iteration 27854, loss = 0.00684098\n",
      "Iteration 27855, loss = 0.00684081\n",
      "Iteration 27856, loss = 0.00684065\n",
      "Iteration 27857, loss = 0.00684048\n",
      "Iteration 27858, loss = 0.00684031\n",
      "Iteration 27859, loss = 0.00684014\n",
      "Iteration 27860, loss = 0.00683998\n",
      "Iteration 27861, loss = 0.00683981\n",
      "Iteration 27862, loss = 0.00683964\n",
      "Iteration 27863, loss = 0.00683947\n",
      "Iteration 27864, loss = 0.00683931\n",
      "Iteration 27865, loss = 0.00683914\n",
      "Iteration 27866, loss = 0.00683897\n",
      "Iteration 27867, loss = 0.00683881\n",
      "Iteration 27868, loss = 0.00683864\n",
      "Iteration 27869, loss = 0.00683847\n",
      "Iteration 27870, loss = 0.00683830\n",
      "Iteration 27871, loss = 0.00683814\n",
      "Iteration 27872, loss = 0.00683797\n",
      "Iteration 27873, loss = 0.00683780\n",
      "Iteration 27874, loss = 0.00683764\n",
      "Iteration 27875, loss = 0.00683747\n",
      "Iteration 27876, loss = 0.00683730\n",
      "Iteration 27877, loss = 0.00683714\n",
      "Iteration 27878, loss = 0.00683697\n",
      "Iteration 27879, loss = 0.00683680\n",
      "Iteration 27880, loss = 0.00683663\n",
      "Iteration 27881, loss = 0.00683647\n",
      "Iteration 27882, loss = 0.00683630\n",
      "Iteration 27883, loss = 0.00683613\n",
      "Iteration 27884, loss = 0.00683597\n",
      "Iteration 27885, loss = 0.00683580\n",
      "Iteration 27886, loss = 0.00683563\n",
      "Iteration 27887, loss = 0.00683547\n",
      "Iteration 27888, loss = 0.00683530\n",
      "Iteration 27889, loss = 0.00683513\n",
      "Iteration 27890, loss = 0.00683497\n",
      "Iteration 27891, loss = 0.00683480\n",
      "Iteration 27892, loss = 0.00683463\n",
      "Iteration 27893, loss = 0.00683447\n",
      "Iteration 27894, loss = 0.00683430\n",
      "Iteration 27895, loss = 0.00683413\n",
      "Iteration 27896, loss = 0.00683397\n",
      "Iteration 27897, loss = 0.00683380\n",
      "Iteration 27898, loss = 0.00683363\n",
      "Iteration 27899, loss = 0.00683347\n",
      "Iteration 27900, loss = 0.00683330\n",
      "Iteration 27901, loss = 0.00683313\n",
      "Iteration 27902, loss = 0.00683297\n",
      "Iteration 27903, loss = 0.00683280\n",
      "Iteration 27904, loss = 0.00683263\n",
      "Iteration 27905, loss = 0.00683247\n",
      "Iteration 27906, loss = 0.00683230\n",
      "Iteration 27907, loss = 0.00683213\n",
      "Iteration 27908, loss = 0.00683197\n",
      "Iteration 27909, loss = 0.00683180\n",
      "Iteration 27910, loss = 0.00683164\n",
      "Iteration 27911, loss = 0.00683147\n",
      "Iteration 27912, loss = 0.00683130\n",
      "Iteration 27913, loss = 0.00683114\n",
      "Iteration 27914, loss = 0.00683097\n",
      "Iteration 27915, loss = 0.00683080\n",
      "Iteration 27916, loss = 0.00683064\n",
      "Iteration 27917, loss = 0.00683047\n",
      "Iteration 27918, loss = 0.00683030\n",
      "Iteration 27919, loss = 0.00683014\n",
      "Iteration 27920, loss = 0.00682997\n",
      "Iteration 27921, loss = 0.00682981\n",
      "Iteration 27922, loss = 0.00682964\n",
      "Iteration 27923, loss = 0.00682947\n",
      "Iteration 27924, loss = 0.00682931\n",
      "Iteration 27925, loss = 0.00682914\n",
      "Iteration 27926, loss = 0.00682898\n",
      "Iteration 27927, loss = 0.00682881\n",
      "Iteration 27928, loss = 0.00682864\n",
      "Iteration 27929, loss = 0.00682848\n",
      "Iteration 27930, loss = 0.00682831\n",
      "Iteration 27931, loss = 0.00682814\n",
      "Iteration 27932, loss = 0.00682798\n",
      "Iteration 27933, loss = 0.00682781\n",
      "Iteration 27934, loss = 0.00682765\n",
      "Iteration 27935, loss = 0.00682748\n",
      "Iteration 27936, loss = 0.00682731\n",
      "Iteration 27937, loss = 0.00682715\n",
      "Iteration 27938, loss = 0.00682698\n",
      "Iteration 27939, loss = 0.00682682\n",
      "Iteration 27940, loss = 0.00682665\n",
      "Iteration 27941, loss = 0.00682648\n",
      "Iteration 27942, loss = 0.00682632\n",
      "Iteration 27943, loss = 0.00682615\n",
      "Iteration 27944, loss = 0.00682599\n",
      "Iteration 27945, loss = 0.00682582\n",
      "Iteration 27946, loss = 0.00682566\n",
      "Iteration 27947, loss = 0.00682549\n",
      "Iteration 27948, loss = 0.00682532\n",
      "Iteration 27949, loss = 0.00682516\n",
      "Iteration 27950, loss = 0.00682499\n",
      "Iteration 27951, loss = 0.00682483\n",
      "Iteration 27952, loss = 0.00682466\n",
      "Iteration 27953, loss = 0.00682450\n",
      "Iteration 27954, loss = 0.00682433\n",
      "Iteration 27955, loss = 0.00682416\n",
      "Iteration 27956, loss = 0.00682400\n",
      "Iteration 27957, loss = 0.00682383\n",
      "Iteration 27958, loss = 0.00682367\n",
      "Iteration 27959, loss = 0.00682350\n",
      "Iteration 27960, loss = 0.00682334\n",
      "Iteration 27961, loss = 0.00682317\n",
      "Iteration 27962, loss = 0.00682300\n",
      "Iteration 27963, loss = 0.00682284\n",
      "Iteration 27964, loss = 0.00682267\n",
      "Iteration 27965, loss = 0.00682251\n",
      "Iteration 27966, loss = 0.00682234\n",
      "Iteration 27967, loss = 0.00682218\n",
      "Iteration 27968, loss = 0.00682201\n",
      "Iteration 27969, loss = 0.00682185\n",
      "Iteration 27970, loss = 0.00682168\n",
      "Iteration 27971, loss = 0.00682152\n",
      "Iteration 27972, loss = 0.00682135\n",
      "Iteration 27973, loss = 0.00682118\n",
      "Iteration 27974, loss = 0.00682102\n",
      "Iteration 27975, loss = 0.00682085\n",
      "Iteration 27976, loss = 0.00682069\n",
      "Iteration 27977, loss = 0.00682052\n",
      "Iteration 27978, loss = 0.00682036\n",
      "Iteration 27979, loss = 0.00682019\n",
      "Iteration 27980, loss = 0.00682003\n",
      "Iteration 27981, loss = 0.00681986\n",
      "Iteration 27982, loss = 0.00681970\n",
      "Iteration 27983, loss = 0.00681953\n",
      "Iteration 27984, loss = 0.00681937\n",
      "Iteration 27985, loss = 0.00681920\n",
      "Iteration 27986, loss = 0.00681904\n",
      "Iteration 27987, loss = 0.00681887\n",
      "Iteration 27988, loss = 0.00681871\n",
      "Iteration 27989, loss = 0.00681854\n",
      "Iteration 27990, loss = 0.00681838\n",
      "Iteration 27991, loss = 0.00681821\n",
      "Iteration 27992, loss = 0.00681805\n",
      "Iteration 27993, loss = 0.00681788\n",
      "Iteration 27994, loss = 0.00681772\n",
      "Iteration 27995, loss = 0.00681755\n",
      "Iteration 27996, loss = 0.00681739\n",
      "Iteration 27997, loss = 0.00681722\n",
      "Iteration 27998, loss = 0.00681706\n",
      "Iteration 27999, loss = 0.00681689\n",
      "Iteration 28000, loss = 0.00681673\n",
      "Iteration 28001, loss = 0.00681656\n",
      "Iteration 28002, loss = 0.00681640\n",
      "Iteration 28003, loss = 0.00681623\n",
      "Iteration 28004, loss = 0.00681607\n",
      "Iteration 28005, loss = 0.00681590\n",
      "Iteration 28006, loss = 0.00681574\n",
      "Iteration 28007, loss = 0.00681557\n",
      "Iteration 28008, loss = 0.00681541\n",
      "Iteration 28009, loss = 0.00681524\n",
      "Iteration 28010, loss = 0.00681508\n",
      "Iteration 28011, loss = 0.00681491\n",
      "Iteration 28012, loss = 0.00681475\n",
      "Iteration 28013, loss = 0.00681458\n",
      "Iteration 28014, loss = 0.00681442\n",
      "Iteration 28015, loss = 0.00681425\n",
      "Iteration 28016, loss = 0.00681409\n",
      "Iteration 28017, loss = 0.00681392\n",
      "Iteration 28018, loss = 0.00681376\n",
      "Iteration 28019, loss = 0.00681359\n",
      "Iteration 28020, loss = 0.00681343\n",
      "Iteration 28021, loss = 0.00681327\n",
      "Iteration 28022, loss = 0.00681310\n",
      "Iteration 28023, loss = 0.00681294\n",
      "Iteration 28024, loss = 0.00681277\n",
      "Iteration 28025, loss = 0.00681261\n",
      "Iteration 28026, loss = 0.00681244\n",
      "Iteration 28027, loss = 0.00681228\n",
      "Iteration 28028, loss = 0.00681211\n",
      "Iteration 28029, loss = 0.00681195\n",
      "Iteration 28030, loss = 0.00681178\n",
      "Iteration 28031, loss = 0.00681162\n",
      "Iteration 28032, loss = 0.00681146\n",
      "Iteration 28033, loss = 0.00681129\n",
      "Iteration 28034, loss = 0.00681113\n",
      "Iteration 28035, loss = 0.00681096\n",
      "Iteration 28036, loss = 0.00681080\n",
      "Iteration 28037, loss = 0.00681063\n",
      "Iteration 28038, loss = 0.00681047\n",
      "Iteration 28039, loss = 0.00681030\n",
      "Iteration 28040, loss = 0.00681014\n",
      "Iteration 28041, loss = 0.00680998\n",
      "Iteration 28042, loss = 0.00680981\n",
      "Iteration 28043, loss = 0.00680965\n",
      "Iteration 28044, loss = 0.00680948\n",
      "Iteration 28045, loss = 0.00680932\n",
      "Iteration 28046, loss = 0.00680916\n",
      "Iteration 28047, loss = 0.00680899\n",
      "Iteration 28048, loss = 0.00680883\n",
      "Iteration 28049, loss = 0.00680866\n",
      "Iteration 28050, loss = 0.00680850\n",
      "Iteration 28051, loss = 0.00680833\n",
      "Iteration 28052, loss = 0.00680817\n",
      "Iteration 28053, loss = 0.00680801\n",
      "Iteration 28054, loss = 0.00680784\n",
      "Iteration 28055, loss = 0.00680768\n",
      "Iteration 28056, loss = 0.00680751\n",
      "Iteration 28057, loss = 0.00680735\n",
      "Iteration 28058, loss = 0.00680719\n",
      "Iteration 28059, loss = 0.00680702\n",
      "Iteration 28060, loss = 0.00680686\n",
      "Iteration 28061, loss = 0.00680669\n",
      "Iteration 28062, loss = 0.00680653\n",
      "Iteration 28063, loss = 0.00680637\n",
      "Iteration 28064, loss = 0.00680620\n",
      "Iteration 28065, loss = 0.00680604\n",
      "Iteration 28066, loss = 0.00680587\n",
      "Iteration 28067, loss = 0.00680571\n",
      "Iteration 28068, loss = 0.00680555\n",
      "Iteration 28069, loss = 0.00680538\n",
      "Iteration 28070, loss = 0.00680522\n",
      "Iteration 28071, loss = 0.00680506\n",
      "Iteration 28072, loss = 0.00680489\n",
      "Iteration 28073, loss = 0.00680473\n",
      "Iteration 28074, loss = 0.00680456\n",
      "Iteration 28075, loss = 0.00680440\n",
      "Iteration 28076, loss = 0.00680424\n",
      "Iteration 28077, loss = 0.00680407\n",
      "Iteration 28078, loss = 0.00680391\n",
      "Iteration 28079, loss = 0.00680375\n",
      "Iteration 28080, loss = 0.00680358\n",
      "Iteration 28081, loss = 0.00680342\n",
      "Iteration 28082, loss = 0.00680325\n",
      "Iteration 28083, loss = 0.00680309\n",
      "Iteration 28084, loss = 0.00680293\n",
      "Iteration 28085, loss = 0.00680276\n",
      "Iteration 28086, loss = 0.00680260\n",
      "Iteration 28087, loss = 0.00680244\n",
      "Iteration 28088, loss = 0.00680227\n",
      "Iteration 28089, loss = 0.00680211\n",
      "Iteration 28090, loss = 0.00680195\n",
      "Iteration 28091, loss = 0.00680178\n",
      "Iteration 28092, loss = 0.00680162\n",
      "Iteration 28093, loss = 0.00680146\n",
      "Iteration 28094, loss = 0.00680129\n",
      "Iteration 28095, loss = 0.00680113\n",
      "Iteration 28096, loss = 0.00680097\n",
      "Iteration 28097, loss = 0.00680080\n",
      "Iteration 28098, loss = 0.00680064\n",
      "Iteration 28099, loss = 0.00680048\n",
      "Iteration 28100, loss = 0.00680031\n",
      "Iteration 28101, loss = 0.00680015\n",
      "Iteration 28102, loss = 0.00679999\n",
      "Iteration 28103, loss = 0.00679982\n",
      "Iteration 28104, loss = 0.00679966\n",
      "Iteration 28105, loss = 0.00679950\n",
      "Iteration 28106, loss = 0.00679933\n",
      "Iteration 28107, loss = 0.00679917\n",
      "Iteration 28108, loss = 0.00679901\n",
      "Iteration 28109, loss = 0.00679884\n",
      "Iteration 28110, loss = 0.00679868\n",
      "Iteration 28111, loss = 0.00679852\n",
      "Iteration 28112, loss = 0.00679835\n",
      "Iteration 28113, loss = 0.00679819\n",
      "Iteration 28114, loss = 0.00679803\n",
      "Iteration 28115, loss = 0.00679786\n",
      "Iteration 28116, loss = 0.00679770\n",
      "Iteration 28117, loss = 0.00679754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28118, loss = 0.00679737\n",
      "Iteration 28119, loss = 0.00679721\n",
      "Iteration 28120, loss = 0.00679705\n",
      "Iteration 28121, loss = 0.00679689\n",
      "Iteration 28122, loss = 0.00679672\n",
      "Iteration 28123, loss = 0.00679656\n",
      "Iteration 28124, loss = 0.00679640\n",
      "Iteration 28125, loss = 0.00679623\n",
      "Iteration 28126, loss = 0.00679607\n",
      "Iteration 28127, loss = 0.00679591\n",
      "Iteration 28128, loss = 0.00679574\n",
      "Iteration 28129, loss = 0.00679558\n",
      "Iteration 28130, loss = 0.00679542\n",
      "Iteration 28131, loss = 0.00679526\n",
      "Iteration 28132, loss = 0.00679509\n",
      "Iteration 28133, loss = 0.00679493\n",
      "Iteration 28134, loss = 0.00679477\n",
      "Iteration 28135, loss = 0.00679461\n",
      "Iteration 28136, loss = 0.00679444\n",
      "Iteration 28137, loss = 0.00679428\n",
      "Iteration 28138, loss = 0.00679412\n",
      "Iteration 28139, loss = 0.00679395\n",
      "Iteration 28140, loss = 0.00679379\n",
      "Iteration 28141, loss = 0.00679363\n",
      "Iteration 28142, loss = 0.00679347\n",
      "Iteration 28143, loss = 0.00679330\n",
      "Iteration 28144, loss = 0.00679314\n",
      "Iteration 28145, loss = 0.00679298\n",
      "Iteration 28146, loss = 0.00679282\n",
      "Iteration 28147, loss = 0.00679265\n",
      "Iteration 28148, loss = 0.00679249\n",
      "Iteration 28149, loss = 0.00679233\n",
      "Iteration 28150, loss = 0.00679217\n",
      "Iteration 28151, loss = 0.00679200\n",
      "Iteration 28152, loss = 0.00679184\n",
      "Iteration 28153, loss = 0.00679168\n",
      "Iteration 28154, loss = 0.00679152\n",
      "Iteration 28155, loss = 0.00679135\n",
      "Iteration 28156, loss = 0.00679119\n",
      "Iteration 28157, loss = 0.00679103\n",
      "Iteration 28158, loss = 0.00679087\n",
      "Iteration 28159, loss = 0.00679070\n",
      "Iteration 28160, loss = 0.00679054\n",
      "Iteration 28161, loss = 0.00679038\n",
      "Iteration 28162, loss = 0.00679022\n",
      "Iteration 28163, loss = 0.00679005\n",
      "Iteration 28164, loss = 0.00678989\n",
      "Iteration 28165, loss = 0.00678973\n",
      "Iteration 28166, loss = 0.00678957\n",
      "Iteration 28167, loss = 0.00678940\n",
      "Iteration 28168, loss = 0.00678924\n",
      "Iteration 28169, loss = 0.00678908\n",
      "Iteration 28170, loss = 0.00678892\n",
      "Iteration 28171, loss = 0.00678876\n",
      "Iteration 28172, loss = 0.00678859\n",
      "Iteration 28173, loss = 0.00678843\n",
      "Iteration 28174, loss = 0.00678827\n",
      "Iteration 28175, loss = 0.00678811\n",
      "Iteration 28176, loss = 0.00678794\n",
      "Iteration 28177, loss = 0.00678778\n",
      "Iteration 28178, loss = 0.00678762\n",
      "Iteration 28179, loss = 0.00678746\n",
      "Iteration 28180, loss = 0.00678730\n",
      "Iteration 28181, loss = 0.00678713\n",
      "Iteration 28182, loss = 0.00678697\n",
      "Iteration 28183, loss = 0.00678681\n",
      "Iteration 28184, loss = 0.00678665\n",
      "Iteration 28185, loss = 0.00678649\n",
      "Iteration 28186, loss = 0.00678632\n",
      "Iteration 28187, loss = 0.00678616\n",
      "Iteration 28188, loss = 0.00678600\n",
      "Iteration 28189, loss = 0.00678584\n",
      "Iteration 28190, loss = 0.00678568\n",
      "Iteration 28191, loss = 0.00678551\n",
      "Iteration 28192, loss = 0.00678535\n",
      "Iteration 28193, loss = 0.00678519\n",
      "Iteration 28194, loss = 0.00678503\n",
      "Iteration 28195, loss = 0.00678487\n",
      "Iteration 28196, loss = 0.00678471\n",
      "Iteration 28197, loss = 0.00678454\n",
      "Iteration 28198, loss = 0.00678438\n",
      "Iteration 28199, loss = 0.00678422\n",
      "Iteration 28200, loss = 0.00678406\n",
      "Iteration 28201, loss = 0.00678390\n",
      "Iteration 28202, loss = 0.00678373\n",
      "Iteration 28203, loss = 0.00678357\n",
      "Iteration 28204, loss = 0.00678341\n",
      "Iteration 28205, loss = 0.00678325\n",
      "Iteration 28206, loss = 0.00678309\n",
      "Iteration 28207, loss = 0.00678293\n",
      "Iteration 28208, loss = 0.00678276\n",
      "Iteration 28209, loss = 0.00678260\n",
      "Iteration 28210, loss = 0.00678244\n",
      "Iteration 28211, loss = 0.00678228\n",
      "Iteration 28212, loss = 0.00678212\n",
      "Iteration 28213, loss = 0.00678196\n",
      "Iteration 28214, loss = 0.00678180\n",
      "Iteration 28215, loss = 0.00678163\n",
      "Iteration 28216, loss = 0.00678147\n",
      "Iteration 28217, loss = 0.00678131\n",
      "Iteration 28218, loss = 0.00678115\n",
      "Iteration 28219, loss = 0.00678099\n",
      "Iteration 28220, loss = 0.00678083\n",
      "Iteration 28221, loss = 0.00678067\n",
      "Iteration 28222, loss = 0.00678050\n",
      "Iteration 28223, loss = 0.00678034\n",
      "Iteration 28224, loss = 0.00678018\n",
      "Iteration 28225, loss = 0.00678002\n",
      "Iteration 28226, loss = 0.00677986\n",
      "Iteration 28227, loss = 0.00677970\n",
      "Iteration 28228, loss = 0.00677954\n",
      "Iteration 28229, loss = 0.00677937\n",
      "Iteration 28230, loss = 0.00677921\n",
      "Iteration 28231, loss = 0.00677905\n",
      "Iteration 28232, loss = 0.00677889\n",
      "Iteration 28233, loss = 0.00677873\n",
      "Iteration 28234, loss = 0.00677857\n",
      "Iteration 28235, loss = 0.00677841\n",
      "Iteration 28236, loss = 0.00677825\n",
      "Iteration 28237, loss = 0.00677808\n",
      "Iteration 28238, loss = 0.00677792\n",
      "Iteration 28239, loss = 0.00677776\n",
      "Iteration 28240, loss = 0.00677760\n",
      "Iteration 28241, loss = 0.00677744\n",
      "Iteration 28242, loss = 0.00677728\n",
      "Iteration 28243, loss = 0.00677712\n",
      "Iteration 28244, loss = 0.00677696\n",
      "Iteration 28245, loss = 0.00677680\n",
      "Iteration 28246, loss = 0.00677663\n",
      "Iteration 28247, loss = 0.00677647\n",
      "Iteration 28248, loss = 0.00677631\n",
      "Iteration 28249, loss = 0.00677615\n",
      "Iteration 28250, loss = 0.00677599\n",
      "Iteration 28251, loss = 0.00677583\n",
      "Iteration 28252, loss = 0.00677567\n",
      "Iteration 28253, loss = 0.00677551\n",
      "Iteration 28254, loss = 0.00677535\n",
      "Iteration 28255, loss = 0.00677519\n",
      "Iteration 28256, loss = 0.00677503\n",
      "Iteration 28257, loss = 0.00677486\n",
      "Iteration 28258, loss = 0.00677470\n",
      "Iteration 28259, loss = 0.00677454\n",
      "Iteration 28260, loss = 0.00677438\n",
      "Iteration 28261, loss = 0.00677422\n",
      "Iteration 28262, loss = 0.00677406\n",
      "Iteration 28263, loss = 0.00677390\n",
      "Iteration 28264, loss = 0.00677374\n",
      "Iteration 28265, loss = 0.00677358\n",
      "Iteration 28266, loss = 0.00677342\n",
      "Iteration 28267, loss = 0.00677326\n",
      "Iteration 28268, loss = 0.00677310\n",
      "Iteration 28269, loss = 0.00677294\n",
      "Iteration 28270, loss = 0.00677277\n",
      "Iteration 28271, loss = 0.00677261\n",
      "Iteration 28272, loss = 0.00677245\n",
      "Iteration 28273, loss = 0.00677229\n",
      "Iteration 28274, loss = 0.00677213\n",
      "Iteration 28275, loss = 0.00677197\n",
      "Iteration 28276, loss = 0.00677181\n",
      "Iteration 28277, loss = 0.00677165\n",
      "Iteration 28278, loss = 0.00677149\n",
      "Iteration 28279, loss = 0.00677133\n",
      "Iteration 28280, loss = 0.00677117\n",
      "Iteration 28281, loss = 0.00677101\n",
      "Iteration 28282, loss = 0.00677085\n",
      "Iteration 28283, loss = 0.00677069\n",
      "Iteration 28284, loss = 0.00677053\n",
      "Iteration 28285, loss = 0.00677037\n",
      "Iteration 28286, loss = 0.00677021\n",
      "Iteration 28287, loss = 0.00677005\n",
      "Iteration 28288, loss = 0.00676989\n",
      "Iteration 28289, loss = 0.00676973\n",
      "Iteration 28290, loss = 0.00676956\n",
      "Iteration 28291, loss = 0.00676940\n",
      "Iteration 28292, loss = 0.00676924\n",
      "Iteration 28293, loss = 0.00676908\n",
      "Iteration 28294, loss = 0.00676892\n",
      "Iteration 28295, loss = 0.00676876\n",
      "Iteration 28296, loss = 0.00676860\n",
      "Iteration 28297, loss = 0.00676844\n",
      "Iteration 28298, loss = 0.00676828\n",
      "Iteration 28299, loss = 0.00676812\n",
      "Iteration 28300, loss = 0.00676796\n",
      "Iteration 28301, loss = 0.00676780\n",
      "Iteration 28302, loss = 0.00676764\n",
      "Iteration 28303, loss = 0.00676748\n",
      "Iteration 28304, loss = 0.00676732\n",
      "Iteration 28305, loss = 0.00676716\n",
      "Iteration 28306, loss = 0.00676700\n",
      "Iteration 28307, loss = 0.00676684\n",
      "Iteration 28308, loss = 0.00676668\n",
      "Iteration 28309, loss = 0.00676652\n",
      "Iteration 28310, loss = 0.00676636\n",
      "Iteration 28311, loss = 0.00676620\n",
      "Iteration 28312, loss = 0.00676604\n",
      "Iteration 28313, loss = 0.00676588\n",
      "Iteration 28314, loss = 0.00676572\n",
      "Iteration 28315, loss = 0.00676556\n",
      "Iteration 28316, loss = 0.00676540\n",
      "Iteration 28317, loss = 0.00676524\n",
      "Iteration 28318, loss = 0.00676508\n",
      "Iteration 28319, loss = 0.00676492\n",
      "Iteration 28320, loss = 0.00676476\n",
      "Iteration 28321, loss = 0.00676460\n",
      "Iteration 28322, loss = 0.00676444\n",
      "Iteration 28323, loss = 0.00676428\n",
      "Iteration 28324, loss = 0.00676412\n",
      "Iteration 28325, loss = 0.00676396\n",
      "Iteration 28326, loss = 0.00676380\n",
      "Iteration 28327, loss = 0.00676364\n",
      "Iteration 28328, loss = 0.00676348\n",
      "Iteration 28329, loss = 0.00676332\n",
      "Iteration 28330, loss = 0.00676316\n",
      "Iteration 28331, loss = 0.00676300\n",
      "Iteration 28332, loss = 0.00676284\n",
      "Iteration 28333, loss = 0.00676268\n",
      "Iteration 28334, loss = 0.00676253\n",
      "Iteration 28335, loss = 0.00676237\n",
      "Iteration 28336, loss = 0.00676221\n",
      "Iteration 28337, loss = 0.00676205\n",
      "Iteration 28338, loss = 0.00676189\n",
      "Iteration 28339, loss = 0.00676173\n",
      "Iteration 28340, loss = 0.00676157\n",
      "Iteration 28341, loss = 0.00676141\n",
      "Iteration 28342, loss = 0.00676125\n",
      "Iteration 28343, loss = 0.00676109\n",
      "Iteration 28344, loss = 0.00676093\n",
      "Iteration 28345, loss = 0.00676077\n",
      "Iteration 28346, loss = 0.00676061\n",
      "Iteration 28347, loss = 0.00676045\n",
      "Iteration 28348, loss = 0.00676029\n",
      "Iteration 28349, loss = 0.00676013\n",
      "Iteration 28350, loss = 0.00675997\n",
      "Iteration 28351, loss = 0.00675981\n",
      "Iteration 28352, loss = 0.00675965\n",
      "Iteration 28353, loss = 0.00675949\n",
      "Iteration 28354, loss = 0.00675934\n",
      "Iteration 28355, loss = 0.00675918\n",
      "Iteration 28356, loss = 0.00675902\n",
      "Iteration 28357, loss = 0.00675886\n",
      "Iteration 28358, loss = 0.00675870\n",
      "Iteration 28359, loss = 0.00675854\n",
      "Iteration 28360, loss = 0.00675838\n",
      "Iteration 28361, loss = 0.00675822\n",
      "Iteration 28362, loss = 0.00675806\n",
      "Iteration 28363, loss = 0.00675790\n",
      "Iteration 28364, loss = 0.00675774\n",
      "Iteration 28365, loss = 0.00675758\n",
      "Iteration 28366, loss = 0.00675742\n",
      "Iteration 28367, loss = 0.00675727\n",
      "Iteration 28368, loss = 0.00675711\n",
      "Iteration 28369, loss = 0.00675695\n",
      "Iteration 28370, loss = 0.00675679\n",
      "Iteration 28371, loss = 0.00675663\n",
      "Iteration 28372, loss = 0.00675647\n",
      "Iteration 28373, loss = 0.00675631\n",
      "Iteration 28374, loss = 0.00675615\n",
      "Iteration 28375, loss = 0.00675599\n",
      "Iteration 28376, loss = 0.00675583\n",
      "Iteration 28377, loss = 0.00675567\n",
      "Iteration 28378, loss = 0.00675552\n",
      "Iteration 28379, loss = 0.00675536\n",
      "Iteration 28380, loss = 0.00675520\n",
      "Iteration 28381, loss = 0.00675504\n",
      "Iteration 28382, loss = 0.00675488\n",
      "Iteration 28383, loss = 0.00675472\n",
      "Iteration 28384, loss = 0.00675456\n",
      "Iteration 28385, loss = 0.00675440\n",
      "Iteration 28386, loss = 0.00675424\n",
      "Iteration 28387, loss = 0.00675409\n",
      "Iteration 28388, loss = 0.00675393\n",
      "Iteration 28389, loss = 0.00675377\n",
      "Iteration 28390, loss = 0.00675361\n",
      "Iteration 28391, loss = 0.00675345\n",
      "Iteration 28392, loss = 0.00675329\n",
      "Iteration 28393, loss = 0.00675313\n",
      "Iteration 28394, loss = 0.00675297\n",
      "Iteration 28395, loss = 0.00675282\n",
      "Iteration 28396, loss = 0.00675266\n",
      "Iteration 28397, loss = 0.00675250\n",
      "Iteration 28398, loss = 0.00675234\n",
      "Iteration 28399, loss = 0.00675218\n",
      "Iteration 28400, loss = 0.00675202\n",
      "Iteration 28401, loss = 0.00675186\n",
      "Iteration 28402, loss = 0.00675170\n",
      "Iteration 28403, loss = 0.00675155\n",
      "Iteration 28404, loss = 0.00675139\n",
      "Iteration 28405, loss = 0.00675123\n",
      "Iteration 28406, loss = 0.00675107\n",
      "Iteration 28407, loss = 0.00675091\n",
      "Iteration 28408, loss = 0.00675075\n",
      "Iteration 28409, loss = 0.00675059\n",
      "Iteration 28410, loss = 0.00675044\n",
      "Iteration 28411, loss = 0.00675028\n",
      "Iteration 28412, loss = 0.00675012\n",
      "Iteration 28413, loss = 0.00674996\n",
      "Iteration 28414, loss = 0.00674980\n",
      "Iteration 28415, loss = 0.00674964\n",
      "Iteration 28416, loss = 0.00674949\n",
      "Iteration 28417, loss = 0.00674933\n",
      "Iteration 28418, loss = 0.00674917\n",
      "Iteration 28419, loss = 0.00674901\n",
      "Iteration 28420, loss = 0.00674885\n",
      "Iteration 28421, loss = 0.00674869\n",
      "Iteration 28422, loss = 0.00674854\n",
      "Iteration 28423, loss = 0.00674838\n",
      "Iteration 28424, loss = 0.00674822\n",
      "Iteration 28425, loss = 0.00674806\n",
      "Iteration 28426, loss = 0.00674790\n",
      "Iteration 28427, loss = 0.00674774\n",
      "Iteration 28428, loss = 0.00674759\n",
      "Iteration 28429, loss = 0.00674743\n",
      "Iteration 28430, loss = 0.00674727\n",
      "Iteration 28431, loss = 0.00674711\n",
      "Iteration 28432, loss = 0.00674695\n",
      "Iteration 28433, loss = 0.00674680\n",
      "Iteration 28434, loss = 0.00674664\n",
      "Iteration 28435, loss = 0.00674648\n",
      "Iteration 28436, loss = 0.00674632\n",
      "Iteration 28437, loss = 0.00674616\n",
      "Iteration 28438, loss = 0.00674600\n",
      "Iteration 28439, loss = 0.00674585\n",
      "Iteration 28440, loss = 0.00674569\n",
      "Iteration 28441, loss = 0.00674553\n",
      "Iteration 28442, loss = 0.00674537\n",
      "Iteration 28443, loss = 0.00674521\n",
      "Iteration 28444, loss = 0.00674506\n",
      "Iteration 28445, loss = 0.00674490\n",
      "Iteration 28446, loss = 0.00674474\n",
      "Iteration 28447, loss = 0.00674458\n",
      "Iteration 28448, loss = 0.00674443\n",
      "Iteration 28449, loss = 0.00674427\n",
      "Iteration 28450, loss = 0.00674411\n",
      "Iteration 28451, loss = 0.00674395\n",
      "Iteration 28452, loss = 0.00674379\n",
      "Iteration 28453, loss = 0.00674364\n",
      "Iteration 28454, loss = 0.00674348\n",
      "Iteration 28455, loss = 0.00674332\n",
      "Iteration 28456, loss = 0.00674316\n",
      "Iteration 28457, loss = 0.00674300\n",
      "Iteration 28458, loss = 0.00674285\n",
      "Iteration 28459, loss = 0.00674269\n",
      "Iteration 28460, loss = 0.00674253\n",
      "Iteration 28461, loss = 0.00674237\n",
      "Iteration 28462, loss = 0.00674222\n",
      "Iteration 28463, loss = 0.00674206\n",
      "Iteration 28464, loss = 0.00674190\n",
      "Iteration 28465, loss = 0.00674174\n",
      "Iteration 28466, loss = 0.00674159\n",
      "Iteration 28467, loss = 0.00674143\n",
      "Iteration 28468, loss = 0.00674127\n",
      "Iteration 28469, loss = 0.00674111\n",
      "Iteration 28470, loss = 0.00674095\n",
      "Iteration 28471, loss = 0.00674080\n",
      "Iteration 28472, loss = 0.00674064\n",
      "Iteration 28473, loss = 0.00674048\n",
      "Iteration 28474, loss = 0.00674032\n",
      "Iteration 28475, loss = 0.00674017\n",
      "Iteration 28476, loss = 0.00674001\n",
      "Iteration 28477, loss = 0.00673985\n",
      "Iteration 28478, loss = 0.00673969\n",
      "Iteration 28479, loss = 0.00673954\n",
      "Iteration 28480, loss = 0.00673938\n",
      "Iteration 28481, loss = 0.00673922\n",
      "Iteration 28482, loss = 0.00673907\n",
      "Iteration 28483, loss = 0.00673891\n",
      "Iteration 28484, loss = 0.00673875\n",
      "Iteration 28485, loss = 0.00673859\n",
      "Iteration 28486, loss = 0.00673844\n",
      "Iteration 28487, loss = 0.00673828\n",
      "Iteration 28488, loss = 0.00673812\n",
      "Iteration 28489, loss = 0.00673796\n",
      "Iteration 28490, loss = 0.00673781\n",
      "Iteration 28491, loss = 0.00673765\n",
      "Iteration 28492, loss = 0.00673749\n",
      "Iteration 28493, loss = 0.00673733\n",
      "Iteration 28494, loss = 0.00673718\n",
      "Iteration 28495, loss = 0.00673702\n",
      "Iteration 28496, loss = 0.00673686\n",
      "Iteration 28497, loss = 0.00673671\n",
      "Iteration 28498, loss = 0.00673655\n",
      "Iteration 28499, loss = 0.00673639\n",
      "Iteration 28500, loss = 0.00673623\n",
      "Iteration 28501, loss = 0.00673608\n",
      "Iteration 28502, loss = 0.00673592\n",
      "Iteration 28503, loss = 0.00673576\n",
      "Iteration 28504, loss = 0.00673561\n",
      "Iteration 28505, loss = 0.00673545\n",
      "Iteration 28506, loss = 0.00673529\n",
      "Iteration 28507, loss = 0.00673514\n",
      "Iteration 28508, loss = 0.00673498\n",
      "Iteration 28509, loss = 0.00673482\n",
      "Iteration 28510, loss = 0.00673466\n",
      "Iteration 28511, loss = 0.00673451\n",
      "Iteration 28512, loss = 0.00673435\n",
      "Iteration 28513, loss = 0.00673419\n",
      "Iteration 28514, loss = 0.00673404\n",
      "Iteration 28515, loss = 0.00673388\n",
      "Iteration 28516, loss = 0.00673372\n",
      "Iteration 28517, loss = 0.00673357\n",
      "Iteration 28518, loss = 0.00673341\n",
      "Iteration 28519, loss = 0.00673325\n",
      "Iteration 28520, loss = 0.00673310\n",
      "Iteration 28521, loss = 0.00673294\n",
      "Iteration 28522, loss = 0.00673278\n",
      "Iteration 28523, loss = 0.00673263\n",
      "Iteration 28524, loss = 0.00673247\n",
      "Iteration 28525, loss = 0.00673231\n",
      "Iteration 28526, loss = 0.00673215\n",
      "Iteration 28527, loss = 0.00673200\n",
      "Iteration 28528, loss = 0.00673184\n",
      "Iteration 28529, loss = 0.00673168\n",
      "Iteration 28530, loss = 0.00673153\n",
      "Iteration 28531, loss = 0.00673137\n",
      "Iteration 28532, loss = 0.00673121\n",
      "Iteration 28533, loss = 0.00673106\n",
      "Iteration 28534, loss = 0.00673090\n",
      "Iteration 28535, loss = 0.00673075\n",
      "Iteration 28536, loss = 0.00673059\n",
      "Iteration 28537, loss = 0.00673043\n",
      "Iteration 28538, loss = 0.00673028\n",
      "Iteration 28539, loss = 0.00673012\n",
      "Iteration 28540, loss = 0.00672996\n",
      "Iteration 28541, loss = 0.00672981\n",
      "Iteration 28542, loss = 0.00672965\n",
      "Iteration 28543, loss = 0.00672949\n",
      "Iteration 28544, loss = 0.00672934\n",
      "Iteration 28545, loss = 0.00672918\n",
      "Iteration 28546, loss = 0.00672902\n",
      "Iteration 28547, loss = 0.00672887\n",
      "Iteration 28548, loss = 0.00672871\n",
      "Iteration 28549, loss = 0.00672855\n",
      "Iteration 28550, loss = 0.00672840\n",
      "Iteration 28551, loss = 0.00672824\n",
      "Iteration 28552, loss = 0.00672809\n",
      "Iteration 28553, loss = 0.00672793\n",
      "Iteration 28554, loss = 0.00672777\n",
      "Iteration 28555, loss = 0.00672762\n",
      "Iteration 28556, loss = 0.00672746\n",
      "Iteration 28557, loss = 0.00672730\n",
      "Iteration 28558, loss = 0.00672715\n",
      "Iteration 28559, loss = 0.00672699\n",
      "Iteration 28560, loss = 0.00672684\n",
      "Iteration 28561, loss = 0.00672668\n",
      "Iteration 28562, loss = 0.00672652\n",
      "Iteration 28563, loss = 0.00672637\n",
      "Iteration 28564, loss = 0.00672621\n",
      "Iteration 28565, loss = 0.00672605\n",
      "Iteration 28566, loss = 0.00672590\n",
      "Iteration 28567, loss = 0.00672574\n",
      "Iteration 28568, loss = 0.00672559\n",
      "Iteration 28569, loss = 0.00672543\n",
      "Iteration 28570, loss = 0.00672527\n",
      "Iteration 28571, loss = 0.00672512\n",
      "Iteration 28572, loss = 0.00672496\n",
      "Iteration 28573, loss = 0.00672481\n",
      "Iteration 28574, loss = 0.00672465\n",
      "Iteration 28575, loss = 0.00672449\n",
      "Iteration 28576, loss = 0.00672434\n",
      "Iteration 28577, loss = 0.00672418\n",
      "Iteration 28578, loss = 0.00672403\n",
      "Iteration 28579, loss = 0.00672387\n",
      "Iteration 28580, loss = 0.00672371\n",
      "Iteration 28581, loss = 0.00672356\n",
      "Iteration 28582, loss = 0.00672340\n",
      "Iteration 28583, loss = 0.00672325\n",
      "Iteration 28584, loss = 0.00672309\n",
      "Iteration 28585, loss = 0.00672293\n",
      "Iteration 28586, loss = 0.00672278\n",
      "Iteration 28587, loss = 0.00672262\n",
      "Iteration 28588, loss = 0.00672247\n",
      "Iteration 28589, loss = 0.00672231\n",
      "Iteration 28590, loss = 0.00672216\n",
      "Iteration 28591, loss = 0.00672200\n",
      "Iteration 28592, loss = 0.00672184\n",
      "Iteration 28593, loss = 0.00672169\n",
      "Iteration 28594, loss = 0.00672153\n",
      "Iteration 28595, loss = 0.00672138\n",
      "Iteration 28596, loss = 0.00672122\n",
      "Iteration 28597, loss = 0.00672107\n",
      "Iteration 28598, loss = 0.00672091\n",
      "Iteration 28599, loss = 0.00672075\n",
      "Iteration 28600, loss = 0.00672060\n",
      "Iteration 28601, loss = 0.00672044\n",
      "Iteration 28602, loss = 0.00672029\n",
      "Iteration 28603, loss = 0.00672013\n",
      "Iteration 28604, loss = 0.00671998\n",
      "Iteration 28605, loss = 0.00671982\n",
      "Iteration 28606, loss = 0.00671967\n",
      "Iteration 28607, loss = 0.00671951\n",
      "Iteration 28608, loss = 0.00671935\n",
      "Iteration 28609, loss = 0.00671920\n",
      "Iteration 28610, loss = 0.00671904\n",
      "Iteration 28611, loss = 0.00671889\n",
      "Iteration 28612, loss = 0.00671873\n",
      "Iteration 28613, loss = 0.00671858\n",
      "Iteration 28614, loss = 0.00671842\n",
      "Iteration 28615, loss = 0.00671827\n",
      "Iteration 28616, loss = 0.00671811\n",
      "Iteration 28617, loss = 0.00671796\n",
      "Iteration 28618, loss = 0.00671780\n",
      "Iteration 28619, loss = 0.00671765\n",
      "Iteration 28620, loss = 0.00671749\n",
      "Iteration 28621, loss = 0.00671733\n",
      "Iteration 28622, loss = 0.00671718\n",
      "Iteration 28623, loss = 0.00671702\n",
      "Iteration 28624, loss = 0.00671687\n",
      "Iteration 28625, loss = 0.00671671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28626, loss = 0.00671656\n",
      "Iteration 28627, loss = 0.00671640\n",
      "Iteration 28628, loss = 0.00671625\n",
      "Iteration 28629, loss = 0.00671609\n",
      "Iteration 28630, loss = 0.00671594\n",
      "Iteration 28631, loss = 0.00671578\n",
      "Iteration 28632, loss = 0.00671563\n",
      "Iteration 28633, loss = 0.00671547\n",
      "Iteration 28634, loss = 0.00671532\n",
      "Iteration 28635, loss = 0.00671516\n",
      "Iteration 28636, loss = 0.00671501\n",
      "Iteration 28637, loss = 0.00671485\n",
      "Iteration 28638, loss = 0.00671470\n",
      "Iteration 28639, loss = 0.00671454\n",
      "Iteration 28640, loss = 0.00671439\n",
      "Iteration 28641, loss = 0.00671423\n",
      "Iteration 28642, loss = 0.00671408\n",
      "Iteration 28643, loss = 0.00671392\n",
      "Iteration 28644, loss = 0.00671377\n",
      "Iteration 28645, loss = 0.00671361\n",
      "Iteration 28646, loss = 0.00671346\n",
      "Iteration 28647, loss = 0.00671330\n",
      "Iteration 28648, loss = 0.00671315\n",
      "Iteration 28649, loss = 0.00671299\n",
      "Iteration 28650, loss = 0.00671284\n",
      "Iteration 28651, loss = 0.00671268\n",
      "Iteration 28652, loss = 0.00671253\n",
      "Iteration 28653, loss = 0.00671237\n",
      "Iteration 28654, loss = 0.00671222\n",
      "Iteration 28655, loss = 0.00671206\n",
      "Iteration 28656, loss = 0.00671191\n",
      "Iteration 28657, loss = 0.00671175\n",
      "Iteration 28658, loss = 0.00671160\n",
      "Iteration 28659, loss = 0.00671144\n",
      "Iteration 28660, loss = 0.00671129\n",
      "Iteration 28661, loss = 0.00671113\n",
      "Iteration 28662, loss = 0.00671098\n",
      "Iteration 28663, loss = 0.00671083\n",
      "Iteration 28664, loss = 0.00671067\n",
      "Iteration 28665, loss = 0.00671052\n",
      "Iteration 28666, loss = 0.00671036\n",
      "Iteration 28667, loss = 0.00671021\n",
      "Iteration 28668, loss = 0.00671005\n",
      "Iteration 28669, loss = 0.00670990\n",
      "Iteration 28670, loss = 0.00670974\n",
      "Iteration 28671, loss = 0.00670959\n",
      "Iteration 28672, loss = 0.00670943\n",
      "Iteration 28673, loss = 0.00670928\n",
      "Iteration 28674, loss = 0.00670912\n",
      "Iteration 28675, loss = 0.00670897\n",
      "Iteration 28676, loss = 0.00670882\n",
      "Iteration 28677, loss = 0.00670866\n",
      "Iteration 28678, loss = 0.00670851\n",
      "Iteration 28679, loss = 0.00670835\n",
      "Iteration 28680, loss = 0.00670820\n",
      "Iteration 28681, loss = 0.00670804\n",
      "Iteration 28682, loss = 0.00670789\n",
      "Iteration 28683, loss = 0.00670773\n",
      "Iteration 28684, loss = 0.00670758\n",
      "Iteration 28685, loss = 0.00670743\n",
      "Iteration 28686, loss = 0.00670727\n",
      "Iteration 28687, loss = 0.00670712\n",
      "Iteration 28688, loss = 0.00670696\n",
      "Iteration 28689, loss = 0.00670681\n",
      "Iteration 28690, loss = 0.00670665\n",
      "Iteration 28691, loss = 0.00670650\n",
      "Iteration 28692, loss = 0.00670635\n",
      "Iteration 28693, loss = 0.00670619\n",
      "Iteration 28694, loss = 0.00670604\n",
      "Iteration 28695, loss = 0.00670588\n",
      "Iteration 28696, loss = 0.00670573\n",
      "Iteration 28697, loss = 0.00670558\n",
      "Iteration 28698, loss = 0.00670542\n",
      "Iteration 28699, loss = 0.00670527\n",
      "Iteration 28700, loss = 0.00670511\n",
      "Iteration 28701, loss = 0.00670496\n",
      "Iteration 28702, loss = 0.00670480\n",
      "Iteration 28703, loss = 0.00670465\n",
      "Iteration 28704, loss = 0.00670450\n",
      "Iteration 28705, loss = 0.00670434\n",
      "Iteration 28706, loss = 0.00670419\n",
      "Iteration 28707, loss = 0.00670403\n",
      "Iteration 28708, loss = 0.00670388\n",
      "Iteration 28709, loss = 0.00670373\n",
      "Iteration 28710, loss = 0.00670357\n",
      "Iteration 28711, loss = 0.00670342\n",
      "Iteration 28712, loss = 0.00670326\n",
      "Iteration 28713, loss = 0.00670311\n",
      "Iteration 28714, loss = 0.00670296\n",
      "Iteration 28715, loss = 0.00670280\n",
      "Iteration 28716, loss = 0.00670265\n",
      "Iteration 28717, loss = 0.00670249\n",
      "Iteration 28718, loss = 0.00670234\n",
      "Iteration 28719, loss = 0.00670219\n",
      "Iteration 28720, loss = 0.00670203\n",
      "Iteration 28721, loss = 0.00670188\n",
      "Iteration 28722, loss = 0.00670173\n",
      "Iteration 28723, loss = 0.00670157\n",
      "Iteration 28724, loss = 0.00670142\n",
      "Iteration 28725, loss = 0.00670126\n",
      "Iteration 28726, loss = 0.00670111\n",
      "Iteration 28727, loss = 0.00670096\n",
      "Iteration 28728, loss = 0.00670080\n",
      "Iteration 28729, loss = 0.00670065\n",
      "Iteration 28730, loss = 0.00670050\n",
      "Iteration 28731, loss = 0.00670034\n",
      "Iteration 28732, loss = 0.00670019\n",
      "Iteration 28733, loss = 0.00670003\n",
      "Iteration 28734, loss = 0.00669988\n",
      "Iteration 28735, loss = 0.00669973\n",
      "Iteration 28736, loss = 0.00669957\n",
      "Iteration 28737, loss = 0.00669942\n",
      "Iteration 28738, loss = 0.00669927\n",
      "Iteration 28739, loss = 0.00669911\n",
      "Iteration 28740, loss = 0.00669896\n",
      "Iteration 28741, loss = 0.00669881\n",
      "Iteration 28742, loss = 0.00669865\n",
      "Iteration 28743, loss = 0.00669850\n",
      "Iteration 28744, loss = 0.00669835\n",
      "Iteration 28745, loss = 0.00669819\n",
      "Iteration 28746, loss = 0.00669804\n",
      "Iteration 28747, loss = 0.00669789\n",
      "Iteration 28748, loss = 0.00669773\n",
      "Iteration 28749, loss = 0.00669758\n",
      "Iteration 28750, loss = 0.00669743\n",
      "Iteration 28751, loss = 0.00669727\n",
      "Iteration 28752, loss = 0.00669712\n",
      "Iteration 28753, loss = 0.00669696\n",
      "Iteration 28754, loss = 0.00669681\n",
      "Iteration 28755, loss = 0.00669666\n",
      "Iteration 28756, loss = 0.00669651\n",
      "Iteration 28757, loss = 0.00669635\n",
      "Iteration 28758, loss = 0.00669620\n",
      "Iteration 28759, loss = 0.00669605\n",
      "Iteration 28760, loss = 0.00669589\n",
      "Iteration 28761, loss = 0.00669574\n",
      "Iteration 28762, loss = 0.00669559\n",
      "Iteration 28763, loss = 0.00669543\n",
      "Iteration 28764, loss = 0.00669528\n",
      "Iteration 28765, loss = 0.00669513\n",
      "Iteration 28766, loss = 0.00669497\n",
      "Iteration 28767, loss = 0.00669482\n",
      "Iteration 28768, loss = 0.00669467\n",
      "Iteration 28769, loss = 0.00669451\n",
      "Iteration 28770, loss = 0.00669436\n",
      "Iteration 28771, loss = 0.00669421\n",
      "Iteration 28772, loss = 0.00669405\n",
      "Iteration 28773, loss = 0.00669390\n",
      "Iteration 28774, loss = 0.00669375\n",
      "Iteration 28775, loss = 0.00669359\n",
      "Iteration 28776, loss = 0.00669344\n",
      "Iteration 28777, loss = 0.00669329\n",
      "Iteration 28778, loss = 0.00669314\n",
      "Iteration 28779, loss = 0.00669298\n",
      "Iteration 28780, loss = 0.00669283\n",
      "Iteration 28781, loss = 0.00669268\n",
      "Iteration 28782, loss = 0.00669252\n",
      "Iteration 28783, loss = 0.00669237\n",
      "Iteration 28784, loss = 0.00669222\n",
      "Iteration 28785, loss = 0.00669207\n",
      "Iteration 28786, loss = 0.00669191\n",
      "Iteration 28787, loss = 0.00669176\n",
      "Iteration 28788, loss = 0.00669161\n",
      "Iteration 28789, loss = 0.00669145\n",
      "Iteration 28790, loss = 0.00669130\n",
      "Iteration 28791, loss = 0.00669115\n",
      "Iteration 28792, loss = 0.00669100\n",
      "Iteration 28793, loss = 0.00669084\n",
      "Iteration 28794, loss = 0.00669069\n",
      "Iteration 28795, loss = 0.00669054\n",
      "Iteration 28796, loss = 0.00669038\n",
      "Iteration 28797, loss = 0.00669023\n",
      "Iteration 28798, loss = 0.00669008\n",
      "Iteration 28799, loss = 0.00668993\n",
      "Iteration 28800, loss = 0.00668977\n",
      "Iteration 28801, loss = 0.00668962\n",
      "Iteration 28802, loss = 0.00668947\n",
      "Iteration 28803, loss = 0.00668932\n",
      "Iteration 28804, loss = 0.00668916\n",
      "Iteration 28805, loss = 0.00668901\n",
      "Iteration 28806, loss = 0.00668886\n",
      "Iteration 28807, loss = 0.00668871\n",
      "Iteration 28808, loss = 0.00668855\n",
      "Iteration 28809, loss = 0.00668840\n",
      "Iteration 28810, loss = 0.00668825\n",
      "Iteration 28811, loss = 0.00668810\n",
      "Iteration 28812, loss = 0.00668794\n",
      "Iteration 28813, loss = 0.00668779\n",
      "Iteration 28814, loss = 0.00668764\n",
      "Iteration 28815, loss = 0.00668749\n",
      "Iteration 28816, loss = 0.00668733\n",
      "Iteration 28817, loss = 0.00668718\n",
      "Iteration 28818, loss = 0.00668703\n",
      "Iteration 28819, loss = 0.00668688\n",
      "Iteration 28820, loss = 0.00668672\n",
      "Iteration 28821, loss = 0.00668657\n",
      "Iteration 28822, loss = 0.00668642\n",
      "Iteration 28823, loss = 0.00668627\n",
      "Iteration 28824, loss = 0.00668611\n",
      "Iteration 28825, loss = 0.00668596\n",
      "Iteration 28826, loss = 0.00668581\n",
      "Iteration 28827, loss = 0.00668566\n",
      "Iteration 28828, loss = 0.00668551\n",
      "Iteration 28829, loss = 0.00668535\n",
      "Iteration 28830, loss = 0.00668520\n",
      "Iteration 28831, loss = 0.00668505\n",
      "Iteration 28832, loss = 0.00668490\n",
      "Iteration 28833, loss = 0.00668474\n",
      "Iteration 28834, loss = 0.00668459\n",
      "Iteration 28835, loss = 0.00668444\n",
      "Iteration 28836, loss = 0.00668429\n",
      "Iteration 28837, loss = 0.00668414\n",
      "Iteration 28838, loss = 0.00668398\n",
      "Iteration 28839, loss = 0.00668383\n",
      "Iteration 28840, loss = 0.00668368\n",
      "Iteration 28841, loss = 0.00668353\n",
      "Iteration 28842, loss = 0.00668338\n",
      "Iteration 28843, loss = 0.00668322\n",
      "Iteration 28844, loss = 0.00668307\n",
      "Iteration 28845, loss = 0.00668292\n",
      "Iteration 28846, loss = 0.00668277\n",
      "Iteration 28847, loss = 0.00668262\n",
      "Iteration 28848, loss = 0.00668246\n",
      "Iteration 28849, loss = 0.00668231\n",
      "Iteration 28850, loss = 0.00668216\n",
      "Iteration 28851, loss = 0.00668201\n",
      "Iteration 28852, loss = 0.00668186\n",
      "Iteration 28853, loss = 0.00668170\n",
      "Iteration 28854, loss = 0.00668155\n",
      "Iteration 28855, loss = 0.00668140\n",
      "Iteration 28856, loss = 0.00668125\n",
      "Iteration 28857, loss = 0.00668110\n",
      "Iteration 28858, loss = 0.00668094\n",
      "Iteration 28859, loss = 0.00668079\n",
      "Iteration 28860, loss = 0.00668064\n",
      "Iteration 28861, loss = 0.00668049\n",
      "Iteration 28862, loss = 0.00668034\n",
      "Iteration 28863, loss = 0.00668019\n",
      "Iteration 28864, loss = 0.00668003\n",
      "Iteration 28865, loss = 0.00667988\n",
      "Iteration 28866, loss = 0.00667973\n",
      "Iteration 28867, loss = 0.00667958\n",
      "Iteration 28868, loss = 0.00667943\n",
      "Iteration 28869, loss = 0.00667928\n",
      "Iteration 28870, loss = 0.00667912\n",
      "Iteration 28871, loss = 0.00667897\n",
      "Iteration 28872, loss = 0.00667882\n",
      "Iteration 28873, loss = 0.00667867\n",
      "Iteration 28874, loss = 0.00667852\n",
      "Iteration 28875, loss = 0.00667837\n",
      "Iteration 28876, loss = 0.00667821\n",
      "Iteration 28877, loss = 0.00667806\n",
      "Iteration 28878, loss = 0.00667791\n",
      "Iteration 28879, loss = 0.00667776\n",
      "Iteration 28880, loss = 0.00667761\n",
      "Iteration 28881, loss = 0.00667746\n",
      "Iteration 28882, loss = 0.00667730\n",
      "Iteration 28883, loss = 0.00667715\n",
      "Iteration 28884, loss = 0.00667700\n",
      "Iteration 28885, loss = 0.00667685\n",
      "Iteration 28886, loss = 0.00667670\n",
      "Iteration 28887, loss = 0.00667655\n",
      "Iteration 28888, loss = 0.00667640\n",
      "Iteration 28889, loss = 0.00667625\n",
      "Iteration 28890, loss = 0.00667609\n",
      "Iteration 28891, loss = 0.00667594\n",
      "Iteration 28892, loss = 0.00667579\n",
      "Iteration 28893, loss = 0.00667564\n",
      "Iteration 28894, loss = 0.00667549\n",
      "Iteration 28895, loss = 0.00667534\n",
      "Iteration 28896, loss = 0.00667519\n",
      "Iteration 28897, loss = 0.00667503\n",
      "Iteration 28898, loss = 0.00667488\n",
      "Iteration 28899, loss = 0.00667473\n",
      "Iteration 28900, loss = 0.00667458\n",
      "Iteration 28901, loss = 0.00667443\n",
      "Iteration 28902, loss = 0.00667428\n",
      "Iteration 28903, loss = 0.00667413\n",
      "Iteration 28904, loss = 0.00667398\n",
      "Iteration 28905, loss = 0.00667382\n",
      "Iteration 28906, loss = 0.00667367\n",
      "Iteration 28907, loss = 0.00667352\n",
      "Iteration 28908, loss = 0.00667337\n",
      "Iteration 28909, loss = 0.00667322\n",
      "Iteration 28910, loss = 0.00667307\n",
      "Iteration 28911, loss = 0.00667292\n",
      "Iteration 28912, loss = 0.00667277\n",
      "Iteration 28913, loss = 0.00667262\n",
      "Iteration 28914, loss = 0.00667247\n",
      "Iteration 28915, loss = 0.00667231\n",
      "Iteration 28916, loss = 0.00667216\n",
      "Iteration 28917, loss = 0.00667201\n",
      "Iteration 28918, loss = 0.00667186\n",
      "Iteration 28919, loss = 0.00667171\n",
      "Iteration 28920, loss = 0.00667156\n",
      "Iteration 28921, loss = 0.00667141\n",
      "Iteration 28922, loss = 0.00667126\n",
      "Iteration 28923, loss = 0.00667111\n",
      "Iteration 28924, loss = 0.00667096\n",
      "Iteration 28925, loss = 0.00667081\n",
      "Iteration 28926, loss = 0.00667065\n",
      "Iteration 28927, loss = 0.00667050\n",
      "Iteration 28928, loss = 0.00667035\n",
      "Iteration 28929, loss = 0.00667020\n",
      "Iteration 28930, loss = 0.00667005\n",
      "Iteration 28931, loss = 0.00666990\n",
      "Iteration 28932, loss = 0.00666975\n",
      "Iteration 28933, loss = 0.00666960\n",
      "Iteration 28934, loss = 0.00666945\n",
      "Iteration 28935, loss = 0.00666930\n",
      "Iteration 28936, loss = 0.00666915\n",
      "Iteration 28937, loss = 0.00666900\n",
      "Iteration 28938, loss = 0.00666885\n",
      "Iteration 28939, loss = 0.00666869\n",
      "Iteration 28940, loss = 0.00666854\n",
      "Iteration 28941, loss = 0.00666839\n",
      "Iteration 28942, loss = 0.00666824\n",
      "Iteration 28943, loss = 0.00666809\n",
      "Iteration 28944, loss = 0.00666794\n",
      "Iteration 28945, loss = 0.00666779\n",
      "Iteration 28946, loss = 0.00666764\n",
      "Iteration 28947, loss = 0.00666749\n",
      "Iteration 28948, loss = 0.00666734\n",
      "Iteration 28949, loss = 0.00666719\n",
      "Iteration 28950, loss = 0.00666704\n",
      "Iteration 28951, loss = 0.00666689\n",
      "Iteration 28952, loss = 0.00666674\n",
      "Iteration 28953, loss = 0.00666659\n",
      "Iteration 28954, loss = 0.00666644\n",
      "Iteration 28955, loss = 0.00666629\n",
      "Iteration 28956, loss = 0.00666614\n",
      "Iteration 28957, loss = 0.00666599\n",
      "Iteration 28958, loss = 0.00666583\n",
      "Iteration 28959, loss = 0.00666568\n",
      "Iteration 28960, loss = 0.00666553\n",
      "Iteration 28961, loss = 0.00666538\n",
      "Iteration 28962, loss = 0.00666523\n",
      "Iteration 28963, loss = 0.00666508\n",
      "Iteration 28964, loss = 0.00666493\n",
      "Iteration 28965, loss = 0.00666478\n",
      "Iteration 28966, loss = 0.00666463\n",
      "Iteration 28967, loss = 0.00666448\n",
      "Iteration 28968, loss = 0.00666433\n",
      "Iteration 28969, loss = 0.00666418\n",
      "Iteration 28970, loss = 0.00666403\n",
      "Iteration 28971, loss = 0.00666388\n",
      "Iteration 28972, loss = 0.00666373\n",
      "Iteration 28973, loss = 0.00666358\n",
      "Iteration 28974, loss = 0.00666343\n",
      "Iteration 28975, loss = 0.00666328\n",
      "Iteration 28976, loss = 0.00666313\n",
      "Iteration 28977, loss = 0.00666298\n",
      "Iteration 28978, loss = 0.00666283\n",
      "Iteration 28979, loss = 0.00666268\n",
      "Iteration 28980, loss = 0.00666253\n",
      "Iteration 28981, loss = 0.00666238\n",
      "Iteration 28982, loss = 0.00666223\n",
      "Iteration 28983, loss = 0.00666208\n",
      "Iteration 28984, loss = 0.00666193\n",
      "Iteration 28985, loss = 0.00666178\n",
      "Iteration 28986, loss = 0.00666163\n",
      "Iteration 28987, loss = 0.00666148\n",
      "Iteration 28988, loss = 0.00666133\n",
      "Iteration 28989, loss = 0.00666118\n",
      "Iteration 28990, loss = 0.00666103\n",
      "Iteration 28991, loss = 0.00666088\n",
      "Iteration 28992, loss = 0.00666073\n",
      "Iteration 28993, loss = 0.00666058\n",
      "Iteration 28994, loss = 0.00666043\n",
      "Iteration 28995, loss = 0.00666028\n",
      "Iteration 28996, loss = 0.00666013\n",
      "Iteration 28997, loss = 0.00665998\n",
      "Iteration 28998, loss = 0.00665983\n",
      "Iteration 28999, loss = 0.00665968\n",
      "Iteration 29000, loss = 0.00665953\n",
      "Iteration 29001, loss = 0.00665938\n",
      "Iteration 29002, loss = 0.00665923\n",
      "Iteration 29003, loss = 0.00665908\n",
      "Iteration 29004, loss = 0.00665893\n",
      "Iteration 29005, loss = 0.00665878\n",
      "Iteration 29006, loss = 0.00665863\n",
      "Iteration 29007, loss = 0.00665848\n",
      "Iteration 29008, loss = 0.00665833\n",
      "Iteration 29009, loss = 0.00665818\n",
      "Iteration 29010, loss = 0.00665803\n",
      "Iteration 29011, loss = 0.00665788\n",
      "Iteration 29012, loss = 0.00665774\n",
      "Iteration 29013, loss = 0.00665759\n",
      "Iteration 29014, loss = 0.00665744\n",
      "Iteration 29015, loss = 0.00665729\n",
      "Iteration 29016, loss = 0.00665714\n",
      "Iteration 29017, loss = 0.00665699\n",
      "Iteration 29018, loss = 0.00665684\n",
      "Iteration 29019, loss = 0.00665669\n",
      "Iteration 29020, loss = 0.00665654\n",
      "Iteration 29021, loss = 0.00665639\n",
      "Iteration 29022, loss = 0.00665624\n",
      "Iteration 29023, loss = 0.00665609\n",
      "Iteration 29024, loss = 0.00665594\n",
      "Iteration 29025, loss = 0.00665579\n",
      "Iteration 29026, loss = 0.00665564\n",
      "Iteration 29027, loss = 0.00665549\n",
      "Iteration 29028, loss = 0.00665534\n",
      "Iteration 29029, loss = 0.00665519\n",
      "Iteration 29030, loss = 0.00665504\n",
      "Iteration 29031, loss = 0.00665489\n",
      "Iteration 29032, loss = 0.00665475\n",
      "Iteration 29033, loss = 0.00665460\n",
      "Iteration 29034, loss = 0.00665445\n",
      "Iteration 29035, loss = 0.00665430\n",
      "Iteration 29036, loss = 0.00665415\n",
      "Iteration 29037, loss = 0.00665400\n",
      "Iteration 29038, loss = 0.00665385\n",
      "Iteration 29039, loss = 0.00665370\n",
      "Iteration 29040, loss = 0.00665355\n",
      "Iteration 29041, loss = 0.00665340\n",
      "Iteration 29042, loss = 0.00665325\n",
      "Iteration 29043, loss = 0.00665310\n",
      "Iteration 29044, loss = 0.00665295\n",
      "Iteration 29045, loss = 0.00665281\n",
      "Iteration 29046, loss = 0.00665266\n",
      "Iteration 29047, loss = 0.00665251\n",
      "Iteration 29048, loss = 0.00665236\n",
      "Iteration 29049, loss = 0.00665221\n",
      "Iteration 29050, loss = 0.00665206\n",
      "Iteration 29051, loss = 0.00665191\n",
      "Iteration 29052, loss = 0.00665176\n",
      "Iteration 29053, loss = 0.00665161\n",
      "Iteration 29054, loss = 0.00665146\n",
      "Iteration 29055, loss = 0.00665131\n",
      "Iteration 29056, loss = 0.00665117\n",
      "Iteration 29057, loss = 0.00665102\n",
      "Iteration 29058, loss = 0.00665087\n",
      "Iteration 29059, loss = 0.00665072\n",
      "Iteration 29060, loss = 0.00665057\n",
      "Iteration 29061, loss = 0.00665042\n",
      "Iteration 29062, loss = 0.00665027\n",
      "Iteration 29063, loss = 0.00665012\n",
      "Iteration 29064, loss = 0.00664997\n",
      "Iteration 29065, loss = 0.00664983\n",
      "Iteration 29066, loss = 0.00664968\n",
      "Iteration 29067, loss = 0.00664953\n",
      "Iteration 29068, loss = 0.00664938\n",
      "Iteration 29069, loss = 0.00664923\n",
      "Iteration 29070, loss = 0.00664908\n",
      "Iteration 29071, loss = 0.00664893\n",
      "Iteration 29072, loss = 0.00664878\n",
      "Iteration 29073, loss = 0.00664863\n",
      "Iteration 29074, loss = 0.00664849\n",
      "Iteration 29075, loss = 0.00664834\n",
      "Iteration 29076, loss = 0.00664819\n",
      "Iteration 29077, loss = 0.00664804\n",
      "Iteration 29078, loss = 0.00664789\n",
      "Iteration 29079, loss = 0.00664774\n",
      "Iteration 29080, loss = 0.00664759\n",
      "Iteration 29081, loss = 0.00664745\n",
      "Iteration 29082, loss = 0.00664730\n",
      "Iteration 29083, loss = 0.00664715\n",
      "Iteration 29084, loss = 0.00664700\n",
      "Iteration 29085, loss = 0.00664685\n",
      "Iteration 29086, loss = 0.00664670\n",
      "Iteration 29087, loss = 0.00664655\n",
      "Iteration 29088, loss = 0.00664641\n",
      "Iteration 29089, loss = 0.00664626\n",
      "Iteration 29090, loss = 0.00664611\n",
      "Iteration 29091, loss = 0.00664596\n",
      "Iteration 29092, loss = 0.00664581\n",
      "Iteration 29093, loss = 0.00664566\n",
      "Iteration 29094, loss = 0.00664551\n",
      "Iteration 29095, loss = 0.00664537\n",
      "Iteration 29096, loss = 0.00664522\n",
      "Iteration 29097, loss = 0.00664507\n",
      "Iteration 29098, loss = 0.00664492\n",
      "Iteration 29099, loss = 0.00664477\n",
      "Iteration 29100, loss = 0.00664462\n",
      "Iteration 29101, loss = 0.00664448\n",
      "Iteration 29102, loss = 0.00664433\n",
      "Iteration 29103, loss = 0.00664418\n",
      "Iteration 29104, loss = 0.00664403\n",
      "Iteration 29105, loss = 0.00664388\n",
      "Iteration 29106, loss = 0.00664373\n",
      "Iteration 29107, loss = 0.00664359\n",
      "Iteration 29108, loss = 0.00664344\n",
      "Iteration 29109, loss = 0.00664329\n",
      "Iteration 29110, loss = 0.00664314\n",
      "Iteration 29111, loss = 0.00664299\n",
      "Iteration 29112, loss = 0.00664284\n",
      "Iteration 29113, loss = 0.00664270\n",
      "Iteration 29114, loss = 0.00664255\n",
      "Iteration 29115, loss = 0.00664240\n",
      "Iteration 29116, loss = 0.00664225\n",
      "Iteration 29117, loss = 0.00664210\n",
      "Iteration 29118, loss = 0.00664196\n",
      "Iteration 29119, loss = 0.00664181\n",
      "Iteration 29120, loss = 0.00664166\n",
      "Iteration 29121, loss = 0.00664151\n",
      "Iteration 29122, loss = 0.00664136\n",
      "Iteration 29123, loss = 0.00664121\n",
      "Iteration 29124, loss = 0.00664107\n",
      "Iteration 29125, loss = 0.00664092\n",
      "Iteration 29126, loss = 0.00664077\n",
      "Iteration 29127, loss = 0.00664062\n",
      "Iteration 29128, loss = 0.00664047\n",
      "Iteration 29129, loss = 0.00664033\n",
      "Iteration 29130, loss = 0.00664018\n",
      "Iteration 29131, loss = 0.00664003\n",
      "Iteration 29132, loss = 0.00663988\n",
      "Iteration 29133, loss = 0.00663974\n",
      "Iteration 29134, loss = 0.00663959\n",
      "Iteration 29135, loss = 0.00663944\n",
      "Iteration 29136, loss = 0.00663929\n",
      "Iteration 29137, loss = 0.00663914\n",
      "Iteration 29138, loss = 0.00663900\n",
      "Iteration 29139, loss = 0.00663885\n",
      "Iteration 29140, loss = 0.00663870\n",
      "Iteration 29141, loss = 0.00663855\n",
      "Iteration 29142, loss = 0.00663840\n",
      "Iteration 29143, loss = 0.00663826\n",
      "Iteration 29144, loss = 0.00663811\n",
      "Iteration 29145, loss = 0.00663796\n",
      "Iteration 29146, loss = 0.00663781\n",
      "Iteration 29147, loss = 0.00663767\n",
      "Iteration 29148, loss = 0.00663752\n",
      "Iteration 29149, loss = 0.00663737\n",
      "Iteration 29150, loss = 0.00663722\n",
      "Iteration 29151, loss = 0.00663707\n",
      "Iteration 29152, loss = 0.00663693\n",
      "Iteration 29153, loss = 0.00663678\n",
      "Iteration 29154, loss = 0.00663663\n",
      "Iteration 29155, loss = 0.00663648\n",
      "Iteration 29156, loss = 0.00663634\n",
      "Iteration 29157, loss = 0.00663619\n",
      "Iteration 29158, loss = 0.00663604\n",
      "Iteration 29159, loss = 0.00663589\n",
      "Iteration 29160, loss = 0.00663575\n",
      "Iteration 29161, loss = 0.00663560\n",
      "Iteration 29162, loss = 0.00663545\n",
      "Iteration 29163, loss = 0.00663530\n",
      "Iteration 29164, loss = 0.00663516\n",
      "Iteration 29165, loss = 0.00663501\n",
      "Iteration 29166, loss = 0.00663486\n",
      "Iteration 29167, loss = 0.00663471\n",
      "Iteration 29168, loss = 0.00663457\n",
      "Iteration 29169, loss = 0.00663442\n",
      "Iteration 29170, loss = 0.00663427\n",
      "Iteration 29171, loss = 0.00663412\n",
      "Iteration 29172, loss = 0.00663398\n",
      "Iteration 29173, loss = 0.00663383\n",
      "Iteration 29174, loss = 0.00663368\n",
      "Iteration 29175, loss = 0.00663354\n",
      "Iteration 29176, loss = 0.00663339\n",
      "Iteration 29177, loss = 0.00663324\n",
      "Iteration 29178, loss = 0.00663309\n",
      "Iteration 29179, loss = 0.00663295\n",
      "Iteration 29180, loss = 0.00663280\n",
      "Iteration 29181, loss = 0.00663265\n",
      "Iteration 29182, loss = 0.00663250\n",
      "Iteration 29183, loss = 0.00663236\n",
      "Iteration 29184, loss = 0.00663221\n",
      "Iteration 29185, loss = 0.00663206\n",
      "Iteration 29186, loss = 0.00663192\n",
      "Iteration 29187, loss = 0.00663177\n",
      "Iteration 29188, loss = 0.00663162\n",
      "Iteration 29189, loss = 0.00663147\n",
      "Iteration 29190, loss = 0.00663133\n",
      "Iteration 29191, loss = 0.00663118\n",
      "Iteration 29192, loss = 0.00663103\n",
      "Iteration 29193, loss = 0.00663089\n",
      "Iteration 29194, loss = 0.00663074\n",
      "Iteration 29195, loss = 0.00663059\n",
      "Iteration 29196, loss = 0.00663044\n",
      "Iteration 29197, loss = 0.00663030\n",
      "Iteration 29198, loss = 0.00663015\n",
      "Iteration 29199, loss = 0.00663000\n",
      "Iteration 29200, loss = 0.00662986\n",
      "Iteration 29201, loss = 0.00662971\n",
      "Iteration 29202, loss = 0.00662956\n",
      "Iteration 29203, loss = 0.00662942\n",
      "Iteration 29204, loss = 0.00662927\n",
      "Iteration 29205, loss = 0.00662912\n",
      "Iteration 29206, loss = 0.00662897\n",
      "Iteration 29207, loss = 0.00662883\n",
      "Iteration 29208, loss = 0.00662868\n",
      "Iteration 29209, loss = 0.00662853\n",
      "Iteration 29210, loss = 0.00662839\n",
      "Iteration 29211, loss = 0.00662824\n",
      "Iteration 29212, loss = 0.00662809\n",
      "Iteration 29213, loss = 0.00662795\n",
      "Iteration 29214, loss = 0.00662780\n",
      "Iteration 29215, loss = 0.00662765\n",
      "Iteration 29216, loss = 0.00662751\n",
      "Iteration 29217, loss = 0.00662736\n",
      "Iteration 29218, loss = 0.00662721\n",
      "Iteration 29219, loss = 0.00662707\n",
      "Iteration 29220, loss = 0.00662692\n",
      "Iteration 29221, loss = 0.00662677\n",
      "Iteration 29222, loss = 0.00662663\n",
      "Iteration 29223, loss = 0.00662648\n",
      "Iteration 29224, loss = 0.00662633\n",
      "Iteration 29225, loss = 0.00662619\n",
      "Iteration 29226, loss = 0.00662604\n",
      "Iteration 29227, loss = 0.00662589\n",
      "Iteration 29228, loss = 0.00662575\n",
      "Iteration 29229, loss = 0.00662560\n",
      "Iteration 29230, loss = 0.00662545\n",
      "Iteration 29231, loss = 0.00662531\n",
      "Iteration 29232, loss = 0.00662516\n",
      "Iteration 29233, loss = 0.00662501\n",
      "Iteration 29234, loss = 0.00662487\n",
      "Iteration 29235, loss = 0.00662472\n",
      "Iteration 29236, loss = 0.00662457\n",
      "Iteration 29237, loss = 0.00662443\n",
      "Iteration 29238, loss = 0.00662428\n",
      "Iteration 29239, loss = 0.00662413\n",
      "Iteration 29240, loss = 0.00662399\n",
      "Iteration 29241, loss = 0.00662384\n",
      "Iteration 29242, loss = 0.00662370\n",
      "Iteration 29243, loss = 0.00662355\n",
      "Iteration 29244, loss = 0.00662340\n",
      "Iteration 29245, loss = 0.00662326\n",
      "Iteration 29246, loss = 0.00662311\n",
      "Iteration 29247, loss = 0.00662296\n",
      "Iteration 29248, loss = 0.00662282\n",
      "Iteration 29249, loss = 0.00662267\n",
      "Iteration 29250, loss = 0.00662252\n",
      "Iteration 29251, loss = 0.00662238\n",
      "Iteration 29252, loss = 0.00662223\n",
      "Iteration 29253, loss = 0.00662209\n",
      "Iteration 29254, loss = 0.00662194\n",
      "Iteration 29255, loss = 0.00662179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29256, loss = 0.00662165\n",
      "Iteration 29257, loss = 0.00662150\n",
      "Iteration 29258, loss = 0.00662135\n",
      "Iteration 29259, loss = 0.00662121\n",
      "Iteration 29260, loss = 0.00662106\n",
      "Iteration 29261, loss = 0.00662092\n",
      "Iteration 29262, loss = 0.00662077\n",
      "Iteration 29263, loss = 0.00662062\n",
      "Iteration 29264, loss = 0.00662048\n",
      "Iteration 29265, loss = 0.00662033\n",
      "Iteration 29266, loss = 0.00662019\n",
      "Iteration 29267, loss = 0.00662004\n",
      "Iteration 29268, loss = 0.00661989\n",
      "Iteration 29269, loss = 0.00661975\n",
      "Iteration 29270, loss = 0.00661960\n",
      "Iteration 29271, loss = 0.00661946\n",
      "Iteration 29272, loss = 0.00661931\n",
      "Iteration 29273, loss = 0.00661916\n",
      "Iteration 29274, loss = 0.00661902\n",
      "Iteration 29275, loss = 0.00661887\n",
      "Iteration 29276, loss = 0.00661873\n",
      "Iteration 29277, loss = 0.00661858\n",
      "Iteration 29278, loss = 0.00661843\n",
      "Iteration 29279, loss = 0.00661829\n",
      "Iteration 29280, loss = 0.00661814\n",
      "Iteration 29281, loss = 0.00661800\n",
      "Iteration 29282, loss = 0.00661785\n",
      "Iteration 29283, loss = 0.00661770\n",
      "Iteration 29284, loss = 0.00661756\n",
      "Iteration 29285, loss = 0.00661741\n",
      "Iteration 29286, loss = 0.00661727\n",
      "Iteration 29287, loss = 0.00661712\n",
      "Iteration 29288, loss = 0.00661698\n",
      "Iteration 29289, loss = 0.00661683\n",
      "Iteration 29290, loss = 0.00661668\n",
      "Iteration 29291, loss = 0.00661654\n",
      "Iteration 29292, loss = 0.00661639\n",
      "Iteration 29293, loss = 0.00661625\n",
      "Iteration 29294, loss = 0.00661610\n",
      "Iteration 29295, loss = 0.00661596\n",
      "Iteration 29296, loss = 0.00661581\n",
      "Iteration 29297, loss = 0.00661566\n",
      "Iteration 29298, loss = 0.00661552\n",
      "Iteration 29299, loss = 0.00661537\n",
      "Iteration 29300, loss = 0.00661523\n",
      "Iteration 29301, loss = 0.00661508\n",
      "Iteration 29302, loss = 0.00661494\n",
      "Iteration 29303, loss = 0.00661479\n",
      "Iteration 29304, loss = 0.00661464\n",
      "Iteration 29305, loss = 0.00661450\n",
      "Iteration 29306, loss = 0.00661435\n",
      "Iteration 29307, loss = 0.00661421\n",
      "Iteration 29308, loss = 0.00661406\n",
      "Iteration 29309, loss = 0.00661392\n",
      "Iteration 29310, loss = 0.00661377\n",
      "Iteration 29311, loss = 0.00661363\n",
      "Iteration 29312, loss = 0.00661348\n",
      "Iteration 29313, loss = 0.00661334\n",
      "Iteration 29314, loss = 0.00661319\n",
      "Iteration 29315, loss = 0.00661304\n",
      "Iteration 29316, loss = 0.00661290\n",
      "Iteration 29317, loss = 0.00661275\n",
      "Iteration 29318, loss = 0.00661261\n",
      "Iteration 29319, loss = 0.00661246\n",
      "Iteration 29320, loss = 0.00661232\n",
      "Iteration 29321, loss = 0.00661217\n",
      "Iteration 29322, loss = 0.00661203\n",
      "Iteration 29323, loss = 0.00661188\n",
      "Iteration 29324, loss = 0.00661174\n",
      "Iteration 29325, loss = 0.00661159\n",
      "Iteration 29326, loss = 0.00661145\n",
      "Iteration 29327, loss = 0.00661130\n",
      "Iteration 29328, loss = 0.00661116\n",
      "Iteration 29329, loss = 0.00661101\n",
      "Iteration 29330, loss = 0.00661086\n",
      "Iteration 29331, loss = 0.00661072\n",
      "Iteration 29332, loss = 0.00661057\n",
      "Iteration 29333, loss = 0.00661043\n",
      "Iteration 29334, loss = 0.00661028\n",
      "Iteration 29335, loss = 0.00661014\n",
      "Iteration 29336, loss = 0.00660999\n",
      "Iteration 29337, loss = 0.00660985\n",
      "Iteration 29338, loss = 0.00660970\n",
      "Iteration 29339, loss = 0.00660956\n",
      "Iteration 29340, loss = 0.00660941\n",
      "Iteration 29341, loss = 0.00660927\n",
      "Iteration 29342, loss = 0.00660912\n",
      "Iteration 29343, loss = 0.00660898\n",
      "Iteration 29344, loss = 0.00660883\n",
      "Iteration 29345, loss = 0.00660869\n",
      "Iteration 29346, loss = 0.00660854\n",
      "Iteration 29347, loss = 0.00660840\n",
      "Iteration 29348, loss = 0.00660825\n",
      "Iteration 29349, loss = 0.00660811\n",
      "Iteration 29350, loss = 0.00660796\n",
      "Iteration 29351, loss = 0.00660782\n",
      "Iteration 29352, loss = 0.00660767\n",
      "Iteration 29353, loss = 0.00660753\n",
      "Iteration 29354, loss = 0.00660738\n",
      "Iteration 29355, loss = 0.00660724\n",
      "Iteration 29356, loss = 0.00660709\n",
      "Iteration 29357, loss = 0.00660695\n",
      "Iteration 29358, loss = 0.00660680\n",
      "Iteration 29359, loss = 0.00660666\n",
      "Iteration 29360, loss = 0.00660651\n",
      "Iteration 29361, loss = 0.00660637\n",
      "Iteration 29362, loss = 0.00660623\n",
      "Iteration 29363, loss = 0.00660608\n",
      "Iteration 29364, loss = 0.00660594\n",
      "Iteration 29365, loss = 0.00660579\n",
      "Iteration 29366, loss = 0.00660565\n",
      "Iteration 29367, loss = 0.00660550\n",
      "Iteration 29368, loss = 0.00660536\n",
      "Iteration 29369, loss = 0.00660521\n",
      "Iteration 29370, loss = 0.00660507\n",
      "Iteration 29371, loss = 0.00660492\n",
      "Iteration 29372, loss = 0.00660478\n",
      "Iteration 29373, loss = 0.00660463\n",
      "Iteration 29374, loss = 0.00660449\n",
      "Iteration 29375, loss = 0.00660434\n",
      "Iteration 29376, loss = 0.00660420\n",
      "Iteration 29377, loss = 0.00660406\n",
      "Iteration 29378, loss = 0.00660391\n",
      "Iteration 29379, loss = 0.00660377\n",
      "Iteration 29380, loss = 0.00660362\n",
      "Iteration 29381, loss = 0.00660348\n",
      "Iteration 29382, loss = 0.00660333\n",
      "Iteration 29383, loss = 0.00660319\n",
      "Iteration 29384, loss = 0.00660304\n",
      "Iteration 29385, loss = 0.00660290\n",
      "Iteration 29386, loss = 0.00660275\n",
      "Iteration 29387, loss = 0.00660261\n",
      "Iteration 29388, loss = 0.00660247\n",
      "Iteration 29389, loss = 0.00660232\n",
      "Iteration 29390, loss = 0.00660218\n",
      "Iteration 29391, loss = 0.00660203\n",
      "Iteration 29392, loss = 0.00660189\n",
      "Iteration 29393, loss = 0.00660174\n",
      "Iteration 29394, loss = 0.00660160\n",
      "Iteration 29395, loss = 0.00660146\n",
      "Iteration 29396, loss = 0.00660131\n",
      "Iteration 29397, loss = 0.00660117\n",
      "Iteration 29398, loss = 0.00660102\n",
      "Iteration 29399, loss = 0.00660088\n",
      "Iteration 29400, loss = 0.00660073\n",
      "Iteration 29401, loss = 0.00660059\n",
      "Iteration 29402, loss = 0.00660045\n",
      "Iteration 29403, loss = 0.00660030\n",
      "Iteration 29404, loss = 0.00660016\n",
      "Iteration 29405, loss = 0.00660001\n",
      "Iteration 29406, loss = 0.00659987\n",
      "Iteration 29407, loss = 0.00659972\n",
      "Iteration 29408, loss = 0.00659958\n",
      "Iteration 29409, loss = 0.00659944\n",
      "Iteration 29410, loss = 0.00659929\n",
      "Iteration 29411, loss = 0.00659915\n",
      "Iteration 29412, loss = 0.00659900\n",
      "Iteration 29413, loss = 0.00659886\n",
      "Iteration 29414, loss = 0.00659872\n",
      "Iteration 29415, loss = 0.00659857\n",
      "Iteration 29416, loss = 0.00659843\n",
      "Iteration 29417, loss = 0.00659828\n",
      "Iteration 29418, loss = 0.00659814\n",
      "Iteration 29419, loss = 0.00659800\n",
      "Iteration 29420, loss = 0.00659785\n",
      "Iteration 29421, loss = 0.00659771\n",
      "Iteration 29422, loss = 0.00659756\n",
      "Iteration 29423, loss = 0.00659742\n",
      "Iteration 29424, loss = 0.00659728\n",
      "Iteration 29425, loss = 0.00659713\n",
      "Iteration 29426, loss = 0.00659699\n",
      "Iteration 29427, loss = 0.00659684\n",
      "Iteration 29428, loss = 0.00659670\n",
      "Iteration 29429, loss = 0.00659656\n",
      "Iteration 29430, loss = 0.00659641\n",
      "Iteration 29431, loss = 0.00659627\n",
      "Iteration 29432, loss = 0.00659612\n",
      "Iteration 29433, loss = 0.00659598\n",
      "Iteration 29434, loss = 0.00659584\n",
      "Iteration 29435, loss = 0.00659569\n",
      "Iteration 29436, loss = 0.00659555\n",
      "Iteration 29437, loss = 0.00659541\n",
      "Iteration 29438, loss = 0.00659526\n",
      "Iteration 29439, loss = 0.00659512\n",
      "Iteration 29440, loss = 0.00659497\n",
      "Iteration 29441, loss = 0.00659483\n",
      "Iteration 29442, loss = 0.00659469\n",
      "Iteration 29443, loss = 0.00659454\n",
      "Iteration 29444, loss = 0.00659440\n",
      "Iteration 29445, loss = 0.00659426\n",
      "Iteration 29446, loss = 0.00659411\n",
      "Iteration 29447, loss = 0.00659397\n",
      "Iteration 29448, loss = 0.00659383\n",
      "Iteration 29449, loss = 0.00659368\n",
      "Iteration 29450, loss = 0.00659354\n",
      "Iteration 29451, loss = 0.00659339\n",
      "Iteration 29452, loss = 0.00659325\n",
      "Iteration 29453, loss = 0.00659311\n",
      "Iteration 29454, loss = 0.00659296\n",
      "Iteration 29455, loss = 0.00659282\n",
      "Iteration 29456, loss = 0.00659268\n",
      "Iteration 29457, loss = 0.00659253\n",
      "Iteration 29458, loss = 0.00659239\n",
      "Iteration 29459, loss = 0.00659225\n",
      "Iteration 29460, loss = 0.00659210\n",
      "Iteration 29461, loss = 0.00659196\n",
      "Iteration 29462, loss = 0.00659182\n",
      "Iteration 29463, loss = 0.00659167\n",
      "Iteration 29464, loss = 0.00659153\n",
      "Iteration 29465, loss = 0.00659139\n",
      "Iteration 29466, loss = 0.00659124\n",
      "Iteration 29467, loss = 0.00659110\n",
      "Iteration 29468, loss = 0.00659096\n",
      "Iteration 29469, loss = 0.00659081\n",
      "Iteration 29470, loss = 0.00659067\n",
      "Iteration 29471, loss = 0.00659053\n",
      "Iteration 29472, loss = 0.00659038\n",
      "Iteration 29473, loss = 0.00659024\n",
      "Iteration 29474, loss = 0.00659010\n",
      "Iteration 29475, loss = 0.00658995\n",
      "Iteration 29476, loss = 0.00658981\n",
      "Iteration 29477, loss = 0.00658967\n",
      "Iteration 29478, loss = 0.00658952\n",
      "Iteration 29479, loss = 0.00658938\n",
      "Iteration 29480, loss = 0.00658924\n",
      "Iteration 29481, loss = 0.00658909\n",
      "Iteration 29482, loss = 0.00658895\n",
      "Iteration 29483, loss = 0.00658881\n",
      "Iteration 29484, loss = 0.00658866\n",
      "Iteration 29485, loss = 0.00658852\n",
      "Iteration 29486, loss = 0.00658838\n",
      "Iteration 29487, loss = 0.00658824\n",
      "Iteration 29488, loss = 0.00658809\n",
      "Iteration 29489, loss = 0.00658795\n",
      "Iteration 29490, loss = 0.00658781\n",
      "Iteration 29491, loss = 0.00658766\n",
      "Iteration 29492, loss = 0.00658752\n",
      "Iteration 29493, loss = 0.00658738\n",
      "Iteration 29494, loss = 0.00658723\n",
      "Iteration 29495, loss = 0.00658709\n",
      "Iteration 29496, loss = 0.00658695\n",
      "Iteration 29497, loss = 0.00658680\n",
      "Iteration 29498, loss = 0.00658666\n",
      "Iteration 29499, loss = 0.00658652\n",
      "Iteration 29500, loss = 0.00658638\n",
      "Iteration 29501, loss = 0.00658623\n",
      "Iteration 29502, loss = 0.00658609\n",
      "Iteration 29503, loss = 0.00658595\n",
      "Iteration 29504, loss = 0.00658580\n",
      "Iteration 29505, loss = 0.00658566\n",
      "Iteration 29506, loss = 0.00658552\n",
      "Iteration 29507, loss = 0.00658538\n",
      "Iteration 29508, loss = 0.00658523\n",
      "Iteration 29509, loss = 0.00658509\n",
      "Iteration 29510, loss = 0.00658495\n",
      "Iteration 29511, loss = 0.00658480\n",
      "Iteration 29512, loss = 0.00658466\n",
      "Iteration 29513, loss = 0.00658452\n",
      "Iteration 29514, loss = 0.00658438\n",
      "Iteration 29515, loss = 0.00658423\n",
      "Iteration 29516, loss = 0.00658409\n",
      "Iteration 29517, loss = 0.00658395\n",
      "Iteration 29518, loss = 0.00658381\n",
      "Iteration 29519, loss = 0.00658366\n",
      "Iteration 29520, loss = 0.00658352\n",
      "Iteration 29521, loss = 0.00658338\n",
      "Iteration 29522, loss = 0.00658324\n",
      "Iteration 29523, loss = 0.00658309\n",
      "Iteration 29524, loss = 0.00658295\n",
      "Iteration 29525, loss = 0.00658281\n",
      "Iteration 29526, loss = 0.00658266\n",
      "Iteration 29527, loss = 0.00658252\n",
      "Iteration 29528, loss = 0.00658238\n",
      "Iteration 29529, loss = 0.00658224\n",
      "Iteration 29530, loss = 0.00658209\n",
      "Iteration 29531, loss = 0.00658195\n",
      "Iteration 29532, loss = 0.00658181\n",
      "Iteration 29533, loss = 0.00658167\n",
      "Iteration 29534, loss = 0.00658152\n",
      "Iteration 29535, loss = 0.00658138\n",
      "Iteration 29536, loss = 0.00658124\n",
      "Iteration 29537, loss = 0.00658110\n",
      "Iteration 29538, loss = 0.00658095\n",
      "Iteration 29539, loss = 0.00658081\n",
      "Iteration 29540, loss = 0.00658067\n",
      "Iteration 29541, loss = 0.00658053\n",
      "Iteration 29542, loss = 0.00658039\n",
      "Iteration 29543, loss = 0.00658024\n",
      "Iteration 29544, loss = 0.00658010\n",
      "Iteration 29545, loss = 0.00657996\n",
      "Iteration 29546, loss = 0.00657982\n",
      "Iteration 29547, loss = 0.00657967\n",
      "Iteration 29548, loss = 0.00657953\n",
      "Iteration 29549, loss = 0.00657939\n",
      "Iteration 29550, loss = 0.00657925\n",
      "Iteration 29551, loss = 0.00657910\n",
      "Iteration 29552, loss = 0.00657896\n",
      "Iteration 29553, loss = 0.00657882\n",
      "Iteration 29554, loss = 0.00657868\n",
      "Iteration 29555, loss = 0.00657854\n",
      "Iteration 29556, loss = 0.00657839\n",
      "Iteration 29557, loss = 0.00657825\n",
      "Iteration 29558, loss = 0.00657811\n",
      "Iteration 29559, loss = 0.00657797\n",
      "Iteration 29560, loss = 0.00657783\n",
      "Iteration 29561, loss = 0.00657768\n",
      "Iteration 29562, loss = 0.00657754\n",
      "Iteration 29563, loss = 0.00657740\n",
      "Iteration 29564, loss = 0.00657726\n",
      "Iteration 29565, loss = 0.00657711\n",
      "Iteration 29566, loss = 0.00657697\n",
      "Iteration 29567, loss = 0.00657683\n",
      "Iteration 29568, loss = 0.00657669\n",
      "Iteration 29569, loss = 0.00657655\n",
      "Iteration 29570, loss = 0.00657640\n",
      "Iteration 29571, loss = 0.00657626\n",
      "Iteration 29572, loss = 0.00657612\n",
      "Iteration 29573, loss = 0.00657598\n",
      "Iteration 29574, loss = 0.00657584\n",
      "Iteration 29575, loss = 0.00657569\n",
      "Iteration 29576, loss = 0.00657555\n",
      "Iteration 29577, loss = 0.00657541\n",
      "Iteration 29578, loss = 0.00657527\n",
      "Iteration 29579, loss = 0.00657513\n",
      "Iteration 29580, loss = 0.00657499\n",
      "Iteration 29581, loss = 0.00657484\n",
      "Iteration 29582, loss = 0.00657470\n",
      "Iteration 29583, loss = 0.00657456\n",
      "Iteration 29584, loss = 0.00657442\n",
      "Iteration 29585, loss = 0.00657428\n",
      "Iteration 29586, loss = 0.00657413\n",
      "Iteration 29587, loss = 0.00657399\n",
      "Iteration 29588, loss = 0.00657385\n",
      "Iteration 29589, loss = 0.00657371\n",
      "Iteration 29590, loss = 0.00657357\n",
      "Iteration 29591, loss = 0.00657343\n",
      "Iteration 29592, loss = 0.00657328\n",
      "Iteration 29593, loss = 0.00657314\n",
      "Iteration 29594, loss = 0.00657300\n",
      "Iteration 29595, loss = 0.00657286\n",
      "Iteration 29596, loss = 0.00657272\n",
      "Iteration 29597, loss = 0.00657258\n",
      "Iteration 29598, loss = 0.00657243\n",
      "Iteration 29599, loss = 0.00657229\n",
      "Iteration 29600, loss = 0.00657215\n",
      "Iteration 29601, loss = 0.00657201\n",
      "Iteration 29602, loss = 0.00657187\n",
      "Iteration 29603, loss = 0.00657173\n",
      "Iteration 29604, loss = 0.00657158\n",
      "Iteration 29605, loss = 0.00657144\n",
      "Iteration 29606, loss = 0.00657130\n",
      "Iteration 29607, loss = 0.00657116\n",
      "Iteration 29608, loss = 0.00657102\n",
      "Iteration 29609, loss = 0.00657088\n",
      "Iteration 29610, loss = 0.00657074\n",
      "Iteration 29611, loss = 0.00657059\n",
      "Iteration 29612, loss = 0.00657045\n",
      "Iteration 29613, loss = 0.00657031\n",
      "Iteration 29614, loss = 0.00657017\n",
      "Iteration 29615, loss = 0.00657003\n",
      "Iteration 29616, loss = 0.00656989\n",
      "Iteration 29617, loss = 0.00656975\n",
      "Iteration 29618, loss = 0.00656960\n",
      "Iteration 29619, loss = 0.00656946\n",
      "Iteration 29620, loss = 0.00656932\n",
      "Iteration 29621, loss = 0.00656918\n",
      "Iteration 29622, loss = 0.00656904\n",
      "Iteration 29623, loss = 0.00656890\n",
      "Iteration 29624, loss = 0.00656876\n",
      "Iteration 29625, loss = 0.00656862\n",
      "Iteration 29626, loss = 0.00656847\n",
      "Iteration 29627, loss = 0.00656833\n",
      "Iteration 29628, loss = 0.00656819\n",
      "Iteration 29629, loss = 0.00656805\n",
      "Iteration 29630, loss = 0.00656791\n",
      "Iteration 29631, loss = 0.00656777\n",
      "Iteration 29632, loss = 0.00656763\n",
      "Iteration 29633, loss = 0.00656749\n",
      "Iteration 29634, loss = 0.00656734\n",
      "Iteration 29635, loss = 0.00656720\n",
      "Iteration 29636, loss = 0.00656706\n",
      "Iteration 29637, loss = 0.00656692\n",
      "Iteration 29638, loss = 0.00656678\n",
      "Iteration 29639, loss = 0.00656664\n",
      "Iteration 29640, loss = 0.00656650\n",
      "Iteration 29641, loss = 0.00656636\n",
      "Iteration 29642, loss = 0.00656622\n",
      "Iteration 29643, loss = 0.00656607\n",
      "Iteration 29644, loss = 0.00656593\n",
      "Iteration 29645, loss = 0.00656579\n",
      "Iteration 29646, loss = 0.00656565\n",
      "Iteration 29647, loss = 0.00656551\n",
      "Iteration 29648, loss = 0.00656537\n",
      "Iteration 29649, loss = 0.00656523\n",
      "Iteration 29650, loss = 0.00656509\n",
      "Iteration 29651, loss = 0.00656495\n",
      "Iteration 29652, loss = 0.00656481\n",
      "Iteration 29653, loss = 0.00656467\n",
      "Iteration 29654, loss = 0.00656452\n",
      "Iteration 29655, loss = 0.00656438\n",
      "Iteration 29656, loss = 0.00656424\n",
      "Iteration 29657, loss = 0.00656410\n",
      "Iteration 29658, loss = 0.00656396\n",
      "Iteration 29659, loss = 0.00656382\n",
      "Iteration 29660, loss = 0.00656368\n",
      "Iteration 29661, loss = 0.00656354\n",
      "Iteration 29662, loss = 0.00656340\n",
      "Iteration 29663, loss = 0.00656326\n",
      "Iteration 29664, loss = 0.00656312\n",
      "Iteration 29665, loss = 0.00656298\n",
      "Iteration 29666, loss = 0.00656283\n",
      "Iteration 29667, loss = 0.00656269\n",
      "Iteration 29668, loss = 0.00656255\n",
      "Iteration 29669, loss = 0.00656241\n",
      "Iteration 29670, loss = 0.00656227\n",
      "Iteration 29671, loss = 0.00656213\n",
      "Iteration 29672, loss = 0.00656199\n",
      "Iteration 29673, loss = 0.00656185\n",
      "Iteration 29674, loss = 0.00656171\n",
      "Iteration 29675, loss = 0.00656157\n",
      "Iteration 29676, loss = 0.00656143\n",
      "Iteration 29677, loss = 0.00656129\n",
      "Iteration 29678, loss = 0.00656115\n",
      "Iteration 29679, loss = 0.00656101\n",
      "Iteration 29680, loss = 0.00656087\n",
      "Iteration 29681, loss = 0.00656073\n",
      "Iteration 29682, loss = 0.00656058\n",
      "Iteration 29683, loss = 0.00656044\n",
      "Iteration 29684, loss = 0.00656030\n",
      "Iteration 29685, loss = 0.00656016\n",
      "Iteration 29686, loss = 0.00656002\n",
      "Iteration 29687, loss = 0.00655988\n",
      "Iteration 29688, loss = 0.00655974\n",
      "Iteration 29689, loss = 0.00655960\n",
      "Iteration 29690, loss = 0.00655946\n",
      "Iteration 29691, loss = 0.00655932\n",
      "Iteration 29692, loss = 0.00655918\n",
      "Iteration 29693, loss = 0.00655904\n",
      "Iteration 29694, loss = 0.00655890\n",
      "Iteration 29695, loss = 0.00655876\n",
      "Iteration 29696, loss = 0.00655862\n",
      "Iteration 29697, loss = 0.00655848\n",
      "Iteration 29698, loss = 0.00655834\n",
      "Iteration 29699, loss = 0.00655820\n",
      "Iteration 29700, loss = 0.00655806\n",
      "Iteration 29701, loss = 0.00655792\n",
      "Iteration 29702, loss = 0.00655778\n",
      "Iteration 29703, loss = 0.00655764\n",
      "Iteration 29704, loss = 0.00655750\n",
      "Iteration 29705, loss = 0.00655736\n",
      "Iteration 29706, loss = 0.00655722\n",
      "Iteration 29707, loss = 0.00655708\n",
      "Iteration 29708, loss = 0.00655694\n",
      "Iteration 29709, loss = 0.00655680\n",
      "Iteration 29710, loss = 0.00655666\n",
      "Iteration 29711, loss = 0.00655652\n",
      "Iteration 29712, loss = 0.00655637\n",
      "Iteration 29713, loss = 0.00655623\n",
      "Iteration 29714, loss = 0.00655609\n",
      "Iteration 29715, loss = 0.00655595\n",
      "Iteration 29716, loss = 0.00655581\n",
      "Iteration 29717, loss = 0.00655567\n",
      "Iteration 29718, loss = 0.00655553\n",
      "Iteration 29719, loss = 0.00655539\n",
      "Iteration 29720, loss = 0.00655525\n",
      "Iteration 29721, loss = 0.00655511\n",
      "Iteration 29722, loss = 0.00655497\n",
      "Iteration 29723, loss = 0.00655483\n",
      "Iteration 29724, loss = 0.00655469\n",
      "Iteration 29725, loss = 0.00655455\n",
      "Iteration 29726, loss = 0.00655441\n",
      "Iteration 29727, loss = 0.00655427\n",
      "Iteration 29728, loss = 0.00655413\n",
      "Iteration 29729, loss = 0.00655399\n",
      "Iteration 29730, loss = 0.00655385\n",
      "Iteration 29731, loss = 0.00655371\n",
      "Iteration 29732, loss = 0.00655357\n",
      "Iteration 29733, loss = 0.00655344\n",
      "Iteration 29734, loss = 0.00655330\n",
      "Iteration 29735, loss = 0.00655316\n",
      "Iteration 29736, loss = 0.00655302\n",
      "Iteration 29737, loss = 0.00655288\n",
      "Iteration 29738, loss = 0.00655274\n",
      "Iteration 29739, loss = 0.00655260\n",
      "Iteration 29740, loss = 0.00655246\n",
      "Iteration 29741, loss = 0.00655232\n",
      "Iteration 29742, loss = 0.00655218\n",
      "Iteration 29743, loss = 0.00655204\n",
      "Iteration 29744, loss = 0.00655190\n",
      "Iteration 29745, loss = 0.00655176\n",
      "Iteration 29746, loss = 0.00655162\n",
      "Iteration 29747, loss = 0.00655148\n",
      "Iteration 29748, loss = 0.00655134\n",
      "Iteration 29749, loss = 0.00655120\n",
      "Iteration 29750, loss = 0.00655106\n",
      "Iteration 29751, loss = 0.00655092\n",
      "Iteration 29752, loss = 0.00655078\n",
      "Iteration 29753, loss = 0.00655064\n",
      "Iteration 29754, loss = 0.00655050\n",
      "Iteration 29755, loss = 0.00655036\n",
      "Iteration 29756, loss = 0.00655022\n",
      "Iteration 29757, loss = 0.00655008\n",
      "Iteration 29758, loss = 0.00654994\n",
      "Iteration 29759, loss = 0.00654980\n",
      "Iteration 29760, loss = 0.00654966\n",
      "Iteration 29761, loss = 0.00654952\n",
      "Iteration 29762, loss = 0.00654938\n",
      "Iteration 29763, loss = 0.00654925\n",
      "Iteration 29764, loss = 0.00654911\n",
      "Iteration 29765, loss = 0.00654897\n",
      "Iteration 29766, loss = 0.00654883\n",
      "Iteration 29767, loss = 0.00654869\n",
      "Iteration 29768, loss = 0.00654855\n",
      "Iteration 29769, loss = 0.00654841\n",
      "Iteration 29770, loss = 0.00654827\n",
      "Iteration 29771, loss = 0.00654813\n",
      "Iteration 29772, loss = 0.00654799\n",
      "Iteration 29773, loss = 0.00654785\n",
      "Iteration 29774, loss = 0.00654771\n",
      "Iteration 29775, loss = 0.00654757\n",
      "Iteration 29776, loss = 0.00654743\n",
      "Iteration 29777, loss = 0.00654729\n",
      "Iteration 29778, loss = 0.00654715\n",
      "Iteration 29779, loss = 0.00654702\n",
      "Iteration 29780, loss = 0.00654688\n",
      "Iteration 29781, loss = 0.00654674\n",
      "Iteration 29782, loss = 0.00654660\n",
      "Iteration 29783, loss = 0.00654646\n",
      "Iteration 29784, loss = 0.00654632\n",
      "Iteration 29785, loss = 0.00654618\n",
      "Iteration 29786, loss = 0.00654604\n",
      "Iteration 29787, loss = 0.00654590\n",
      "Iteration 29788, loss = 0.00654576\n",
      "Iteration 29789, loss = 0.00654562\n",
      "Iteration 29790, loss = 0.00654548\n",
      "Iteration 29791, loss = 0.00654535\n",
      "Iteration 29792, loss = 0.00654521\n",
      "Iteration 29793, loss = 0.00654507\n",
      "Iteration 29794, loss = 0.00654493\n",
      "Iteration 29795, loss = 0.00654479\n",
      "Iteration 29796, loss = 0.00654465\n",
      "Iteration 29797, loss = 0.00654451\n",
      "Iteration 29798, loss = 0.00654437\n",
      "Iteration 29799, loss = 0.00654423\n",
      "Iteration 29800, loss = 0.00654409\n",
      "Iteration 29801, loss = 0.00654395\n",
      "Iteration 29802, loss = 0.00654382\n",
      "Iteration 29803, loss = 0.00654368\n",
      "Iteration 29804, loss = 0.00654354\n",
      "Iteration 29805, loss = 0.00654340\n",
      "Iteration 29806, loss = 0.00654326\n",
      "Iteration 29807, loss = 0.00654312\n",
      "Iteration 29808, loss = 0.00654298\n",
      "Iteration 29809, loss = 0.00654284\n",
      "Iteration 29810, loss = 0.00654270\n",
      "Iteration 29811, loss = 0.00654257\n",
      "Iteration 29812, loss = 0.00654243\n",
      "Iteration 29813, loss = 0.00654229\n",
      "Iteration 29814, loss = 0.00654215\n",
      "Iteration 29815, loss = 0.00654201\n",
      "Iteration 29816, loss = 0.00654187\n",
      "Iteration 29817, loss = 0.00654173\n",
      "Iteration 29818, loss = 0.00654159\n",
      "Iteration 29819, loss = 0.00654145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29820, loss = 0.00654132\n",
      "Iteration 29821, loss = 0.00654118\n",
      "Iteration 29822, loss = 0.00654104\n",
      "Iteration 29823, loss = 0.00654090\n",
      "Iteration 29824, loss = 0.00654076\n",
      "Iteration 29825, loss = 0.00654062\n",
      "Iteration 29826, loss = 0.00654048\n",
      "Iteration 29827, loss = 0.00654035\n",
      "Iteration 29828, loss = 0.00654021\n",
      "Iteration 29829, loss = 0.00654007\n",
      "Iteration 29830, loss = 0.00653993\n",
      "Iteration 29831, loss = 0.00653979\n",
      "Iteration 29832, loss = 0.00653965\n",
      "Iteration 29833, loss = 0.00653951\n",
      "Iteration 29834, loss = 0.00653938\n",
      "Iteration 29835, loss = 0.00653924\n",
      "Iteration 29836, loss = 0.00653910\n",
      "Iteration 29837, loss = 0.00653896\n",
      "Iteration 29838, loss = 0.00653882\n",
      "Iteration 29839, loss = 0.00653868\n",
      "Iteration 29840, loss = 0.00653854\n",
      "Iteration 29841, loss = 0.00653841\n",
      "Iteration 29842, loss = 0.00653827\n",
      "Iteration 29843, loss = 0.00653813\n",
      "Iteration 29844, loss = 0.00653799\n",
      "Iteration 29845, loss = 0.00653785\n",
      "Iteration 29846, loss = 0.00653771\n",
      "Iteration 29847, loss = 0.00653757\n",
      "Iteration 29848, loss = 0.00653744\n",
      "Iteration 29849, loss = 0.00653730\n",
      "Iteration 29850, loss = 0.00653716\n",
      "Iteration 29851, loss = 0.00653702\n",
      "Iteration 29852, loss = 0.00653688\n",
      "Iteration 29853, loss = 0.00653674\n",
      "Iteration 29854, loss = 0.00653661\n",
      "Iteration 29855, loss = 0.00653647\n",
      "Iteration 29856, loss = 0.00653633\n",
      "Iteration 29857, loss = 0.00653619\n",
      "Iteration 29858, loss = 0.00653605\n",
      "Iteration 29859, loss = 0.00653592\n",
      "Iteration 29860, loss = 0.00653578\n",
      "Iteration 29861, loss = 0.00653564\n",
      "Iteration 29862, loss = 0.00653550\n",
      "Iteration 29863, loss = 0.00653536\n",
      "Iteration 29864, loss = 0.00653522\n",
      "Iteration 29865, loss = 0.00653509\n",
      "Iteration 29866, loss = 0.00653495\n",
      "Iteration 29867, loss = 0.00653481\n",
      "Iteration 29868, loss = 0.00653467\n",
      "Iteration 29869, loss = 0.00653453\n",
      "Iteration 29870, loss = 0.00653440\n",
      "Iteration 29871, loss = 0.00653426\n",
      "Iteration 29872, loss = 0.00653412\n",
      "Iteration 29873, loss = 0.00653398\n",
      "Iteration 29874, loss = 0.00653384\n",
      "Iteration 29875, loss = 0.00653370\n",
      "Iteration 29876, loss = 0.00653357\n",
      "Iteration 29877, loss = 0.00653343\n",
      "Iteration 29878, loss = 0.00653329\n",
      "Iteration 29879, loss = 0.00653315\n",
      "Iteration 29880, loss = 0.00653301\n",
      "Iteration 29881, loss = 0.00653288\n",
      "Iteration 29882, loss = 0.00653274\n",
      "Iteration 29883, loss = 0.00653260\n",
      "Iteration 29884, loss = 0.00653246\n",
      "Iteration 29885, loss = 0.00653233\n",
      "Iteration 29886, loss = 0.00653219\n",
      "Iteration 29887, loss = 0.00653205\n",
      "Iteration 29888, loss = 0.00653191\n",
      "Iteration 29889, loss = 0.00653177\n",
      "Iteration 29890, loss = 0.00653164\n",
      "Iteration 29891, loss = 0.00653150\n",
      "Iteration 29892, loss = 0.00653136\n",
      "Iteration 29893, loss = 0.00653122\n",
      "Iteration 29894, loss = 0.00653108\n",
      "Iteration 29895, loss = 0.00653095\n",
      "Iteration 29896, loss = 0.00653081\n",
      "Iteration 29897, loss = 0.00653067\n",
      "Iteration 29898, loss = 0.00653053\n",
      "Iteration 29899, loss = 0.00653040\n",
      "Iteration 29900, loss = 0.00653026\n",
      "Iteration 29901, loss = 0.00653012\n",
      "Iteration 29902, loss = 0.00652998\n",
      "Iteration 29903, loss = 0.00652984\n",
      "Iteration 29904, loss = 0.00652971\n",
      "Iteration 29905, loss = 0.00652957\n",
      "Iteration 29906, loss = 0.00652943\n",
      "Iteration 29907, loss = 0.00652929\n",
      "Iteration 29908, loss = 0.00652916\n",
      "Iteration 29909, loss = 0.00652902\n",
      "Iteration 29910, loss = 0.00652888\n",
      "Iteration 29911, loss = 0.00652874\n",
      "Iteration 29912, loss = 0.00652861\n",
      "Iteration 29913, loss = 0.00652847\n",
      "Iteration 29914, loss = 0.00652833\n",
      "Iteration 29915, loss = 0.00652819\n",
      "Iteration 29916, loss = 0.00652806\n",
      "Iteration 29917, loss = 0.00652792\n",
      "Iteration 29918, loss = 0.00652778\n",
      "Iteration 29919, loss = 0.00652764\n",
      "Iteration 29920, loss = 0.00652751\n",
      "Iteration 29921, loss = 0.00652737\n",
      "Iteration 29922, loss = 0.00652723\n",
      "Iteration 29923, loss = 0.00652709\n",
      "Iteration 29924, loss = 0.00652696\n",
      "Iteration 29925, loss = 0.00652682\n",
      "Iteration 29926, loss = 0.00652668\n",
      "Iteration 29927, loss = 0.00652654\n",
      "Iteration 29928, loss = 0.00652641\n",
      "Iteration 29929, loss = 0.00652627\n",
      "Iteration 29930, loss = 0.00652613\n",
      "Iteration 29931, loss = 0.00652599\n",
      "Iteration 29932, loss = 0.00652586\n",
      "Iteration 29933, loss = 0.00652572\n",
      "Iteration 29934, loss = 0.00652558\n",
      "Iteration 29935, loss = 0.00652545\n",
      "Iteration 29936, loss = 0.00652531\n",
      "Iteration 29937, loss = 0.00652517\n",
      "Iteration 29938, loss = 0.00652503\n",
      "Iteration 29939, loss = 0.00652490\n",
      "Iteration 29940, loss = 0.00652476\n",
      "Iteration 29941, loss = 0.00652462\n",
      "Iteration 29942, loss = 0.00652448\n",
      "Iteration 29943, loss = 0.00652435\n",
      "Iteration 29944, loss = 0.00652421\n",
      "Iteration 29945, loss = 0.00652407\n",
      "Iteration 29946, loss = 0.00652394\n",
      "Iteration 29947, loss = 0.00652380\n",
      "Iteration 29948, loss = 0.00652366\n",
      "Iteration 29949, loss = 0.00652352\n",
      "Iteration 29950, loss = 0.00652339\n",
      "Iteration 29951, loss = 0.00652325\n",
      "Iteration 29952, loss = 0.00652311\n",
      "Iteration 29953, loss = 0.00652298\n",
      "Iteration 29954, loss = 0.00652284\n",
      "Iteration 29955, loss = 0.00652270\n",
      "Iteration 29956, loss = 0.00652257\n",
      "Iteration 29957, loss = 0.00652243\n",
      "Iteration 29958, loss = 0.00652229\n",
      "Iteration 29959, loss = 0.00652215\n",
      "Iteration 29960, loss = 0.00652202\n",
      "Iteration 29961, loss = 0.00652188\n",
      "Iteration 29962, loss = 0.00652174\n",
      "Iteration 29963, loss = 0.00652161\n",
      "Iteration 29964, loss = 0.00652147\n",
      "Iteration 29965, loss = 0.00652133\n",
      "Iteration 29966, loss = 0.00652120\n",
      "Iteration 29967, loss = 0.00652106\n",
      "Iteration 29968, loss = 0.00652092\n",
      "Iteration 29969, loss = 0.00652078\n",
      "Iteration 29970, loss = 0.00652065\n",
      "Iteration 29971, loss = 0.00652051\n",
      "Iteration 29972, loss = 0.00652037\n",
      "Iteration 29973, loss = 0.00652024\n",
      "Iteration 29974, loss = 0.00652010\n",
      "Iteration 29975, loss = 0.00651996\n",
      "Iteration 29976, loss = 0.00651983\n",
      "Iteration 29977, loss = 0.00651969\n",
      "Iteration 29978, loss = 0.00651955\n",
      "Iteration 29979, loss = 0.00651942\n",
      "Iteration 29980, loss = 0.00651928\n",
      "Iteration 29981, loss = 0.00651914\n",
      "Iteration 29982, loss = 0.00651901\n",
      "Iteration 29983, loss = 0.00651887\n",
      "Iteration 29984, loss = 0.00651873\n",
      "Iteration 29985, loss = 0.00651860\n",
      "Iteration 29986, loss = 0.00651846\n",
      "Iteration 29987, loss = 0.00651832\n",
      "Iteration 29988, loss = 0.00651819\n",
      "Iteration 29989, loss = 0.00651805\n",
      "Iteration 29990, loss = 0.00651791\n",
      "Iteration 29991, loss = 0.00651778\n",
      "Iteration 29992, loss = 0.00651764\n",
      "Iteration 29993, loss = 0.00651750\n",
      "Iteration 29994, loss = 0.00651737\n",
      "Iteration 29995, loss = 0.00651723\n",
      "Iteration 29996, loss = 0.00651709\n",
      "Iteration 29997, loss = 0.00651696\n",
      "Iteration 29998, loss = 0.00651682\n",
      "Iteration 29999, loss = 0.00651668\n",
      "Iteration 30000, loss = 0.00651655\n",
      "Iteration 30001, loss = 0.00651641\n",
      "Iteration 30002, loss = 0.00651628\n",
      "Iteration 30003, loss = 0.00651614\n",
      "Iteration 30004, loss = 0.00651600\n",
      "Iteration 30005, loss = 0.00651587\n",
      "Iteration 30006, loss = 0.00651573\n",
      "Iteration 30007, loss = 0.00651559\n",
      "Iteration 30008, loss = 0.00651546\n",
      "Iteration 30009, loss = 0.00651532\n",
      "Iteration 30010, loss = 0.00651518\n",
      "Iteration 30011, loss = 0.00651505\n",
      "Iteration 30012, loss = 0.00651491\n",
      "Iteration 30013, loss = 0.00651478\n",
      "Iteration 30014, loss = 0.00651464\n",
      "Iteration 30015, loss = 0.00651450\n",
      "Iteration 30016, loss = 0.00651437\n",
      "Iteration 30017, loss = 0.00651423\n",
      "Iteration 30018, loss = 0.00651409\n",
      "Iteration 30019, loss = 0.00651396\n",
      "Iteration 30020, loss = 0.00651382\n",
      "Iteration 30021, loss = 0.00651369\n",
      "Iteration 30022, loss = 0.00651355\n",
      "Iteration 30023, loss = 0.00651341\n",
      "Iteration 30024, loss = 0.00651328\n",
      "Iteration 30025, loss = 0.00651314\n",
      "Iteration 30026, loss = 0.00651300\n",
      "Iteration 30027, loss = 0.00651287\n",
      "Iteration 30028, loss = 0.00651273\n",
      "Iteration 30029, loss = 0.00651260\n",
      "Iteration 30030, loss = 0.00651246\n",
      "Iteration 30031, loss = 0.00651232\n",
      "Iteration 30032, loss = 0.00651219\n",
      "Iteration 30033, loss = 0.00651205\n",
      "Iteration 30034, loss = 0.00651192\n",
      "Iteration 30035, loss = 0.00651178\n",
      "Iteration 30036, loss = 0.00651164\n",
      "Iteration 30037, loss = 0.00651151\n",
      "Iteration 30038, loss = 0.00651137\n",
      "Iteration 30039, loss = 0.00651124\n",
      "Iteration 30040, loss = 0.00651110\n",
      "Iteration 30041, loss = 0.00651096\n",
      "Iteration 30042, loss = 0.00651083\n",
      "Iteration 30043, loss = 0.00651069\n",
      "Iteration 30044, loss = 0.00651056\n",
      "Iteration 30045, loss = 0.00651042\n",
      "Iteration 30046, loss = 0.00651028\n",
      "Iteration 30047, loss = 0.00651015\n",
      "Iteration 30048, loss = 0.00651001\n",
      "Iteration 30049, loss = 0.00650988\n",
      "Iteration 30050, loss = 0.00650974\n",
      "Iteration 30051, loss = 0.00650960\n",
      "Iteration 30052, loss = 0.00650947\n",
      "Iteration 30053, loss = 0.00650933\n",
      "Iteration 30054, loss = 0.00650920\n",
      "Iteration 30055, loss = 0.00650906\n",
      "Iteration 30056, loss = 0.00650893\n",
      "Iteration 30057, loss = 0.00650879\n",
      "Iteration 30058, loss = 0.00650865\n",
      "Iteration 30059, loss = 0.00650852\n",
      "Iteration 30060, loss = 0.00650838\n",
      "Iteration 30061, loss = 0.00650825\n",
      "Iteration 30062, loss = 0.00650811\n",
      "Iteration 30063, loss = 0.00650798\n",
      "Iteration 30064, loss = 0.00650784\n",
      "Iteration 30065, loss = 0.00650770\n",
      "Iteration 30066, loss = 0.00650757\n",
      "Iteration 30067, loss = 0.00650743\n",
      "Iteration 30068, loss = 0.00650730\n",
      "Iteration 30069, loss = 0.00650716\n",
      "Iteration 30070, loss = 0.00650703\n",
      "Iteration 30071, loss = 0.00650689\n",
      "Iteration 30072, loss = 0.00650676\n",
      "Iteration 30073, loss = 0.00650662\n",
      "Iteration 30074, loss = 0.00650648\n",
      "Iteration 30075, loss = 0.00650635\n",
      "Iteration 30076, loss = 0.00650621\n",
      "Iteration 30077, loss = 0.00650608\n",
      "Iteration 30078, loss = 0.00650594\n",
      "Iteration 30079, loss = 0.00650581\n",
      "Iteration 30080, loss = 0.00650567\n",
      "Iteration 30081, loss = 0.00650554\n",
      "Iteration 30082, loss = 0.00650540\n",
      "Iteration 30083, loss = 0.00650526\n",
      "Iteration 30084, loss = 0.00650513\n",
      "Iteration 30085, loss = 0.00650499\n",
      "Iteration 30086, loss = 0.00650486\n",
      "Iteration 30087, loss = 0.00650472\n",
      "Iteration 30088, loss = 0.00650459\n",
      "Iteration 30089, loss = 0.00650445\n",
      "Iteration 30090, loss = 0.00650432\n",
      "Iteration 30091, loss = 0.00650418\n",
      "Iteration 30092, loss = 0.00650405\n",
      "Iteration 30093, loss = 0.00650391\n",
      "Iteration 30094, loss = 0.00650378\n",
      "Iteration 30095, loss = 0.00650364\n",
      "Iteration 30096, loss = 0.00650351\n",
      "Iteration 30097, loss = 0.00650337\n",
      "Iteration 30098, loss = 0.00650323\n",
      "Iteration 30099, loss = 0.00650310\n",
      "Iteration 30100, loss = 0.00650296\n",
      "Iteration 30101, loss = 0.00650283\n",
      "Iteration 30102, loss = 0.00650269\n",
      "Iteration 30103, loss = 0.00650256\n",
      "Iteration 30104, loss = 0.00650242\n",
      "Iteration 30105, loss = 0.00650229\n",
      "Iteration 30106, loss = 0.00650215\n",
      "Iteration 30107, loss = 0.00650202\n",
      "Iteration 30108, loss = 0.00650188\n",
      "Iteration 30109, loss = 0.00650175\n",
      "Iteration 30110, loss = 0.00650161\n",
      "Iteration 30111, loss = 0.00650148\n",
      "Iteration 30112, loss = 0.00650134\n",
      "Iteration 30113, loss = 0.00650121\n",
      "Iteration 30114, loss = 0.00650107\n",
      "Iteration 30115, loss = 0.00650094\n",
      "Iteration 30116, loss = 0.00650080\n",
      "Iteration 30117, loss = 0.00650067\n",
      "Iteration 30118, loss = 0.00650053\n",
      "Iteration 30119, loss = 0.00650040\n",
      "Iteration 30120, loss = 0.00650026\n",
      "Iteration 30121, loss = 0.00650013\n",
      "Iteration 30122, loss = 0.00649999\n",
      "Iteration 30123, loss = 0.00649986\n",
      "Iteration 30124, loss = 0.00649972\n",
      "Iteration 30125, loss = 0.00649959\n",
      "Iteration 30126, loss = 0.00649945\n",
      "Iteration 30127, loss = 0.00649932\n",
      "Iteration 30128, loss = 0.00649918\n",
      "Iteration 30129, loss = 0.00649905\n",
      "Iteration 30130, loss = 0.00649891\n",
      "Iteration 30131, loss = 0.00649878\n",
      "Iteration 30132, loss = 0.00649864\n",
      "Iteration 30133, loss = 0.00649851\n",
      "Iteration 30134, loss = 0.00649837\n",
      "Iteration 30135, loss = 0.00649824\n",
      "Iteration 30136, loss = 0.00649810\n",
      "Iteration 30137, loss = 0.00649797\n",
      "Iteration 30138, loss = 0.00649783\n",
      "Iteration 30139, loss = 0.00649770\n",
      "Iteration 30140, loss = 0.00649757\n",
      "Iteration 30141, loss = 0.00649743\n",
      "Iteration 30142, loss = 0.00649730\n",
      "Iteration 30143, loss = 0.00649716\n",
      "Iteration 30144, loss = 0.00649703\n",
      "Iteration 30145, loss = 0.00649689\n",
      "Iteration 30146, loss = 0.00649676\n",
      "Iteration 30147, loss = 0.00649662\n",
      "Iteration 30148, loss = 0.00649649\n",
      "Iteration 30149, loss = 0.00649635\n",
      "Iteration 30150, loss = 0.00649622\n",
      "Iteration 30151, loss = 0.00649608\n",
      "Iteration 30152, loss = 0.00649595\n",
      "Iteration 30153, loss = 0.00649581\n",
      "Iteration 30154, loss = 0.00649568\n",
      "Iteration 30155, loss = 0.00649555\n",
      "Iteration 30156, loss = 0.00649541\n",
      "Iteration 30157, loss = 0.00649528\n",
      "Iteration 30158, loss = 0.00649514\n",
      "Iteration 30159, loss = 0.00649501\n",
      "Iteration 30160, loss = 0.00649487\n",
      "Iteration 30161, loss = 0.00649474\n",
      "Iteration 30162, loss = 0.00649460\n",
      "Iteration 30163, loss = 0.00649447\n",
      "Iteration 30164, loss = 0.00649434\n",
      "Iteration 30165, loss = 0.00649420\n",
      "Iteration 30166, loss = 0.00649407\n",
      "Iteration 30167, loss = 0.00649393\n",
      "Iteration 30168, loss = 0.00649380\n",
      "Iteration 30169, loss = 0.00649366\n",
      "Iteration 30170, loss = 0.00649353\n",
      "Iteration 30171, loss = 0.00649339\n",
      "Iteration 30172, loss = 0.00649326\n",
      "Iteration 30173, loss = 0.00649313\n",
      "Iteration 30174, loss = 0.00649299\n",
      "Iteration 30175, loss = 0.00649286\n",
      "Iteration 30176, loss = 0.00649272\n",
      "Iteration 30177, loss = 0.00649259\n",
      "Iteration 30178, loss = 0.00649245\n",
      "Iteration 30179, loss = 0.00649232\n",
      "Iteration 30180, loss = 0.00649219\n",
      "Iteration 30181, loss = 0.00649205\n",
      "Iteration 30182, loss = 0.00649192\n",
      "Iteration 30183, loss = 0.00649178\n",
      "Iteration 30184, loss = 0.00649165\n",
      "Iteration 30185, loss = 0.00649151\n",
      "Iteration 30186, loss = 0.00649138\n",
      "Iteration 30187, loss = 0.00649125\n",
      "Iteration 30188, loss = 0.00649111\n",
      "Iteration 30189, loss = 0.00649098\n",
      "Iteration 30190, loss = 0.00649084\n",
      "Iteration 30191, loss = 0.00649071\n",
      "Iteration 30192, loss = 0.00649058\n",
      "Iteration 30193, loss = 0.00649044\n",
      "Iteration 30194, loss = 0.00649031\n",
      "Iteration 30195, loss = 0.00649017\n",
      "Iteration 30196, loss = 0.00649004\n",
      "Iteration 30197, loss = 0.00648991\n",
      "Iteration 30198, loss = 0.00648977\n",
      "Iteration 30199, loss = 0.00648964\n",
      "Iteration 30200, loss = 0.00648950\n",
      "Iteration 30201, loss = 0.00648937\n",
      "Iteration 30202, loss = 0.00648924\n",
      "Iteration 30203, loss = 0.00648910\n",
      "Iteration 30204, loss = 0.00648897\n",
      "Iteration 30205, loss = 0.00648883\n",
      "Iteration 30206, loss = 0.00648870\n",
      "Iteration 30207, loss = 0.00648857\n",
      "Iteration 30208, loss = 0.00648843\n",
      "Iteration 30209, loss = 0.00648830\n",
      "Iteration 30210, loss = 0.00648816\n",
      "Iteration 30211, loss = 0.00648803\n",
      "Iteration 30212, loss = 0.00648790\n",
      "Iteration 30213, loss = 0.00648776\n",
      "Iteration 30214, loss = 0.00648763\n",
      "Iteration 30215, loss = 0.00648749\n",
      "Iteration 30216, loss = 0.00648736\n",
      "Iteration 30217, loss = 0.00648723\n",
      "Iteration 30218, loss = 0.00648709\n",
      "Iteration 30219, loss = 0.00648696\n",
      "Iteration 30220, loss = 0.00648683\n",
      "Iteration 30221, loss = 0.00648669\n",
      "Iteration 30222, loss = 0.00648656\n",
      "Iteration 30223, loss = 0.00648642\n",
      "Iteration 30224, loss = 0.00648629\n",
      "Iteration 30225, loss = 0.00648616\n",
      "Iteration 30226, loss = 0.00648602\n",
      "Iteration 30227, loss = 0.00648589\n",
      "Iteration 30228, loss = 0.00648576\n",
      "Iteration 30229, loss = 0.00648562\n",
      "Iteration 30230, loss = 0.00648549\n",
      "Iteration 30231, loss = 0.00648536\n",
      "Iteration 30232, loss = 0.00648522\n",
      "Iteration 30233, loss = 0.00648509\n",
      "Iteration 30234, loss = 0.00648495\n",
      "Iteration 30235, loss = 0.00648482\n",
      "Iteration 30236, loss = 0.00648469\n",
      "Iteration 30237, loss = 0.00648455\n",
      "Iteration 30238, loss = 0.00648442\n",
      "Iteration 30239, loss = 0.00648429\n",
      "Iteration 30240, loss = 0.00648415\n",
      "Iteration 30241, loss = 0.00648402\n",
      "Iteration 30242, loss = 0.00648389\n",
      "Iteration 30243, loss = 0.00648375\n",
      "Iteration 30244, loss = 0.00648362\n",
      "Iteration 30245, loss = 0.00648349\n",
      "Iteration 30246, loss = 0.00648335\n",
      "Iteration 30247, loss = 0.00648322\n",
      "Iteration 30248, loss = 0.00648309\n",
      "Iteration 30249, loss = 0.00648295\n",
      "Iteration 30250, loss = 0.00648282\n",
      "Iteration 30251, loss = 0.00648269\n",
      "Iteration 30252, loss = 0.00648255\n",
      "Iteration 30253, loss = 0.00648242\n",
      "Iteration 30254, loss = 0.00648229\n",
      "Iteration 30255, loss = 0.00648215\n",
      "Iteration 30256, loss = 0.00648202\n",
      "Iteration 30257, loss = 0.00648189\n",
      "Iteration 30258, loss = 0.00648175\n",
      "Iteration 30259, loss = 0.00648162\n",
      "Iteration 30260, loss = 0.00648149\n",
      "Iteration 30261, loss = 0.00648135\n",
      "Iteration 30262, loss = 0.00648122\n",
      "Iteration 30263, loss = 0.00648109\n",
      "Iteration 30264, loss = 0.00648095\n",
      "Iteration 30265, loss = 0.00648082\n",
      "Iteration 30266, loss = 0.00648069\n",
      "Iteration 30267, loss = 0.00648055\n",
      "Iteration 30268, loss = 0.00648042\n",
      "Iteration 30269, loss = 0.00648029\n",
      "Iteration 30270, loss = 0.00648015\n",
      "Iteration 30271, loss = 0.00648002\n",
      "Iteration 30272, loss = 0.00647989\n",
      "Iteration 30273, loss = 0.00647975\n",
      "Iteration 30274, loss = 0.00647962\n",
      "Iteration 30275, loss = 0.00647949\n",
      "Iteration 30276, loss = 0.00647935\n",
      "Iteration 30277, loss = 0.00647922\n",
      "Iteration 30278, loss = 0.00647909\n",
      "Iteration 30279, loss = 0.00647896\n",
      "Iteration 30280, loss = 0.00647882\n",
      "Iteration 30281, loss = 0.00647869\n",
      "Iteration 30282, loss = 0.00647856\n",
      "Iteration 30283, loss = 0.00647842\n",
      "Iteration 30284, loss = 0.00647829\n",
      "Iteration 30285, loss = 0.00647816\n",
      "Iteration 30286, loss = 0.00647802\n",
      "Iteration 30287, loss = 0.00647789\n",
      "Iteration 30288, loss = 0.00647776\n",
      "Iteration 30289, loss = 0.00647763\n",
      "Iteration 30290, loss = 0.00647749\n",
      "Iteration 30291, loss = 0.00647736\n",
      "Iteration 30292, loss = 0.00647723\n",
      "Iteration 30293, loss = 0.00647709\n",
      "Iteration 30294, loss = 0.00647696\n",
      "Iteration 30295, loss = 0.00647683\n",
      "Iteration 30296, loss = 0.00647670\n",
      "Iteration 30297, loss = 0.00647656\n",
      "Iteration 30298, loss = 0.00647643\n",
      "Iteration 30299, loss = 0.00647630\n",
      "Iteration 30300, loss = 0.00647616\n",
      "Iteration 30301, loss = 0.00647603\n",
      "Iteration 30302, loss = 0.00647590\n",
      "Iteration 30303, loss = 0.00647577\n",
      "Iteration 30304, loss = 0.00647563\n",
      "Iteration 30305, loss = 0.00647550\n",
      "Iteration 30306, loss = 0.00647537\n",
      "Iteration 30307, loss = 0.00647524\n",
      "Iteration 30308, loss = 0.00647510\n",
      "Iteration 30309, loss = 0.00647497\n",
      "Iteration 30310, loss = 0.00647484\n",
      "Iteration 30311, loss = 0.00647470\n",
      "Iteration 30312, loss = 0.00647457\n",
      "Iteration 30313, loss = 0.00647444\n",
      "Iteration 30314, loss = 0.00647431\n",
      "Iteration 30315, loss = 0.00647417\n",
      "Iteration 30316, loss = 0.00647404\n",
      "Iteration 30317, loss = 0.00647391\n",
      "Iteration 30318, loss = 0.00647378\n",
      "Iteration 30319, loss = 0.00647364\n",
      "Iteration 30320, loss = 0.00647351\n",
      "Iteration 30321, loss = 0.00647338\n",
      "Iteration 30322, loss = 0.00647325\n",
      "Iteration 30323, loss = 0.00647311\n",
      "Iteration 30324, loss = 0.00647298\n",
      "Iteration 30325, loss = 0.00647285\n",
      "Iteration 30326, loss = 0.00647272\n",
      "Iteration 30327, loss = 0.00647258\n",
      "Iteration 30328, loss = 0.00647245\n",
      "Iteration 30329, loss = 0.00647232\n",
      "Iteration 30330, loss = 0.00647219\n",
      "Iteration 30331, loss = 0.00647205\n",
      "Iteration 30332, loss = 0.00647192\n",
      "Iteration 30333, loss = 0.00647179\n",
      "Iteration 30334, loss = 0.00647166\n",
      "Iteration 30335, loss = 0.00647152\n",
      "Iteration 30336, loss = 0.00647139\n",
      "Iteration 30337, loss = 0.00647126\n",
      "Iteration 30338, loss = 0.00647113\n",
      "Iteration 30339, loss = 0.00647099\n",
      "Iteration 30340, loss = 0.00647086\n",
      "Iteration 30341, loss = 0.00647073\n",
      "Iteration 30342, loss = 0.00647060\n",
      "Iteration 30343, loss = 0.00647047\n",
      "Iteration 30344, loss = 0.00647033\n",
      "Iteration 30345, loss = 0.00647020\n",
      "Iteration 30346, loss = 0.00647007\n",
      "Iteration 30347, loss = 0.00646994\n",
      "Iteration 30348, loss = 0.00646980\n",
      "Iteration 30349, loss = 0.00646967\n",
      "Iteration 30350, loss = 0.00646954\n",
      "Iteration 30351, loss = 0.00646941\n",
      "Iteration 30352, loss = 0.00646928\n",
      "Iteration 30353, loss = 0.00646914\n",
      "Iteration 30354, loss = 0.00646901\n",
      "Iteration 30355, loss = 0.00646888\n",
      "Iteration 30356, loss = 0.00646875\n",
      "Iteration 30357, loss = 0.00646861\n",
      "Iteration 30358, loss = 0.00646848\n",
      "Iteration 30359, loss = 0.00646835\n",
      "Iteration 30360, loss = 0.00646822\n",
      "Iteration 30361, loss = 0.00646809\n",
      "Iteration 30362, loss = 0.00646795\n",
      "Iteration 30363, loss = 0.00646782\n",
      "Iteration 30364, loss = 0.00646769\n",
      "Iteration 30365, loss = 0.00646756\n",
      "Iteration 30366, loss = 0.00646743\n",
      "Iteration 30367, loss = 0.00646729\n",
      "Iteration 30368, loss = 0.00646716\n",
      "Iteration 30369, loss = 0.00646703\n",
      "Iteration 30370, loss = 0.00646690\n",
      "Iteration 30371, loss = 0.00646677\n",
      "Iteration 30372, loss = 0.00646663\n",
      "Iteration 30373, loss = 0.00646650\n",
      "Iteration 30374, loss = 0.00646637\n",
      "Iteration 30375, loss = 0.00646624\n",
      "Iteration 30376, loss = 0.00646611\n",
      "Iteration 30377, loss = 0.00646598\n",
      "Iteration 30378, loss = 0.00646584\n",
      "Iteration 30379, loss = 0.00646571\n",
      "Iteration 30380, loss = 0.00646558\n",
      "Iteration 30381, loss = 0.00646545\n",
      "Iteration 30382, loss = 0.00646532\n",
      "Iteration 30383, loss = 0.00646518\n",
      "Iteration 30384, loss = 0.00646505\n",
      "Iteration 30385, loss = 0.00646492\n",
      "Iteration 30386, loss = 0.00646479\n",
      "Iteration 30387, loss = 0.00646466\n",
      "Iteration 30388, loss = 0.00646453\n",
      "Iteration 30389, loss = 0.00646439\n",
      "Iteration 30390, loss = 0.00646426\n",
      "Iteration 30391, loss = 0.00646413\n",
      "Iteration 30392, loss = 0.00646400\n",
      "Iteration 30393, loss = 0.00646387\n",
      "Iteration 30394, loss = 0.00646374\n",
      "Iteration 30395, loss = 0.00646360\n",
      "Iteration 30396, loss = 0.00646347\n",
      "Iteration 30397, loss = 0.00646334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30398, loss = 0.00646321\n",
      "Iteration 30399, loss = 0.00646308\n",
      "Iteration 30400, loss = 0.00646295\n",
      "Iteration 30401, loss = 0.00646281\n",
      "Iteration 30402, loss = 0.00646268\n",
      "Iteration 30403, loss = 0.00646255\n",
      "Iteration 30404, loss = 0.00646242\n",
      "Iteration 30405, loss = 0.00646229\n",
      "Iteration 30406, loss = 0.00646216\n",
      "Iteration 30407, loss = 0.00646202\n",
      "Iteration 30408, loss = 0.00646189\n",
      "Iteration 30409, loss = 0.00646176\n",
      "Iteration 30410, loss = 0.00646163\n",
      "Iteration 30411, loss = 0.00646150\n",
      "Iteration 30412, loss = 0.00646137\n",
      "Iteration 30413, loss = 0.00646124\n",
      "Iteration 30414, loss = 0.00646110\n",
      "Iteration 30415, loss = 0.00646097\n",
      "Iteration 30416, loss = 0.00646084\n",
      "Iteration 30417, loss = 0.00646071\n",
      "Iteration 30418, loss = 0.00646058\n",
      "Iteration 30419, loss = 0.00646045\n",
      "Iteration 30420, loss = 0.00646032\n",
      "Iteration 30421, loss = 0.00646018\n",
      "Iteration 30422, loss = 0.00646005\n",
      "Iteration 30423, loss = 0.00645992\n",
      "Iteration 30424, loss = 0.00645979\n",
      "Iteration 30425, loss = 0.00645966\n",
      "Iteration 30426, loss = 0.00645953\n",
      "Iteration 30427, loss = 0.00645940\n",
      "Iteration 30428, loss = 0.00645927\n",
      "Iteration 30429, loss = 0.00645913\n",
      "Iteration 30430, loss = 0.00645900\n",
      "Iteration 30431, loss = 0.00645887\n",
      "Iteration 30432, loss = 0.00645874\n",
      "Iteration 30433, loss = 0.00645861\n",
      "Iteration 30434, loss = 0.00645848\n",
      "Iteration 30435, loss = 0.00645835\n",
      "Iteration 30436, loss = 0.00645822\n",
      "Iteration 30437, loss = 0.00645808\n",
      "Iteration 30438, loss = 0.00645795\n",
      "Iteration 30439, loss = 0.00645782\n",
      "Iteration 30440, loss = 0.00645769\n",
      "Iteration 30441, loss = 0.00645756\n",
      "Iteration 30442, loss = 0.00645743\n",
      "Iteration 30443, loss = 0.00645730\n",
      "Iteration 30444, loss = 0.00645717\n",
      "Iteration 30445, loss = 0.00645704\n",
      "Iteration 30446, loss = 0.00645690\n",
      "Iteration 30447, loss = 0.00645677\n",
      "Iteration 30448, loss = 0.00645664\n",
      "Iteration 30449, loss = 0.00645651\n",
      "Iteration 30450, loss = 0.00645638\n",
      "Iteration 30451, loss = 0.00645625\n",
      "Iteration 30452, loss = 0.00645612\n",
      "Iteration 30453, loss = 0.00645599\n",
      "Iteration 30454, loss = 0.00645586\n",
      "Iteration 30455, loss = 0.00645573\n",
      "Iteration 30456, loss = 0.00645560\n",
      "Iteration 30457, loss = 0.00645546\n",
      "Iteration 30458, loss = 0.00645533\n",
      "Iteration 30459, loss = 0.00645520\n",
      "Iteration 30460, loss = 0.00645507\n",
      "Iteration 30461, loss = 0.00645494\n",
      "Iteration 30462, loss = 0.00645481\n",
      "Iteration 30463, loss = 0.00645468\n",
      "Iteration 30464, loss = 0.00645455\n",
      "Iteration 30465, loss = 0.00645442\n",
      "Iteration 30466, loss = 0.00645429\n",
      "Iteration 30467, loss = 0.00645416\n",
      "Iteration 30468, loss = 0.00645402\n",
      "Iteration 30469, loss = 0.00645389\n",
      "Iteration 30470, loss = 0.00645376\n",
      "Iteration 30471, loss = 0.00645363\n",
      "Iteration 30472, loss = 0.00645350\n",
      "Iteration 30473, loss = 0.00645337\n",
      "Iteration 30474, loss = 0.00645324\n",
      "Iteration 30475, loss = 0.00645311\n",
      "Iteration 30476, loss = 0.00645298\n",
      "Iteration 30477, loss = 0.00645285\n",
      "Iteration 30478, loss = 0.00645272\n",
      "Iteration 30479, loss = 0.00645259\n",
      "Iteration 30480, loss = 0.00645246\n",
      "Iteration 30481, loss = 0.00645233\n",
      "Iteration 30482, loss = 0.00645220\n",
      "Iteration 30483, loss = 0.00645206\n",
      "Iteration 30484, loss = 0.00645193\n",
      "Iteration 30485, loss = 0.00645180\n",
      "Iteration 30486, loss = 0.00645167\n",
      "Iteration 30487, loss = 0.00645154\n",
      "Iteration 30488, loss = 0.00645141\n",
      "Iteration 30489, loss = 0.00645128\n",
      "Iteration 30490, loss = 0.00645115\n",
      "Iteration 30491, loss = 0.00645102\n",
      "Iteration 30492, loss = 0.00645089\n",
      "Iteration 30493, loss = 0.00645076\n",
      "Iteration 30494, loss = 0.00645063\n",
      "Iteration 30495, loss = 0.00645050\n",
      "Iteration 30496, loss = 0.00645037\n",
      "Iteration 30497, loss = 0.00645024\n",
      "Iteration 30498, loss = 0.00645011\n",
      "Iteration 30499, loss = 0.00644998\n",
      "Iteration 30500, loss = 0.00644985\n",
      "Iteration 30501, loss = 0.00644972\n",
      "Iteration 30502, loss = 0.00644959\n",
      "Iteration 30503, loss = 0.00644945\n",
      "Iteration 30504, loss = 0.00644932\n",
      "Iteration 30505, loss = 0.00644919\n",
      "Iteration 30506, loss = 0.00644906\n",
      "Iteration 30507, loss = 0.00644893\n",
      "Iteration 30508, loss = 0.00644880\n",
      "Iteration 30509, loss = 0.00644867\n",
      "Iteration 30510, loss = 0.00644854\n",
      "Iteration 30511, loss = 0.00644841\n",
      "Iteration 30512, loss = 0.00644828\n",
      "Iteration 30513, loss = 0.00644815\n",
      "Iteration 30514, loss = 0.00644802\n",
      "Iteration 30515, loss = 0.00644789\n",
      "Iteration 30516, loss = 0.00644776\n",
      "Iteration 30517, loss = 0.00644763\n",
      "Iteration 30518, loss = 0.00644750\n",
      "Iteration 30519, loss = 0.00644737\n",
      "Iteration 30520, loss = 0.00644724\n",
      "Iteration 30521, loss = 0.00644711\n",
      "Iteration 30522, loss = 0.00644698\n",
      "Iteration 30523, loss = 0.00644685\n",
      "Iteration 30524, loss = 0.00644672\n",
      "Iteration 30525, loss = 0.00644659\n",
      "Iteration 30526, loss = 0.00644646\n",
      "Iteration 30527, loss = 0.00644633\n",
      "Iteration 30528, loss = 0.00644620\n",
      "Iteration 30529, loss = 0.00644607\n",
      "Iteration 30530, loss = 0.00644594\n",
      "Iteration 30531, loss = 0.00644581\n",
      "Iteration 30532, loss = 0.00644568\n",
      "Iteration 30533, loss = 0.00644555\n",
      "Iteration 30534, loss = 0.00644542\n",
      "Iteration 30535, loss = 0.00644529\n",
      "Iteration 30536, loss = 0.00644516\n",
      "Iteration 30537, loss = 0.00644503\n",
      "Iteration 30538, loss = 0.00644490\n",
      "Iteration 30539, loss = 0.00644477\n",
      "Iteration 30540, loss = 0.00644464\n",
      "Iteration 30541, loss = 0.00644451\n",
      "Iteration 30542, loss = 0.00644438\n",
      "Iteration 30543, loss = 0.00644425\n",
      "Iteration 30544, loss = 0.00644412\n",
      "Iteration 30545, loss = 0.00644399\n",
      "Iteration 30546, loss = 0.00644386\n",
      "Iteration 30547, loss = 0.00644373\n",
      "Iteration 30548, loss = 0.00644360\n",
      "Iteration 30549, loss = 0.00644347\n",
      "Iteration 30550, loss = 0.00644334\n",
      "Iteration 30551, loss = 0.00644321\n",
      "Iteration 30552, loss = 0.00644308\n",
      "Iteration 30553, loss = 0.00644295\n",
      "Iteration 30554, loss = 0.00644282\n",
      "Iteration 30555, loss = 0.00644269\n",
      "Iteration 30556, loss = 0.00644256\n",
      "Iteration 30557, loss = 0.00644243\n",
      "Iteration 30558, loss = 0.00644230\n",
      "Iteration 30559, loss = 0.00644217\n",
      "Iteration 30560, loss = 0.00644204\n",
      "Iteration 30561, loss = 0.00644191\n",
      "Iteration 30562, loss = 0.00644178\n",
      "Iteration 30563, loss = 0.00644165\n",
      "Iteration 30564, loss = 0.00644152\n",
      "Iteration 30565, loss = 0.00644139\n",
      "Iteration 30566, loss = 0.00644126\n",
      "Iteration 30567, loss = 0.00644113\n",
      "Iteration 30568, loss = 0.00644101\n",
      "Iteration 30569, loss = 0.00644088\n",
      "Iteration 30570, loss = 0.00644075\n",
      "Iteration 30571, loss = 0.00644062\n",
      "Iteration 30572, loss = 0.00644049\n",
      "Iteration 30573, loss = 0.00644036\n",
      "Iteration 30574, loss = 0.00644023\n",
      "Iteration 30575, loss = 0.00644010\n",
      "Iteration 30576, loss = 0.00643997\n",
      "Iteration 30577, loss = 0.00643984\n",
      "Iteration 30578, loss = 0.00643971\n",
      "Iteration 30579, loss = 0.00643958\n",
      "Iteration 30580, loss = 0.00643945\n",
      "Iteration 30581, loss = 0.00643932\n",
      "Iteration 30582, loss = 0.00643919\n",
      "Iteration 30583, loss = 0.00643906\n",
      "Iteration 30584, loss = 0.00643893\n",
      "Iteration 30585, loss = 0.00643880\n",
      "Iteration 30586, loss = 0.00643867\n",
      "Iteration 30587, loss = 0.00643854\n",
      "Iteration 30588, loss = 0.00643842\n",
      "Iteration 30589, loss = 0.00643829\n",
      "Iteration 30590, loss = 0.00643816\n",
      "Iteration 30591, loss = 0.00643803\n",
      "Iteration 30592, loss = 0.00643790\n",
      "Iteration 30593, loss = 0.00643777\n",
      "Iteration 30594, loss = 0.00643764\n",
      "Iteration 30595, loss = 0.00643751\n",
      "Iteration 30596, loss = 0.00643738\n",
      "Iteration 30597, loss = 0.00643725\n",
      "Iteration 30598, loss = 0.00643712\n",
      "Iteration 30599, loss = 0.00643699\n",
      "Iteration 30600, loss = 0.00643686\n",
      "Iteration 30601, loss = 0.00643673\n",
      "Iteration 30602, loss = 0.00643661\n",
      "Iteration 30603, loss = 0.00643648\n",
      "Iteration 30604, loss = 0.00643635\n",
      "Iteration 30605, loss = 0.00643622\n",
      "Iteration 30606, loss = 0.00643609\n",
      "Iteration 30607, loss = 0.00643596\n",
      "Iteration 30608, loss = 0.00643583\n",
      "Iteration 30609, loss = 0.00643570\n",
      "Iteration 30610, loss = 0.00643557\n",
      "Iteration 30611, loss = 0.00643544\n",
      "Iteration 30612, loss = 0.00643531\n",
      "Iteration 30613, loss = 0.00643518\n",
      "Iteration 30614, loss = 0.00643506\n",
      "Iteration 30615, loss = 0.00643493\n",
      "Iteration 30616, loss = 0.00643480\n",
      "Iteration 30617, loss = 0.00643467\n",
      "Iteration 30618, loss = 0.00643454\n",
      "Iteration 30619, loss = 0.00643441\n",
      "Iteration 30620, loss = 0.00643428\n",
      "Iteration 30621, loss = 0.00643415\n",
      "Iteration 30622, loss = 0.00643402\n",
      "Iteration 30623, loss = 0.00643389\n",
      "Iteration 30624, loss = 0.00643377\n",
      "Iteration 30625, loss = 0.00643364\n",
      "Iteration 30626, loss = 0.00643351\n",
      "Iteration 30627, loss = 0.00643338\n",
      "Iteration 30628, loss = 0.00643325\n",
      "Iteration 30629, loss = 0.00643312\n",
      "Iteration 30630, loss = 0.00643299\n",
      "Iteration 30631, loss = 0.00643286\n",
      "Iteration 30632, loss = 0.00643273\n",
      "Iteration 30633, loss = 0.00643260\n",
      "Iteration 30634, loss = 0.00643248\n",
      "Iteration 30635, loss = 0.00643235\n",
      "Iteration 30636, loss = 0.00643222\n",
      "Iteration 30637, loss = 0.00643209\n",
      "Iteration 30638, loss = 0.00643196\n",
      "Iteration 30639, loss = 0.00643183\n",
      "Iteration 30640, loss = 0.00643170\n",
      "Iteration 30641, loss = 0.00643157\n",
      "Iteration 30642, loss = 0.00643145\n",
      "Iteration 30643, loss = 0.00643132\n",
      "Iteration 30644, loss = 0.00643119\n",
      "Iteration 30645, loss = 0.00643106\n",
      "Iteration 30646, loss = 0.00643093\n",
      "Iteration 30647, loss = 0.00643080\n",
      "Iteration 30648, loss = 0.00643067\n",
      "Iteration 30649, loss = 0.00643054\n",
      "Iteration 30650, loss = 0.00643042\n",
      "Iteration 30651, loss = 0.00643029\n",
      "Iteration 30652, loss = 0.00643016\n",
      "Iteration 30653, loss = 0.00643003\n",
      "Iteration 30654, loss = 0.00642990\n",
      "Iteration 30655, loss = 0.00642977\n",
      "Iteration 30656, loss = 0.00642964\n",
      "Iteration 30657, loss = 0.00642952\n",
      "Iteration 30658, loss = 0.00642939\n",
      "Iteration 30659, loss = 0.00642926\n",
      "Iteration 30660, loss = 0.00642913\n",
      "Iteration 30661, loss = 0.00642900\n",
      "Iteration 30662, loss = 0.00642887\n",
      "Iteration 30663, loss = 0.00642874\n",
      "Iteration 30664, loss = 0.00642862\n",
      "Iteration 30665, loss = 0.00642849\n",
      "Iteration 30666, loss = 0.00642836\n",
      "Iteration 30667, loss = 0.00642823\n",
      "Iteration 30668, loss = 0.00642810\n",
      "Iteration 30669, loss = 0.00642797\n",
      "Iteration 30670, loss = 0.00642785\n",
      "Iteration 30671, loss = 0.00642772\n",
      "Iteration 30672, loss = 0.00642759\n",
      "Iteration 30673, loss = 0.00642746\n",
      "Iteration 30674, loss = 0.00642733\n",
      "Iteration 30675, loss = 0.00642720\n",
      "Iteration 30676, loss = 0.00642707\n",
      "Iteration 30677, loss = 0.00642695\n",
      "Iteration 30678, loss = 0.00642682\n",
      "Iteration 30679, loss = 0.00642669\n",
      "Iteration 30680, loss = 0.00642656\n",
      "Iteration 30681, loss = 0.00642643\n",
      "Iteration 30682, loss = 0.00642630\n",
      "Iteration 30683, loss = 0.00642618\n",
      "Iteration 30684, loss = 0.00642605\n",
      "Iteration 30685, loss = 0.00642592\n",
      "Iteration 30686, loss = 0.00642579\n",
      "Iteration 30687, loss = 0.00642566\n",
      "Iteration 30688, loss = 0.00642554\n",
      "Iteration 30689, loss = 0.00642541\n",
      "Iteration 30690, loss = 0.00642528\n",
      "Iteration 30691, loss = 0.00642515\n",
      "Iteration 30692, loss = 0.00642502\n",
      "Iteration 30693, loss = 0.00642489\n",
      "Iteration 30694, loss = 0.00642477\n",
      "Iteration 30695, loss = 0.00642464\n",
      "Iteration 30696, loss = 0.00642451\n",
      "Iteration 30697, loss = 0.00642438\n",
      "Iteration 30698, loss = 0.00642425\n",
      "Iteration 30699, loss = 0.00642413\n",
      "Iteration 30700, loss = 0.00642400\n",
      "Iteration 30701, loss = 0.00642387\n",
      "Iteration 30702, loss = 0.00642374\n",
      "Iteration 30703, loss = 0.00642361\n",
      "Iteration 30704, loss = 0.00642348\n",
      "Iteration 30705, loss = 0.00642336\n",
      "Iteration 30706, loss = 0.00642323\n",
      "Iteration 30707, loss = 0.00642310\n",
      "Iteration 30708, loss = 0.00642297\n",
      "Iteration 30709, loss = 0.00642284\n",
      "Iteration 30710, loss = 0.00642272\n",
      "Iteration 30711, loss = 0.00642259\n",
      "Iteration 30712, loss = 0.00642246\n",
      "Iteration 30713, loss = 0.00642233\n",
      "Iteration 30714, loss = 0.00642221\n",
      "Iteration 30715, loss = 0.00642208\n",
      "Iteration 30716, loss = 0.00642195\n",
      "Iteration 30717, loss = 0.00642182\n",
      "Iteration 30718, loss = 0.00642169\n",
      "Iteration 30719, loss = 0.00642157\n",
      "Iteration 30720, loss = 0.00642144\n",
      "Iteration 30721, loss = 0.00642131\n",
      "Iteration 30722, loss = 0.00642118\n",
      "Iteration 30723, loss = 0.00642105\n",
      "Iteration 30724, loss = 0.00642093\n",
      "Iteration 30725, loss = 0.00642080\n",
      "Iteration 30726, loss = 0.00642067\n",
      "Iteration 30727, loss = 0.00642054\n",
      "Iteration 30728, loss = 0.00642042\n",
      "Iteration 30729, loss = 0.00642029\n",
      "Iteration 30730, loss = 0.00642016\n",
      "Iteration 30731, loss = 0.00642003\n",
      "Iteration 30732, loss = 0.00641990\n",
      "Iteration 30733, loss = 0.00641978\n",
      "Iteration 30734, loss = 0.00641965\n",
      "Iteration 30735, loss = 0.00641952\n",
      "Iteration 30736, loss = 0.00641939\n",
      "Iteration 30737, loss = 0.00641927\n",
      "Iteration 30738, loss = 0.00641914\n",
      "Iteration 30739, loss = 0.00641901\n",
      "Iteration 30740, loss = 0.00641888\n",
      "Iteration 30741, loss = 0.00641876\n",
      "Iteration 30742, loss = 0.00641863\n",
      "Iteration 30743, loss = 0.00641850\n",
      "Iteration 30744, loss = 0.00641837\n",
      "Iteration 30745, loss = 0.00641824\n",
      "Iteration 30746, loss = 0.00641812\n",
      "Iteration 30747, loss = 0.00641799\n",
      "Iteration 30748, loss = 0.00641786\n",
      "Iteration 30749, loss = 0.00641773\n",
      "Iteration 30750, loss = 0.00641761\n",
      "Iteration 30751, loss = 0.00641748\n",
      "Iteration 30752, loss = 0.00641735\n",
      "Iteration 30753, loss = 0.00641722\n",
      "Iteration 30754, loss = 0.00641710\n",
      "Iteration 30755, loss = 0.00641697\n",
      "Iteration 30756, loss = 0.00641684\n",
      "Iteration 30757, loss = 0.00641671\n",
      "Iteration 30758, loss = 0.00641659\n",
      "Iteration 30759, loss = 0.00641646\n",
      "Iteration 30760, loss = 0.00641633\n",
      "Iteration 30761, loss = 0.00641621\n",
      "Iteration 30762, loss = 0.00641608\n",
      "Iteration 30763, loss = 0.00641595\n",
      "Iteration 30764, loss = 0.00641582\n",
      "Iteration 30765, loss = 0.00641570\n",
      "Iteration 30766, loss = 0.00641557\n",
      "Iteration 30767, loss = 0.00641544\n",
      "Iteration 30768, loss = 0.00641531\n",
      "Iteration 30769, loss = 0.00641519\n",
      "Iteration 30770, loss = 0.00641506\n",
      "Iteration 30771, loss = 0.00641493\n",
      "Iteration 30772, loss = 0.00641480\n",
      "Iteration 30773, loss = 0.00641468\n",
      "Iteration 30774, loss = 0.00641455\n",
      "Iteration 30775, loss = 0.00641442\n",
      "Iteration 30776, loss = 0.00641430\n",
      "Iteration 30777, loss = 0.00641417\n",
      "Iteration 30778, loss = 0.00641404\n",
      "Iteration 30779, loss = 0.00641391\n",
      "Iteration 30780, loss = 0.00641379\n",
      "Iteration 30781, loss = 0.00641366\n",
      "Iteration 30782, loss = 0.00641353\n",
      "Iteration 30783, loss = 0.00641341\n",
      "Iteration 30784, loss = 0.00641328\n",
      "Iteration 30785, loss = 0.00641315\n",
      "Iteration 30786, loss = 0.00641302\n",
      "Iteration 30787, loss = 0.00641290\n",
      "Iteration 30788, loss = 0.00641277\n",
      "Iteration 30789, loss = 0.00641264\n",
      "Iteration 30790, loss = 0.00641252\n",
      "Iteration 30791, loss = 0.00641239\n",
      "Iteration 30792, loss = 0.00641226\n",
      "Iteration 30793, loss = 0.00641213\n",
      "Iteration 30794, loss = 0.00641201\n",
      "Iteration 30795, loss = 0.00641188\n",
      "Iteration 30796, loss = 0.00641175\n",
      "Iteration 30797, loss = 0.00641163\n",
      "Iteration 30798, loss = 0.00641150\n",
      "Iteration 30799, loss = 0.00641137\n",
      "Iteration 30800, loss = 0.00641125\n",
      "Iteration 30801, loss = 0.00641112\n",
      "Iteration 30802, loss = 0.00641099\n",
      "Iteration 30803, loss = 0.00641086\n",
      "Iteration 30804, loss = 0.00641074\n",
      "Iteration 30805, loss = 0.00641061\n",
      "Iteration 30806, loss = 0.00641048\n",
      "Iteration 30807, loss = 0.00641036\n",
      "Iteration 30808, loss = 0.00641023\n",
      "Iteration 30809, loss = 0.00641010\n",
      "Iteration 30810, loss = 0.00640998\n",
      "Iteration 30811, loss = 0.00640985\n",
      "Iteration 30812, loss = 0.00640972\n",
      "Iteration 30813, loss = 0.00640960\n",
      "Iteration 30814, loss = 0.00640947\n",
      "Iteration 30815, loss = 0.00640934\n",
      "Iteration 30816, loss = 0.00640922\n",
      "Iteration 30817, loss = 0.00640909\n",
      "Iteration 30818, loss = 0.00640896\n",
      "Iteration 30819, loss = 0.00640884\n",
      "Iteration 30820, loss = 0.00640871\n",
      "Iteration 30821, loss = 0.00640858\n",
      "Iteration 30822, loss = 0.00640846\n",
      "Iteration 30823, loss = 0.00640833\n",
      "Iteration 30824, loss = 0.00640820\n",
      "Iteration 30825, loss = 0.00640808\n",
      "Iteration 30826, loss = 0.00640795\n",
      "Iteration 30827, loss = 0.00640782\n",
      "Iteration 30828, loss = 0.00640770\n",
      "Iteration 30829, loss = 0.00640757\n",
      "Iteration 30830, loss = 0.00640744\n",
      "Iteration 30831, loss = 0.00640732\n",
      "Iteration 30832, loss = 0.00640719\n",
      "Iteration 30833, loss = 0.00640706\n",
      "Iteration 30834, loss = 0.00640694\n",
      "Iteration 30835, loss = 0.00640681\n",
      "Iteration 30836, loss = 0.00640668\n",
      "Iteration 30837, loss = 0.00640656\n",
      "Iteration 30838, loss = 0.00640643\n",
      "Iteration 30839, loss = 0.00640630\n",
      "Iteration 30840, loss = 0.00640618\n",
      "Iteration 30841, loss = 0.00640605\n",
      "Iteration 30842, loss = 0.00640592\n",
      "Iteration 30843, loss = 0.00640580\n",
      "Iteration 30844, loss = 0.00640567\n",
      "Iteration 30845, loss = 0.00640554\n",
      "Iteration 30846, loss = 0.00640542\n",
      "Iteration 30847, loss = 0.00640529\n",
      "Iteration 30848, loss = 0.00640517\n",
      "Iteration 30849, loss = 0.00640504\n",
      "Iteration 30850, loss = 0.00640491\n",
      "Iteration 30851, loss = 0.00640479\n",
      "Iteration 30852, loss = 0.00640466\n",
      "Iteration 30853, loss = 0.00640453\n",
      "Iteration 30854, loss = 0.00640441\n",
      "Iteration 30855, loss = 0.00640428\n",
      "Iteration 30856, loss = 0.00640415\n",
      "Iteration 30857, loss = 0.00640403\n",
      "Iteration 30858, loss = 0.00640390\n",
      "Iteration 30859, loss = 0.00640378\n",
      "Iteration 30860, loss = 0.00640365\n",
      "Iteration 30861, loss = 0.00640352\n",
      "Iteration 30862, loss = 0.00640340\n",
      "Iteration 30863, loss = 0.00640327\n",
      "Iteration 30864, loss = 0.00640314\n",
      "Iteration 30865, loss = 0.00640302\n",
      "Iteration 30866, loss = 0.00640289\n",
      "Iteration 30867, loss = 0.00640277\n",
      "Iteration 30868, loss = 0.00640264\n",
      "Iteration 30869, loss = 0.00640251\n",
      "Iteration 30870, loss = 0.00640239\n",
      "Iteration 30871, loss = 0.00640226\n",
      "Iteration 30872, loss = 0.00640213\n",
      "Iteration 30873, loss = 0.00640201\n",
      "Iteration 30874, loss = 0.00640188\n",
      "Iteration 30875, loss = 0.00640176\n",
      "Iteration 30876, loss = 0.00640163\n",
      "Iteration 30877, loss = 0.00640150\n",
      "Iteration 30878, loss = 0.00640138\n",
      "Iteration 30879, loss = 0.00640125\n",
      "Iteration 30880, loss = 0.00640113\n",
      "Iteration 30881, loss = 0.00640100\n",
      "Iteration 30882, loss = 0.00640087\n",
      "Iteration 30883, loss = 0.00640075\n",
      "Iteration 30884, loss = 0.00640062\n",
      "Iteration 30885, loss = 0.00640050\n",
      "Iteration 30886, loss = 0.00640037\n",
      "Iteration 30887, loss = 0.00640024\n",
      "Iteration 30888, loss = 0.00640012\n",
      "Iteration 30889, loss = 0.00639999\n",
      "Iteration 30890, loss = 0.00639987\n",
      "Iteration 30891, loss = 0.00639974\n",
      "Iteration 30892, loss = 0.00639961\n",
      "Iteration 30893, loss = 0.00639949\n",
      "Iteration 30894, loss = 0.00639936\n",
      "Iteration 30895, loss = 0.00639924\n",
      "Iteration 30896, loss = 0.00639911\n",
      "Iteration 30897, loss = 0.00639898\n",
      "Iteration 30898, loss = 0.00639886\n",
      "Iteration 30899, loss = 0.00639873\n",
      "Iteration 30900, loss = 0.00639861\n",
      "Iteration 30901, loss = 0.00639848\n",
      "Iteration 30902, loss = 0.00639836\n",
      "Iteration 30903, loss = 0.00639823\n",
      "Iteration 30904, loss = 0.00639810\n",
      "Iteration 30905, loss = 0.00639798\n",
      "Iteration 30906, loss = 0.00639785\n",
      "Iteration 30907, loss = 0.00639773\n",
      "Iteration 30908, loss = 0.00639760\n",
      "Iteration 30909, loss = 0.00639748\n",
      "Iteration 30910, loss = 0.00639735\n",
      "Iteration 30911, loss = 0.00639722\n",
      "Iteration 30912, loss = 0.00639710\n",
      "Iteration 30913, loss = 0.00639697\n",
      "Iteration 30914, loss = 0.00639685\n",
      "Iteration 30915, loss = 0.00639672\n",
      "Iteration 30916, loss = 0.00639660\n",
      "Iteration 30917, loss = 0.00639647\n",
      "Iteration 30918, loss = 0.00639634\n",
      "Iteration 30919, loss = 0.00639622\n",
      "Iteration 30920, loss = 0.00639609\n",
      "Iteration 30921, loss = 0.00639597\n",
      "Iteration 30922, loss = 0.00639584\n",
      "Iteration 30923, loss = 0.00639572\n",
      "Iteration 30924, loss = 0.00639559\n",
      "Iteration 30925, loss = 0.00639547\n",
      "Iteration 30926, loss = 0.00639534\n",
      "Iteration 30927, loss = 0.00639521\n",
      "Iteration 30928, loss = 0.00639509\n",
      "Iteration 30929, loss = 0.00639496\n",
      "Iteration 30930, loss = 0.00639484\n",
      "Iteration 30931, loss = 0.00639471\n",
      "Iteration 30932, loss = 0.00639459\n",
      "Iteration 30933, loss = 0.00639446\n",
      "Iteration 30934, loss = 0.00639434\n",
      "Iteration 30935, loss = 0.00639421\n",
      "Iteration 30936, loss = 0.00639408\n",
      "Iteration 30937, loss = 0.00639396\n",
      "Iteration 30938, loss = 0.00639383\n",
      "Iteration 30939, loss = 0.00639371\n",
      "Iteration 30940, loss = 0.00639358\n",
      "Iteration 30941, loss = 0.00639346\n",
      "Iteration 30942, loss = 0.00639333\n",
      "Iteration 30943, loss = 0.00639321\n",
      "Iteration 30944, loss = 0.00639308\n",
      "Iteration 30945, loss = 0.00639296\n",
      "Iteration 30946, loss = 0.00639283\n",
      "Iteration 30947, loss = 0.00639271\n",
      "Iteration 30948, loss = 0.00639258\n",
      "Iteration 30949, loss = 0.00639246\n",
      "Iteration 30950, loss = 0.00639233\n",
      "Iteration 30951, loss = 0.00639220\n",
      "Iteration 30952, loss = 0.00639208\n",
      "Iteration 30953, loss = 0.00639195\n",
      "Iteration 30954, loss = 0.00639183\n",
      "Iteration 30955, loss = 0.00639170\n",
      "Iteration 30956, loss = 0.00639158\n",
      "Iteration 30957, loss = 0.00639145\n",
      "Iteration 30958, loss = 0.00639133\n",
      "Iteration 30959, loss = 0.00639120\n",
      "Iteration 30960, loss = 0.00639108\n",
      "Iteration 30961, loss = 0.00639095\n",
      "Iteration 30962, loss = 0.00639083\n",
      "Iteration 30963, loss = 0.00639070\n",
      "Iteration 30964, loss = 0.00639058\n",
      "Iteration 30965, loss = 0.00639045\n",
      "Iteration 30966, loss = 0.00639033\n",
      "Iteration 30967, loss = 0.00639020\n",
      "Iteration 30968, loss = 0.00639008\n",
      "Iteration 30969, loss = 0.00638995\n",
      "Iteration 30970, loss = 0.00638983\n",
      "Iteration 30971, loss = 0.00638970\n",
      "Iteration 30972, loss = 0.00638958\n",
      "Iteration 30973, loss = 0.00638945\n",
      "Iteration 30974, loss = 0.00638933\n",
      "Iteration 30975, loss = 0.00638920\n",
      "Iteration 30976, loss = 0.00638908\n",
      "Iteration 30977, loss = 0.00638895\n",
      "Iteration 30978, loss = 0.00638883\n",
      "Iteration 30979, loss = 0.00638870\n",
      "Iteration 30980, loss = 0.00638858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30981, loss = 0.00638845\n",
      "Iteration 30982, loss = 0.00638833\n",
      "Iteration 30983, loss = 0.00638820\n",
      "Iteration 30984, loss = 0.00638808\n",
      "Iteration 30985, loss = 0.00638795\n",
      "Iteration 30986, loss = 0.00638783\n",
      "Iteration 30987, loss = 0.00638770\n",
      "Iteration 30988, loss = 0.00638758\n",
      "Iteration 30989, loss = 0.00638745\n",
      "Iteration 30990, loss = 0.00638733\n",
      "Iteration 30991, loss = 0.00638720\n",
      "Iteration 30992, loss = 0.00638708\n",
      "Iteration 30993, loss = 0.00638695\n",
      "Iteration 30994, loss = 0.00638683\n",
      "Iteration 30995, loss = 0.00638670\n",
      "Iteration 30996, loss = 0.00638658\n",
      "Iteration 30997, loss = 0.00638645\n",
      "Iteration 30998, loss = 0.00638633\n",
      "Iteration 30999, loss = 0.00638621\n",
      "Iteration 31000, loss = 0.00638608\n",
      "Iteration 31001, loss = 0.00638596\n",
      "Iteration 31002, loss = 0.00638583\n",
      "Iteration 31003, loss = 0.00638571\n",
      "Iteration 31004, loss = 0.00638558\n",
      "Iteration 31005, loss = 0.00638546\n",
      "Iteration 31006, loss = 0.00638533\n",
      "Iteration 31007, loss = 0.00638521\n",
      "Iteration 31008, loss = 0.00638508\n",
      "Iteration 31009, loss = 0.00638496\n",
      "Iteration 31010, loss = 0.00638483\n",
      "Iteration 31011, loss = 0.00638471\n",
      "Iteration 31012, loss = 0.00638458\n",
      "Iteration 31013, loss = 0.00638446\n",
      "Iteration 31014, loss = 0.00638434\n",
      "Iteration 31015, loss = 0.00638421\n",
      "Iteration 31016, loss = 0.00638409\n",
      "Iteration 31017, loss = 0.00638396\n",
      "Iteration 31018, loss = 0.00638384\n",
      "Iteration 31019, loss = 0.00638371\n",
      "Iteration 31020, loss = 0.00638359\n",
      "Iteration 31021, loss = 0.00638346\n",
      "Iteration 31022, loss = 0.00638334\n",
      "Iteration 31023, loss = 0.00638322\n",
      "Iteration 31024, loss = 0.00638309\n",
      "Iteration 31025, loss = 0.00638297\n",
      "Iteration 31026, loss = 0.00638284\n",
      "Iteration 31027, loss = 0.00638272\n",
      "Iteration 31028, loss = 0.00638259\n",
      "Iteration 31029, loss = 0.00638247\n",
      "Iteration 31030, loss = 0.00638234\n",
      "Iteration 31031, loss = 0.00638222\n",
      "Iteration 31032, loss = 0.00638210\n",
      "Iteration 31033, loss = 0.00638197\n",
      "Iteration 31034, loss = 0.00638185\n",
      "Iteration 31035, loss = 0.00638172\n",
      "Iteration 31036, loss = 0.00638160\n",
      "Iteration 31037, loss = 0.00638147\n",
      "Iteration 31038, loss = 0.00638135\n",
      "Iteration 31039, loss = 0.00638123\n",
      "Iteration 31040, loss = 0.00638110\n",
      "Iteration 31041, loss = 0.00638098\n",
      "Iteration 31042, loss = 0.00638085\n",
      "Iteration 31043, loss = 0.00638073\n",
      "Iteration 31044, loss = 0.00638060\n",
      "Iteration 31045, loss = 0.00638048\n",
      "Iteration 31046, loss = 0.00638036\n",
      "Iteration 31047, loss = 0.00638023\n",
      "Iteration 31048, loss = 0.00638011\n",
      "Iteration 31049, loss = 0.00637998\n",
      "Iteration 31050, loss = 0.00637986\n",
      "Iteration 31051, loss = 0.00637973\n",
      "Iteration 31052, loss = 0.00637961\n",
      "Iteration 31053, loss = 0.00637949\n",
      "Iteration 31054, loss = 0.00637936\n",
      "Iteration 31055, loss = 0.00637924\n",
      "Iteration 31056, loss = 0.00637911\n",
      "Iteration 31057, loss = 0.00637899\n",
      "Iteration 31058, loss = 0.00637887\n",
      "Iteration 31059, loss = 0.00637874\n",
      "Iteration 31060, loss = 0.00637862\n",
      "Iteration 31061, loss = 0.00637849\n",
      "Iteration 31062, loss = 0.00637837\n",
      "Iteration 31063, loss = 0.00637825\n",
      "Iteration 31064, loss = 0.00637812\n",
      "Iteration 31065, loss = 0.00637800\n",
      "Iteration 31066, loss = 0.00637787\n",
      "Iteration 31067, loss = 0.00637775\n",
      "Iteration 31068, loss = 0.00637763\n",
      "Iteration 31069, loss = 0.00637750\n",
      "Iteration 31070, loss = 0.00637738\n",
      "Iteration 31071, loss = 0.00637725\n",
      "Iteration 31072, loss = 0.00637713\n",
      "Iteration 31073, loss = 0.00637701\n",
      "Iteration 31074, loss = 0.00637688\n",
      "Iteration 31075, loss = 0.00637676\n",
      "Iteration 31076, loss = 0.00637663\n",
      "Iteration 31077, loss = 0.00637651\n",
      "Iteration 31078, loss = 0.00637639\n",
      "Iteration 31079, loss = 0.00637626\n",
      "Iteration 31080, loss = 0.00637614\n",
      "Iteration 31081, loss = 0.00637602\n",
      "Iteration 31082, loss = 0.00637589\n",
      "Iteration 31083, loss = 0.00637577\n",
      "Iteration 31084, loss = 0.00637564\n",
      "Iteration 31085, loss = 0.00637552\n",
      "Iteration 31086, loss = 0.00637540\n",
      "Iteration 31087, loss = 0.00637527\n",
      "Iteration 31088, loss = 0.00637515\n",
      "Iteration 31089, loss = 0.00637502\n",
      "Iteration 31090, loss = 0.00637490\n",
      "Iteration 31091, loss = 0.00637478\n",
      "Iteration 31092, loss = 0.00637465\n",
      "Iteration 31093, loss = 0.00637453\n",
      "Iteration 31094, loss = 0.00637441\n",
      "Iteration 31095, loss = 0.00637428\n",
      "Iteration 31096, loss = 0.00637416\n",
      "Iteration 31097, loss = 0.00637404\n",
      "Iteration 31098, loss = 0.00637391\n",
      "Iteration 31099, loss = 0.00637379\n",
      "Iteration 31100, loss = 0.00637366\n",
      "Iteration 31101, loss = 0.00637354\n",
      "Iteration 31102, loss = 0.00637342\n",
      "Iteration 31103, loss = 0.00637329\n",
      "Iteration 31104, loss = 0.00637317\n",
      "Iteration 31105, loss = 0.00637305\n",
      "Iteration 31106, loss = 0.00637292\n",
      "Iteration 31107, loss = 0.00637280\n",
      "Iteration 31108, loss = 0.00637268\n",
      "Iteration 31109, loss = 0.00637255\n",
      "Iteration 31110, loss = 0.00637243\n",
      "Iteration 31111, loss = 0.00637231\n",
      "Iteration 31112, loss = 0.00637218\n",
      "Iteration 31113, loss = 0.00637206\n",
      "Iteration 31114, loss = 0.00637194\n",
      "Iteration 31115, loss = 0.00637181\n",
      "Iteration 31116, loss = 0.00637169\n",
      "Iteration 31117, loss = 0.00637156\n",
      "Iteration 31118, loss = 0.00637144\n",
      "Iteration 31119, loss = 0.00637132\n",
      "Iteration 31120, loss = 0.00637119\n",
      "Iteration 31121, loss = 0.00637107\n",
      "Iteration 31122, loss = 0.00637095\n",
      "Iteration 31123, loss = 0.00637082\n",
      "Iteration 31124, loss = 0.00637070\n",
      "Iteration 31125, loss = 0.00637058\n",
      "Iteration 31126, loss = 0.00637045\n",
      "Iteration 31127, loss = 0.00637033\n",
      "Iteration 31128, loss = 0.00637021\n",
      "Iteration 31129, loss = 0.00637008\n",
      "Iteration 31130, loss = 0.00636996\n",
      "Iteration 31131, loss = 0.00636984\n",
      "Iteration 31132, loss = 0.00636971\n",
      "Iteration 31133, loss = 0.00636959\n",
      "Iteration 31134, loss = 0.00636947\n",
      "Iteration 31135, loss = 0.00636934\n",
      "Iteration 31136, loss = 0.00636922\n",
      "Iteration 31137, loss = 0.00636910\n",
      "Iteration 31138, loss = 0.00636898\n",
      "Iteration 31139, loss = 0.00636885\n",
      "Iteration 31140, loss = 0.00636873\n",
      "Iteration 31141, loss = 0.00636861\n",
      "Iteration 31142, loss = 0.00636848\n",
      "Iteration 31143, loss = 0.00636836\n",
      "Iteration 31144, loss = 0.00636824\n",
      "Iteration 31145, loss = 0.00636811\n",
      "Iteration 31146, loss = 0.00636799\n",
      "Iteration 31147, loss = 0.00636787\n",
      "Iteration 31148, loss = 0.00636774\n",
      "Iteration 31149, loss = 0.00636762\n",
      "Iteration 31150, loss = 0.00636750\n",
      "Iteration 31151, loss = 0.00636737\n",
      "Iteration 31152, loss = 0.00636725\n",
      "Iteration 31153, loss = 0.00636713\n",
      "Iteration 31154, loss = 0.00636701\n",
      "Iteration 31155, loss = 0.00636688\n",
      "Iteration 31156, loss = 0.00636676\n",
      "Iteration 31157, loss = 0.00636664\n",
      "Iteration 31158, loss = 0.00636651\n",
      "Iteration 31159, loss = 0.00636639\n",
      "Iteration 31160, loss = 0.00636627\n",
      "Iteration 31161, loss = 0.00636614\n",
      "Iteration 31162, loss = 0.00636602\n",
      "Iteration 31163, loss = 0.00636590\n",
      "Iteration 31164, loss = 0.00636578\n",
      "Iteration 31165, loss = 0.00636565\n",
      "Iteration 31166, loss = 0.00636553\n",
      "Iteration 31167, loss = 0.00636541\n",
      "Iteration 31168, loss = 0.00636528\n",
      "Iteration 31169, loss = 0.00636516\n",
      "Iteration 31170, loss = 0.00636504\n",
      "Iteration 31171, loss = 0.00636492\n",
      "Iteration 31172, loss = 0.00636479\n",
      "Iteration 31173, loss = 0.00636467\n",
      "Iteration 31174, loss = 0.00636455\n",
      "Iteration 31175, loss = 0.00636442\n",
      "Iteration 31176, loss = 0.00636430\n",
      "Iteration 31177, loss = 0.00636418\n",
      "Iteration 31178, loss = 0.00636406\n",
      "Iteration 31179, loss = 0.00636393\n",
      "Iteration 31180, loss = 0.00636381\n",
      "Iteration 31181, loss = 0.00636369\n",
      "Iteration 31182, loss = 0.00636357\n",
      "Iteration 31183, loss = 0.00636344\n",
      "Iteration 31184, loss = 0.00636332\n",
      "Iteration 31185, loss = 0.00636320\n",
      "Iteration 31186, loss = 0.00636307\n",
      "Iteration 31187, loss = 0.00636295\n",
      "Iteration 31188, loss = 0.00636283\n",
      "Iteration 31189, loss = 0.00636271\n",
      "Iteration 31190, loss = 0.00636258\n",
      "Iteration 31191, loss = 0.00636246\n",
      "Iteration 31192, loss = 0.00636234\n",
      "Iteration 31193, loss = 0.00636222\n",
      "Iteration 31194, loss = 0.00636209\n",
      "Iteration 31195, loss = 0.00636197\n",
      "Iteration 31196, loss = 0.00636185\n",
      "Iteration 31197, loss = 0.00636173\n",
      "Iteration 31198, loss = 0.00636160\n",
      "Iteration 31199, loss = 0.00636148\n",
      "Iteration 31200, loss = 0.00636136\n",
      "Iteration 31201, loss = 0.00636124\n",
      "Iteration 31202, loss = 0.00636111\n",
      "Iteration 31203, loss = 0.00636099\n",
      "Iteration 31204, loss = 0.00636087\n",
      "Iteration 31205, loss = 0.00636075\n",
      "Iteration 31206, loss = 0.00636062\n",
      "Iteration 31207, loss = 0.00636050\n",
      "Iteration 31208, loss = 0.00636038\n",
      "Iteration 31209, loss = 0.00636026\n",
      "Iteration 31210, loss = 0.00636013\n",
      "Iteration 31211, loss = 0.00636001\n",
      "Iteration 31212, loss = 0.00635989\n",
      "Iteration 31213, loss = 0.00635977\n",
      "Iteration 31214, loss = 0.00635964\n",
      "Iteration 31215, loss = 0.00635952\n",
      "Iteration 31216, loss = 0.00635940\n",
      "Iteration 31217, loss = 0.00635928\n",
      "Iteration 31218, loss = 0.00635915\n",
      "Iteration 31219, loss = 0.00635903\n",
      "Iteration 31220, loss = 0.00635891\n",
      "Iteration 31221, loss = 0.00635879\n",
      "Iteration 31222, loss = 0.00635866\n",
      "Iteration 31223, loss = 0.00635854\n",
      "Iteration 31224, loss = 0.00635842\n",
      "Iteration 31225, loss = 0.00635830\n",
      "Iteration 31226, loss = 0.00635818\n",
      "Iteration 31227, loss = 0.00635805\n",
      "Iteration 31228, loss = 0.00635793\n",
      "Iteration 31229, loss = 0.00635781\n",
      "Iteration 31230, loss = 0.00635769\n",
      "Iteration 31231, loss = 0.00635756\n",
      "Iteration 31232, loss = 0.00635744\n",
      "Iteration 31233, loss = 0.00635732\n",
      "Iteration 31234, loss = 0.00635720\n",
      "Iteration 31235, loss = 0.00635708\n",
      "Iteration 31236, loss = 0.00635695\n",
      "Iteration 31237, loss = 0.00635683\n",
      "Iteration 31238, loss = 0.00635671\n",
      "Iteration 31239, loss = 0.00635659\n",
      "Iteration 31240, loss = 0.00635647\n",
      "Iteration 31241, loss = 0.00635634\n",
      "Iteration 31242, loss = 0.00635622\n",
      "Iteration 31243, loss = 0.00635610\n",
      "Iteration 31244, loss = 0.00635598\n",
      "Iteration 31245, loss = 0.00635586\n",
      "Iteration 31246, loss = 0.00635573\n",
      "Iteration 31247, loss = 0.00635561\n",
      "Iteration 31248, loss = 0.00635549\n",
      "Iteration 31249, loss = 0.00635537\n",
      "Iteration 31250, loss = 0.00635524\n",
      "Iteration 31251, loss = 0.00635512\n",
      "Iteration 31252, loss = 0.00635500\n",
      "Iteration 31253, loss = 0.00635488\n",
      "Iteration 31254, loss = 0.00635476\n",
      "Iteration 31255, loss = 0.00635464\n",
      "Iteration 31256, loss = 0.00635451\n",
      "Iteration 31257, loss = 0.00635439\n",
      "Iteration 31258, loss = 0.00635427\n",
      "Iteration 31259, loss = 0.00635415\n",
      "Iteration 31260, loss = 0.00635403\n",
      "Iteration 31261, loss = 0.00635390\n",
      "Iteration 31262, loss = 0.00635378\n",
      "Iteration 31263, loss = 0.00635366\n",
      "Iteration 31264, loss = 0.00635354\n",
      "Iteration 31265, loss = 0.00635342\n",
      "Iteration 31266, loss = 0.00635329\n",
      "Iteration 31267, loss = 0.00635317\n",
      "Iteration 31268, loss = 0.00635305\n",
      "Iteration 31269, loss = 0.00635293\n",
      "Iteration 31270, loss = 0.00635281\n",
      "Iteration 31271, loss = 0.00635269\n",
      "Iteration 31272, loss = 0.00635256\n",
      "Iteration 31273, loss = 0.00635244\n",
      "Iteration 31274, loss = 0.00635232\n",
      "Iteration 31275, loss = 0.00635220\n",
      "Iteration 31276, loss = 0.00635208\n",
      "Iteration 31277, loss = 0.00635196\n",
      "Iteration 31278, loss = 0.00635183\n",
      "Iteration 31279, loss = 0.00635171\n",
      "Iteration 31280, loss = 0.00635159\n",
      "Iteration 31281, loss = 0.00635147\n",
      "Iteration 31282, loss = 0.00635135\n",
      "Iteration 31283, loss = 0.00635123\n",
      "Iteration 31284, loss = 0.00635110\n",
      "Iteration 31285, loss = 0.00635098\n",
      "Iteration 31286, loss = 0.00635086\n",
      "Iteration 31287, loss = 0.00635074\n",
      "Iteration 31288, loss = 0.00635062\n",
      "Iteration 31289, loss = 0.00635050\n",
      "Iteration 31290, loss = 0.00635037\n",
      "Iteration 31291, loss = 0.00635025\n",
      "Iteration 31292, loss = 0.00635013\n",
      "Iteration 31293, loss = 0.00635001\n",
      "Iteration 31294, loss = 0.00634989\n",
      "Iteration 31295, loss = 0.00634977\n",
      "Iteration 31296, loss = 0.00634964\n",
      "Iteration 31297, loss = 0.00634952\n",
      "Iteration 31298, loss = 0.00634940\n",
      "Iteration 31299, loss = 0.00634928\n",
      "Iteration 31300, loss = 0.00634916\n",
      "Iteration 31301, loss = 0.00634904\n",
      "Iteration 31302, loss = 0.00634892\n",
      "Iteration 31303, loss = 0.00634879\n",
      "Iteration 31304, loss = 0.00634867\n",
      "Iteration 31305, loss = 0.00634855\n",
      "Iteration 31306, loss = 0.00634843\n",
      "Iteration 31307, loss = 0.00634831\n",
      "Iteration 31308, loss = 0.00634819\n",
      "Iteration 31309, loss = 0.00634807\n",
      "Iteration 31310, loss = 0.00634794\n",
      "Iteration 31311, loss = 0.00634782\n",
      "Iteration 31312, loss = 0.00634770\n",
      "Iteration 31313, loss = 0.00634758\n",
      "Iteration 31314, loss = 0.00634746\n",
      "Iteration 31315, loss = 0.00634734\n",
      "Iteration 31316, loss = 0.00634722\n",
      "Iteration 31317, loss = 0.00634710\n",
      "Iteration 31318, loss = 0.00634697\n",
      "Iteration 31319, loss = 0.00634685\n",
      "Iteration 31320, loss = 0.00634673\n",
      "Iteration 31321, loss = 0.00634661\n",
      "Iteration 31322, loss = 0.00634649\n",
      "Iteration 31323, loss = 0.00634637\n",
      "Iteration 31324, loss = 0.00634625\n",
      "Iteration 31325, loss = 0.00634613\n",
      "Iteration 31326, loss = 0.00634600\n",
      "Iteration 31327, loss = 0.00634588\n",
      "Iteration 31328, loss = 0.00634576\n",
      "Iteration 31329, loss = 0.00634564\n",
      "Iteration 31330, loss = 0.00634552\n",
      "Iteration 31331, loss = 0.00634540\n",
      "Iteration 31332, loss = 0.00634528\n",
      "Iteration 31333, loss = 0.00634516\n",
      "Iteration 31334, loss = 0.00634504\n",
      "Iteration 31335, loss = 0.00634491\n",
      "Iteration 31336, loss = 0.00634479\n",
      "Iteration 31337, loss = 0.00634467\n",
      "Iteration 31338, loss = 0.00634455\n",
      "Iteration 31339, loss = 0.00634443\n",
      "Iteration 31340, loss = 0.00634431\n",
      "Iteration 31341, loss = 0.00634419\n",
      "Iteration 31342, loss = 0.00634407\n",
      "Iteration 31343, loss = 0.00634395\n",
      "Iteration 31344, loss = 0.00634383\n",
      "Iteration 31345, loss = 0.00634370\n",
      "Iteration 31346, loss = 0.00634358\n",
      "Iteration 31347, loss = 0.00634346\n",
      "Iteration 31348, loss = 0.00634334\n",
      "Iteration 31349, loss = 0.00634322\n",
      "Iteration 31350, loss = 0.00634310\n",
      "Iteration 31351, loss = 0.00634298\n",
      "Iteration 31352, loss = 0.00634286\n",
      "Iteration 31353, loss = 0.00634274\n",
      "Iteration 31354, loss = 0.00634262\n",
      "Iteration 31355, loss = 0.00634250\n",
      "Iteration 31356, loss = 0.00634237\n",
      "Iteration 31357, loss = 0.00634225\n",
      "Iteration 31358, loss = 0.00634213\n",
      "Iteration 31359, loss = 0.00634201\n",
      "Iteration 31360, loss = 0.00634189\n",
      "Iteration 31361, loss = 0.00634177\n",
      "Iteration 31362, loss = 0.00634165\n",
      "Iteration 31363, loss = 0.00634153\n",
      "Iteration 31364, loss = 0.00634141\n",
      "Iteration 31365, loss = 0.00634129\n",
      "Iteration 31366, loss = 0.00634117\n",
      "Iteration 31367, loss = 0.00634105\n",
      "Iteration 31368, loss = 0.00634092\n",
      "Iteration 31369, loss = 0.00634080\n",
      "Iteration 31370, loss = 0.00634068\n",
      "Iteration 31371, loss = 0.00634056\n",
      "Iteration 31372, loss = 0.00634044\n",
      "Iteration 31373, loss = 0.00634032\n",
      "Iteration 31374, loss = 0.00634020\n",
      "Iteration 31375, loss = 0.00634008\n",
      "Iteration 31376, loss = 0.00633996\n",
      "Iteration 31377, loss = 0.00633984\n",
      "Iteration 31378, loss = 0.00633972\n",
      "Iteration 31379, loss = 0.00633960\n",
      "Iteration 31380, loss = 0.00633948\n",
      "Iteration 31381, loss = 0.00633936\n",
      "Iteration 31382, loss = 0.00633924\n",
      "Iteration 31383, loss = 0.00633911\n",
      "Iteration 31384, loss = 0.00633899\n",
      "Iteration 31385, loss = 0.00633887\n",
      "Iteration 31386, loss = 0.00633875\n",
      "Iteration 31387, loss = 0.00633863\n",
      "Iteration 31388, loss = 0.00633851\n",
      "Iteration 31389, loss = 0.00633839\n",
      "Iteration 31390, loss = 0.00633827\n",
      "Iteration 31391, loss = 0.00633815\n",
      "Iteration 31392, loss = 0.00633803\n",
      "Iteration 31393, loss = 0.00633791\n",
      "Iteration 31394, loss = 0.00633779\n",
      "Iteration 31395, loss = 0.00633767\n",
      "Iteration 31396, loss = 0.00633755\n",
      "Iteration 31397, loss = 0.00633743\n",
      "Iteration 31398, loss = 0.00633731\n",
      "Iteration 31399, loss = 0.00633719\n",
      "Iteration 31400, loss = 0.00633707\n",
      "Iteration 31401, loss = 0.00633695\n",
      "Iteration 31402, loss = 0.00633683\n",
      "Iteration 31403, loss = 0.00633671\n",
      "Iteration 31404, loss = 0.00633659\n",
      "Iteration 31405, loss = 0.00633646\n",
      "Iteration 31406, loss = 0.00633634\n",
      "Iteration 31407, loss = 0.00633622\n",
      "Iteration 31408, loss = 0.00633610\n",
      "Iteration 31409, loss = 0.00633598\n",
      "Iteration 31410, loss = 0.00633586\n",
      "Iteration 31411, loss = 0.00633574\n",
      "Iteration 31412, loss = 0.00633562\n",
      "Iteration 31413, loss = 0.00633550\n",
      "Iteration 31414, loss = 0.00633538\n",
      "Iteration 31415, loss = 0.00633526\n",
      "Iteration 31416, loss = 0.00633514\n",
      "Iteration 31417, loss = 0.00633502\n",
      "Iteration 31418, loss = 0.00633490\n",
      "Iteration 31419, loss = 0.00633478\n",
      "Iteration 31420, loss = 0.00633466\n",
      "Iteration 31421, loss = 0.00633454\n",
      "Iteration 31422, loss = 0.00633442\n",
      "Iteration 31423, loss = 0.00633430\n",
      "Iteration 31424, loss = 0.00633418\n",
      "Iteration 31425, loss = 0.00633406\n",
      "Iteration 31426, loss = 0.00633394\n",
      "Iteration 31427, loss = 0.00633382\n",
      "Iteration 31428, loss = 0.00633370\n",
      "Iteration 31429, loss = 0.00633358\n",
      "Iteration 31430, loss = 0.00633346\n",
      "Iteration 31431, loss = 0.00633334\n",
      "Iteration 31432, loss = 0.00633322\n",
      "Iteration 31433, loss = 0.00633310\n",
      "Iteration 31434, loss = 0.00633298\n",
      "Iteration 31435, loss = 0.00633286\n",
      "Iteration 31436, loss = 0.00633274\n",
      "Iteration 31437, loss = 0.00633262\n",
      "Iteration 31438, loss = 0.00633250\n",
      "Iteration 31439, loss = 0.00633238\n",
      "Iteration 31440, loss = 0.00633226\n",
      "Iteration 31441, loss = 0.00633214\n",
      "Iteration 31442, loss = 0.00633202\n",
      "Iteration 31443, loss = 0.00633190\n",
      "Iteration 31444, loss = 0.00633178\n",
      "Iteration 31445, loss = 0.00633166\n",
      "Iteration 31446, loss = 0.00633154\n",
      "Iteration 31447, loss = 0.00633142\n",
      "Iteration 31448, loss = 0.00633130\n",
      "Iteration 31449, loss = 0.00633118\n",
      "Iteration 31450, loss = 0.00633106\n",
      "Iteration 31451, loss = 0.00633094\n",
      "Iteration 31452, loss = 0.00633082\n",
      "Iteration 31453, loss = 0.00633070\n",
      "Iteration 31454, loss = 0.00633058\n",
      "Iteration 31455, loss = 0.00633046\n",
      "Iteration 31456, loss = 0.00633034\n",
      "Iteration 31457, loss = 0.00633022\n",
      "Iteration 31458, loss = 0.00633010\n",
      "Iteration 31459, loss = 0.00632998\n",
      "Iteration 31460, loss = 0.00632986\n",
      "Iteration 31461, loss = 0.00632974\n",
      "Iteration 31462, loss = 0.00632962\n",
      "Iteration 31463, loss = 0.00632950\n",
      "Iteration 31464, loss = 0.00632938\n",
      "Iteration 31465, loss = 0.00632926\n",
      "Iteration 31466, loss = 0.00632914\n",
      "Iteration 31467, loss = 0.00632902\n",
      "Iteration 31468, loss = 0.00632890\n",
      "Iteration 31469, loss = 0.00632878\n",
      "Iteration 31470, loss = 0.00632866\n",
      "Iteration 31471, loss = 0.00632855\n",
      "Iteration 31472, loss = 0.00632843\n",
      "Iteration 31473, loss = 0.00632831\n",
      "Iteration 31474, loss = 0.00632819\n",
      "Iteration 31475, loss = 0.00632807\n",
      "Iteration 31476, loss = 0.00632795\n",
      "Iteration 31477, loss = 0.00632783\n",
      "Iteration 31478, loss = 0.00632771\n",
      "Iteration 31479, loss = 0.00632759\n",
      "Iteration 31480, loss = 0.00632747\n",
      "Iteration 31481, loss = 0.00632735\n",
      "Iteration 31482, loss = 0.00632723\n",
      "Iteration 31483, loss = 0.00632711\n",
      "Iteration 31484, loss = 0.00632699\n",
      "Iteration 31485, loss = 0.00632687\n",
      "Iteration 31486, loss = 0.00632675\n",
      "Iteration 31487, loss = 0.00632663\n",
      "Iteration 31488, loss = 0.00632651\n",
      "Iteration 31489, loss = 0.00632639\n",
      "Iteration 31490, loss = 0.00632627\n",
      "Iteration 31491, loss = 0.00632615\n",
      "Iteration 31492, loss = 0.00632603\n",
      "Iteration 31493, loss = 0.00632592\n",
      "Iteration 31494, loss = 0.00632580\n",
      "Iteration 31495, loss = 0.00632568\n",
      "Iteration 31496, loss = 0.00632556\n",
      "Iteration 31497, loss = 0.00632544\n",
      "Iteration 31498, loss = 0.00632532\n",
      "Iteration 31499, loss = 0.00632520\n",
      "Iteration 31500, loss = 0.00632508\n",
      "Iteration 31501, loss = 0.00632496\n",
      "Iteration 31502, loss = 0.00632484\n",
      "Iteration 31503, loss = 0.00632472\n",
      "Iteration 31504, loss = 0.00632460\n",
      "Iteration 31505, loss = 0.00632448\n",
      "Iteration 31506, loss = 0.00632436\n",
      "Iteration 31507, loss = 0.00632424\n",
      "Iteration 31508, loss = 0.00632413\n",
      "Iteration 31509, loss = 0.00632401\n",
      "Iteration 31510, loss = 0.00632389\n",
      "Iteration 31511, loss = 0.00632377\n",
      "Iteration 31512, loss = 0.00632365\n",
      "Iteration 31513, loss = 0.00632353\n",
      "Iteration 31514, loss = 0.00632341\n",
      "Iteration 31515, loss = 0.00632329\n",
      "Iteration 31516, loss = 0.00632317\n",
      "Iteration 31517, loss = 0.00632305\n",
      "Iteration 31518, loss = 0.00632293\n",
      "Iteration 31519, loss = 0.00632281\n",
      "Iteration 31520, loss = 0.00632269\n",
      "Iteration 31521, loss = 0.00632258\n",
      "Iteration 31522, loss = 0.00632246\n",
      "Iteration 31523, loss = 0.00632234\n",
      "Iteration 31524, loss = 0.00632222\n",
      "Iteration 31525, loss = 0.00632210\n",
      "Iteration 31526, loss = 0.00632198\n",
      "Iteration 31527, loss = 0.00632186\n",
      "Iteration 31528, loss = 0.00632174\n",
      "Iteration 31529, loss = 0.00632162\n",
      "Iteration 31530, loss = 0.00632150\n",
      "Iteration 31531, loss = 0.00632138\n",
      "Iteration 31532, loss = 0.00632127\n",
      "Iteration 31533, loss = 0.00632115\n",
      "Iteration 31534, loss = 0.00632103\n",
      "Iteration 31535, loss = 0.00632091\n",
      "Iteration 31536, loss = 0.00632079\n",
      "Iteration 31537, loss = 0.00632067\n",
      "Iteration 31538, loss = 0.00632055\n",
      "Iteration 31539, loss = 0.00632043\n",
      "Iteration 31540, loss = 0.00632031\n",
      "Iteration 31541, loss = 0.00632020\n",
      "Iteration 31542, loss = 0.00632008\n",
      "Iteration 31543, loss = 0.00631996\n",
      "Iteration 31544, loss = 0.00631984\n",
      "Iteration 31545, loss = 0.00631972\n",
      "Iteration 31546, loss = 0.00631960\n",
      "Iteration 31547, loss = 0.00631948\n",
      "Iteration 31548, loss = 0.00631936\n",
      "Iteration 31549, loss = 0.00631924\n",
      "Iteration 31550, loss = 0.00631913\n",
      "Iteration 31551, loss = 0.00631901\n",
      "Iteration 31552, loss = 0.00631889\n",
      "Iteration 31553, loss = 0.00631877\n",
      "Iteration 31554, loss = 0.00631865\n",
      "Iteration 31555, loss = 0.00631853\n",
      "Iteration 31556, loss = 0.00631841\n",
      "Iteration 31557, loss = 0.00631829\n",
      "Iteration 31558, loss = 0.00631818\n",
      "Iteration 31559, loss = 0.00631806\n",
      "Iteration 31560, loss = 0.00631794\n",
      "Iteration 31561, loss = 0.00631782\n",
      "Iteration 31562, loss = 0.00631770\n",
      "Iteration 31563, loss = 0.00631758\n",
      "Iteration 31564, loss = 0.00631746\n",
      "Iteration 31565, loss = 0.00631734\n",
      "Iteration 31566, loss = 0.00631723\n",
      "Iteration 31567, loss = 0.00631711\n",
      "Iteration 31568, loss = 0.00631699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31569, loss = 0.00631687\n",
      "Iteration 31570, loss = 0.00631675\n",
      "Iteration 31571, loss = 0.00631663\n",
      "Iteration 31572, loss = 0.00631651\n",
      "Iteration 31573, loss = 0.00631640\n",
      "Iteration 31574, loss = 0.00631628\n",
      "Iteration 31575, loss = 0.00631616\n",
      "Iteration 31576, loss = 0.00631604\n",
      "Iteration 31577, loss = 0.00631592\n",
      "Iteration 31578, loss = 0.00631580\n",
      "Iteration 31579, loss = 0.00631568\n",
      "Iteration 31580, loss = 0.00631557\n",
      "Iteration 31581, loss = 0.00631545\n",
      "Iteration 31582, loss = 0.00631533\n",
      "Iteration 31583, loss = 0.00631521\n",
      "Iteration 31584, loss = 0.00631509\n",
      "Iteration 31585, loss = 0.00631497\n",
      "Iteration 31586, loss = 0.00631485\n",
      "Iteration 31587, loss = 0.00631474\n",
      "Iteration 31588, loss = 0.00631462\n",
      "Iteration 31589, loss = 0.00631450\n",
      "Iteration 31590, loss = 0.00631438\n",
      "Iteration 31591, loss = 0.00631426\n",
      "Iteration 31592, loss = 0.00631414\n",
      "Iteration 31593, loss = 0.00631403\n",
      "Iteration 31594, loss = 0.00631391\n",
      "Iteration 31595, loss = 0.00631379\n",
      "Iteration 31596, loss = 0.00631367\n",
      "Iteration 31597, loss = 0.00631355\n",
      "Iteration 31598, loss = 0.00631343\n",
      "Iteration 31599, loss = 0.00631332\n",
      "Iteration 31600, loss = 0.00631320\n",
      "Iteration 31601, loss = 0.00631308\n",
      "Iteration 31602, loss = 0.00631296\n",
      "Iteration 31603, loss = 0.00631284\n",
      "Iteration 31604, loss = 0.00631272\n",
      "Iteration 31605, loss = 0.00631261\n",
      "Iteration 31606, loss = 0.00631249\n",
      "Iteration 31607, loss = 0.00631237\n",
      "Iteration 31608, loss = 0.00631225\n",
      "Iteration 31609, loss = 0.00631213\n",
      "Iteration 31610, loss = 0.00631201\n",
      "Iteration 31611, loss = 0.00631190\n",
      "Iteration 31612, loss = 0.00631178\n",
      "Iteration 31613, loss = 0.00631166\n",
      "Iteration 31614, loss = 0.00631154\n",
      "Iteration 31615, loss = 0.00631142\n",
      "Iteration 31616, loss = 0.00631131\n",
      "Iteration 31617, loss = 0.00631119\n",
      "Iteration 31618, loss = 0.00631107\n",
      "Iteration 31619, loss = 0.00631095\n",
      "Iteration 31620, loss = 0.00631083\n",
      "Iteration 31621, loss = 0.00631071\n",
      "Iteration 31622, loss = 0.00631060\n",
      "Iteration 31623, loss = 0.00631048\n",
      "Iteration 31624, loss = 0.00631036\n",
      "Iteration 31625, loss = 0.00631024\n",
      "Iteration 31626, loss = 0.00631012\n",
      "Iteration 31627, loss = 0.00631001\n",
      "Iteration 31628, loss = 0.00630989\n",
      "Iteration 31629, loss = 0.00630977\n",
      "Iteration 31630, loss = 0.00630965\n",
      "Iteration 31631, loss = 0.00630953\n",
      "Iteration 31632, loss = 0.00630942\n",
      "Iteration 31633, loss = 0.00630930\n",
      "Iteration 31634, loss = 0.00630918\n",
      "Iteration 31635, loss = 0.00630906\n",
      "Iteration 31636, loss = 0.00630894\n",
      "Iteration 31637, loss = 0.00630883\n",
      "Iteration 31638, loss = 0.00630871\n",
      "Iteration 31639, loss = 0.00630859\n",
      "Iteration 31640, loss = 0.00630847\n",
      "Iteration 31641, loss = 0.00630835\n",
      "Iteration 31642, loss = 0.00630824\n",
      "Iteration 31643, loss = 0.00630812\n",
      "Iteration 31644, loss = 0.00630800\n",
      "Iteration 31645, loss = 0.00630788\n",
      "Iteration 31646, loss = 0.00630777\n",
      "Iteration 31647, loss = 0.00630765\n",
      "Iteration 31648, loss = 0.00630753\n",
      "Iteration 31649, loss = 0.00630741\n",
      "Iteration 31650, loss = 0.00630729\n",
      "Iteration 31651, loss = 0.00630718\n",
      "Iteration 31652, loss = 0.00630706\n",
      "Iteration 31653, loss = 0.00630694\n",
      "Iteration 31654, loss = 0.00630682\n",
      "Iteration 31655, loss = 0.00630671\n",
      "Iteration 31656, loss = 0.00630659\n",
      "Iteration 31657, loss = 0.00630647\n",
      "Iteration 31658, loss = 0.00630635\n",
      "Iteration 31659, loss = 0.00630623\n",
      "Iteration 31660, loss = 0.00630612\n",
      "Iteration 31661, loss = 0.00630600\n",
      "Iteration 31662, loss = 0.00630588\n",
      "Iteration 31663, loss = 0.00630576\n",
      "Iteration 31664, loss = 0.00630565\n",
      "Iteration 31665, loss = 0.00630553\n",
      "Iteration 31666, loss = 0.00630541\n",
      "Iteration 31667, loss = 0.00630529\n",
      "Iteration 31668, loss = 0.00630518\n",
      "Iteration 31669, loss = 0.00630506\n",
      "Iteration 31670, loss = 0.00630494\n",
      "Iteration 31671, loss = 0.00630482\n",
      "Iteration 31672, loss = 0.00630471\n",
      "Iteration 31673, loss = 0.00630459\n",
      "Iteration 31674, loss = 0.00630447\n",
      "Iteration 31675, loss = 0.00630435\n",
      "Iteration 31676, loss = 0.00630423\n",
      "Iteration 31677, loss = 0.00630412\n",
      "Iteration 31678, loss = 0.00630400\n",
      "Iteration 31679, loss = 0.00630388\n",
      "Iteration 31680, loss = 0.00630376\n",
      "Iteration 31681, loss = 0.00630365\n",
      "Iteration 31682, loss = 0.00630353\n",
      "Iteration 31683, loss = 0.00630341\n",
      "Iteration 31684, loss = 0.00630329\n",
      "Iteration 31685, loss = 0.00630318\n",
      "Iteration 31686, loss = 0.00630306\n",
      "Iteration 31687, loss = 0.00630294\n",
      "Iteration 31688, loss = 0.00630283\n",
      "Iteration 31689, loss = 0.00630271\n",
      "Iteration 31690, loss = 0.00630259\n",
      "Iteration 31691, loss = 0.00630247\n",
      "Iteration 31692, loss = 0.00630236\n",
      "Iteration 31693, loss = 0.00630224\n",
      "Iteration 31694, loss = 0.00630212\n",
      "Iteration 31695, loss = 0.00630200\n",
      "Iteration 31696, loss = 0.00630189\n",
      "Iteration 31697, loss = 0.00630177\n",
      "Iteration 31698, loss = 0.00630165\n",
      "Iteration 31699, loss = 0.00630153\n",
      "Iteration 31700, loss = 0.00630142\n",
      "Iteration 31701, loss = 0.00630130\n",
      "Iteration 31702, loss = 0.00630118\n",
      "Iteration 31703, loss = 0.00630107\n",
      "Iteration 31704, loss = 0.00630095\n",
      "Iteration 31705, loss = 0.00630083\n",
      "Iteration 31706, loss = 0.00630071\n",
      "Iteration 31707, loss = 0.00630060\n",
      "Iteration 31708, loss = 0.00630048\n",
      "Iteration 31709, loss = 0.00630036\n",
      "Iteration 31710, loss = 0.00630024\n",
      "Iteration 31711, loss = 0.00630013\n",
      "Iteration 31712, loss = 0.00630001\n",
      "Iteration 31713, loss = 0.00629989\n",
      "Iteration 31714, loss = 0.00629978\n",
      "Iteration 31715, loss = 0.00629966\n",
      "Iteration 31716, loss = 0.00629954\n",
      "Iteration 31717, loss = 0.00629942\n",
      "Iteration 31718, loss = 0.00629931\n",
      "Iteration 31719, loss = 0.00629919\n",
      "Iteration 31720, loss = 0.00629907\n",
      "Iteration 31721, loss = 0.00629896\n",
      "Iteration 31722, loss = 0.00629884\n",
      "Iteration 31723, loss = 0.00629872\n",
      "Iteration 31724, loss = 0.00629861\n",
      "Iteration 31725, loss = 0.00629849\n",
      "Iteration 31726, loss = 0.00629837\n",
      "Iteration 31727, loss = 0.00629825\n",
      "Iteration 31728, loss = 0.00629814\n",
      "Iteration 31729, loss = 0.00629802\n",
      "Iteration 31730, loss = 0.00629790\n",
      "Iteration 31731, loss = 0.00629779\n",
      "Iteration 31732, loss = 0.00629767\n",
      "Iteration 31733, loss = 0.00629755\n",
      "Iteration 31734, loss = 0.00629744\n",
      "Iteration 31735, loss = 0.00629732\n",
      "Iteration 31736, loss = 0.00629720\n",
      "Iteration 31737, loss = 0.00629708\n",
      "Iteration 31738, loss = 0.00629697\n",
      "Iteration 31739, loss = 0.00629685\n",
      "Iteration 31740, loss = 0.00629673\n",
      "Iteration 31741, loss = 0.00629662\n",
      "Iteration 31742, loss = 0.00629650\n",
      "Iteration 31743, loss = 0.00629638\n",
      "Iteration 31744, loss = 0.00629627\n",
      "Iteration 31745, loss = 0.00629615\n",
      "Iteration 31746, loss = 0.00629603\n",
      "Iteration 31747, loss = 0.00629592\n",
      "Iteration 31748, loss = 0.00629580\n",
      "Iteration 31749, loss = 0.00629568\n",
      "Iteration 31750, loss = 0.00629557\n",
      "Iteration 31751, loss = 0.00629545\n",
      "Iteration 31752, loss = 0.00629533\n",
      "Iteration 31753, loss = 0.00629522\n",
      "Iteration 31754, loss = 0.00629510\n",
      "Iteration 31755, loss = 0.00629498\n",
      "Iteration 31756, loss = 0.00629486\n",
      "Iteration 31757, loss = 0.00629475\n",
      "Iteration 31758, loss = 0.00629463\n",
      "Iteration 31759, loss = 0.00629451\n",
      "Iteration 31760, loss = 0.00629440\n",
      "Iteration 31761, loss = 0.00629428\n",
      "Iteration 31762, loss = 0.00629416\n",
      "Iteration 31763, loss = 0.00629405\n",
      "Iteration 31764, loss = 0.00629393\n",
      "Iteration 31765, loss = 0.00629381\n",
      "Iteration 31766, loss = 0.00629370\n",
      "Iteration 31767, loss = 0.00629358\n",
      "Iteration 31768, loss = 0.00629346\n",
      "Iteration 31769, loss = 0.00629335\n",
      "Iteration 31770, loss = 0.00629323\n",
      "Iteration 31771, loss = 0.00629312\n",
      "Iteration 31772, loss = 0.00629300\n",
      "Iteration 31773, loss = 0.00629288\n",
      "Iteration 31774, loss = 0.00629277\n",
      "Iteration 31775, loss = 0.00629265\n",
      "Iteration 31776, loss = 0.00629253\n",
      "Iteration 31777, loss = 0.00629242\n",
      "Iteration 31778, loss = 0.00629230\n",
      "Iteration 31779, loss = 0.00629218\n",
      "Iteration 31780, loss = 0.00629207\n",
      "Iteration 31781, loss = 0.00629195\n",
      "Iteration 31782, loss = 0.00629183\n",
      "Iteration 31783, loss = 0.00629172\n",
      "Iteration 31784, loss = 0.00629160\n",
      "Iteration 31785, loss = 0.00629148\n",
      "Iteration 31786, loss = 0.00629137\n",
      "Iteration 31787, loss = 0.00629125\n",
      "Iteration 31788, loss = 0.00629113\n",
      "Iteration 31789, loss = 0.00629102\n",
      "Iteration 31790, loss = 0.00629090\n",
      "Iteration 31791, loss = 0.00629079\n",
      "Iteration 31792, loss = 0.00629067\n",
      "Iteration 31793, loss = 0.00629055\n",
      "Iteration 31794, loss = 0.00629044\n",
      "Iteration 31795, loss = 0.00629032\n",
      "Iteration 31796, loss = 0.00629020\n",
      "Iteration 31797, loss = 0.00629009\n",
      "Iteration 31798, loss = 0.00628997\n",
      "Iteration 31799, loss = 0.00628986\n",
      "Iteration 31800, loss = 0.00628974\n",
      "Iteration 31801, loss = 0.00628962\n",
      "Iteration 31802, loss = 0.00628951\n",
      "Iteration 31803, loss = 0.00628939\n",
      "Iteration 31804, loss = 0.00628927\n",
      "Iteration 31805, loss = 0.00628916\n",
      "Iteration 31806, loss = 0.00628904\n",
      "Iteration 31807, loss = 0.00628893\n",
      "Iteration 31808, loss = 0.00628881\n",
      "Iteration 31809, loss = 0.00628869\n",
      "Iteration 31810, loss = 0.00628858\n",
      "Iteration 31811, loss = 0.00628846\n",
      "Iteration 31812, loss = 0.00628834\n",
      "Iteration 31813, loss = 0.00628823\n",
      "Iteration 31814, loss = 0.00628811\n",
      "Iteration 31815, loss = 0.00628800\n",
      "Iteration 31816, loss = 0.00628788\n",
      "Iteration 31817, loss = 0.00628776\n",
      "Iteration 31818, loss = 0.00628765\n",
      "Iteration 31819, loss = 0.00628753\n",
      "Iteration 31820, loss = 0.00628742\n",
      "Iteration 31821, loss = 0.00628730\n",
      "Iteration 31822, loss = 0.00628718\n",
      "Iteration 31823, loss = 0.00628707\n",
      "Iteration 31824, loss = 0.00628695\n",
      "Iteration 31825, loss = 0.00628684\n",
      "Iteration 31826, loss = 0.00628672\n",
      "Iteration 31827, loss = 0.00628660\n",
      "Iteration 31828, loss = 0.00628649\n",
      "Iteration 31829, loss = 0.00628637\n",
      "Iteration 31830, loss = 0.00628626\n",
      "Iteration 31831, loss = 0.00628614\n",
      "Iteration 31832, loss = 0.00628602\n",
      "Iteration 31833, loss = 0.00628591\n",
      "Iteration 31834, loss = 0.00628579\n",
      "Iteration 31835, loss = 0.00628568\n",
      "Iteration 31836, loss = 0.00628556\n",
      "Iteration 31837, loss = 0.00628544\n",
      "Iteration 31838, loss = 0.00628533\n",
      "Iteration 31839, loss = 0.00628521\n",
      "Iteration 31840, loss = 0.00628510\n",
      "Iteration 31841, loss = 0.00628498\n",
      "Iteration 31842, loss = 0.00628486\n",
      "Iteration 31843, loss = 0.00628475\n",
      "Iteration 31844, loss = 0.00628463\n",
      "Iteration 31845, loss = 0.00628452\n",
      "Iteration 31846, loss = 0.00628440\n",
      "Iteration 31847, loss = 0.00628428\n",
      "Iteration 31848, loss = 0.00628417\n",
      "Iteration 31849, loss = 0.00628405\n",
      "Iteration 31850, loss = 0.00628394\n",
      "Iteration 31851, loss = 0.00628382\n",
      "Iteration 31852, loss = 0.00628371\n",
      "Iteration 31853, loss = 0.00628359\n",
      "Iteration 31854, loss = 0.00628347\n",
      "Iteration 31855, loss = 0.00628336\n",
      "Iteration 31856, loss = 0.00628324\n",
      "Iteration 31857, loss = 0.00628313\n",
      "Iteration 31858, loss = 0.00628301\n",
      "Iteration 31859, loss = 0.00628290\n",
      "Iteration 31860, loss = 0.00628278\n",
      "Iteration 31861, loss = 0.00628266\n",
      "Iteration 31862, loss = 0.00628255\n",
      "Iteration 31863, loss = 0.00628243\n",
      "Iteration 31864, loss = 0.00628232\n",
      "Iteration 31865, loss = 0.00628220\n",
      "Iteration 31866, loss = 0.00628209\n",
      "Iteration 31867, loss = 0.00628197\n",
      "Iteration 31868, loss = 0.00628186\n",
      "Iteration 31869, loss = 0.00628174\n",
      "Iteration 31870, loss = 0.00628162\n",
      "Iteration 31871, loss = 0.00628151\n",
      "Iteration 31872, loss = 0.00628139\n",
      "Iteration 31873, loss = 0.00628128\n",
      "Iteration 31874, loss = 0.00628116\n",
      "Iteration 31875, loss = 0.00628105\n",
      "Iteration 31876, loss = 0.00628093\n",
      "Iteration 31877, loss = 0.00628082\n",
      "Iteration 31878, loss = 0.00628070\n",
      "Iteration 31879, loss = 0.00628058\n",
      "Iteration 31880, loss = 0.00628047\n",
      "Iteration 31881, loss = 0.00628035\n",
      "Iteration 31882, loss = 0.00628024\n",
      "Iteration 31883, loss = 0.00628012\n",
      "Iteration 31884, loss = 0.00628001\n",
      "Iteration 31885, loss = 0.00627989\n",
      "Iteration 31886, loss = 0.00627978\n",
      "Iteration 31887, loss = 0.00627966\n",
      "Iteration 31888, loss = 0.00627955\n",
      "Iteration 31889, loss = 0.00627943\n",
      "Iteration 31890, loss = 0.00627931\n",
      "Iteration 31891, loss = 0.00627920\n",
      "Iteration 31892, loss = 0.00627908\n",
      "Iteration 31893, loss = 0.00627897\n",
      "Iteration 31894, loss = 0.00627885\n",
      "Iteration 31895, loss = 0.00627874\n",
      "Iteration 31896, loss = 0.00627862\n",
      "Iteration 31897, loss = 0.00627851\n",
      "Iteration 31898, loss = 0.00627839\n",
      "Iteration 31899, loss = 0.00627828\n",
      "Iteration 31900, loss = 0.00627816\n",
      "Iteration 31901, loss = 0.00627805\n",
      "Iteration 31902, loss = 0.00627793\n",
      "Iteration 31903, loss = 0.00627782\n",
      "Iteration 31904, loss = 0.00627770\n",
      "Iteration 31905, loss = 0.00627758\n",
      "Iteration 31906, loss = 0.00627747\n",
      "Iteration 31907, loss = 0.00627735\n",
      "Iteration 31908, loss = 0.00627724\n",
      "Iteration 31909, loss = 0.00627712\n",
      "Iteration 31910, loss = 0.00627701\n",
      "Iteration 31911, loss = 0.00627689\n",
      "Iteration 31912, loss = 0.00627678\n",
      "Iteration 31913, loss = 0.00627666\n",
      "Iteration 31914, loss = 0.00627655\n",
      "Iteration 31915, loss = 0.00627643\n",
      "Iteration 31916, loss = 0.00627632\n",
      "Iteration 31917, loss = 0.00627620\n",
      "Iteration 31918, loss = 0.00627609\n",
      "Iteration 31919, loss = 0.00627597\n",
      "Iteration 31920, loss = 0.00627586\n",
      "Iteration 31921, loss = 0.00627574\n",
      "Iteration 31922, loss = 0.00627563\n",
      "Iteration 31923, loss = 0.00627551\n",
      "Iteration 31924, loss = 0.00627540\n",
      "Iteration 31925, loss = 0.00627528\n",
      "Iteration 31926, loss = 0.00627517\n",
      "Iteration 31927, loss = 0.00627505\n",
      "Iteration 31928, loss = 0.00627494\n",
      "Iteration 31929, loss = 0.00627482\n",
      "Iteration 31930, loss = 0.00627471\n",
      "Iteration 31931, loss = 0.00627459\n",
      "Iteration 31932, loss = 0.00627448\n",
      "Iteration 31933, loss = 0.00627436\n",
      "Iteration 31934, loss = 0.00627425\n",
      "Iteration 31935, loss = 0.00627413\n",
      "Iteration 31936, loss = 0.00627402\n",
      "Iteration 31937, loss = 0.00627390\n",
      "Iteration 31938, loss = 0.00627379\n",
      "Iteration 31939, loss = 0.00627367\n",
      "Iteration 31940, loss = 0.00627356\n",
      "Iteration 31941, loss = 0.00627344\n",
      "Iteration 31942, loss = 0.00627333\n",
      "Iteration 31943, loss = 0.00627321\n",
      "Iteration 31944, loss = 0.00627310\n",
      "Iteration 31945, loss = 0.00627298\n",
      "Iteration 31946, loss = 0.00627287\n",
      "Iteration 31947, loss = 0.00627275\n",
      "Iteration 31948, loss = 0.00627264\n",
      "Iteration 31949, loss = 0.00627252\n",
      "Iteration 31950, loss = 0.00627241\n",
      "Iteration 31951, loss = 0.00627229\n",
      "Iteration 31952, loss = 0.00627218\n",
      "Iteration 31953, loss = 0.00627207\n",
      "Iteration 31954, loss = 0.00627195\n",
      "Iteration 31955, loss = 0.00627184\n",
      "Iteration 31956, loss = 0.00627172\n",
      "Iteration 31957, loss = 0.00627161\n",
      "Iteration 31958, loss = 0.00627149\n",
      "Iteration 31959, loss = 0.00627138\n",
      "Iteration 31960, loss = 0.00627126\n",
      "Iteration 31961, loss = 0.00627115\n",
      "Iteration 31962, loss = 0.00627103\n",
      "Iteration 31963, loss = 0.00627092\n",
      "Iteration 31964, loss = 0.00627080\n",
      "Iteration 31965, loss = 0.00627069\n",
      "Iteration 31966, loss = 0.00627057\n",
      "Iteration 31967, loss = 0.00627046\n",
      "Iteration 31968, loss = 0.00627035\n",
      "Iteration 31969, loss = 0.00627023\n",
      "Iteration 31970, loss = 0.00627012\n",
      "Iteration 31971, loss = 0.00627000\n",
      "Iteration 31972, loss = 0.00626989\n",
      "Iteration 31973, loss = 0.00626977\n",
      "Iteration 31974, loss = 0.00626966\n",
      "Iteration 31975, loss = 0.00626954\n",
      "Iteration 31976, loss = 0.00626943\n",
      "Iteration 31977, loss = 0.00626931\n",
      "Iteration 31978, loss = 0.00626920\n",
      "Iteration 31979, loss = 0.00626909\n",
      "Iteration 31980, loss = 0.00626897\n",
      "Iteration 31981, loss = 0.00626886\n",
      "Iteration 31982, loss = 0.00626874\n",
      "Iteration 31983, loss = 0.00626863\n",
      "Iteration 31984, loss = 0.00626851\n",
      "Iteration 31985, loss = 0.00626840\n",
      "Iteration 31986, loss = 0.00626828\n",
      "Iteration 31987, loss = 0.00626817\n",
      "Iteration 31988, loss = 0.00626805\n",
      "Iteration 31989, loss = 0.00626794\n",
      "Iteration 31990, loss = 0.00626783\n",
      "Iteration 31991, loss = 0.00626771\n",
      "Iteration 31992, loss = 0.00626760\n",
      "Iteration 31993, loss = 0.00626748\n",
      "Iteration 31994, loss = 0.00626737\n",
      "Iteration 31995, loss = 0.00626725\n",
      "Iteration 31996, loss = 0.00626714\n",
      "Iteration 31997, loss = 0.00626703\n",
      "Iteration 31998, loss = 0.00626691\n",
      "Iteration 31999, loss = 0.00626680\n",
      "Iteration 32000, loss = 0.00626668\n",
      "Iteration 32001, loss = 0.00626657\n",
      "Iteration 32002, loss = 0.00626645\n",
      "Iteration 32003, loss = 0.00626634\n",
      "Iteration 32004, loss = 0.00626623\n",
      "Iteration 32005, loss = 0.00626611\n",
      "Iteration 32006, loss = 0.00626600\n",
      "Iteration 32007, loss = 0.00626588\n",
      "Iteration 32008, loss = 0.00626577\n",
      "Iteration 32009, loss = 0.00626565\n",
      "Iteration 32010, loss = 0.00626554\n",
      "Iteration 32011, loss = 0.00626543\n",
      "Iteration 32012, loss = 0.00626531\n",
      "Iteration 32013, loss = 0.00626520\n",
      "Iteration 32014, loss = 0.00626508\n",
      "Iteration 32015, loss = 0.00626497\n",
      "Iteration 32016, loss = 0.00626486\n",
      "Iteration 32017, loss = 0.00626474\n",
      "Iteration 32018, loss = 0.00626463\n",
      "Iteration 32019, loss = 0.00626451\n",
      "Iteration 32020, loss = 0.00626440\n",
      "Iteration 32021, loss = 0.00626428\n",
      "Iteration 32022, loss = 0.00626417\n",
      "Iteration 32023, loss = 0.00626406\n",
      "Iteration 32024, loss = 0.00626394\n",
      "Iteration 32025, loss = 0.00626383\n",
      "Iteration 32026, loss = 0.00626371\n",
      "Iteration 32027, loss = 0.00626360\n",
      "Iteration 32028, loss = 0.00626349\n",
      "Iteration 32029, loss = 0.00626337\n",
      "Iteration 32030, loss = 0.00626326\n",
      "Iteration 32031, loss = 0.00626314\n",
      "Iteration 32032, loss = 0.00626303\n",
      "Iteration 32033, loss = 0.00626292\n",
      "Iteration 32034, loss = 0.00626280\n",
      "Iteration 32035, loss = 0.00626269\n",
      "Iteration 32036, loss = 0.00626257\n",
      "Iteration 32037, loss = 0.00626246\n",
      "Iteration 32038, loss = 0.00626235\n",
      "Iteration 32039, loss = 0.00626223\n",
      "Iteration 32040, loss = 0.00626212\n",
      "Iteration 32041, loss = 0.00626201\n",
      "Iteration 32042, loss = 0.00626189\n",
      "Iteration 32043, loss = 0.00626178\n",
      "Iteration 32044, loss = 0.00626166\n",
      "Iteration 32045, loss = 0.00626155\n",
      "Iteration 32046, loss = 0.00626144\n",
      "Iteration 32047, loss = 0.00626132\n",
      "Iteration 32048, loss = 0.00626121\n",
      "Iteration 32049, loss = 0.00626109\n",
      "Iteration 32050, loss = 0.00626098\n",
      "Iteration 32051, loss = 0.00626087\n",
      "Iteration 32052, loss = 0.00626075\n",
      "Iteration 32053, loss = 0.00626064\n",
      "Iteration 32054, loss = 0.00626053\n",
      "Iteration 32055, loss = 0.00626041\n",
      "Iteration 32056, loss = 0.00626030\n",
      "Iteration 32057, loss = 0.00626018\n",
      "Iteration 32058, loss = 0.00626007\n",
      "Iteration 32059, loss = 0.00625996\n",
      "Iteration 32060, loss = 0.00625984\n",
      "Iteration 32061, loss = 0.00625973\n",
      "Iteration 32062, loss = 0.00625962\n",
      "Iteration 32063, loss = 0.00625950\n",
      "Iteration 32064, loss = 0.00625939\n",
      "Iteration 32065, loss = 0.00625927\n",
      "Iteration 32066, loss = 0.00625916\n",
      "Iteration 32067, loss = 0.00625905\n",
      "Iteration 32068, loss = 0.00625893\n",
      "Iteration 32069, loss = 0.00625882\n",
      "Iteration 32070, loss = 0.00625871\n",
      "Iteration 32071, loss = 0.00625859\n",
      "Iteration 32072, loss = 0.00625848\n",
      "Iteration 32073, loss = 0.00625837\n",
      "Iteration 32074, loss = 0.00625825\n",
      "Iteration 32075, loss = 0.00625814\n",
      "Iteration 32076, loss = 0.00625802\n",
      "Iteration 32077, loss = 0.00625791\n",
      "Iteration 32078, loss = 0.00625780\n",
      "Iteration 32079, loss = 0.00625768\n",
      "Iteration 32080, loss = 0.00625757\n",
      "Iteration 32081, loss = 0.00625746\n",
      "Iteration 32082, loss = 0.00625734\n",
      "Iteration 32083, loss = 0.00625723\n",
      "Iteration 32084, loss = 0.00625712\n",
      "Iteration 32085, loss = 0.00625700\n",
      "Iteration 32086, loss = 0.00625689\n",
      "Iteration 32087, loss = 0.00625678\n",
      "Iteration 32088, loss = 0.00625666\n",
      "Iteration 32089, loss = 0.00625655\n",
      "Iteration 32090, loss = 0.00625644\n",
      "Iteration 32091, loss = 0.00625632\n",
      "Iteration 32092, loss = 0.00625621\n",
      "Iteration 32093, loss = 0.00625610\n",
      "Iteration 32094, loss = 0.00625598\n",
      "Iteration 32095, loss = 0.00625587\n",
      "Iteration 32096, loss = 0.00625576\n",
      "Iteration 32097, loss = 0.00625564\n",
      "Iteration 32098, loss = 0.00625553\n",
      "Iteration 32099, loss = 0.00625542\n",
      "Iteration 32100, loss = 0.00625530\n",
      "Iteration 32101, loss = 0.00625519\n",
      "Iteration 32102, loss = 0.00625508\n",
      "Iteration 32103, loss = 0.00625496\n",
      "Iteration 32104, loss = 0.00625485\n",
      "Iteration 32105, loss = 0.00625474\n",
      "Iteration 32106, loss = 0.00625462\n",
      "Iteration 32107, loss = 0.00625451\n",
      "Iteration 32108, loss = 0.00625440\n",
      "Iteration 32109, loss = 0.00625428\n",
      "Iteration 32110, loss = 0.00625417\n",
      "Iteration 32111, loss = 0.00625406\n",
      "Iteration 32112, loss = 0.00625394\n",
      "Iteration 32113, loss = 0.00625383\n",
      "Iteration 32114, loss = 0.00625372\n",
      "Iteration 32115, loss = 0.00625360\n",
      "Iteration 32116, loss = 0.00625349\n",
      "Iteration 32117, loss = 0.00625338\n",
      "Iteration 32118, loss = 0.00625326\n",
      "Iteration 32119, loss = 0.00625315\n",
      "Iteration 32120, loss = 0.00625304\n",
      "Iteration 32121, loss = 0.00625293\n",
      "Iteration 32122, loss = 0.00625281\n",
      "Iteration 32123, loss = 0.00625270\n",
      "Iteration 32124, loss = 0.00625259\n",
      "Iteration 32125, loss = 0.00625247\n",
      "Iteration 32126, loss = 0.00625236\n",
      "Iteration 32127, loss = 0.00625225\n",
      "Iteration 32128, loss = 0.00625213\n",
      "Iteration 32129, loss = 0.00625202\n",
      "Iteration 32130, loss = 0.00625191\n",
      "Iteration 32131, loss = 0.00625179\n",
      "Iteration 32132, loss = 0.00625168\n",
      "Iteration 32133, loss = 0.00625157\n",
      "Iteration 32134, loss = 0.00625146\n",
      "Iteration 32135, loss = 0.00625134\n",
      "Iteration 32136, loss = 0.00625123\n",
      "Iteration 32137, loss = 0.00625112\n",
      "Iteration 32138, loss = 0.00625100\n",
      "Iteration 32139, loss = 0.00625089\n",
      "Iteration 32140, loss = 0.00625078\n",
      "Iteration 32141, loss = 0.00625066\n",
      "Iteration 32142, loss = 0.00625055\n",
      "Iteration 32143, loss = 0.00625044\n",
      "Iteration 32144, loss = 0.00625033\n",
      "Iteration 32145, loss = 0.00625021\n",
      "Iteration 32146, loss = 0.00625010\n",
      "Iteration 32147, loss = 0.00624999\n",
      "Iteration 32148, loss = 0.00624987\n",
      "Iteration 32149, loss = 0.00624976\n",
      "Iteration 32150, loss = 0.00624965\n",
      "Iteration 32151, loss = 0.00624954\n",
      "Iteration 32152, loss = 0.00624942\n",
      "Iteration 32153, loss = 0.00624931\n",
      "Iteration 32154, loss = 0.00624920\n",
      "Iteration 32155, loss = 0.00624908\n",
      "Iteration 32156, loss = 0.00624897\n",
      "Iteration 32157, loss = 0.00624886\n",
      "Iteration 32158, loss = 0.00624875\n",
      "Iteration 32159, loss = 0.00624863\n",
      "Iteration 32160, loss = 0.00624852\n",
      "Iteration 32161, loss = 0.00624841\n",
      "Iteration 32162, loss = 0.00624830\n",
      "Iteration 32163, loss = 0.00624818\n",
      "Iteration 32164, loss = 0.00624807\n",
      "Iteration 32165, loss = 0.00624796\n",
      "Iteration 32166, loss = 0.00624784\n",
      "Iteration 32167, loss = 0.00624773\n",
      "Iteration 32168, loss = 0.00624762\n",
      "Iteration 32169, loss = 0.00624751\n",
      "Iteration 32170, loss = 0.00624739\n",
      "Iteration 32171, loss = 0.00624728\n",
      "Iteration 32172, loss = 0.00624717\n",
      "Iteration 32173, loss = 0.00624706\n",
      "Iteration 32174, loss = 0.00624694\n",
      "Iteration 32175, loss = 0.00624683\n",
      "Iteration 32176, loss = 0.00624672\n",
      "Iteration 32177, loss = 0.00624661\n",
      "Iteration 32178, loss = 0.00624649\n",
      "Iteration 32179, loss = 0.00624638\n",
      "Iteration 32180, loss = 0.00624627\n",
      "Iteration 32181, loss = 0.00624616\n",
      "Iteration 32182, loss = 0.00624604\n",
      "Iteration 32183, loss = 0.00624593\n",
      "Iteration 32184, loss = 0.00624582\n",
      "Iteration 32185, loss = 0.00624571\n",
      "Iteration 32186, loss = 0.00624559\n",
      "Iteration 32187, loss = 0.00624548\n",
      "Iteration 32188, loss = 0.00624537\n",
      "Iteration 32189, loss = 0.00624526\n",
      "Iteration 32190, loss = 0.00624514\n",
      "Iteration 32191, loss = 0.00624503\n",
      "Iteration 32192, loss = 0.00624492\n",
      "Iteration 32193, loss = 0.00624481\n",
      "Iteration 32194, loss = 0.00624469\n",
      "Iteration 32195, loss = 0.00624458\n",
      "Iteration 32196, loss = 0.00624447\n",
      "Iteration 32197, loss = 0.00624436\n",
      "Iteration 32198, loss = 0.00624424\n",
      "Iteration 32199, loss = 0.00624413\n",
      "Iteration 32200, loss = 0.00624402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32201, loss = 0.00624391\n",
      "Iteration 32202, loss = 0.00624379\n",
      "Iteration 32203, loss = 0.00624368\n",
      "Iteration 32204, loss = 0.00624357\n",
      "Iteration 32205, loss = 0.00624346\n",
      "Iteration 32206, loss = 0.00624335\n",
      "Iteration 32207, loss = 0.00624323\n",
      "Iteration 32208, loss = 0.00624312\n",
      "Iteration 32209, loss = 0.00624301\n",
      "Iteration 32210, loss = 0.00624290\n",
      "Iteration 32211, loss = 0.00624278\n",
      "Iteration 32212, loss = 0.00624267\n",
      "Iteration 32213, loss = 0.00624256\n",
      "Iteration 32214, loss = 0.00624245\n",
      "Iteration 32215, loss = 0.00624234\n",
      "Iteration 32216, loss = 0.00624222\n",
      "Iteration 32217, loss = 0.00624211\n",
      "Iteration 32218, loss = 0.00624200\n",
      "Iteration 32219, loss = 0.00624189\n",
      "Iteration 32220, loss = 0.00624177\n",
      "Iteration 32221, loss = 0.00624166\n",
      "Iteration 32222, loss = 0.00624155\n",
      "Iteration 32223, loss = 0.00624144\n",
      "Iteration 32224, loss = 0.00624133\n",
      "Iteration 32225, loss = 0.00624121\n",
      "Iteration 32226, loss = 0.00624110\n",
      "Iteration 32227, loss = 0.00624099\n",
      "Iteration 32228, loss = 0.00624088\n",
      "Iteration 32229, loss = 0.00624077\n",
      "Iteration 32230, loss = 0.00624065\n",
      "Iteration 32231, loss = 0.00624054\n",
      "Iteration 32232, loss = 0.00624043\n",
      "Iteration 32233, loss = 0.00624032\n",
      "Iteration 32234, loss = 0.00624021\n",
      "Iteration 32235, loss = 0.00624009\n",
      "Iteration 32236, loss = 0.00623998\n",
      "Iteration 32237, loss = 0.00623987\n",
      "Iteration 32238, loss = 0.00623976\n",
      "Iteration 32239, loss = 0.00623965\n",
      "Iteration 32240, loss = 0.00623953\n",
      "Iteration 32241, loss = 0.00623942\n",
      "Iteration 32242, loss = 0.00623931\n",
      "Iteration 32243, loss = 0.00623920\n",
      "Iteration 32244, loss = 0.00623909\n",
      "Iteration 32245, loss = 0.00623897\n",
      "Iteration 32246, loss = 0.00623886\n",
      "Iteration 32247, loss = 0.00623875\n",
      "Iteration 32248, loss = 0.00623864\n",
      "Iteration 32249, loss = 0.00623853\n",
      "Iteration 32250, loss = 0.00623841\n",
      "Iteration 32251, loss = 0.00623830\n",
      "Iteration 32252, loss = 0.00623819\n",
      "Iteration 32253, loss = 0.00623808\n",
      "Iteration 32254, loss = 0.00623797\n",
      "Iteration 32255, loss = 0.00623785\n",
      "Iteration 32256, loss = 0.00623774\n",
      "Iteration 32257, loss = 0.00623763\n",
      "Iteration 32258, loss = 0.00623752\n",
      "Iteration 32259, loss = 0.00623741\n",
      "Iteration 32260, loss = 0.00623730\n",
      "Iteration 32261, loss = 0.00623718\n",
      "Iteration 32262, loss = 0.00623707\n",
      "Iteration 32263, loss = 0.00623696\n",
      "Iteration 32264, loss = 0.00623685\n",
      "Iteration 32265, loss = 0.00623674\n",
      "Iteration 32266, loss = 0.00623663\n",
      "Iteration 32267, loss = 0.00623651\n",
      "Iteration 32268, loss = 0.00623640\n",
      "Iteration 32269, loss = 0.00623629\n",
      "Iteration 32270, loss = 0.00623618\n",
      "Iteration 32271, loss = 0.00623607\n",
      "Iteration 32272, loss = 0.00623596\n",
      "Iteration 32273, loss = 0.00623584\n",
      "Iteration 32274, loss = 0.00623573\n",
      "Iteration 32275, loss = 0.00623562\n",
      "Iteration 32276, loss = 0.00623551\n",
      "Iteration 32277, loss = 0.00623540\n",
      "Iteration 32278, loss = 0.00623529\n",
      "Iteration 32279, loss = 0.00623517\n",
      "Iteration 32280, loss = 0.00623506\n",
      "Iteration 32281, loss = 0.00623495\n",
      "Iteration 32282, loss = 0.00623484\n",
      "Iteration 32283, loss = 0.00623473\n",
      "Iteration 32284, loss = 0.00623462\n",
      "Iteration 32285, loss = 0.00623450\n",
      "Iteration 32286, loss = 0.00623439\n",
      "Iteration 32287, loss = 0.00623428\n",
      "Iteration 32288, loss = 0.00623417\n",
      "Iteration 32289, loss = 0.00623406\n",
      "Iteration 32290, loss = 0.00623395\n",
      "Iteration 32291, loss = 0.00623384\n",
      "Iteration 32292, loss = 0.00623372\n",
      "Iteration 32293, loss = 0.00623361\n",
      "Iteration 32294, loss = 0.00623350\n",
      "Iteration 32295, loss = 0.00623339\n",
      "Iteration 32296, loss = 0.00623328\n",
      "Iteration 32297, loss = 0.00623317\n",
      "Iteration 32298, loss = 0.00623306\n",
      "Iteration 32299, loss = 0.00623294\n",
      "Iteration 32300, loss = 0.00623283\n",
      "Iteration 32301, loss = 0.00623272\n",
      "Iteration 32302, loss = 0.00623261\n",
      "Iteration 32303, loss = 0.00623250\n",
      "Iteration 32304, loss = 0.00623239\n",
      "Iteration 32305, loss = 0.00623228\n",
      "Iteration 32306, loss = 0.00623216\n",
      "Iteration 32307, loss = 0.00623205\n",
      "Iteration 32308, loss = 0.00623194\n",
      "Iteration 32309, loss = 0.00623183\n",
      "Iteration 32310, loss = 0.00623172\n",
      "Iteration 32311, loss = 0.00623161\n",
      "Iteration 32312, loss = 0.00623150\n",
      "Iteration 32313, loss = 0.00623138\n",
      "Iteration 32314, loss = 0.00623127\n",
      "Iteration 32315, loss = 0.00623116\n",
      "Iteration 32316, loss = 0.00623105\n",
      "Iteration 32317, loss = 0.00623094\n",
      "Iteration 32318, loss = 0.00623083\n",
      "Iteration 32319, loss = 0.00623072\n",
      "Iteration 32320, loss = 0.00623061\n",
      "Iteration 32321, loss = 0.00623050\n",
      "Iteration 32322, loss = 0.00623038\n",
      "Iteration 32323, loss = 0.00623027\n",
      "Iteration 32324, loss = 0.00623016\n",
      "Iteration 32325, loss = 0.00623005\n",
      "Iteration 32326, loss = 0.00622994\n",
      "Iteration 32327, loss = 0.00622983\n",
      "Iteration 32328, loss = 0.00622972\n",
      "Iteration 32329, loss = 0.00622961\n",
      "Iteration 32330, loss = 0.00622949\n",
      "Iteration 32331, loss = 0.00622938\n",
      "Iteration 32332, loss = 0.00622927\n",
      "Iteration 32333, loss = 0.00622916\n",
      "Iteration 32334, loss = 0.00622905\n",
      "Iteration 32335, loss = 0.00622894\n",
      "Iteration 32336, loss = 0.00622883\n",
      "Iteration 32337, loss = 0.00622872\n",
      "Iteration 32338, loss = 0.00622861\n",
      "Iteration 32339, loss = 0.00622849\n",
      "Iteration 32340, loss = 0.00622838\n",
      "Iteration 32341, loss = 0.00622827\n",
      "Iteration 32342, loss = 0.00622816\n",
      "Iteration 32343, loss = 0.00622805\n",
      "Iteration 32344, loss = 0.00622794\n",
      "Iteration 32345, loss = 0.00622783\n",
      "Iteration 32346, loss = 0.00622772\n",
      "Iteration 32347, loss = 0.00622761\n",
      "Iteration 32348, loss = 0.00622750\n",
      "Iteration 32349, loss = 0.00622739\n",
      "Iteration 32350, loss = 0.00622727\n",
      "Iteration 32351, loss = 0.00622716\n",
      "Iteration 32352, loss = 0.00622705\n",
      "Iteration 32353, loss = 0.00622694\n",
      "Iteration 32354, loss = 0.00622683\n",
      "Iteration 32355, loss = 0.00622672\n",
      "Iteration 32356, loss = 0.00622661\n",
      "Iteration 32357, loss = 0.00622650\n",
      "Iteration 32358, loss = 0.00622639\n",
      "Iteration 32359, loss = 0.00622628\n",
      "Iteration 32360, loss = 0.00622617\n",
      "Iteration 32361, loss = 0.00622605\n",
      "Iteration 32362, loss = 0.00622594\n",
      "Iteration 32363, loss = 0.00622583\n",
      "Iteration 32364, loss = 0.00622572\n",
      "Iteration 32365, loss = 0.00622561\n",
      "Iteration 32366, loss = 0.00622550\n",
      "Iteration 32367, loss = 0.00622539\n",
      "Iteration 32368, loss = 0.00622528\n",
      "Iteration 32369, loss = 0.00622517\n",
      "Iteration 32370, loss = 0.00622506\n",
      "Iteration 32371, loss = 0.00622495\n",
      "Iteration 32372, loss = 0.00622484\n",
      "Iteration 32373, loss = 0.00622473\n",
      "Iteration 32374, loss = 0.00622461\n",
      "Iteration 32375, loss = 0.00622450\n",
      "Iteration 32376, loss = 0.00622439\n",
      "Iteration 32377, loss = 0.00622428\n",
      "Iteration 32378, loss = 0.00622417\n",
      "Iteration 32379, loss = 0.00622406\n",
      "Iteration 32380, loss = 0.00622395\n",
      "Iteration 32381, loss = 0.00622384\n",
      "Iteration 32382, loss = 0.00622373\n",
      "Iteration 32383, loss = 0.00622362\n",
      "Iteration 32384, loss = 0.00622351\n",
      "Iteration 32385, loss = 0.00622340\n",
      "Iteration 32386, loss = 0.00622329\n",
      "Iteration 32387, loss = 0.00622318\n",
      "Iteration 32388, loss = 0.00622307\n",
      "Iteration 32389, loss = 0.00622296\n",
      "Iteration 32390, loss = 0.00622284\n",
      "Iteration 32391, loss = 0.00622273\n",
      "Iteration 32392, loss = 0.00622262\n",
      "Iteration 32393, loss = 0.00622251\n",
      "Iteration 32394, loss = 0.00622240\n",
      "Iteration 32395, loss = 0.00622229\n",
      "Iteration 32396, loss = 0.00622218\n",
      "Iteration 32397, loss = 0.00622207\n",
      "Iteration 32398, loss = 0.00622196\n",
      "Iteration 32399, loss = 0.00622185\n",
      "Iteration 32400, loss = 0.00622174\n",
      "Iteration 32401, loss = 0.00622163\n",
      "Iteration 32402, loss = 0.00622152\n",
      "Iteration 32403, loss = 0.00622141\n",
      "Iteration 32404, loss = 0.00622130\n",
      "Iteration 32405, loss = 0.00622119\n",
      "Iteration 32406, loss = 0.00622108\n",
      "Iteration 32407, loss = 0.00622097\n",
      "Iteration 32408, loss = 0.00622086\n",
      "Iteration 32409, loss = 0.00622075\n",
      "Iteration 32410, loss = 0.00622064\n",
      "Iteration 32411, loss = 0.00622053\n",
      "Iteration 32412, loss = 0.00622042\n",
      "Iteration 32413, loss = 0.00622030\n",
      "Iteration 32414, loss = 0.00622019\n",
      "Iteration 32415, loss = 0.00622008\n",
      "Iteration 32416, loss = 0.00621997\n",
      "Iteration 32417, loss = 0.00621986\n",
      "Iteration 32418, loss = 0.00621975\n",
      "Iteration 32419, loss = 0.00621964\n",
      "Iteration 32420, loss = 0.00621953\n",
      "Iteration 32421, loss = 0.00621942\n",
      "Iteration 32422, loss = 0.00621931\n",
      "Iteration 32423, loss = 0.00621920\n",
      "Iteration 32424, loss = 0.00621909\n",
      "Iteration 32425, loss = 0.00621898\n",
      "Iteration 32426, loss = 0.00621887\n",
      "Iteration 32427, loss = 0.00621876\n",
      "Iteration 32428, loss = 0.00621865\n",
      "Iteration 32429, loss = 0.00621854\n",
      "Iteration 32430, loss = 0.00621843\n",
      "Iteration 32431, loss = 0.00621832\n",
      "Iteration 32432, loss = 0.00621821\n",
      "Iteration 32433, loss = 0.00621810\n",
      "Iteration 32434, loss = 0.00621799\n",
      "Iteration 32435, loss = 0.00621788\n",
      "Iteration 32436, loss = 0.00621777\n",
      "Iteration 32437, loss = 0.00621766\n",
      "Iteration 32438, loss = 0.00621755\n",
      "Iteration 32439, loss = 0.00621744\n",
      "Iteration 32440, loss = 0.00621733\n",
      "Iteration 32441, loss = 0.00621722\n",
      "Iteration 32442, loss = 0.00621711\n",
      "Iteration 32443, loss = 0.00621700\n",
      "Iteration 32444, loss = 0.00621689\n",
      "Iteration 32445, loss = 0.00621678\n",
      "Iteration 32446, loss = 0.00621667\n",
      "Iteration 32447, loss = 0.00621656\n",
      "Iteration 32448, loss = 0.00621645\n",
      "Iteration 32449, loss = 0.00621634\n",
      "Iteration 32450, loss = 0.00621623\n",
      "Iteration 32451, loss = 0.00621612\n",
      "Iteration 32452, loss = 0.00621601\n",
      "Iteration 32453, loss = 0.00621590\n",
      "Iteration 32454, loss = 0.00621579\n",
      "Iteration 32455, loss = 0.00621568\n",
      "Iteration 32456, loss = 0.00621557\n",
      "Iteration 32457, loss = 0.00621546\n",
      "Iteration 32458, loss = 0.00621535\n",
      "Iteration 32459, loss = 0.00621524\n",
      "Iteration 32460, loss = 0.00621513\n",
      "Iteration 32461, loss = 0.00621502\n",
      "Iteration 32462, loss = 0.00621491\n",
      "Iteration 32463, loss = 0.00621480\n",
      "Iteration 32464, loss = 0.00621469\n",
      "Iteration 32465, loss = 0.00621458\n",
      "Iteration 32466, loss = 0.00621447\n",
      "Iteration 32467, loss = 0.00621436\n",
      "Iteration 32468, loss = 0.00621425\n",
      "Iteration 32469, loss = 0.00621414\n",
      "Iteration 32470, loss = 0.00621403\n",
      "Iteration 32471, loss = 0.00621392\n",
      "Iteration 32472, loss = 0.00621381\n",
      "Iteration 32473, loss = 0.00621370\n",
      "Iteration 32474, loss = 0.00621359\n",
      "Iteration 32475, loss = 0.00621348\n",
      "Iteration 32476, loss = 0.00621337\n",
      "Iteration 32477, loss = 0.00621326\n",
      "Iteration 32478, loss = 0.00621315\n",
      "Iteration 32479, loss = 0.00621304\n",
      "Iteration 32480, loss = 0.00621293\n",
      "Iteration 32481, loss = 0.00621282\n",
      "Iteration 32482, loss = 0.00621271\n",
      "Iteration 32483, loss = 0.00621260\n",
      "Iteration 32484, loss = 0.00621249\n",
      "Iteration 32485, loss = 0.00621239\n",
      "Iteration 32486, loss = 0.00621228\n",
      "Iteration 32487, loss = 0.00621217\n",
      "Iteration 32488, loss = 0.00621206\n",
      "Iteration 32489, loss = 0.00621195\n",
      "Iteration 32490, loss = 0.00621184\n",
      "Iteration 32491, loss = 0.00621173\n",
      "Iteration 32492, loss = 0.00621162\n",
      "Iteration 32493, loss = 0.00621151\n",
      "Iteration 32494, loss = 0.00621140\n",
      "Iteration 32495, loss = 0.00621129\n",
      "Iteration 32496, loss = 0.00621118\n",
      "Iteration 32497, loss = 0.00621107\n",
      "Iteration 32498, loss = 0.00621096\n",
      "Iteration 32499, loss = 0.00621085\n",
      "Iteration 32500, loss = 0.00621074\n",
      "Iteration 32501, loss = 0.00621063\n",
      "Iteration 32502, loss = 0.00621052\n",
      "Iteration 32503, loss = 0.00621041\n",
      "Iteration 32504, loss = 0.00621030\n",
      "Iteration 32505, loss = 0.00621019\n",
      "Iteration 32506, loss = 0.00621008\n",
      "Iteration 32507, loss = 0.00620997\n",
      "Iteration 32508, loss = 0.00620987\n",
      "Iteration 32509, loss = 0.00620976\n",
      "Iteration 32510, loss = 0.00620965\n",
      "Iteration 32511, loss = 0.00620954\n",
      "Iteration 32512, loss = 0.00620943\n",
      "Iteration 32513, loss = 0.00620932\n",
      "Iteration 32514, loss = 0.00620921\n",
      "Iteration 32515, loss = 0.00620910\n",
      "Iteration 32516, loss = 0.00620899\n",
      "Iteration 32517, loss = 0.00620888\n",
      "Iteration 32518, loss = 0.00620877\n",
      "Iteration 32519, loss = 0.00620866\n",
      "Iteration 32520, loss = 0.00620855\n",
      "Iteration 32521, loss = 0.00620844\n",
      "Iteration 32522, loss = 0.00620833\n",
      "Iteration 32523, loss = 0.00620822\n",
      "Iteration 32524, loss = 0.00620812\n",
      "Iteration 32525, loss = 0.00620801\n",
      "Iteration 32526, loss = 0.00620790\n",
      "Iteration 32527, loss = 0.00620779\n",
      "Iteration 32528, loss = 0.00620768\n",
      "Iteration 32529, loss = 0.00620757\n",
      "Iteration 32530, loss = 0.00620746\n",
      "Iteration 32531, loss = 0.00620735\n",
      "Iteration 32532, loss = 0.00620724\n",
      "Iteration 32533, loss = 0.00620713\n",
      "Iteration 32534, loss = 0.00620702\n",
      "Iteration 32535, loss = 0.00620691\n",
      "Iteration 32536, loss = 0.00620680\n",
      "Iteration 32537, loss = 0.00620670\n",
      "Iteration 32538, loss = 0.00620659\n",
      "Iteration 32539, loss = 0.00620648\n",
      "Iteration 32540, loss = 0.00620637\n",
      "Iteration 32541, loss = 0.00620626\n",
      "Iteration 32542, loss = 0.00620615\n",
      "Iteration 32543, loss = 0.00620604\n",
      "Iteration 32544, loss = 0.00620593\n",
      "Iteration 32545, loss = 0.00620582\n",
      "Iteration 32546, loss = 0.00620571\n",
      "Iteration 32547, loss = 0.00620560\n",
      "Iteration 32548, loss = 0.00620549\n",
      "Iteration 32549, loss = 0.00620539\n",
      "Iteration 32550, loss = 0.00620528\n",
      "Iteration 32551, loss = 0.00620517\n",
      "Iteration 32552, loss = 0.00620506\n",
      "Iteration 32553, loss = 0.00620495\n",
      "Iteration 32554, loss = 0.00620484\n",
      "Iteration 32555, loss = 0.00620473\n",
      "Iteration 32556, loss = 0.00620462\n",
      "Iteration 32557, loss = 0.00620451\n",
      "Iteration 32558, loss = 0.00620440\n",
      "Iteration 32559, loss = 0.00620430\n",
      "Iteration 32560, loss = 0.00620419\n",
      "Iteration 32561, loss = 0.00620408\n",
      "Iteration 32562, loss = 0.00620397\n",
      "Iteration 32563, loss = 0.00620386\n",
      "Iteration 32564, loss = 0.00620375\n",
      "Iteration 32565, loss = 0.00620364\n",
      "Iteration 32566, loss = 0.00620353\n",
      "Iteration 32567, loss = 0.00620342\n",
      "Iteration 32568, loss = 0.00620332\n",
      "Iteration 32569, loss = 0.00620321\n",
      "Iteration 32570, loss = 0.00620310\n",
      "Iteration 32571, loss = 0.00620299\n",
      "Iteration 32572, loss = 0.00620288\n",
      "Iteration 32573, loss = 0.00620277\n",
      "Iteration 32574, loss = 0.00620266\n",
      "Iteration 32575, loss = 0.00620255\n",
      "Iteration 32576, loss = 0.00620244\n",
      "Iteration 32577, loss = 0.00620234\n",
      "Iteration 32578, loss = 0.00620223\n",
      "Iteration 32579, loss = 0.00620212\n",
      "Iteration 32580, loss = 0.00620201\n",
      "Iteration 32581, loss = 0.00620190\n",
      "Iteration 32582, loss = 0.00620179\n",
      "Iteration 32583, loss = 0.00620168\n",
      "Iteration 32584, loss = 0.00620157\n",
      "Iteration 32585, loss = 0.00620147\n",
      "Iteration 32586, loss = 0.00620136\n",
      "Iteration 32587, loss = 0.00620125\n",
      "Iteration 32588, loss = 0.00620114\n",
      "Iteration 32589, loss = 0.00620103\n",
      "Iteration 32590, loss = 0.00620092\n",
      "Iteration 32591, loss = 0.00620081\n",
      "Iteration 32592, loss = 0.00620070\n",
      "Iteration 32593, loss = 0.00620060\n",
      "Iteration 32594, loss = 0.00620049\n",
      "Iteration 32595, loss = 0.00620038\n",
      "Iteration 32596, loss = 0.00620027\n",
      "Iteration 32597, loss = 0.00620016\n",
      "Iteration 32598, loss = 0.00620005\n",
      "Iteration 32599, loss = 0.00619994\n",
      "Iteration 32600, loss = 0.00619984\n",
      "Iteration 32601, loss = 0.00619973\n",
      "Iteration 32602, loss = 0.00619962\n",
      "Iteration 32603, loss = 0.00619951\n",
      "Iteration 32604, loss = 0.00619940\n",
      "Iteration 32605, loss = 0.00619929\n",
      "Iteration 32606, loss = 0.00619918\n",
      "Iteration 32607, loss = 0.00619908\n",
      "Iteration 32608, loss = 0.00619897\n",
      "Iteration 32609, loss = 0.00619886\n",
      "Iteration 32610, loss = 0.00619875\n",
      "Iteration 32611, loss = 0.00619864\n",
      "Iteration 32612, loss = 0.00619853\n",
      "Iteration 32613, loss = 0.00619842\n",
      "Iteration 32614, loss = 0.00619832\n",
      "Iteration 32615, loss = 0.00619821\n",
      "Iteration 32616, loss = 0.00619810\n",
      "Iteration 32617, loss = 0.00619799\n",
      "Iteration 32618, loss = 0.00619788\n",
      "Iteration 32619, loss = 0.00619777\n",
      "Iteration 32620, loss = 0.00619767\n",
      "Iteration 32621, loss = 0.00619756\n",
      "Iteration 32622, loss = 0.00619745\n",
      "Iteration 32623, loss = 0.00619734\n",
      "Iteration 32624, loss = 0.00619723\n",
      "Iteration 32625, loss = 0.00619712\n",
      "Iteration 32626, loss = 0.00619701\n",
      "Iteration 32627, loss = 0.00619691\n",
      "Iteration 32628, loss = 0.00619680\n",
      "Iteration 32629, loss = 0.00619669\n",
      "Iteration 32630, loss = 0.00619658\n",
      "Iteration 32631, loss = 0.00619647\n",
      "Iteration 32632, loss = 0.00619636\n",
      "Iteration 32633, loss = 0.00619626\n",
      "Iteration 32634, loss = 0.00619615\n",
      "Iteration 32635, loss = 0.00619604\n",
      "Iteration 32636, loss = 0.00619593\n",
      "Iteration 32637, loss = 0.00619582\n",
      "Iteration 32638, loss = 0.00619571\n",
      "Iteration 32639, loss = 0.00619561\n",
      "Iteration 32640, loss = 0.00619550\n",
      "Iteration 32641, loss = 0.00619539\n",
      "Iteration 32642, loss = 0.00619528\n",
      "Iteration 32643, loss = 0.00619517\n",
      "Iteration 32644, loss = 0.00619507\n",
      "Iteration 32645, loss = 0.00619496\n",
      "Iteration 32646, loss = 0.00619485\n",
      "Iteration 32647, loss = 0.00619474\n",
      "Iteration 32648, loss = 0.00619463\n",
      "Iteration 32649, loss = 0.00619452\n",
      "Iteration 32650, loss = 0.00619442\n",
      "Iteration 32651, loss = 0.00619431\n",
      "Iteration 32652, loss = 0.00619420\n",
      "Iteration 32653, loss = 0.00619409\n",
      "Iteration 32654, loss = 0.00619398\n",
      "Iteration 32655, loss = 0.00619388\n",
      "Iteration 32656, loss = 0.00619377\n",
      "Iteration 32657, loss = 0.00619366\n",
      "Iteration 32658, loss = 0.00619355\n",
      "Iteration 32659, loss = 0.00619344\n",
      "Iteration 32660, loss = 0.00619334\n",
      "Iteration 32661, loss = 0.00619323\n",
      "Iteration 32662, loss = 0.00619312\n",
      "Iteration 32663, loss = 0.00619301\n",
      "Iteration 32664, loss = 0.00619290\n",
      "Iteration 32665, loss = 0.00619280\n",
      "Iteration 32666, loss = 0.00619269\n",
      "Iteration 32667, loss = 0.00619258\n",
      "Iteration 32668, loss = 0.00619247\n",
      "Iteration 32669, loss = 0.00619236\n",
      "Iteration 32670, loss = 0.00619226\n",
      "Iteration 32671, loss = 0.00619215\n",
      "Iteration 32672, loss = 0.00619204\n",
      "Iteration 32673, loss = 0.00619193\n",
      "Iteration 32674, loss = 0.00619182\n",
      "Iteration 32675, loss = 0.00619172\n",
      "Iteration 32676, loss = 0.00619161\n",
      "Iteration 32677, loss = 0.00619150\n",
      "Iteration 32678, loss = 0.00619139\n",
      "Iteration 32679, loss = 0.00619128\n",
      "Iteration 32680, loss = 0.00619118\n",
      "Iteration 32681, loss = 0.00619107\n",
      "Iteration 32682, loss = 0.00619096\n",
      "Iteration 32683, loss = 0.00619085\n",
      "Iteration 32684, loss = 0.00619074\n",
      "Iteration 32685, loss = 0.00619064\n",
      "Iteration 32686, loss = 0.00619053\n",
      "Iteration 32687, loss = 0.00619042\n",
      "Iteration 32688, loss = 0.00619031\n",
      "Iteration 32689, loss = 0.00619021\n",
      "Iteration 32690, loss = 0.00619010\n",
      "Iteration 32691, loss = 0.00618999\n",
      "Iteration 32692, loss = 0.00618988\n",
      "Iteration 32693, loss = 0.00618977\n",
      "Iteration 32694, loss = 0.00618967\n",
      "Iteration 32695, loss = 0.00618956\n",
      "Iteration 32696, loss = 0.00618945\n",
      "Iteration 32697, loss = 0.00618934\n",
      "Iteration 32698, loss = 0.00618924\n",
      "Iteration 32699, loss = 0.00618913\n",
      "Iteration 32700, loss = 0.00618902\n",
      "Iteration 32701, loss = 0.00618891\n",
      "Iteration 32702, loss = 0.00618881\n",
      "Iteration 32703, loss = 0.00618870\n",
      "Iteration 32704, loss = 0.00618859\n",
      "Iteration 32705, loss = 0.00618848\n",
      "Iteration 32706, loss = 0.00618837\n",
      "Iteration 32707, loss = 0.00618827\n",
      "Iteration 32708, loss = 0.00618816\n",
      "Iteration 32709, loss = 0.00618805\n",
      "Iteration 32710, loss = 0.00618794\n",
      "Iteration 32711, loss = 0.00618784\n",
      "Iteration 32712, loss = 0.00618773\n",
      "Iteration 32713, loss = 0.00618762\n",
      "Iteration 32714, loss = 0.00618751\n",
      "Iteration 32715, loss = 0.00618741\n",
      "Iteration 32716, loss = 0.00618730\n",
      "Iteration 32717, loss = 0.00618719\n",
      "Iteration 32718, loss = 0.00618708\n",
      "Iteration 32719, loss = 0.00618698\n",
      "Iteration 32720, loss = 0.00618687\n",
      "Iteration 32721, loss = 0.00618676\n",
      "Iteration 32722, loss = 0.00618665\n",
      "Iteration 32723, loss = 0.00618655\n",
      "Iteration 32724, loss = 0.00618644\n",
      "Iteration 32725, loss = 0.00618633\n",
      "Iteration 32726, loss = 0.00618622\n",
      "Iteration 32727, loss = 0.00618612\n",
      "Iteration 32728, loss = 0.00618601\n",
      "Iteration 32729, loss = 0.00618590\n",
      "Iteration 32730, loss = 0.00618579\n",
      "Iteration 32731, loss = 0.00618569\n",
      "Iteration 32732, loss = 0.00618558\n",
      "Iteration 32733, loss = 0.00618547\n",
      "Iteration 32734, loss = 0.00618536\n",
      "Iteration 32735, loss = 0.00618526\n",
      "Iteration 32736, loss = 0.00618515\n",
      "Iteration 32737, loss = 0.00618504\n",
      "Iteration 32738, loss = 0.00618494\n",
      "Iteration 32739, loss = 0.00618483\n",
      "Iteration 32740, loss = 0.00618472\n",
      "Iteration 32741, loss = 0.00618461\n",
      "Iteration 32742, loss = 0.00618451\n",
      "Iteration 32743, loss = 0.00618440\n",
      "Iteration 32744, loss = 0.00618429\n",
      "Iteration 32745, loss = 0.00618418\n",
      "Iteration 32746, loss = 0.00618408\n",
      "Iteration 32747, loss = 0.00618397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32748, loss = 0.00618386\n",
      "Iteration 32749, loss = 0.00618375\n",
      "Iteration 32750, loss = 0.00618365\n",
      "Iteration 32751, loss = 0.00618354\n",
      "Iteration 32752, loss = 0.00618343\n",
      "Iteration 32753, loss = 0.00618333\n",
      "Iteration 32754, loss = 0.00618322\n",
      "Iteration 32755, loss = 0.00618311\n",
      "Iteration 32756, loss = 0.00618300\n",
      "Iteration 32757, loss = 0.00618290\n",
      "Iteration 32758, loss = 0.00618279\n",
      "Iteration 32759, loss = 0.00618268\n",
      "Iteration 32760, loss = 0.00618258\n",
      "Iteration 32761, loss = 0.00618247\n",
      "Iteration 32762, loss = 0.00618236\n",
      "Iteration 32763, loss = 0.00618225\n",
      "Iteration 32764, loss = 0.00618215\n",
      "Iteration 32765, loss = 0.00618204\n",
      "Iteration 32766, loss = 0.00618193\n",
      "Iteration 32767, loss = 0.00618183\n",
      "Iteration 32768, loss = 0.00618172\n",
      "Iteration 32769, loss = 0.00618161\n",
      "Iteration 32770, loss = 0.00618150\n",
      "Iteration 32771, loss = 0.00618140\n",
      "Iteration 32772, loss = 0.00618129\n",
      "Iteration 32773, loss = 0.00618118\n",
      "Iteration 32774, loss = 0.00618108\n",
      "Iteration 32775, loss = 0.00618097\n",
      "Iteration 32776, loss = 0.00618086\n",
      "Iteration 32777, loss = 0.00618076\n",
      "Iteration 32778, loss = 0.00618065\n",
      "Iteration 32779, loss = 0.00618054\n",
      "Iteration 32780, loss = 0.00618043\n",
      "Iteration 32781, loss = 0.00618033\n",
      "Iteration 32782, loss = 0.00618022\n",
      "Iteration 32783, loss = 0.00618011\n",
      "Iteration 32784, loss = 0.00618001\n",
      "Iteration 32785, loss = 0.00617990\n",
      "Iteration 32786, loss = 0.00617979\n",
      "Iteration 32787, loss = 0.00617969\n",
      "Iteration 32788, loss = 0.00617958\n",
      "Iteration 32789, loss = 0.00617947\n",
      "Iteration 32790, loss = 0.00617937\n",
      "Iteration 32791, loss = 0.00617926\n",
      "Iteration 32792, loss = 0.00617915\n",
      "Iteration 32793, loss = 0.00617904\n",
      "Iteration 32794, loss = 0.00617894\n",
      "Iteration 32795, loss = 0.00617883\n",
      "Iteration 32796, loss = 0.00617872\n",
      "Iteration 32797, loss = 0.00617862\n",
      "Iteration 32798, loss = 0.00617851\n",
      "Iteration 32799, loss = 0.00617840\n",
      "Iteration 32800, loss = 0.00617830\n",
      "Iteration 32801, loss = 0.00617819\n",
      "Iteration 32802, loss = 0.00617808\n",
      "Iteration 32803, loss = 0.00617798\n",
      "Iteration 32804, loss = 0.00617787\n",
      "Iteration 32805, loss = 0.00617776\n",
      "Iteration 32806, loss = 0.00617766\n",
      "Iteration 32807, loss = 0.00617755\n",
      "Iteration 32808, loss = 0.00617744\n",
      "Iteration 32809, loss = 0.00617734\n",
      "Iteration 32810, loss = 0.00617723\n",
      "Iteration 32811, loss = 0.00617712\n",
      "Iteration 32812, loss = 0.00617702\n",
      "Iteration 32813, loss = 0.00617691\n",
      "Iteration 32814, loss = 0.00617680\n",
      "Iteration 32815, loss = 0.00617670\n",
      "Iteration 32816, loss = 0.00617659\n",
      "Iteration 32817, loss = 0.00617648\n",
      "Iteration 32818, loss = 0.00617638\n",
      "Iteration 32819, loss = 0.00617627\n",
      "Iteration 32820, loss = 0.00617616\n",
      "Iteration 32821, loss = 0.00617606\n",
      "Iteration 32822, loss = 0.00617595\n",
      "Iteration 32823, loss = 0.00617584\n",
      "Iteration 32824, loss = 0.00617574\n",
      "Iteration 32825, loss = 0.00617563\n",
      "Iteration 32826, loss = 0.00617552\n",
      "Iteration 32827, loss = 0.00617542\n",
      "Iteration 32828, loss = 0.00617531\n",
      "Iteration 32829, loss = 0.00617520\n",
      "Iteration 32830, loss = 0.00617510\n",
      "Iteration 32831, loss = 0.00617499\n",
      "Iteration 32832, loss = 0.00617488\n",
      "Iteration 32833, loss = 0.00617478\n",
      "Iteration 32834, loss = 0.00617467\n",
      "Iteration 32835, loss = 0.00617456\n",
      "Iteration 32836, loss = 0.00617446\n",
      "Iteration 32837, loss = 0.00617435\n",
      "Iteration 32838, loss = 0.00617425\n",
      "Iteration 32839, loss = 0.00617414\n",
      "Iteration 32840, loss = 0.00617403\n",
      "Iteration 32841, loss = 0.00617393\n",
      "Iteration 32842, loss = 0.00617382\n",
      "Iteration 32843, loss = 0.00617371\n",
      "Iteration 32844, loss = 0.00617361\n",
      "Iteration 32845, loss = 0.00617350\n",
      "Iteration 32846, loss = 0.00617339\n",
      "Iteration 32847, loss = 0.00617329\n",
      "Iteration 32848, loss = 0.00617318\n",
      "Iteration 32849, loss = 0.00617308\n",
      "Iteration 32850, loss = 0.00617297\n",
      "Iteration 32851, loss = 0.00617286\n",
      "Iteration 32852, loss = 0.00617276\n",
      "Iteration 32853, loss = 0.00617265\n",
      "Iteration 32854, loss = 0.00617254\n",
      "Iteration 32855, loss = 0.00617244\n",
      "Iteration 32856, loss = 0.00617233\n",
      "Iteration 32857, loss = 0.00617222\n",
      "Iteration 32858, loss = 0.00617212\n",
      "Iteration 32859, loss = 0.00617201\n",
      "Iteration 32860, loss = 0.00617191\n",
      "Iteration 32861, loss = 0.00617180\n",
      "Iteration 32862, loss = 0.00617169\n",
      "Iteration 32863, loss = 0.00617159\n",
      "Iteration 32864, loss = 0.00617148\n",
      "Iteration 32865, loss = 0.00617137\n",
      "Iteration 32866, loss = 0.00617127\n",
      "Iteration 32867, loss = 0.00617116\n",
      "Iteration 32868, loss = 0.00617106\n",
      "Iteration 32869, loss = 0.00617095\n",
      "Iteration 32870, loss = 0.00617084\n",
      "Iteration 32871, loss = 0.00617074\n",
      "Iteration 32872, loss = 0.00617063\n",
      "Iteration 32873, loss = 0.00617053\n",
      "Iteration 32874, loss = 0.00617042\n",
      "Iteration 32875, loss = 0.00617031\n",
      "Iteration 32876, loss = 0.00617021\n",
      "Iteration 32877, loss = 0.00617010\n",
      "Iteration 32878, loss = 0.00616999\n",
      "Iteration 32879, loss = 0.00616989\n",
      "Iteration 32880, loss = 0.00616978\n",
      "Iteration 32881, loss = 0.00616968\n",
      "Iteration 32882, loss = 0.00616957\n",
      "Iteration 32883, loss = 0.00616946\n",
      "Iteration 32884, loss = 0.00616936\n",
      "Iteration 32885, loss = 0.00616925\n",
      "Iteration 32886, loss = 0.00616915\n",
      "Iteration 32887, loss = 0.00616904\n",
      "Iteration 32888, loss = 0.00616893\n",
      "Iteration 32889, loss = 0.00616883\n",
      "Iteration 32890, loss = 0.00616872\n",
      "Iteration 32891, loss = 0.00616862\n",
      "Iteration 32892, loss = 0.00616851\n",
      "Iteration 32893, loss = 0.00616840\n",
      "Iteration 32894, loss = 0.00616830\n",
      "Iteration 32895, loss = 0.00616819\n",
      "Iteration 32896, loss = 0.00616809\n",
      "Iteration 32897, loss = 0.00616798\n",
      "Iteration 32898, loss = 0.00616787\n",
      "Iteration 32899, loss = 0.00616777\n",
      "Iteration 32900, loss = 0.00616766\n",
      "Iteration 32901, loss = 0.00616756\n",
      "Iteration 32902, loss = 0.00616745\n",
      "Iteration 32903, loss = 0.00616735\n",
      "Iteration 32904, loss = 0.00616724\n",
      "Iteration 32905, loss = 0.00616713\n",
      "Iteration 32906, loss = 0.00616703\n",
      "Iteration 32907, loss = 0.00616692\n",
      "Iteration 32908, loss = 0.00616682\n",
      "Iteration 32909, loss = 0.00616671\n",
      "Iteration 32910, loss = 0.00616660\n",
      "Iteration 32911, loss = 0.00616650\n",
      "Iteration 32912, loss = 0.00616639\n",
      "Iteration 32913, loss = 0.00616629\n",
      "Iteration 32914, loss = 0.00616618\n",
      "Iteration 32915, loss = 0.00616608\n",
      "Iteration 32916, loss = 0.00616597\n",
      "Iteration 32917, loss = 0.00616586\n",
      "Iteration 32918, loss = 0.00616576\n",
      "Iteration 32919, loss = 0.00616565\n",
      "Iteration 32920, loss = 0.00616555\n",
      "Iteration 32921, loss = 0.00616544\n",
      "Iteration 32922, loss = 0.00616534\n",
      "Iteration 32923, loss = 0.00616523\n",
      "Iteration 32924, loss = 0.00616512\n",
      "Iteration 32925, loss = 0.00616502\n",
      "Iteration 32926, loss = 0.00616491\n",
      "Iteration 32927, loss = 0.00616481\n",
      "Iteration 32928, loss = 0.00616470\n",
      "Iteration 32929, loss = 0.00616460\n",
      "Iteration 32930, loss = 0.00616449\n",
      "Iteration 32931, loss = 0.00616438\n",
      "Iteration 32932, loss = 0.00616428\n",
      "Iteration 32933, loss = 0.00616417\n",
      "Iteration 32934, loss = 0.00616407\n",
      "Iteration 32935, loss = 0.00616396\n",
      "Iteration 32936, loss = 0.00616386\n",
      "Iteration 32937, loss = 0.00616375\n",
      "Iteration 32938, loss = 0.00616364\n",
      "Iteration 32939, loss = 0.00616354\n",
      "Iteration 32940, loss = 0.00616343\n",
      "Iteration 32941, loss = 0.00616333\n",
      "Iteration 32942, loss = 0.00616322\n",
      "Iteration 32943, loss = 0.00616312\n",
      "Iteration 32944, loss = 0.00616301\n",
      "Iteration 32945, loss = 0.00616291\n",
      "Iteration 32946, loss = 0.00616280\n",
      "Iteration 32947, loss = 0.00616270\n",
      "Iteration 32948, loss = 0.00616259\n",
      "Iteration 32949, loss = 0.00616248\n",
      "Iteration 32950, loss = 0.00616238\n",
      "Iteration 32951, loss = 0.00616227\n",
      "Iteration 32952, loss = 0.00616217\n",
      "Iteration 32953, loss = 0.00616206\n",
      "Iteration 32954, loss = 0.00616196\n",
      "Iteration 32955, loss = 0.00616185\n",
      "Iteration 32956, loss = 0.00616175\n",
      "Iteration 32957, loss = 0.00616164\n",
      "Iteration 32958, loss = 0.00616154\n",
      "Iteration 32959, loss = 0.00616143\n",
      "Iteration 32960, loss = 0.00616132\n",
      "Iteration 32961, loss = 0.00616122\n",
      "Iteration 32962, loss = 0.00616111\n",
      "Iteration 32963, loss = 0.00616101\n",
      "Iteration 32964, loss = 0.00616090\n",
      "Iteration 32965, loss = 0.00616080\n",
      "Iteration 32966, loss = 0.00616069\n",
      "Iteration 32967, loss = 0.00616059\n",
      "Iteration 32968, loss = 0.00616048\n",
      "Iteration 32969, loss = 0.00616038\n",
      "Iteration 32970, loss = 0.00616027\n",
      "Iteration 32971, loss = 0.00616017\n",
      "Iteration 32972, loss = 0.00616006\n",
      "Iteration 32973, loss = 0.00615996\n",
      "Iteration 32974, loss = 0.00615985\n",
      "Iteration 32975, loss = 0.00615975\n",
      "Iteration 32976, loss = 0.00615964\n",
      "Iteration 32977, loss = 0.00615953\n",
      "Iteration 32978, loss = 0.00615943\n",
      "Iteration 32979, loss = 0.00615932\n",
      "Iteration 32980, loss = 0.00615922\n",
      "Iteration 32981, loss = 0.00615911\n",
      "Iteration 32982, loss = 0.00615901\n",
      "Iteration 32983, loss = 0.00615890\n",
      "Iteration 32984, loss = 0.00615880\n",
      "Iteration 32985, loss = 0.00615869\n",
      "Iteration 32986, loss = 0.00615859\n",
      "Iteration 32987, loss = 0.00615848\n",
      "Iteration 32988, loss = 0.00615838\n",
      "Iteration 32989, loss = 0.00615827\n",
      "Iteration 32990, loss = 0.00615817\n",
      "Iteration 32991, loss = 0.00615806\n",
      "Iteration 32992, loss = 0.00615796\n",
      "Iteration 32993, loss = 0.00615785\n",
      "Iteration 32994, loss = 0.00615775\n",
      "Iteration 32995, loss = 0.00615764\n",
      "Iteration 32996, loss = 0.00615754\n",
      "Iteration 32997, loss = 0.00615743\n",
      "Iteration 32998, loss = 0.00615733\n",
      "Iteration 32999, loss = 0.00615722\n",
      "Iteration 33000, loss = 0.00615712\n",
      "Iteration 33001, loss = 0.00615701\n",
      "Iteration 33002, loss = 0.00615691\n",
      "Iteration 33003, loss = 0.00615680\n",
      "Iteration 33004, loss = 0.00615670\n",
      "Iteration 33005, loss = 0.00615659\n",
      "Iteration 33006, loss = 0.00615649\n",
      "Iteration 33007, loss = 0.00615638\n",
      "Iteration 33008, loss = 0.00615628\n",
      "Iteration 33009, loss = 0.00615617\n",
      "Iteration 33010, loss = 0.00615607\n",
      "Iteration 33011, loss = 0.00615596\n",
      "Iteration 33012, loss = 0.00615586\n",
      "Iteration 33013, loss = 0.00615575\n",
      "Iteration 33014, loss = 0.00615565\n",
      "Iteration 33015, loss = 0.00615554\n",
      "Iteration 33016, loss = 0.00615544\n",
      "Iteration 33017, loss = 0.00615533\n",
      "Iteration 33018, loss = 0.00615523\n",
      "Iteration 33019, loss = 0.00615512\n",
      "Iteration 33020, loss = 0.00615502\n",
      "Iteration 33021, loss = 0.00615491\n",
      "Iteration 33022, loss = 0.00615481\n",
      "Iteration 33023, loss = 0.00615470\n",
      "Iteration 33024, loss = 0.00615460\n",
      "Iteration 33025, loss = 0.00615449\n",
      "Iteration 33026, loss = 0.00615439\n",
      "Iteration 33027, loss = 0.00615428\n",
      "Iteration 33028, loss = 0.00615418\n",
      "Iteration 33029, loss = 0.00615408\n",
      "Iteration 33030, loss = 0.00615397\n",
      "Iteration 33031, loss = 0.00615387\n",
      "Iteration 33032, loss = 0.00615376\n",
      "Iteration 33033, loss = 0.00615366\n",
      "Iteration 33034, loss = 0.00615355\n",
      "Iteration 33035, loss = 0.00615345\n",
      "Iteration 33036, loss = 0.00615334\n",
      "Iteration 33037, loss = 0.00615324\n",
      "Iteration 33038, loss = 0.00615313\n",
      "Iteration 33039, loss = 0.00615303\n",
      "Iteration 33040, loss = 0.00615292\n",
      "Iteration 33041, loss = 0.00615282\n",
      "Iteration 33042, loss = 0.00615271\n",
      "Iteration 33043, loss = 0.00615261\n",
      "Iteration 33044, loss = 0.00615250\n",
      "Iteration 33045, loss = 0.00615240\n",
      "Iteration 33046, loss = 0.00615230\n",
      "Iteration 33047, loss = 0.00615219\n",
      "Iteration 33048, loss = 0.00615209\n",
      "Iteration 33049, loss = 0.00615198\n",
      "Iteration 33050, loss = 0.00615188\n",
      "Iteration 33051, loss = 0.00615177\n",
      "Iteration 33052, loss = 0.00615167\n",
      "Iteration 33053, loss = 0.00615156\n",
      "Iteration 33054, loss = 0.00615146\n",
      "Iteration 33055, loss = 0.00615135\n",
      "Iteration 33056, loss = 0.00615125\n",
      "Iteration 33057, loss = 0.00615115\n",
      "Iteration 33058, loss = 0.00615104\n",
      "Iteration 33059, loss = 0.00615094\n",
      "Iteration 33060, loss = 0.00615083\n",
      "Iteration 33061, loss = 0.00615073\n",
      "Iteration 33062, loss = 0.00615062\n",
      "Iteration 33063, loss = 0.00615052\n",
      "Iteration 33064, loss = 0.00615041\n",
      "Iteration 33065, loss = 0.00615031\n",
      "Iteration 33066, loss = 0.00615020\n",
      "Iteration 33067, loss = 0.00615010\n",
      "Iteration 33068, loss = 0.00615000\n",
      "Iteration 33069, loss = 0.00614989\n",
      "Iteration 33070, loss = 0.00614979\n",
      "Iteration 33071, loss = 0.00614968\n",
      "Iteration 33072, loss = 0.00614958\n",
      "Iteration 33073, loss = 0.00614947\n",
      "Iteration 33074, loss = 0.00614937\n",
      "Iteration 33075, loss = 0.00614927\n",
      "Iteration 33076, loss = 0.00614916\n",
      "Iteration 33077, loss = 0.00614906\n",
      "Iteration 33078, loss = 0.00614895\n",
      "Iteration 33079, loss = 0.00614885\n",
      "Iteration 33080, loss = 0.00614874\n",
      "Iteration 33081, loss = 0.00614864\n",
      "Iteration 33082, loss = 0.00614853\n",
      "Iteration 33083, loss = 0.00614843\n",
      "Iteration 33084, loss = 0.00614833\n",
      "Iteration 33085, loss = 0.00614822\n",
      "Iteration 33086, loss = 0.00614812\n",
      "Iteration 33087, loss = 0.00614801\n",
      "Iteration 33088, loss = 0.00614791\n",
      "Iteration 33089, loss = 0.00614780\n",
      "Iteration 33090, loss = 0.00614770\n",
      "Iteration 33091, loss = 0.00614760\n",
      "Iteration 33092, loss = 0.00614749\n",
      "Iteration 33093, loss = 0.00614739\n",
      "Iteration 33094, loss = 0.00614728\n",
      "Iteration 33095, loss = 0.00614718\n",
      "Iteration 33096, loss = 0.00614708\n",
      "Iteration 33097, loss = 0.00614697\n",
      "Iteration 33098, loss = 0.00614687\n",
      "Iteration 33099, loss = 0.00614676\n",
      "Iteration 33100, loss = 0.00614666\n",
      "Iteration 33101, loss = 0.00614655\n",
      "Iteration 33102, loss = 0.00614645\n",
      "Iteration 33103, loss = 0.00614635\n",
      "Iteration 33104, loss = 0.00614624\n",
      "Iteration 33105, loss = 0.00614614\n",
      "Iteration 33106, loss = 0.00614603\n",
      "Iteration 33107, loss = 0.00614593\n",
      "Iteration 33108, loss = 0.00614583\n",
      "Iteration 33109, loss = 0.00614572\n",
      "Iteration 33110, loss = 0.00614562\n",
      "Iteration 33111, loss = 0.00614551\n",
      "Iteration 33112, loss = 0.00614541\n",
      "Iteration 33113, loss = 0.00614531\n",
      "Iteration 33114, loss = 0.00614520\n",
      "Iteration 33115, loss = 0.00614510\n",
      "Iteration 33116, loss = 0.00614499\n",
      "Iteration 33117, loss = 0.00614489\n",
      "Iteration 33118, loss = 0.00614479\n",
      "Iteration 33119, loss = 0.00614468\n",
      "Iteration 33120, loss = 0.00614458\n",
      "Iteration 33121, loss = 0.00614447\n",
      "Iteration 33122, loss = 0.00614437\n",
      "Iteration 33123, loss = 0.00614427\n",
      "Iteration 33124, loss = 0.00614416\n",
      "Iteration 33125, loss = 0.00614406\n",
      "Iteration 33126, loss = 0.00614395\n",
      "Iteration 33127, loss = 0.00614385\n",
      "Iteration 33128, loss = 0.00614375\n",
      "Iteration 33129, loss = 0.00614364\n",
      "Iteration 33130, loss = 0.00614354\n",
      "Iteration 33131, loss = 0.00614343\n",
      "Iteration 33132, loss = 0.00614333\n",
      "Iteration 33133, loss = 0.00614323\n",
      "Iteration 33134, loss = 0.00614312\n",
      "Iteration 33135, loss = 0.00614302\n",
      "Iteration 33136, loss = 0.00614292\n",
      "Iteration 33137, loss = 0.00614281\n",
      "Iteration 33138, loss = 0.00614271\n",
      "Iteration 33139, loss = 0.00614260\n",
      "Iteration 33140, loss = 0.00614250\n",
      "Iteration 33141, loss = 0.00614240\n",
      "Iteration 33142, loss = 0.00614229\n",
      "Iteration 33143, loss = 0.00614219\n",
      "Iteration 33144, loss = 0.00614208\n",
      "Iteration 33145, loss = 0.00614198\n",
      "Iteration 33146, loss = 0.00614188\n",
      "Iteration 33147, loss = 0.00614177\n",
      "Iteration 33148, loss = 0.00614167\n",
      "Iteration 33149, loss = 0.00614157\n",
      "Iteration 33150, loss = 0.00614146\n",
      "Iteration 33151, loss = 0.00614136\n",
      "Iteration 33152, loss = 0.00614125\n",
      "Iteration 33153, loss = 0.00614115\n",
      "Iteration 33154, loss = 0.00614105\n",
      "Iteration 33155, loss = 0.00614094\n",
      "Iteration 33156, loss = 0.00614084\n",
      "Iteration 33157, loss = 0.00614074\n",
      "Iteration 33158, loss = 0.00614063\n",
      "Iteration 33159, loss = 0.00614053\n",
      "Iteration 33160, loss = 0.00614043\n",
      "Iteration 33161, loss = 0.00614032\n",
      "Iteration 33162, loss = 0.00614022\n",
      "Iteration 33163, loss = 0.00614011\n",
      "Iteration 33164, loss = 0.00614001\n",
      "Iteration 33165, loss = 0.00613991\n",
      "Iteration 33166, loss = 0.00613980\n",
      "Iteration 33167, loss = 0.00613970\n",
      "Iteration 33168, loss = 0.00613960\n",
      "Iteration 33169, loss = 0.00613949\n",
      "Iteration 33170, loss = 0.00613939\n",
      "Iteration 33171, loss = 0.00613929\n",
      "Iteration 33172, loss = 0.00613918\n",
      "Iteration 33173, loss = 0.00613908\n",
      "Iteration 33174, loss = 0.00613898\n",
      "Iteration 33175, loss = 0.00613887\n",
      "Iteration 33176, loss = 0.00613877\n",
      "Iteration 33177, loss = 0.00613867\n",
      "Iteration 33178, loss = 0.00613856\n",
      "Iteration 33179, loss = 0.00613846\n",
      "Iteration 33180, loss = 0.00613835\n",
      "Iteration 33181, loss = 0.00613825\n",
      "Iteration 33182, loss = 0.00613815\n",
      "Iteration 33183, loss = 0.00613804\n",
      "Iteration 33184, loss = 0.00613794\n",
      "Iteration 33185, loss = 0.00613784\n",
      "Iteration 33186, loss = 0.00613773\n",
      "Iteration 33187, loss = 0.00613763\n",
      "Iteration 33188, loss = 0.00613753\n",
      "Iteration 33189, loss = 0.00613742\n",
      "Iteration 33190, loss = 0.00613732\n",
      "Iteration 33191, loss = 0.00613722\n",
      "Iteration 33192, loss = 0.00613711\n",
      "Iteration 33193, loss = 0.00613701\n",
      "Iteration 33194, loss = 0.00613691\n",
      "Iteration 33195, loss = 0.00613680\n",
      "Iteration 33196, loss = 0.00613670\n",
      "Iteration 33197, loss = 0.00613660\n",
      "Iteration 33198, loss = 0.00613649\n",
      "Iteration 33199, loss = 0.00613639\n",
      "Iteration 33200, loss = 0.00613629\n",
      "Iteration 33201, loss = 0.00613618\n",
      "Iteration 33202, loss = 0.00613608\n",
      "Iteration 33203, loss = 0.00613598\n",
      "Iteration 33204, loss = 0.00613587\n",
      "Iteration 33205, loss = 0.00613577\n",
      "Iteration 33206, loss = 0.00613567\n",
      "Iteration 33207, loss = 0.00613556\n",
      "Iteration 33208, loss = 0.00613546\n",
      "Iteration 33209, loss = 0.00613536\n",
      "Iteration 33210, loss = 0.00613525\n",
      "Iteration 33211, loss = 0.00613515\n",
      "Iteration 33212, loss = 0.00613505\n",
      "Iteration 33213, loss = 0.00613495\n",
      "Iteration 33214, loss = 0.00613484\n",
      "Iteration 33215, loss = 0.00613474\n",
      "Iteration 33216, loss = 0.00613464\n",
      "Iteration 33217, loss = 0.00613453\n",
      "Iteration 33218, loss = 0.00613443\n",
      "Iteration 33219, loss = 0.00613433\n",
      "Iteration 33220, loss = 0.00613422\n",
      "Iteration 33221, loss = 0.00613412\n",
      "Iteration 33222, loss = 0.00613402\n",
      "Iteration 33223, loss = 0.00613391\n",
      "Iteration 33224, loss = 0.00613381\n",
      "Iteration 33225, loss = 0.00613371\n",
      "Iteration 33226, loss = 0.00613360\n",
      "Iteration 33227, loss = 0.00613350\n",
      "Iteration 33228, loss = 0.00613340\n",
      "Iteration 33229, loss = 0.00613330\n",
      "Iteration 33230, loss = 0.00613319\n",
      "Iteration 33231, loss = 0.00613309\n",
      "Iteration 33232, loss = 0.00613299\n",
      "Iteration 33233, loss = 0.00613288\n",
      "Iteration 33234, loss = 0.00613278\n",
      "Iteration 33235, loss = 0.00613268\n",
      "Iteration 33236, loss = 0.00613257\n",
      "Iteration 33237, loss = 0.00613247\n",
      "Iteration 33238, loss = 0.00613237\n",
      "Iteration 33239, loss = 0.00613227\n",
      "Iteration 33240, loss = 0.00613216\n",
      "Iteration 33241, loss = 0.00613206\n",
      "Iteration 33242, loss = 0.00613196\n",
      "Iteration 33243, loss = 0.00613185\n",
      "Iteration 33244, loss = 0.00613175\n",
      "Iteration 33245, loss = 0.00613165\n",
      "Iteration 33246, loss = 0.00613155\n",
      "Iteration 33247, loss = 0.00613144\n",
      "Iteration 33248, loss = 0.00613134\n",
      "Iteration 33249, loss = 0.00613124\n",
      "Iteration 33250, loss = 0.00613113\n",
      "Iteration 33251, loss = 0.00613103\n",
      "Iteration 33252, loss = 0.00613093\n",
      "Iteration 33253, loss = 0.00613082\n",
      "Iteration 33254, loss = 0.00613072\n",
      "Iteration 33255, loss = 0.00613062\n",
      "Iteration 33256, loss = 0.00613052\n",
      "Iteration 33257, loss = 0.00613041\n",
      "Iteration 33258, loss = 0.00613031\n",
      "Iteration 33259, loss = 0.00613021\n",
      "Iteration 33260, loss = 0.00613011\n",
      "Iteration 33261, loss = 0.00613000\n",
      "Iteration 33262, loss = 0.00612990\n",
      "Iteration 33263, loss = 0.00612980\n",
      "Iteration 33264, loss = 0.00612969\n",
      "Iteration 33265, loss = 0.00612959\n",
      "Iteration 33266, loss = 0.00612949\n",
      "Iteration 33267, loss = 0.00612939\n",
      "Iteration 33268, loss = 0.00612928\n",
      "Iteration 33269, loss = 0.00612918\n",
      "Iteration 33270, loss = 0.00612908\n",
      "Iteration 33271, loss = 0.00612898\n",
      "Iteration 33272, loss = 0.00612887\n",
      "Iteration 33273, loss = 0.00612877\n",
      "Iteration 33274, loss = 0.00612867\n",
      "Iteration 33275, loss = 0.00612856\n",
      "Iteration 33276, loss = 0.00612846\n",
      "Iteration 33277, loss = 0.00612836\n",
      "Iteration 33278, loss = 0.00612826\n",
      "Iteration 33279, loss = 0.00612815\n",
      "Iteration 33280, loss = 0.00612805\n",
      "Iteration 33281, loss = 0.00612795\n",
      "Iteration 33282, loss = 0.00612785\n",
      "Iteration 33283, loss = 0.00612774\n",
      "Iteration 33284, loss = 0.00612764\n",
      "Iteration 33285, loss = 0.00612754\n",
      "Iteration 33286, loss = 0.00612744\n",
      "Iteration 33287, loss = 0.00612733\n",
      "Iteration 33288, loss = 0.00612723\n",
      "Iteration 33289, loss = 0.00612713\n",
      "Iteration 33290, loss = 0.00612703\n",
      "Iteration 33291, loss = 0.00612692\n",
      "Iteration 33292, loss = 0.00612682\n",
      "Iteration 33293, loss = 0.00612672\n",
      "Iteration 33294, loss = 0.00612662\n",
      "Iteration 33295, loss = 0.00612651\n",
      "Iteration 33296, loss = 0.00612641\n",
      "Iteration 33297, loss = 0.00612631\n",
      "Iteration 33298, loss = 0.00612621\n",
      "Iteration 33299, loss = 0.00612610\n",
      "Iteration 33300, loss = 0.00612600\n",
      "Iteration 33301, loss = 0.00612590\n",
      "Iteration 33302, loss = 0.00612580\n",
      "Iteration 33303, loss = 0.00612569\n",
      "Iteration 33304, loss = 0.00612559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33305, loss = 0.00612549\n",
      "Iteration 33306, loss = 0.00612539\n",
      "Iteration 33307, loss = 0.00612528\n",
      "Iteration 33308, loss = 0.00612518\n",
      "Iteration 33309, loss = 0.00612508\n",
      "Iteration 33310, loss = 0.00612498\n",
      "Iteration 33311, loss = 0.00612487\n",
      "Iteration 33312, loss = 0.00612477\n",
      "Iteration 33313, loss = 0.00612467\n",
      "Iteration 33314, loss = 0.00612457\n",
      "Iteration 33315, loss = 0.00612447\n",
      "Iteration 33316, loss = 0.00612436\n",
      "Iteration 33317, loss = 0.00612426\n",
      "Iteration 33318, loss = 0.00612416\n",
      "Iteration 33319, loss = 0.00612406\n",
      "Iteration 33320, loss = 0.00612395\n",
      "Iteration 33321, loss = 0.00612385\n",
      "Iteration 33322, loss = 0.00612375\n",
      "Iteration 33323, loss = 0.00612365\n",
      "Iteration 33324, loss = 0.00612355\n",
      "Iteration 33325, loss = 0.00612344\n",
      "Iteration 33326, loss = 0.00612334\n",
      "Iteration 33327, loss = 0.00612324\n",
      "Iteration 33328, loss = 0.00612314\n",
      "Iteration 33329, loss = 0.00612303\n",
      "Iteration 33330, loss = 0.00612293\n",
      "Iteration 33331, loss = 0.00612283\n",
      "Iteration 33332, loss = 0.00612273\n",
      "Iteration 33333, loss = 0.00612263\n",
      "Iteration 33334, loss = 0.00612252\n",
      "Iteration 33335, loss = 0.00612242\n",
      "Iteration 33336, loss = 0.00612232\n",
      "Iteration 33337, loss = 0.00612222\n",
      "Iteration 33338, loss = 0.00612211\n",
      "Iteration 33339, loss = 0.00612201\n",
      "Iteration 33340, loss = 0.00612191\n",
      "Iteration 33341, loss = 0.00612181\n",
      "Iteration 33342, loss = 0.00612171\n",
      "Iteration 33343, loss = 0.00612160\n",
      "Iteration 33344, loss = 0.00612150\n",
      "Iteration 33345, loss = 0.00612140\n",
      "Iteration 33346, loss = 0.00612130\n",
      "Iteration 33347, loss = 0.00612120\n",
      "Iteration 33348, loss = 0.00612109\n",
      "Iteration 33349, loss = 0.00612099\n",
      "Iteration 33350, loss = 0.00612089\n",
      "Iteration 33351, loss = 0.00612079\n",
      "Iteration 33352, loss = 0.00612069\n",
      "Iteration 33353, loss = 0.00612058\n",
      "Iteration 33354, loss = 0.00612048\n",
      "Iteration 33355, loss = 0.00612038\n",
      "Iteration 33356, loss = 0.00612028\n",
      "Iteration 33357, loss = 0.00612018\n",
      "Iteration 33358, loss = 0.00612007\n",
      "Iteration 33359, loss = 0.00611997\n",
      "Iteration 33360, loss = 0.00611987\n",
      "Iteration 33361, loss = 0.00611977\n",
      "Iteration 33362, loss = 0.00611967\n",
      "Iteration 33363, loss = 0.00611956\n",
      "Iteration 33364, loss = 0.00611946\n",
      "Iteration 33365, loss = 0.00611936\n",
      "Iteration 33366, loss = 0.00611926\n",
      "Iteration 33367, loss = 0.00611916\n",
      "Iteration 33368, loss = 0.00611906\n",
      "Iteration 33369, loss = 0.00611895\n",
      "Iteration 33370, loss = 0.00611885\n",
      "Iteration 33371, loss = 0.00611875\n",
      "Iteration 33372, loss = 0.00611865\n",
      "Iteration 33373, loss = 0.00611855\n",
      "Iteration 33374, loss = 0.00611844\n",
      "Iteration 33375, loss = 0.00611834\n",
      "Iteration 33376, loss = 0.00611824\n",
      "Iteration 33377, loss = 0.00611814\n",
      "Iteration 33378, loss = 0.00611804\n",
      "Iteration 33379, loss = 0.00611794\n",
      "Iteration 33380, loss = 0.00611783\n",
      "Iteration 33381, loss = 0.00611773\n",
      "Iteration 33382, loss = 0.00611763\n",
      "Iteration 33383, loss = 0.00611753\n",
      "Iteration 33384, loss = 0.00611743\n",
      "Iteration 33385, loss = 0.00611732\n",
      "Iteration 33386, loss = 0.00611722\n",
      "Iteration 33387, loss = 0.00611712\n",
      "Iteration 33388, loss = 0.00611702\n",
      "Iteration 33389, loss = 0.00611692\n",
      "Iteration 33390, loss = 0.00611682\n",
      "Iteration 33391, loss = 0.00611671\n",
      "Iteration 33392, loss = 0.00611661\n",
      "Iteration 33393, loss = 0.00611651\n",
      "Iteration 33394, loss = 0.00611641\n",
      "Iteration 33395, loss = 0.00611631\n",
      "Iteration 33396, loss = 0.00611621\n",
      "Iteration 33397, loss = 0.00611610\n",
      "Iteration 33398, loss = 0.00611600\n",
      "Iteration 33399, loss = 0.00611590\n",
      "Iteration 33400, loss = 0.00611580\n",
      "Iteration 33401, loss = 0.00611570\n",
      "Iteration 33402, loss = 0.00611560\n",
      "Iteration 33403, loss = 0.00611549\n",
      "Iteration 33404, loss = 0.00611539\n",
      "Iteration 33405, loss = 0.00611529\n",
      "Iteration 33406, loss = 0.00611519\n",
      "Iteration 33407, loss = 0.00611509\n",
      "Iteration 33408, loss = 0.00611499\n",
      "Iteration 33409, loss = 0.00611489\n",
      "Iteration 33410, loss = 0.00611478\n",
      "Iteration 33411, loss = 0.00611468\n",
      "Iteration 33412, loss = 0.00611458\n",
      "Iteration 33413, loss = 0.00611448\n",
      "Iteration 33414, loss = 0.00611438\n",
      "Iteration 33415, loss = 0.00611428\n",
      "Iteration 33416, loss = 0.00611418\n",
      "Iteration 33417, loss = 0.00611407\n",
      "Iteration 33418, loss = 0.00611397\n",
      "Iteration 33419, loss = 0.00611387\n",
      "Iteration 33420, loss = 0.00611377\n",
      "Iteration 33421, loss = 0.00611367\n",
      "Iteration 33422, loss = 0.00611357\n",
      "Iteration 33423, loss = 0.00611347\n",
      "Iteration 33424, loss = 0.00611336\n",
      "Iteration 33425, loss = 0.00611326\n",
      "Iteration 33426, loss = 0.00611316\n",
      "Iteration 33427, loss = 0.00611306\n",
      "Iteration 33428, loss = 0.00611296\n",
      "Iteration 33429, loss = 0.00611286\n",
      "Iteration 33430, loss = 0.00611276\n",
      "Iteration 33431, loss = 0.00611265\n",
      "Iteration 33432, loss = 0.00611255\n",
      "Iteration 33433, loss = 0.00611245\n",
      "Iteration 33434, loss = 0.00611235\n",
      "Iteration 33435, loss = 0.00611225\n",
      "Iteration 33436, loss = 0.00611215\n",
      "Iteration 33437, loss = 0.00611205\n",
      "Iteration 33438, loss = 0.00611195\n",
      "Iteration 33439, loss = 0.00611184\n",
      "Iteration 33440, loss = 0.00611174\n",
      "Iteration 33441, loss = 0.00611164\n",
      "Iteration 33442, loss = 0.00611154\n",
      "Iteration 33443, loss = 0.00611144\n",
      "Iteration 33444, loss = 0.00611134\n",
      "Iteration 33445, loss = 0.00611124\n",
      "Iteration 33446, loss = 0.00611114\n",
      "Iteration 33447, loss = 0.00611103\n",
      "Iteration 33448, loss = 0.00611093\n",
      "Iteration 33449, loss = 0.00611083\n",
      "Iteration 33450, loss = 0.00611073\n",
      "Iteration 33451, loss = 0.00611063\n",
      "Iteration 33452, loss = 0.00611053\n",
      "Iteration 33453, loss = 0.00611043\n",
      "Iteration 33454, loss = 0.00611033\n",
      "Iteration 33455, loss = 0.00611022\n",
      "Iteration 33456, loss = 0.00611012\n",
      "Iteration 33457, loss = 0.00611002\n",
      "Iteration 33458, loss = 0.00610992\n",
      "Iteration 33459, loss = 0.00610982\n",
      "Iteration 33460, loss = 0.00610972\n",
      "Iteration 33461, loss = 0.00610962\n",
      "Iteration 33462, loss = 0.00610952\n",
      "Iteration 33463, loss = 0.00610942\n",
      "Iteration 33464, loss = 0.00610931\n",
      "Iteration 33465, loss = 0.00610921\n",
      "Iteration 33466, loss = 0.00610911\n",
      "Iteration 33467, loss = 0.00610901\n",
      "Iteration 33468, loss = 0.00610891\n",
      "Iteration 33469, loss = 0.00610881\n",
      "Iteration 33470, loss = 0.00610871\n",
      "Iteration 33471, loss = 0.00610861\n",
      "Iteration 33472, loss = 0.00610851\n",
      "Iteration 33473, loss = 0.00610841\n",
      "Iteration 33474, loss = 0.00610830\n",
      "Iteration 33475, loss = 0.00610820\n",
      "Iteration 33476, loss = 0.00610810\n",
      "Iteration 33477, loss = 0.00610800\n",
      "Iteration 33478, loss = 0.00610790\n",
      "Iteration 33479, loss = 0.00610780\n",
      "Iteration 33480, loss = 0.00610770\n",
      "Iteration 33481, loss = 0.00610760\n",
      "Iteration 33482, loss = 0.00610750\n",
      "Iteration 33483, loss = 0.00610740\n",
      "Iteration 33484, loss = 0.00610730\n",
      "Iteration 33485, loss = 0.00610719\n",
      "Iteration 33486, loss = 0.00610709\n",
      "Iteration 33487, loss = 0.00610699\n",
      "Iteration 33488, loss = 0.00610689\n",
      "Iteration 33489, loss = 0.00610679\n",
      "Iteration 33490, loss = 0.00610669\n",
      "Iteration 33491, loss = 0.00610659\n",
      "Iteration 33492, loss = 0.00610649\n",
      "Iteration 33493, loss = 0.00610639\n",
      "Iteration 33494, loss = 0.00610629\n",
      "Iteration 33495, loss = 0.00610619\n",
      "Iteration 33496, loss = 0.00610609\n",
      "Iteration 33497, loss = 0.00610598\n",
      "Iteration 33498, loss = 0.00610588\n",
      "Iteration 33499, loss = 0.00610578\n",
      "Iteration 33500, loss = 0.00610568\n",
      "Iteration 33501, loss = 0.00610558\n",
      "Iteration 33502, loss = 0.00610548\n",
      "Iteration 33503, loss = 0.00610538\n",
      "Iteration 33504, loss = 0.00610528\n",
      "Iteration 33505, loss = 0.00610518\n",
      "Iteration 33506, loss = 0.00610508\n",
      "Iteration 33507, loss = 0.00610498\n",
      "Iteration 33508, loss = 0.00610488\n",
      "Iteration 33509, loss = 0.00610478\n",
      "Iteration 33510, loss = 0.00610467\n",
      "Iteration 33511, loss = 0.00610457\n",
      "Iteration 33512, loss = 0.00610447\n",
      "Iteration 33513, loss = 0.00610437\n",
      "Iteration 33514, loss = 0.00610427\n",
      "Iteration 33515, loss = 0.00610417\n",
      "Iteration 33516, loss = 0.00610407\n",
      "Iteration 33517, loss = 0.00610397\n",
      "Iteration 33518, loss = 0.00610387\n",
      "Iteration 33519, loss = 0.00610377\n",
      "Iteration 33520, loss = 0.00610367\n",
      "Iteration 33521, loss = 0.00610357\n",
      "Iteration 33522, loss = 0.00610347\n",
      "Iteration 33523, loss = 0.00610337\n",
      "Iteration 33524, loss = 0.00610327\n",
      "Iteration 33525, loss = 0.00610317\n",
      "Iteration 33526, loss = 0.00610307\n",
      "Iteration 33527, loss = 0.00610296\n",
      "Iteration 33528, loss = 0.00610286\n",
      "Iteration 33529, loss = 0.00610276\n",
      "Iteration 33530, loss = 0.00610266\n",
      "Iteration 33531, loss = 0.00610256\n",
      "Iteration 33532, loss = 0.00610246\n",
      "Iteration 33533, loss = 0.00610236\n",
      "Iteration 33534, loss = 0.00610226\n",
      "Iteration 33535, loss = 0.00610216\n",
      "Iteration 33536, loss = 0.00610206\n",
      "Iteration 33537, loss = 0.00610196\n",
      "Iteration 33538, loss = 0.00610186\n",
      "Iteration 33539, loss = 0.00610176\n",
      "Iteration 33540, loss = 0.00610166\n",
      "Iteration 33541, loss = 0.00610156\n",
      "Iteration 33542, loss = 0.00610146\n",
      "Iteration 33543, loss = 0.00610136\n",
      "Iteration 33544, loss = 0.00610126\n",
      "Iteration 33545, loss = 0.00610116\n",
      "Iteration 33546, loss = 0.00610106\n",
      "Iteration 33547, loss = 0.00610096\n",
      "Iteration 33548, loss = 0.00610085\n",
      "Iteration 33549, loss = 0.00610075\n",
      "Iteration 33550, loss = 0.00610065\n",
      "Iteration 33551, loss = 0.00610055\n",
      "Iteration 33552, loss = 0.00610045\n",
      "Iteration 33553, loss = 0.00610035\n",
      "Iteration 33554, loss = 0.00610025\n",
      "Iteration 33555, loss = 0.00610015\n",
      "Iteration 33556, loss = 0.00610005\n",
      "Iteration 33557, loss = 0.00609995\n",
      "Iteration 33558, loss = 0.00609985\n",
      "Iteration 33559, loss = 0.00609975\n",
      "Iteration 33560, loss = 0.00609965\n",
      "Iteration 33561, loss = 0.00609955\n",
      "Iteration 33562, loss = 0.00609945\n",
      "Iteration 33563, loss = 0.00609935\n",
      "Iteration 33564, loss = 0.00609925\n",
      "Iteration 33565, loss = 0.00609915\n",
      "Iteration 33566, loss = 0.00609905\n",
      "Iteration 33567, loss = 0.00609895\n",
      "Iteration 33568, loss = 0.00609885\n",
      "Iteration 33569, loss = 0.00609875\n",
      "Iteration 33570, loss = 0.00609865\n",
      "Iteration 33571, loss = 0.00609855\n",
      "Iteration 33572, loss = 0.00609845\n",
      "Iteration 33573, loss = 0.00609835\n",
      "Iteration 33574, loss = 0.00609825\n",
      "Iteration 33575, loss = 0.00609815\n",
      "Iteration 33576, loss = 0.00609805\n",
      "Iteration 33577, loss = 0.00609795\n",
      "Iteration 33578, loss = 0.00609785\n",
      "Iteration 33579, loss = 0.00609775\n",
      "Iteration 33580, loss = 0.00609765\n",
      "Iteration 33581, loss = 0.00609755\n",
      "Iteration 33582, loss = 0.00609745\n",
      "Iteration 33583, loss = 0.00609735\n",
      "Iteration 33584, loss = 0.00609725\n",
      "Iteration 33585, loss = 0.00609715\n",
      "Iteration 33586, loss = 0.00609705\n",
      "Iteration 33587, loss = 0.00609695\n",
      "Iteration 33588, loss = 0.00609685\n",
      "Iteration 33589, loss = 0.00609675\n",
      "Iteration 33590, loss = 0.00609665\n",
      "Iteration 33591, loss = 0.00609655\n",
      "Iteration 33592, loss = 0.00609645\n",
      "Iteration 33593, loss = 0.00609635\n",
      "Iteration 33594, loss = 0.00609625\n",
      "Iteration 33595, loss = 0.00609615\n",
      "Iteration 33596, loss = 0.00609605\n",
      "Iteration 33597, loss = 0.00609595\n",
      "Iteration 33598, loss = 0.00609585\n",
      "Iteration 33599, loss = 0.00609575\n",
      "Iteration 33600, loss = 0.00609565\n",
      "Iteration 33601, loss = 0.00609555\n",
      "Iteration 33602, loss = 0.00609545\n",
      "Iteration 33603, loss = 0.00609535\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=1)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(5,),activation='logistic', solver='sgd', alpha=0.0001,tol=0.0000001,\n",
    "                    learning_rate='constant', learning_rate_init=0.01,verbose=True,\n",
    "                    random_state=1, max_iter=500000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -0.97720073,   7.38223558,  -5.42587177,   4.62925756,\n",
      "         -0.32552534],\n",
      "       [  1.51743417, -10.43266202,   7.63933553,  -6.79530903,\n",
      "          0.71467124]]), array([[  2.27084972],\n",
      "       [-12.86201445],\n",
      "       [ 10.02763603],\n",
      "       [ -7.92693412],\n",
      "       [  1.09303577]])]\n",
      "[array([ 0.21798924, -0.5363608 ,  0.45682523, -0.06069621, -0.2803301 ]), array([2.87302814])]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coefs_)\n",
    "print(clf.intercepts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.95383632e-06 9.99990046e-01]\n",
      " [3.73464994e-04 9.99626535e-01]\n",
      " [9.43408069e-01 5.65919308e-02]]\n",
      "[1 1 0]\n",
      "训练集准确率:1.000000\n",
      "测试集准确率:1.000000\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict_proba(X_test))\n",
    "print(clf.predict(X_test))\n",
    "print(\"训练集准确率:%f\" %clf.score(X_train, y_train))\n",
    "print(\"测试集准确率:%f\" %clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_decision_boundary(pred_func):\n",
    "    x_min, x_max = min([x[0] for x in X]) - .1, max([x[0] for x in X]) + .1 \n",
    "    y_min, y_max = min([x[1] for x in X]) - .1, max([x[1] for x in X]) + .1 \n",
    "    h = 0.001\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) \n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()]) \n",
    "    Z = Z.reshape(xx.shape) \n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter([x[0] for x in X], [x[1] for x in X], c=y) \n",
    "    \n",
    "plot_decision_boundary(lambda x: clf.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.85276127\n",
      "Iteration 2, loss = 0.85095075\n",
      "Iteration 3, loss = 0.84839386\n",
      "Iteration 4, loss = 0.84519093\n",
      "Iteration 5, loss = 0.84143679\n",
      "Iteration 6, loss = 0.83722014\n",
      "Iteration 7, loss = 0.83262313\n",
      "Iteration 8, loss = 0.82772126\n",
      "Iteration 9, loss = 0.82258331\n",
      "Iteration 10, loss = 0.81727148\n",
      "Iteration 11, loss = 0.81184156\n",
      "Iteration 12, loss = 0.80634326\n",
      "Iteration 13, loss = 0.80082044\n",
      "Iteration 14, loss = 0.79531153\n",
      "Iteration 15, loss = 0.78984989\n",
      "Iteration 16, loss = 0.78446415\n",
      "Iteration 17, loss = 0.77917862\n",
      "Iteration 18, loss = 0.77401367\n",
      "Iteration 19, loss = 0.76898607\n",
      "Iteration 20, loss = 0.76410934\n",
      "Iteration 21, loss = 0.75939412\n",
      "Iteration 22, loss = 0.75484844\n",
      "Iteration 23, loss = 0.75047804\n",
      "Iteration 24, loss = 0.74628664\n",
      "Iteration 25, loss = 0.74227617\n",
      "Iteration 26, loss = 0.73844704\n",
      "Iteration 27, loss = 0.73479836\n",
      "Iteration 28, loss = 0.73132809\n",
      "Iteration 29, loss = 0.72803323\n",
      "Iteration 30, loss = 0.72491003\n",
      "Iteration 31, loss = 0.72195405\n",
      "Iteration 32, loss = 0.71916037\n",
      "Iteration 33, loss = 0.71652362\n",
      "Iteration 34, loss = 0.71403816\n",
      "Iteration 35, loss = 0.71169811\n",
      "Iteration 36, loss = 0.70949746\n",
      "Iteration 37, loss = 0.70743010\n",
      "Iteration 38, loss = 0.70548994\n",
      "Iteration 39, loss = 0.70367087\n",
      "Iteration 40, loss = 0.70196688\n",
      "Iteration 41, loss = 0.70037206\n",
      "Iteration 42, loss = 0.69888062\n",
      "Iteration 43, loss = 0.69748693\n",
      "Iteration 44, loss = 0.69618553\n",
      "Iteration 45, loss = 0.69497115\n",
      "Iteration 46, loss = 0.69383872\n",
      "Iteration 47, loss = 0.69278335\n",
      "Iteration 48, loss = 0.69180039\n",
      "Iteration 49, loss = 0.69088538\n",
      "Iteration 50, loss = 0.69003408\n",
      "Iteration 51, loss = 0.68924245\n",
      "Iteration 52, loss = 0.68850666\n",
      "Iteration 53, loss = 0.68782308\n",
      "Iteration 54, loss = 0.68718828\n",
      "Iteration 55, loss = 0.68659902\n",
      "Iteration 56, loss = 0.68605225\n",
      "Iteration 57, loss = 0.68554509\n",
      "Iteration 58, loss = 0.68507483\n",
      "Iteration 59, loss = 0.68463893\n",
      "Iteration 60, loss = 0.68423500\n",
      "Iteration 61, loss = 0.68386082\n",
      "Iteration 62, loss = 0.68351429\n",
      "Iteration 63, loss = 0.68319344\n",
      "Iteration 64, loss = 0.68289645\n",
      "Iteration 65, loss = 0.68262161\n",
      "Iteration 66, loss = 0.68236731\n",
      "Iteration 67, loss = 0.68213208\n",
      "Iteration 68, loss = 0.68191451\n",
      "Iteration 69, loss = 0.68171333\n",
      "Iteration 70, loss = 0.68152733\n",
      "Iteration 71, loss = 0.68135538\n",
      "Iteration 72, loss = 0.68119644\n",
      "Iteration 73, loss = 0.68104955\n",
      "Iteration 74, loss = 0.68091381\n",
      "Iteration 75, loss = 0.68078839\n",
      "Iteration 76, loss = 0.68067250\n",
      "Iteration 77, loss = 0.68056544\n",
      "Iteration 78, loss = 0.68046653\n",
      "Iteration 79, loss = 0.68037516\n",
      "Iteration 80, loss = 0.68029075\n",
      "Iteration 81, loss = 0.68021278\n",
      "Iteration 82, loss = 0.68014075\n",
      "Iteration 83, loss = 0.68007421\n",
      "Iteration 84, loss = 0.68001273\n",
      "Iteration 85, loss = 0.67995594\n",
      "Iteration 86, loss = 0.67990347\n",
      "Iteration 87, loss = 0.67985498\n",
      "Iteration 88, loss = 0.67981017\n",
      "Iteration 89, loss = 0.67976875\n",
      "Iteration 90, loss = 0.67973047\n",
      "Iteration 91, loss = 0.67969507\n",
      "Iteration 92, loss = 0.67966234\n",
      "Iteration 93, loss = 0.67963206\n",
      "Iteration 94, loss = 0.67960405\n",
      "Iteration 95, loss = 0.67957813\n",
      "Iteration 96, loss = 0.67955413\n",
      "Iteration 97, loss = 0.67953191\n",
      "Iteration 98, loss = 0.67951132\n",
      "Iteration 99, loss = 0.67949225\n",
      "Iteration 100, loss = 0.67947456\n",
      "Iteration 101, loss = 0.67945815\n",
      "Iteration 102, loss = 0.67944292\n",
      "Iteration 103, loss = 0.67942878\n",
      "Iteration 104, loss = 0.67941565\n",
      "Iteration 105, loss = 0.67940343\n",
      "Iteration 106, loss = 0.67939207\n",
      "Iteration 107, loss = 0.67938148\n",
      "Iteration 108, loss = 0.67937162\n",
      "Iteration 109, loss = 0.67936243\n",
      "Iteration 110, loss = 0.67935385\n",
      "Iteration 111, loss = 0.67934583\n",
      "Iteration 112, loss = 0.67933833\n",
      "Iteration 113, loss = 0.67933130\n",
      "Iteration 114, loss = 0.67932472\n",
      "Iteration 115, loss = 0.67931855\n",
      "Iteration 116, loss = 0.67931274\n",
      "Iteration 117, loss = 0.67930728\n",
      "Iteration 118, loss = 0.67930214\n",
      "Iteration 119, loss = 0.67929729\n",
      "Iteration 120, loss = 0.67929271\n",
      "Iteration 121, loss = 0.67928837\n",
      "Iteration 122, loss = 0.67928426\n",
      "Iteration 123, loss = 0.67928036\n",
      "Iteration 124, loss = 0.67927665\n",
      "Iteration 125, loss = 0.67927313\n",
      "Iteration 126, loss = 0.67926976\n",
      "Iteration 127, loss = 0.67926654\n",
      "Iteration 128, loss = 0.67926347\n",
      "Iteration 129, loss = 0.67926052\n",
      "Iteration 130, loss = 0.67925769\n",
      "Iteration 131, loss = 0.67925496\n",
      "Iteration 132, loss = 0.67925234\n",
      "Iteration 133, loss = 0.67924980\n",
      "Iteration 134, loss = 0.67924735\n",
      "Iteration 135, loss = 0.67924498\n",
      "Iteration 136, loss = 0.67924268\n",
      "Iteration 137, loss = 0.67924045\n",
      "Iteration 138, loss = 0.67923828\n",
      "Iteration 139, loss = 0.67923616\n",
      "Iteration 140, loss = 0.67923409\n",
      "Iteration 141, loss = 0.67923207\n",
      "Iteration 142, loss = 0.67923009\n",
      "Iteration 143, loss = 0.67922816\n",
      "Iteration 144, loss = 0.67922626\n",
      "Iteration 145, loss = 0.67922439\n",
      "Iteration 146, loss = 0.67922256\n",
      "Iteration 147, loss = 0.67922075\n",
      "Iteration 148, loss = 0.67921897\n",
      "Iteration 149, loss = 0.67921722\n",
      "Iteration 150, loss = 0.67921549\n",
      "Iteration 151, loss = 0.67921377\n",
      "Iteration 152, loss = 0.67921208\n",
      "Iteration 153, loss = 0.67921040\n",
      "Iteration 154, loss = 0.67920874\n",
      "Iteration 155, loss = 0.67920710\n",
      "Iteration 156, loss = 0.67920547\n",
      "Iteration 157, loss = 0.67920385\n",
      "Iteration 158, loss = 0.67920224\n",
      "Iteration 159, loss = 0.67920065\n",
      "Iteration 160, loss = 0.67919906\n",
      "Iteration 161, loss = 0.67919748\n",
      "Iteration 162, loss = 0.67919591\n",
      "Iteration 163, loss = 0.67919435\n",
      "Iteration 164, loss = 0.67919279\n",
      "Iteration 165, loss = 0.67919125\n",
      "Iteration 166, loss = 0.67918970\n",
      "Iteration 167, loss = 0.67918817\n",
      "Iteration 168, loss = 0.67918663\n",
      "Iteration 169, loss = 0.67918511\n",
      "Iteration 170, loss = 0.67918358\n",
      "Iteration 171, loss = 0.67918206\n",
      "Iteration 172, loss = 0.67918055\n",
      "Iteration 173, loss = 0.67917903\n",
      "Iteration 174, loss = 0.67917752\n",
      "Iteration 175, loss = 0.67917602\n",
      "Iteration 176, loss = 0.67917451\n",
      "Iteration 177, loss = 0.67917301\n",
      "Iteration 178, loss = 0.67917151\n",
      "Iteration 179, loss = 0.67917001\n",
      "Iteration 180, loss = 0.67916852\n",
      "Iteration 181, loss = 0.67916702\n",
      "Iteration 182, loss = 0.67916553\n",
      "Iteration 183, loss = 0.67916404\n",
      "Iteration 184, loss = 0.67916255\n",
      "Iteration 185, loss = 0.67916106\n",
      "Iteration 186, loss = 0.67915957\n",
      "Iteration 187, loss = 0.67915808\n",
      "Iteration 188, loss = 0.67915659\n",
      "Iteration 189, loss = 0.67915511\n",
      "Iteration 190, loss = 0.67915362\n",
      "Iteration 191, loss = 0.67915214\n",
      "Iteration 192, loss = 0.67915065\n",
      "Iteration 193, loss = 0.67914917\n",
      "Iteration 194, loss = 0.67914769\n",
      "Iteration 195, loss = 0.67914620\n",
      "Iteration 196, loss = 0.67914472\n",
      "Iteration 197, loss = 0.67914324\n",
      "Iteration 198, loss = 0.67914176\n",
      "Iteration 199, loss = 0.67914027\n",
      "Iteration 200, loss = 0.67913879\n",
      "Iteration 201, loss = 0.67913731\n",
      "Iteration 202, loss = 0.67913583\n",
      "Iteration 203, loss = 0.67913435\n",
      "Iteration 204, loss = 0.67913287\n",
      "Iteration 205, loss = 0.67913138\n",
      "Iteration 206, loss = 0.67912990\n",
      "Iteration 207, loss = 0.67912842\n",
      "Iteration 208, loss = 0.67912694\n",
      "Iteration 209, loss = 0.67912546\n",
      "Iteration 210, loss = 0.67912397\n",
      "Iteration 211, loss = 0.67912249\n",
      "Iteration 212, loss = 0.67912101\n",
      "Iteration 213, loss = 0.67911953\n",
      "Iteration 214, loss = 0.67911805\n",
      "Iteration 215, loss = 0.67911656\n",
      "Iteration 216, loss = 0.67911508\n",
      "Iteration 217, loss = 0.67911360\n",
      "Iteration 218, loss = 0.67911211\n",
      "Iteration 219, loss = 0.67911063\n",
      "Iteration 220, loss = 0.67910914\n",
      "Iteration 221, loss = 0.67910766\n",
      "Iteration 222, loss = 0.67910618\n",
      "Iteration 223, loss = 0.67910469\n",
      "Iteration 224, loss = 0.67910320\n",
      "Iteration 225, loss = 0.67910172\n",
      "Iteration 226, loss = 0.67910023\n",
      "Iteration 227, loss = 0.67909875\n",
      "Iteration 228, loss = 0.67909726\n",
      "Iteration 229, loss = 0.67909577\n",
      "Iteration 230, loss = 0.67909429\n",
      "Iteration 231, loss = 0.67909280\n",
      "Iteration 232, loss = 0.67909131\n",
      "Iteration 233, loss = 0.67908982\n",
      "Iteration 234, loss = 0.67908833\n",
      "Iteration 235, loss = 0.67908684\n",
      "Iteration 236, loss = 0.67908535\n",
      "Iteration 237, loss = 0.67908386\n",
      "Iteration 238, loss = 0.67908237\n",
      "Iteration 239, loss = 0.67908088\n",
      "Iteration 240, loss = 0.67907939\n",
      "Iteration 241, loss = 0.67907789\n",
      "Iteration 242, loss = 0.67907640\n",
      "Iteration 243, loss = 0.67907491\n",
      "Iteration 244, loss = 0.67907341\n",
      "Iteration 245, loss = 0.67907192\n",
      "Iteration 246, loss = 0.67907042\n",
      "Iteration 247, loss = 0.67906893\n",
      "Iteration 248, loss = 0.67906743\n",
      "Iteration 249, loss = 0.67906594\n",
      "Iteration 250, loss = 0.67906444\n",
      "Iteration 251, loss = 0.67906294\n",
      "Iteration 252, loss = 0.67906144\n",
      "Iteration 253, loss = 0.67905994\n",
      "Iteration 254, loss = 0.67905844\n",
      "Iteration 255, loss = 0.67905694\n",
      "Iteration 256, loss = 0.67905544\n",
      "Iteration 257, loss = 0.67905394\n",
      "Iteration 258, loss = 0.67905244\n",
      "Iteration 259, loss = 0.67905094\n",
      "Iteration 260, loss = 0.67904943\n",
      "Iteration 261, loss = 0.67904793\n",
      "Iteration 262, loss = 0.67904643\n",
      "Iteration 263, loss = 0.67904492\n",
      "Iteration 264, loss = 0.67904341\n",
      "Iteration 265, loss = 0.67904191\n",
      "Iteration 266, loss = 0.67904040\n",
      "Iteration 267, loss = 0.67903889\n",
      "Iteration 268, loss = 0.67903738\n",
      "Iteration 269, loss = 0.67903588\n",
      "Iteration 270, loss = 0.67903437\n",
      "Iteration 271, loss = 0.67903285\n",
      "Iteration 272, loss = 0.67903134\n",
      "Iteration 273, loss = 0.67902983\n",
      "Iteration 274, loss = 0.67902832\n",
      "Iteration 275, loss = 0.67902680\n",
      "Iteration 276, loss = 0.67902529\n",
      "Iteration 277, loss = 0.67902378\n",
      "Iteration 278, loss = 0.67902226\n",
      "Iteration 279, loss = 0.67902074\n",
      "Iteration 280, loss = 0.67901922\n",
      "Iteration 281, loss = 0.67901771\n",
      "Iteration 282, loss = 0.67901619\n",
      "Iteration 283, loss = 0.67901467\n",
      "Iteration 284, loss = 0.67901315\n",
      "Iteration 285, loss = 0.67901163\n",
      "Iteration 286, loss = 0.67901010\n",
      "Iteration 287, loss = 0.67900858\n",
      "Iteration 288, loss = 0.67900706\n",
      "Iteration 289, loss = 0.67900553\n",
      "Iteration 290, loss = 0.67900400\n",
      "Iteration 291, loss = 0.67900248\n",
      "Iteration 292, loss = 0.67900095\n",
      "Iteration 293, loss = 0.67899942\n",
      "Iteration 294, loss = 0.67899789\n",
      "Iteration 295, loss = 0.67899636\n",
      "Iteration 296, loss = 0.67899483\n",
      "Iteration 297, loss = 0.67899330\n",
      "Iteration 298, loss = 0.67899177\n",
      "Iteration 299, loss = 0.67899023\n",
      "Iteration 300, loss = 0.67898870\n",
      "Iteration 301, loss = 0.67898716\n",
      "Iteration 302, loss = 0.67898562\n",
      "Iteration 303, loss = 0.67898409\n",
      "Iteration 304, loss = 0.67898255\n",
      "Iteration 305, loss = 0.67898101\n",
      "Iteration 306, loss = 0.67897947\n",
      "Iteration 307, loss = 0.67897793\n",
      "Iteration 308, loss = 0.67897638\n",
      "Iteration 309, loss = 0.67897484\n",
      "Iteration 310, loss = 0.67897330\n",
      "Iteration 311, loss = 0.67897175\n",
      "Iteration 312, loss = 0.67897020\n",
      "Iteration 313, loss = 0.67896866\n",
      "Iteration 314, loss = 0.67896711\n",
      "Iteration 315, loss = 0.67896556\n",
      "Iteration 316, loss = 0.67896401\n",
      "Iteration 317, loss = 0.67896245\n",
      "Iteration 318, loss = 0.67896090\n",
      "Iteration 319, loss = 0.67895935\n",
      "Iteration 320, loss = 0.67895779\n",
      "Iteration 321, loss = 0.67895624\n",
      "Iteration 322, loss = 0.67895468\n",
      "Iteration 323, loss = 0.67895312\n",
      "Iteration 324, loss = 0.67895156\n",
      "Iteration 325, loss = 0.67895000\n",
      "Iteration 326, loss = 0.67894844\n",
      "Iteration 327, loss = 0.67894687\n",
      "Iteration 328, loss = 0.67894531\n",
      "Iteration 329, loss = 0.67894374\n",
      "Iteration 330, loss = 0.67894218\n",
      "Iteration 331, loss = 0.67894061\n",
      "Iteration 332, loss = 0.67893904\n",
      "Iteration 333, loss = 0.67893747\n",
      "Iteration 334, loss = 0.67893590\n",
      "Iteration 335, loss = 0.67893433\n",
      "Iteration 336, loss = 0.67893275\n",
      "Iteration 337, loss = 0.67893118\n",
      "Iteration 338, loss = 0.67892960\n",
      "Iteration 339, loss = 0.67892803\n",
      "Iteration 340, loss = 0.67892645\n",
      "Iteration 341, loss = 0.67892487\n",
      "Iteration 342, loss = 0.67892329\n",
      "Iteration 343, loss = 0.67892170\n",
      "Iteration 344, loss = 0.67892012\n",
      "Iteration 345, loss = 0.67891853\n",
      "Iteration 346, loss = 0.67891695\n",
      "Iteration 347, loss = 0.67891536\n",
      "Iteration 348, loss = 0.67891377\n",
      "Iteration 349, loss = 0.67891218\n",
      "Iteration 350, loss = 0.67891059\n",
      "Iteration 351, loss = 0.67890900\n",
      "Iteration 352, loss = 0.67890740\n",
      "Iteration 353, loss = 0.67890581\n",
      "Iteration 354, loss = 0.67890421\n",
      "Iteration 355, loss = 0.67890261\n",
      "Iteration 356, loss = 0.67890101\n",
      "Iteration 357, loss = 0.67889941\n",
      "Iteration 358, loss = 0.67889781\n",
      "Iteration 359, loss = 0.67889620\n",
      "Iteration 360, loss = 0.67889460\n",
      "Iteration 361, loss = 0.67889299\n",
      "Iteration 362, loss = 0.67889138\n",
      "Iteration 363, loss = 0.67888977\n",
      "Iteration 364, loss = 0.67888816\n",
      "Iteration 365, loss = 0.67888655\n",
      "Iteration 366, loss = 0.67888494\n",
      "Iteration 367, loss = 0.67888332\n",
      "Iteration 368, loss = 0.67888171\n",
      "Iteration 369, loss = 0.67888009\n",
      "Iteration 370, loss = 0.67887847\n",
      "Iteration 371, loss = 0.67887685\n",
      "Iteration 372, loss = 0.67887522\n",
      "Iteration 373, loss = 0.67887360\n",
      "Iteration 374, loss = 0.67887197\n",
      "Iteration 375, loss = 0.67887035\n",
      "Iteration 376, loss = 0.67886872\n",
      "Iteration 377, loss = 0.67886709\n",
      "Iteration 378, loss = 0.67886545\n",
      "Iteration 379, loss = 0.67886382\n",
      "Iteration 380, loss = 0.67886219\n",
      "Iteration 381, loss = 0.67886055\n",
      "Iteration 382, loss = 0.67885891\n",
      "Iteration 383, loss = 0.67885727\n",
      "Iteration 384, loss = 0.67885563\n",
      "Iteration 385, loss = 0.67885399\n",
      "Iteration 386, loss = 0.67885234\n",
      "Iteration 387, loss = 0.67885070\n",
      "Iteration 388, loss = 0.67884905\n",
      "Iteration 389, loss = 0.67884740\n",
      "Iteration 390, loss = 0.67884575\n",
      "Iteration 391, loss = 0.67884409\n",
      "Iteration 392, loss = 0.67884244\n",
      "Iteration 393, loss = 0.67884078\n",
      "Iteration 394, loss = 0.67883913\n",
      "Iteration 395, loss = 0.67883747\n",
      "Iteration 396, loss = 0.67883580\n",
      "Iteration 397, loss = 0.67883414\n",
      "Iteration 398, loss = 0.67883248\n",
      "Iteration 399, loss = 0.67883081\n",
      "Iteration 400, loss = 0.67882914\n",
      "Iteration 401, loss = 0.67882747\n",
      "Iteration 402, loss = 0.67882580\n",
      "Iteration 403, loss = 0.67882413\n",
      "Iteration 404, loss = 0.67882245\n",
      "Iteration 405, loss = 0.67882078\n",
      "Iteration 406, loss = 0.67881910\n",
      "Iteration 407, loss = 0.67881742\n",
      "Iteration 408, loss = 0.67881573\n",
      "Iteration 409, loss = 0.67881405\n",
      "Iteration 410, loss = 0.67881236\n",
      "Iteration 411, loss = 0.67881068\n",
      "Iteration 412, loss = 0.67880899\n",
      "Iteration 413, loss = 0.67880729\n",
      "Iteration 414, loss = 0.67880560\n",
      "Iteration 415, loss = 0.67880391\n",
      "Iteration 416, loss = 0.67880221\n",
      "Iteration 417, loss = 0.67880051\n",
      "Iteration 418, loss = 0.67879881\n",
      "Iteration 419, loss = 0.67879711\n",
      "Iteration 420, loss = 0.67879540\n",
      "Iteration 421, loss = 0.67879370\n",
      "Iteration 422, loss = 0.67879199\n",
      "Iteration 423, loss = 0.67879028\n",
      "Iteration 424, loss = 0.67878856\n",
      "Iteration 425, loss = 0.67878685\n",
      "Iteration 426, loss = 0.67878513\n",
      "Iteration 427, loss = 0.67878342\n",
      "Iteration 428, loss = 0.67878170\n",
      "Iteration 429, loss = 0.67877997\n",
      "Iteration 430, loss = 0.67877825\n",
      "Iteration 431, loss = 0.67877652\n",
      "Iteration 432, loss = 0.67877480\n",
      "Iteration 433, loss = 0.67877306\n",
      "Iteration 434, loss = 0.67877133\n",
      "Iteration 435, loss = 0.67876960\n",
      "Iteration 436, loss = 0.67876786\n",
      "Iteration 437, loss = 0.67876612\n",
      "Iteration 438, loss = 0.67876438\n",
      "Iteration 439, loss = 0.67876264\n",
      "Iteration 440, loss = 0.67876090\n",
      "Iteration 441, loss = 0.67875915\n",
      "Iteration 442, loss = 0.67875740\n",
      "Iteration 443, loss = 0.67875565\n",
      "Iteration 444, loss = 0.67875390\n",
      "Iteration 445, loss = 0.67875214\n",
      "Iteration 446, loss = 0.67875039\n",
      "Iteration 447, loss = 0.67874863\n",
      "Iteration 448, loss = 0.67874686\n",
      "Iteration 449, loss = 0.67874510\n",
      "Iteration 450, loss = 0.67874334\n",
      "Iteration 451, loss = 0.67874157\n",
      "Iteration 452, loss = 0.67873980\n",
      "Iteration 453, loss = 0.67873802\n",
      "Iteration 454, loss = 0.67873625\n",
      "Iteration 455, loss = 0.67873447\n",
      "Iteration 456, loss = 0.67873269\n",
      "Iteration 457, loss = 0.67873091\n",
      "Iteration 458, loss = 0.67872913\n",
      "Iteration 459, loss = 0.67872734\n",
      "Iteration 460, loss = 0.67872556\n",
      "Iteration 461, loss = 0.67872377\n",
      "Iteration 462, loss = 0.67872197\n",
      "Iteration 463, loss = 0.67872018\n",
      "Iteration 464, loss = 0.67871838\n",
      "Iteration 465, loss = 0.67871658\n",
      "Iteration 466, loss = 0.67871478\n",
      "Iteration 467, loss = 0.67871298\n",
      "Iteration 468, loss = 0.67871117\n",
      "Iteration 469, loss = 0.67870936\n",
      "Iteration 470, loss = 0.67870755\n",
      "Iteration 471, loss = 0.67870574\n",
      "Iteration 472, loss = 0.67870392\n",
      "Iteration 473, loss = 0.67870210\n",
      "Iteration 474, loss = 0.67870028\n",
      "Iteration 475, loss = 0.67869846\n",
      "Iteration 476, loss = 0.67869663\n",
      "Iteration 477, loss = 0.67869481\n",
      "Iteration 478, loss = 0.67869298\n",
      "Iteration 479, loss = 0.67869114\n",
      "Iteration 480, loss = 0.67868931\n",
      "Iteration 481, loss = 0.67868747\n",
      "Iteration 482, loss = 0.67868563\n",
      "Iteration 483, loss = 0.67868379\n",
      "Iteration 484, loss = 0.67868194\n",
      "Iteration 485, loss = 0.67868010\n",
      "Iteration 486, loss = 0.67867825\n",
      "Iteration 487, loss = 0.67867639\n",
      "Iteration 488, loss = 0.67867454\n",
      "Iteration 489, loss = 0.67867268\n",
      "Iteration 490, loss = 0.67867082\n",
      "Iteration 491, loss = 0.67866896\n",
      "Iteration 492, loss = 0.67866709\n",
      "Iteration 493, loss = 0.67866522\n",
      "Iteration 494, loss = 0.67866335\n",
      "Iteration 495, loss = 0.67866148\n",
      "Iteration 496, loss = 0.67865961\n",
      "Iteration 497, loss = 0.67865773\n",
      "Iteration 498, loss = 0.67865585\n",
      "Iteration 499, loss = 0.67865396\n",
      "Iteration 500, loss = 0.67865208\n",
      "Iteration 501, loss = 0.67865019\n",
      "Iteration 502, loss = 0.67864830\n",
      "Iteration 503, loss = 0.67864640\n",
      "Iteration 504, loss = 0.67864451\n",
      "Iteration 505, loss = 0.67864261\n",
      "Iteration 506, loss = 0.67864071\n",
      "Iteration 507, loss = 0.67863880\n",
      "Iteration 508, loss = 0.67863689\n",
      "Iteration 509, loss = 0.67863498\n",
      "Iteration 510, loss = 0.67863307\n",
      "Iteration 511, loss = 0.67863115\n",
      "Iteration 512, loss = 0.67862924\n",
      "Iteration 513, loss = 0.67862731\n",
      "Iteration 514, loss = 0.67862539\n",
      "Iteration 515, loss = 0.67862346\n",
      "Iteration 516, loss = 0.67862153\n",
      "Iteration 517, loss = 0.67861960\n",
      "Iteration 518, loss = 0.67861767\n",
      "Iteration 519, loss = 0.67861573\n",
      "Iteration 520, loss = 0.67861379\n",
      "Iteration 521, loss = 0.67861184\n",
      "Iteration 522, loss = 0.67860990\n",
      "Iteration 523, loss = 0.67860795\n",
      "Iteration 524, loss = 0.67860600\n",
      "Iteration 525, loss = 0.67860404\n",
      "Iteration 526, loss = 0.67860208\n",
      "Iteration 527, loss = 0.67860012\n",
      "Iteration 528, loss = 0.67859816\n",
      "Iteration 529, loss = 0.67859619\n",
      "Iteration 530, loss = 0.67859422\n",
      "Iteration 531, loss = 0.67859225\n",
      "Iteration 532, loss = 0.67859027\n",
      "Iteration 533, loss = 0.67858829\n",
      "Iteration 534, loss = 0.67858631\n",
      "Iteration 535, loss = 0.67858433\n",
      "Iteration 536, loss = 0.67858234\n",
      "Iteration 537, loss = 0.67858035\n",
      "Iteration 538, loss = 0.67857836\n",
      "Iteration 539, loss = 0.67857636\n",
      "Iteration 540, loss = 0.67857436\n",
      "Iteration 541, loss = 0.67857236\n",
      "Iteration 542, loss = 0.67857035\n",
      "Iteration 543, loss = 0.67856834\n",
      "Iteration 544, loss = 0.67856633\n",
      "Iteration 545, loss = 0.67856432\n",
      "Iteration 546, loss = 0.67856230\n",
      "Iteration 547, loss = 0.67856028\n",
      "Iteration 548, loss = 0.67855825\n",
      "Iteration 549, loss = 0.67855622\n",
      "Iteration 550, loss = 0.67855419\n",
      "Iteration 551, loss = 0.67855216\n",
      "Iteration 552, loss = 0.67855012\n",
      "Iteration 553, loss = 0.67854808\n",
      "Iteration 554, loss = 0.67854604\n",
      "Iteration 555, loss = 0.67854399\n",
      "Iteration 556, loss = 0.67854194\n",
      "Iteration 557, loss = 0.67853989\n",
      "Iteration 558, loss = 0.67853783\n",
      "Iteration 559, loss = 0.67853577\n",
      "Iteration 560, loss = 0.67853371\n",
      "Iteration 561, loss = 0.67853164\n",
      "Iteration 562, loss = 0.67852957\n",
      "Iteration 563, loss = 0.67852750\n",
      "Iteration 564, loss = 0.67852543\n",
      "Iteration 565, loss = 0.67852335\n",
      "Iteration 566, loss = 0.67852126\n",
      "Iteration 567, loss = 0.67851918\n",
      "Iteration 568, loss = 0.67851709\n",
      "Iteration 569, loss = 0.67851500\n",
      "Iteration 570, loss = 0.67851290\n",
      "Iteration 571, loss = 0.67851080\n",
      "Iteration 572, loss = 0.67850870\n",
      "Iteration 573, loss = 0.67850659\n",
      "Iteration 574, loss = 0.67850448\n",
      "Iteration 575, loss = 0.67850237\n",
      "Iteration 576, loss = 0.67850025\n",
      "Iteration 577, loss = 0.67849813\n",
      "Iteration 578, loss = 0.67849601\n",
      "Iteration 579, loss = 0.67849388\n",
      "Iteration 580, loss = 0.67849175\n",
      "Iteration 581, loss = 0.67848962\n",
      "Iteration 582, loss = 0.67848748\n",
      "Iteration 583, loss = 0.67848534\n",
      "Iteration 584, loss = 0.67848320\n",
      "Iteration 585, loss = 0.67848105\n",
      "Iteration 586, loss = 0.67847890\n",
      "Iteration 587, loss = 0.67847674\n",
      "Iteration 588, loss = 0.67847458\n",
      "Iteration 589, loss = 0.67847242\n",
      "Iteration 590, loss = 0.67847025\n",
      "Iteration 591, loss = 0.67846809\n",
      "Iteration 592, loss = 0.67846591\n",
      "Iteration 593, loss = 0.67846374\n",
      "Iteration 594, loss = 0.67846156\n",
      "Iteration 595, loss = 0.67845937\n",
      "Iteration 596, loss = 0.67845718\n",
      "Iteration 597, loss = 0.67845499\n",
      "Iteration 598, loss = 0.67845280\n",
      "Iteration 599, loss = 0.67845060\n",
      "Iteration 600, loss = 0.67844840\n",
      "Iteration 601, loss = 0.67844619\n",
      "Iteration 602, loss = 0.67844398\n",
      "Iteration 603, loss = 0.67844177\n",
      "Iteration 604, loss = 0.67843955\n",
      "Iteration 605, loss = 0.67843733\n",
      "Iteration 606, loss = 0.67843510\n",
      "Iteration 607, loss = 0.67843288\n",
      "Iteration 608, loss = 0.67843064\n",
      "Iteration 609, loss = 0.67842841\n",
      "Iteration 610, loss = 0.67842617\n",
      "Iteration 611, loss = 0.67842392\n",
      "Iteration 612, loss = 0.67842167\n",
      "Iteration 613, loss = 0.67841942\n",
      "Iteration 614, loss = 0.67841717\n",
      "Iteration 615, loss = 0.67841491\n",
      "Iteration 616, loss = 0.67841264\n",
      "Iteration 617, loss = 0.67841038\n",
      "Iteration 618, loss = 0.67840811\n",
      "Iteration 619, loss = 0.67840583\n",
      "Iteration 620, loss = 0.67840355\n",
      "Iteration 621, loss = 0.67840127\n",
      "Iteration 622, loss = 0.67839898\n",
      "Iteration 623, loss = 0.67839669\n",
      "Iteration 624, loss = 0.67839440\n",
      "Iteration 625, loss = 0.67839210\n",
      "Iteration 626, loss = 0.67838979\n",
      "Iteration 627, loss = 0.67838749\n",
      "Iteration 628, loss = 0.67838517\n",
      "Iteration 629, loss = 0.67838286\n",
      "Iteration 630, loss = 0.67838054\n",
      "Iteration 631, loss = 0.67837822\n",
      "Iteration 632, loss = 0.67837589\n",
      "Iteration 633, loss = 0.67837356\n",
      "Iteration 634, loss = 0.67837122\n",
      "Iteration 635, loss = 0.67836888\n",
      "Iteration 636, loss = 0.67836654\n",
      "Iteration 637, loss = 0.67836419\n",
      "Iteration 638, loss = 0.67836184\n",
      "Iteration 639, loss = 0.67835948\n",
      "Iteration 640, loss = 0.67835712\n",
      "Iteration 641, loss = 0.67835475\n",
      "Iteration 642, loss = 0.67835238\n",
      "Iteration 643, loss = 0.67835001\n",
      "Iteration 644, loss = 0.67834763\n",
      "Iteration 645, loss = 0.67834525\n",
      "Iteration 646, loss = 0.67834286\n",
      "Iteration 647, loss = 0.67834047\n",
      "Iteration 648, loss = 0.67833808\n",
      "Iteration 649, loss = 0.67833568\n",
      "Iteration 650, loss = 0.67833328\n",
      "Iteration 651, loss = 0.67833087\n",
      "Iteration 652, loss = 0.67832846\n",
      "Iteration 653, loss = 0.67832604\n",
      "Iteration 654, loss = 0.67832362\n",
      "Iteration 655, loss = 0.67832119\n",
      "Iteration 656, loss = 0.67831876\n",
      "Iteration 657, loss = 0.67831633\n",
      "Iteration 658, loss = 0.67831389\n",
      "Iteration 659, loss = 0.67831145\n",
      "Iteration 660, loss = 0.67830900\n",
      "Iteration 661, loss = 0.67830655\n",
      "Iteration 662, loss = 0.67830409\n",
      "Iteration 663, loss = 0.67830163\n",
      "Iteration 664, loss = 0.67829916\n",
      "Iteration 665, loss = 0.67829669\n",
      "Iteration 666, loss = 0.67829422\n",
      "Iteration 667, loss = 0.67829174\n",
      "Iteration 668, loss = 0.67828925\n",
      "Iteration 669, loss = 0.67828676\n",
      "Iteration 670, loss = 0.67828427\n",
      "Iteration 671, loss = 0.67828177\n",
      "Iteration 672, loss = 0.67827927\n",
      "Iteration 673, loss = 0.67827676\n",
      "Iteration 674, loss = 0.67827425\n",
      "Iteration 675, loss = 0.67827173\n",
      "Iteration 676, loss = 0.67826921\n",
      "Iteration 677, loss = 0.67826669\n",
      "Iteration 678, loss = 0.67826416\n",
      "Iteration 679, loss = 0.67826162\n",
      "Iteration 680, loss = 0.67825908\n",
      "Iteration 681, loss = 0.67825654\n",
      "Iteration 682, loss = 0.67825399\n",
      "Iteration 683, loss = 0.67825143\n",
      "Iteration 684, loss = 0.67824887\n",
      "Iteration 685, loss = 0.67824631\n",
      "Iteration 686, loss = 0.67824374\n",
      "Iteration 687, loss = 0.67824117\n",
      "Iteration 688, loss = 0.67823859\n",
      "Iteration 689, loss = 0.67823600\n",
      "Iteration 690, loss = 0.67823342\n",
      "Iteration 691, loss = 0.67823082\n",
      "Iteration 692, loss = 0.67822822\n",
      "Iteration 693, loss = 0.67822562\n",
      "Iteration 694, loss = 0.67822301\n",
      "Iteration 695, loss = 0.67822040\n",
      "Iteration 696, loss = 0.67821778\n",
      "Iteration 697, loss = 0.67821516\n",
      "Iteration 698, loss = 0.67821253\n",
      "Iteration 699, loss = 0.67820990\n",
      "Iteration 700, loss = 0.67820726\n",
      "Iteration 701, loss = 0.67820462\n",
      "Iteration 702, loss = 0.67820197\n",
      "Iteration 703, loss = 0.67819931\n",
      "Iteration 704, loss = 0.67819666\n",
      "Iteration 705, loss = 0.67819399\n",
      "Iteration 706, loss = 0.67819132\n",
      "Iteration 707, loss = 0.67818865\n",
      "Iteration 708, loss = 0.67818597\n",
      "Iteration 709, loss = 0.67818329\n",
      "Iteration 710, loss = 0.67818060\n",
      "Iteration 711, loss = 0.67817790\n",
      "Iteration 712, loss = 0.67817520\n",
      "Iteration 713, loss = 0.67817250\n",
      "Iteration 714, loss = 0.67816979\n",
      "Iteration 715, loss = 0.67816707\n",
      "Iteration 716, loss = 0.67816435\n",
      "Iteration 717, loss = 0.67816162\n",
      "Iteration 718, loss = 0.67815889\n",
      "Iteration 719, loss = 0.67815616\n",
      "Iteration 720, loss = 0.67815341\n",
      "Iteration 721, loss = 0.67815067\n",
      "Iteration 722, loss = 0.67814791\n",
      "Iteration 723, loss = 0.67814516\n",
      "Iteration 724, loss = 0.67814239\n",
      "Iteration 725, loss = 0.67813962\n",
      "Iteration 726, loss = 0.67813685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 727, loss = 0.67813407\n",
      "Iteration 728, loss = 0.67813128\n",
      "Iteration 729, loss = 0.67812849\n",
      "Iteration 730, loss = 0.67812570\n",
      "Iteration 731, loss = 0.67812289\n",
      "Iteration 732, loss = 0.67812009\n",
      "Iteration 733, loss = 0.67811727\n",
      "Iteration 734, loss = 0.67811445\n",
      "Iteration 735, loss = 0.67811163\n",
      "Iteration 736, loss = 0.67810880\n",
      "Iteration 737, loss = 0.67810597\n",
      "Iteration 738, loss = 0.67810312\n",
      "Iteration 739, loss = 0.67810028\n",
      "Iteration 740, loss = 0.67809743\n",
      "Iteration 741, loss = 0.67809457\n",
      "Iteration 742, loss = 0.67809170\n",
      "Iteration 743, loss = 0.67808883\n",
      "Iteration 744, loss = 0.67808596\n",
      "Iteration 745, loss = 0.67808308\n",
      "Iteration 746, loss = 0.67808019\n",
      "Iteration 747, loss = 0.67807730\n",
      "Iteration 748, loss = 0.67807440\n",
      "Iteration 749, loss = 0.67807150\n",
      "Iteration 750, loss = 0.67806859\n",
      "Iteration 751, loss = 0.67806567\n",
      "Iteration 752, loss = 0.67806275\n",
      "Iteration 753, loss = 0.67805982\n",
      "Iteration 754, loss = 0.67805689\n",
      "Iteration 755, loss = 0.67805395\n",
      "Iteration 756, loss = 0.67805101\n",
      "Iteration 757, loss = 0.67804805\n",
      "Iteration 758, loss = 0.67804510\n",
      "Iteration 759, loss = 0.67804213\n",
      "Iteration 760, loss = 0.67803917\n",
      "Iteration 761, loss = 0.67803619\n",
      "Iteration 762, loss = 0.67803321\n",
      "Iteration 763, loss = 0.67803022\n",
      "Iteration 764, loss = 0.67802723\n",
      "Iteration 765, loss = 0.67802423\n",
      "Iteration 766, loss = 0.67802122\n",
      "Iteration 767, loss = 0.67801821\n",
      "Iteration 768, loss = 0.67801519\n",
      "Iteration 769, loss = 0.67801217\n",
      "Iteration 770, loss = 0.67800914\n",
      "Iteration 771, loss = 0.67800610\n",
      "Iteration 772, loss = 0.67800306\n",
      "Iteration 773, loss = 0.67800001\n",
      "Iteration 774, loss = 0.67799696\n",
      "Iteration 775, loss = 0.67799390\n",
      "Iteration 776, loss = 0.67799083\n",
      "Iteration 777, loss = 0.67798776\n",
      "Iteration 778, loss = 0.67798468\n",
      "Iteration 779, loss = 0.67798159\n",
      "Iteration 780, loss = 0.67797850\n",
      "Iteration 781, loss = 0.67797540\n",
      "Iteration 782, loss = 0.67797229\n",
      "Iteration 783, loss = 0.67796918\n",
      "Iteration 784, loss = 0.67796606\n",
      "Iteration 785, loss = 0.67796294\n",
      "Iteration 786, loss = 0.67795981\n",
      "Iteration 787, loss = 0.67795667\n",
      "Iteration 788, loss = 0.67795353\n",
      "Iteration 789, loss = 0.67795038\n",
      "Iteration 790, loss = 0.67794722\n",
      "Iteration 791, loss = 0.67794405\n",
      "Iteration 792, loss = 0.67794088\n",
      "Iteration 793, loss = 0.67793771\n",
      "Iteration 794, loss = 0.67793452\n",
      "Iteration 795, loss = 0.67793133\n",
      "Iteration 796, loss = 0.67792814\n",
      "Iteration 797, loss = 0.67792493\n",
      "Iteration 798, loss = 0.67792172\n",
      "Iteration 799, loss = 0.67791851\n",
      "Iteration 800, loss = 0.67791528\n",
      "Iteration 801, loss = 0.67791205\n",
      "Iteration 802, loss = 0.67790882\n",
      "Iteration 803, loss = 0.67790557\n",
      "Iteration 804, loss = 0.67790232\n",
      "Iteration 805, loss = 0.67789907\n",
      "Iteration 806, loss = 0.67789580\n",
      "Iteration 807, loss = 0.67789253\n",
      "Iteration 808, loss = 0.67788925\n",
      "Iteration 809, loss = 0.67788597\n",
      "Iteration 810, loss = 0.67788268\n",
      "Iteration 811, loss = 0.67787938\n",
      "Iteration 812, loss = 0.67787607\n",
      "Iteration 813, loss = 0.67787276\n",
      "Iteration 814, loss = 0.67786944\n",
      "Iteration 815, loss = 0.67786611\n",
      "Iteration 816, loss = 0.67786278\n",
      "Iteration 817, loss = 0.67785944\n",
      "Iteration 818, loss = 0.67785609\n",
      "Iteration 819, loss = 0.67785274\n",
      "Iteration 820, loss = 0.67784937\n",
      "Iteration 821, loss = 0.67784600\n",
      "Iteration 822, loss = 0.67784263\n",
      "Iteration 823, loss = 0.67783925\n",
      "Iteration 824, loss = 0.67783585\n",
      "Iteration 825, loss = 0.67783246\n",
      "Iteration 826, loss = 0.67782905\n",
      "Iteration 827, loss = 0.67782564\n",
      "Iteration 828, loss = 0.67782222\n",
      "Iteration 829, loss = 0.67781879\n",
      "Iteration 830, loss = 0.67781536\n",
      "Iteration 831, loss = 0.67781192\n",
      "Iteration 832, loss = 0.67780847\n",
      "Iteration 833, loss = 0.67780501\n",
      "Iteration 834, loss = 0.67780155\n",
      "Iteration 835, loss = 0.67779808\n",
      "Iteration 836, loss = 0.67779460\n",
      "Iteration 837, loss = 0.67779111\n",
      "Iteration 838, loss = 0.67778762\n",
      "Iteration 839, loss = 0.67778412\n",
      "Iteration 840, loss = 0.67778061\n",
      "Iteration 841, loss = 0.67777710\n",
      "Iteration 842, loss = 0.67777357\n",
      "Iteration 843, loss = 0.67777004\n",
      "Iteration 844, loss = 0.67776650\n",
      "Iteration 845, loss = 0.67776296\n",
      "Iteration 846, loss = 0.67775940\n",
      "Iteration 847, loss = 0.67775584\n",
      "Iteration 848, loss = 0.67775227\n",
      "Iteration 849, loss = 0.67774869\n",
      "Iteration 850, loss = 0.67774511\n",
      "Iteration 851, loss = 0.67774152\n",
      "Iteration 852, loss = 0.67773792\n",
      "Iteration 853, loss = 0.67773431\n",
      "Iteration 854, loss = 0.67773069\n",
      "Iteration 855, loss = 0.67772707\n",
      "Iteration 856, loss = 0.67772344\n",
      "Iteration 857, loss = 0.67771980\n",
      "Iteration 858, loss = 0.67771615\n",
      "Iteration 859, loss = 0.67771250\n",
      "Iteration 860, loss = 0.67770883\n",
      "Iteration 861, loss = 0.67770516\n",
      "Iteration 862, loss = 0.67770148\n",
      "Iteration 863, loss = 0.67769780\n",
      "Iteration 864, loss = 0.67769410\n",
      "Iteration 865, loss = 0.67769040\n",
      "Iteration 866, loss = 0.67768669\n",
      "Iteration 867, loss = 0.67768297\n",
      "Iteration 868, loss = 0.67767924\n",
      "Iteration 869, loss = 0.67767550\n",
      "Iteration 870, loss = 0.67767176\n",
      "Iteration 871, loss = 0.67766801\n",
      "Iteration 872, loss = 0.67766425\n",
      "Iteration 873, loss = 0.67766048\n",
      "Iteration 874, loss = 0.67765670\n",
      "Iteration 875, loss = 0.67765292\n",
      "Iteration 876, loss = 0.67764912\n",
      "Iteration 877, loss = 0.67764532\n",
      "Iteration 878, loss = 0.67764151\n",
      "Iteration 879, loss = 0.67763769\n",
      "Iteration 880, loss = 0.67763386\n",
      "Iteration 881, loss = 0.67763003\n",
      "Iteration 882, loss = 0.67762619\n",
      "Iteration 883, loss = 0.67762233\n",
      "Iteration 884, loss = 0.67761847\n",
      "Iteration 885, loss = 0.67761460\n",
      "Iteration 886, loss = 0.67761073\n",
      "Iteration 887, loss = 0.67760684\n",
      "Iteration 888, loss = 0.67760294\n",
      "Iteration 889, loss = 0.67759904\n",
      "Iteration 890, loss = 0.67759513\n",
      "Iteration 891, loss = 0.67759121\n",
      "Iteration 892, loss = 0.67758728\n",
      "Iteration 893, loss = 0.67758334\n",
      "Iteration 894, loss = 0.67757939\n",
      "Iteration 895, loss = 0.67757544\n",
      "Iteration 896, loss = 0.67757147\n",
      "Iteration 897, loss = 0.67756750\n",
      "Iteration 898, loss = 0.67756352\n",
      "Iteration 899, loss = 0.67755952\n",
      "Iteration 900, loss = 0.67755552\n",
      "Iteration 901, loss = 0.67755152\n",
      "Iteration 902, loss = 0.67754750\n",
      "Iteration 903, loss = 0.67754347\n",
      "Iteration 904, loss = 0.67753943\n",
      "Iteration 905, loss = 0.67753539\n",
      "Iteration 906, loss = 0.67753134\n",
      "Iteration 907, loss = 0.67752727\n",
      "Iteration 908, loss = 0.67752320\n",
      "Iteration 909, loss = 0.67751912\n",
      "Iteration 910, loss = 0.67751503\n",
      "Iteration 911, loss = 0.67751093\n",
      "Iteration 912, loss = 0.67750682\n",
      "Iteration 913, loss = 0.67750270\n",
      "Iteration 914, loss = 0.67749858\n",
      "Iteration 915, loss = 0.67749444\n",
      "Iteration 916, loss = 0.67749029\n",
      "Iteration 917, loss = 0.67748614\n",
      "Iteration 918, loss = 0.67748198\n",
      "Iteration 919, loss = 0.67747780\n",
      "Iteration 920, loss = 0.67747362\n",
      "Iteration 921, loss = 0.67746943\n",
      "Iteration 922, loss = 0.67746523\n",
      "Iteration 923, loss = 0.67746101\n",
      "Iteration 924, loss = 0.67745679\n",
      "Iteration 925, loss = 0.67745256\n",
      "Iteration 926, loss = 0.67744832\n",
      "Iteration 927, loss = 0.67744407\n",
      "Iteration 928, loss = 0.67743981\n",
      "Iteration 929, loss = 0.67743555\n",
      "Iteration 930, loss = 0.67743127\n",
      "Iteration 931, loss = 0.67742698\n",
      "Iteration 932, loss = 0.67742268\n",
      "Iteration 933, loss = 0.67741837\n",
      "Iteration 934, loss = 0.67741406\n",
      "Iteration 935, loss = 0.67740973\n",
      "Iteration 936, loss = 0.67740539\n",
      "Iteration 937, loss = 0.67740105\n",
      "Iteration 938, loss = 0.67739669\n",
      "Iteration 939, loss = 0.67739232\n",
      "Iteration 940, loss = 0.67738795\n",
      "Iteration 941, loss = 0.67738356\n",
      "Iteration 942, loss = 0.67737916\n",
      "Iteration 943, loss = 0.67737476\n",
      "Iteration 944, loss = 0.67737034\n",
      "Iteration 945, loss = 0.67736592\n",
      "Iteration 946, loss = 0.67736148\n",
      "Iteration 947, loss = 0.67735703\n",
      "Iteration 948, loss = 0.67735258\n",
      "Iteration 949, loss = 0.67734811\n",
      "Iteration 950, loss = 0.67734363\n",
      "Iteration 951, loss = 0.67733914\n",
      "Iteration 952, loss = 0.67733465\n",
      "Iteration 953, loss = 0.67733014\n",
      "Iteration 954, loss = 0.67732562\n",
      "Iteration 955, loss = 0.67732109\n",
      "Iteration 956, loss = 0.67731655\n",
      "Iteration 957, loss = 0.67731200\n",
      "Iteration 958, loss = 0.67730744\n",
      "Iteration 959, loss = 0.67730287\n",
      "Iteration 960, loss = 0.67729829\n",
      "Iteration 961, loss = 0.67729370\n",
      "Iteration 962, loss = 0.67728910\n",
      "Iteration 963, loss = 0.67728449\n",
      "Iteration 964, loss = 0.67727987\n",
      "Iteration 965, loss = 0.67727523\n",
      "Iteration 966, loss = 0.67727059\n",
      "Iteration 967, loss = 0.67726593\n",
      "Iteration 968, loss = 0.67726127\n",
      "Iteration 969, loss = 0.67725659\n",
      "Iteration 970, loss = 0.67725190\n",
      "Iteration 971, loss = 0.67724721\n",
      "Iteration 972, loss = 0.67724250\n",
      "Iteration 973, loss = 0.67723778\n",
      "Iteration 974, loss = 0.67723305\n",
      "Iteration 975, loss = 0.67722831\n",
      "Iteration 976, loss = 0.67722355\n",
      "Iteration 977, loss = 0.67721879\n",
      "Iteration 978, loss = 0.67721402\n",
      "Iteration 979, loss = 0.67720923\n",
      "Iteration 980, loss = 0.67720443\n",
      "Iteration 981, loss = 0.67719963\n",
      "Iteration 982, loss = 0.67719481\n",
      "Iteration 983, loss = 0.67718998\n",
      "Iteration 984, loss = 0.67718514\n",
      "Iteration 985, loss = 0.67718028\n",
      "Iteration 986, loss = 0.67717542\n",
      "Iteration 987, loss = 0.67717055\n",
      "Iteration 988, loss = 0.67716566\n",
      "Iteration 989, loss = 0.67716076\n",
      "Iteration 990, loss = 0.67715585\n",
      "Iteration 991, loss = 0.67715093\n",
      "Iteration 992, loss = 0.67714600\n",
      "Iteration 993, loss = 0.67714106\n",
      "Iteration 994, loss = 0.67713610\n",
      "Iteration 995, loss = 0.67713114\n",
      "Iteration 996, loss = 0.67712616\n",
      "Iteration 997, loss = 0.67712117\n",
      "Iteration 998, loss = 0.67711617\n",
      "Iteration 999, loss = 0.67711115\n",
      "Iteration 1000, loss = 0.67710613\n",
      "Iteration 1001, loss = 0.67710109\n",
      "Iteration 1002, loss = 0.67709604\n",
      "Iteration 1003, loss = 0.67709098\n",
      "Iteration 1004, loss = 0.67708591\n",
      "Iteration 1005, loss = 0.67708083\n",
      "Iteration 1006, loss = 0.67707573\n",
      "Iteration 1007, loss = 0.67707063\n",
      "Iteration 1008, loss = 0.67706551\n",
      "Iteration 1009, loss = 0.67706038\n",
      "Iteration 1010, loss = 0.67705523\n",
      "Iteration 1011, loss = 0.67705008\n",
      "Iteration 1012, loss = 0.67704491\n",
      "Iteration 1013, loss = 0.67703973\n",
      "Iteration 1014, loss = 0.67703454\n",
      "Iteration 1015, loss = 0.67702933\n",
      "Iteration 1016, loss = 0.67702412\n",
      "Iteration 1017, loss = 0.67701889\n",
      "Iteration 1018, loss = 0.67701365\n",
      "Iteration 1019, loss = 0.67700839\n",
      "Iteration 1020, loss = 0.67700313\n",
      "Iteration 1021, loss = 0.67699785\n",
      "Iteration 1022, loss = 0.67699256\n",
      "Iteration 1023, loss = 0.67698726\n",
      "Iteration 1024, loss = 0.67698194\n",
      "Iteration 1025, loss = 0.67697661\n",
      "Iteration 1026, loss = 0.67697127\n",
      "Iteration 1027, loss = 0.67696592\n",
      "Iteration 1028, loss = 0.67696056\n",
      "Iteration 1029, loss = 0.67695518\n",
      "Iteration 1030, loss = 0.67694979\n",
      "Iteration 1031, loss = 0.67694438\n",
      "Iteration 1032, loss = 0.67693897\n",
      "Iteration 1033, loss = 0.67693354\n",
      "Iteration 1034, loss = 0.67692809\n",
      "Iteration 1035, loss = 0.67692264\n",
      "Iteration 1036, loss = 0.67691717\n",
      "Iteration 1037, loss = 0.67691169\n",
      "Iteration 1038, loss = 0.67690620\n",
      "Iteration 1039, loss = 0.67690069\n",
      "Iteration 1040, loss = 0.67689517\n",
      "Iteration 1041, loss = 0.67688964\n",
      "Iteration 1042, loss = 0.67688409\n",
      "Iteration 1043, loss = 0.67687853\n",
      "Iteration 1044, loss = 0.67687296\n",
      "Iteration 1045, loss = 0.67686737\n",
      "Iteration 1046, loss = 0.67686177\n",
      "Iteration 1047, loss = 0.67685616\n",
      "Iteration 1048, loss = 0.67685053\n",
      "Iteration 1049, loss = 0.67684489\n",
      "Iteration 1050, loss = 0.67683924\n",
      "Iteration 1051, loss = 0.67683357\n",
      "Iteration 1052, loss = 0.67682790\n",
      "Iteration 1053, loss = 0.67682220\n",
      "Iteration 1054, loss = 0.67681650\n",
      "Iteration 1055, loss = 0.67681078\n",
      "Iteration 1056, loss = 0.67680504\n",
      "Iteration 1057, loss = 0.67679929\n",
      "Iteration 1058, loss = 0.67679353\n",
      "Iteration 1059, loss = 0.67678776\n",
      "Iteration 1060, loss = 0.67678197\n",
      "Iteration 1061, loss = 0.67677617\n",
      "Iteration 1062, loss = 0.67677035\n",
      "Iteration 1063, loss = 0.67676452\n",
      "Iteration 1064, loss = 0.67675867\n",
      "Iteration 1065, loss = 0.67675281\n",
      "Iteration 1066, loss = 0.67674694\n",
      "Iteration 1067, loss = 0.67674106\n",
      "Iteration 1068, loss = 0.67673516\n",
      "Iteration 1069, loss = 0.67672924\n",
      "Iteration 1070, loss = 0.67672331\n",
      "Iteration 1071, loss = 0.67671737\n",
      "Iteration 1072, loss = 0.67671141\n",
      "Iteration 1073, loss = 0.67670544\n",
      "Iteration 1074, loss = 0.67669945\n",
      "Iteration 1075, loss = 0.67669345\n",
      "Iteration 1076, loss = 0.67668744\n",
      "Iteration 1077, loss = 0.67668141\n",
      "Iteration 1078, loss = 0.67667536\n",
      "Iteration 1079, loss = 0.67666931\n",
      "Iteration 1080, loss = 0.67666323\n",
      "Iteration 1081, loss = 0.67665715\n",
      "Iteration 1082, loss = 0.67665104\n",
      "Iteration 1083, loss = 0.67664493\n",
      "Iteration 1084, loss = 0.67663880\n",
      "Iteration 1085, loss = 0.67663265\n",
      "Iteration 1086, loss = 0.67662649\n",
      "Iteration 1087, loss = 0.67662031\n",
      "Iteration 1088, loss = 0.67661412\n",
      "Iteration 1089, loss = 0.67660792\n",
      "Iteration 1090, loss = 0.67660169\n",
      "Iteration 1091, loss = 0.67659546\n",
      "Iteration 1092, loss = 0.67658921\n",
      "Iteration 1093, loss = 0.67658294\n",
      "Iteration 1094, loss = 0.67657666\n",
      "Iteration 1095, loss = 0.67657036\n",
      "Iteration 1096, loss = 0.67656405\n",
      "Iteration 1097, loss = 0.67655773\n",
      "Iteration 1098, loss = 0.67655138\n",
      "Iteration 1099, loss = 0.67654503\n",
      "Iteration 1100, loss = 0.67653865\n",
      "Iteration 1101, loss = 0.67653227\n",
      "Iteration 1102, loss = 0.67652586\n",
      "Iteration 1103, loss = 0.67651944\n",
      "Iteration 1104, loss = 0.67651301\n",
      "Iteration 1105, loss = 0.67650656\n",
      "Iteration 1106, loss = 0.67650009\n",
      "Iteration 1107, loss = 0.67649361\n",
      "Iteration 1108, loss = 0.67648711\n",
      "Iteration 1109, loss = 0.67648060\n",
      "Iteration 1110, loss = 0.67647407\n",
      "Iteration 1111, loss = 0.67646753\n",
      "Iteration 1112, loss = 0.67646097\n",
      "Iteration 1113, loss = 0.67645439\n",
      "Iteration 1114, loss = 0.67644780\n",
      "Iteration 1115, loss = 0.67644119\n",
      "Iteration 1116, loss = 0.67643456\n",
      "Iteration 1117, loss = 0.67642792\n",
      "Iteration 1118, loss = 0.67642127\n",
      "Iteration 1119, loss = 0.67641459\n",
      "Iteration 1120, loss = 0.67640790\n",
      "Iteration 1121, loss = 0.67640120\n",
      "Iteration 1122, loss = 0.67639448\n",
      "Iteration 1123, loss = 0.67638774\n",
      "Iteration 1124, loss = 0.67638098\n",
      "Iteration 1125, loss = 0.67637421\n",
      "Iteration 1126, loss = 0.67636742\n",
      "Iteration 1127, loss = 0.67636062\n",
      "Iteration 1128, loss = 0.67635380\n",
      "Iteration 1129, loss = 0.67634696\n",
      "Iteration 1130, loss = 0.67634011\n",
      "Iteration 1131, loss = 0.67633324\n",
      "Iteration 1132, loss = 0.67632635\n",
      "Iteration 1133, loss = 0.67631944\n",
      "Iteration 1134, loss = 0.67631252\n",
      "Iteration 1135, loss = 0.67630558\n",
      "Iteration 1136, loss = 0.67629863\n",
      "Iteration 1137, loss = 0.67629166\n",
      "Iteration 1138, loss = 0.67628467\n",
      "Iteration 1139, loss = 0.67627766\n",
      "Iteration 1140, loss = 0.67627064\n",
      "Iteration 1141, loss = 0.67626359\n",
      "Iteration 1142, loss = 0.67625654\n",
      "Iteration 1143, loss = 0.67624946\n",
      "Iteration 1144, loss = 0.67624237\n",
      "Iteration 1145, loss = 0.67623526\n",
      "Iteration 1146, loss = 0.67622813\n",
      "Iteration 1147, loss = 0.67622099\n",
      "Iteration 1148, loss = 0.67621382\n",
      "Iteration 1149, loss = 0.67620664\n",
      "Iteration 1150, loss = 0.67619945\n",
      "Iteration 1151, loss = 0.67619223\n",
      "Iteration 1152, loss = 0.67618500\n",
      "Iteration 1153, loss = 0.67617775\n",
      "Iteration 1154, loss = 0.67617048\n",
      "Iteration 1155, loss = 0.67616319\n",
      "Iteration 1156, loss = 0.67615589\n",
      "Iteration 1157, loss = 0.67614856\n",
      "Iteration 1158, loss = 0.67614122\n",
      "Iteration 1159, loss = 0.67613387\n",
      "Iteration 1160, loss = 0.67612649\n",
      "Iteration 1161, loss = 0.67611909\n",
      "Iteration 1162, loss = 0.67611168\n",
      "Iteration 1163, loss = 0.67610425\n",
      "Iteration 1164, loss = 0.67609680\n",
      "Iteration 1165, loss = 0.67608933\n",
      "Iteration 1166, loss = 0.67608185\n",
      "Iteration 1167, loss = 0.67607434\n",
      "Iteration 1168, loss = 0.67606682\n",
      "Iteration 1169, loss = 0.67605928\n",
      "Iteration 1170, loss = 0.67605172\n",
      "Iteration 1171, loss = 0.67604414\n",
      "Iteration 1172, loss = 0.67603654\n",
      "Iteration 1173, loss = 0.67602892\n",
      "Iteration 1174, loss = 0.67602129\n",
      "Iteration 1175, loss = 0.67601363\n",
      "Iteration 1176, loss = 0.67600596\n",
      "Iteration 1177, loss = 0.67599827\n",
      "Iteration 1178, loss = 0.67599056\n",
      "Iteration 1179, loss = 0.67598283\n",
      "Iteration 1180, loss = 0.67597508\n",
      "Iteration 1181, loss = 0.67596731\n",
      "Iteration 1182, loss = 0.67595952\n",
      "Iteration 1183, loss = 0.67595172\n",
      "Iteration 1184, loss = 0.67594389\n",
      "Iteration 1185, loss = 0.67593605\n",
      "Iteration 1186, loss = 0.67592818\n",
      "Iteration 1187, loss = 0.67592030\n",
      "Iteration 1188, loss = 0.67591239\n",
      "Iteration 1189, loss = 0.67590447\n",
      "Iteration 1190, loss = 0.67589653\n",
      "Iteration 1191, loss = 0.67588857\n",
      "Iteration 1192, loss = 0.67588058\n",
      "Iteration 1193, loss = 0.67587258\n",
      "Iteration 1194, loss = 0.67586456\n",
      "Iteration 1195, loss = 0.67585652\n",
      "Iteration 1196, loss = 0.67584846\n",
      "Iteration 1197, loss = 0.67584037\n",
      "Iteration 1198, loss = 0.67583227\n",
      "Iteration 1199, loss = 0.67582415\n",
      "Iteration 1200, loss = 0.67581601\n",
      "Iteration 1201, loss = 0.67580785\n",
      "Iteration 1202, loss = 0.67579966\n",
      "Iteration 1203, loss = 0.67579146\n",
      "Iteration 1204, loss = 0.67578324\n",
      "Iteration 1205, loss = 0.67577499\n",
      "Iteration 1206, loss = 0.67576673\n",
      "Iteration 1207, loss = 0.67575845\n",
      "Iteration 1208, loss = 0.67575014\n",
      "Iteration 1209, loss = 0.67574181\n",
      "Iteration 1210, loss = 0.67573347\n",
      "Iteration 1211, loss = 0.67572510\n",
      "Iteration 1212, loss = 0.67571671\n",
      "Iteration 1213, loss = 0.67570830\n",
      "Iteration 1214, loss = 0.67569987\n",
      "Iteration 1215, loss = 0.67569142\n",
      "Iteration 1216, loss = 0.67568295\n",
      "Iteration 1217, loss = 0.67567446\n",
      "Iteration 1218, loss = 0.67566594\n",
      "Iteration 1219, loss = 0.67565741\n",
      "Iteration 1220, loss = 0.67564885\n",
      "Iteration 1221, loss = 0.67564027\n",
      "Iteration 1222, loss = 0.67563167\n",
      "Iteration 1223, loss = 0.67562305\n",
      "Iteration 1224, loss = 0.67561441\n",
      "Iteration 1225, loss = 0.67560575\n",
      "Iteration 1226, loss = 0.67559706\n",
      "Iteration 1227, loss = 0.67558835\n",
      "Iteration 1228, loss = 0.67557962\n",
      "Iteration 1229, loss = 0.67557087\n",
      "Iteration 1230, loss = 0.67556210\n",
      "Iteration 1231, loss = 0.67555331\n",
      "Iteration 1232, loss = 0.67554449\n",
      "Iteration 1233, loss = 0.67553565\n",
      "Iteration 1234, loss = 0.67552679\n",
      "Iteration 1235, loss = 0.67551791\n",
      "Iteration 1236, loss = 0.67550901\n",
      "Iteration 1237, loss = 0.67550008\n",
      "Iteration 1238, loss = 0.67549113\n",
      "Iteration 1239, loss = 0.67548216\n",
      "Iteration 1240, loss = 0.67547316\n",
      "Iteration 1241, loss = 0.67546415\n",
      "Iteration 1242, loss = 0.67545511\n",
      "Iteration 1243, loss = 0.67544605\n",
      "Iteration 1244, loss = 0.67543696\n",
      "Iteration 1245, loss = 0.67542785\n",
      "Iteration 1246, loss = 0.67541872\n",
      "Iteration 1247, loss = 0.67540957\n",
      "Iteration 1248, loss = 0.67540040\n",
      "Iteration 1249, loss = 0.67539120\n",
      "Iteration 1250, loss = 0.67538198\n",
      "Iteration 1251, loss = 0.67537273\n",
      "Iteration 1252, loss = 0.67536346\n",
      "Iteration 1253, loss = 0.67535417\n",
      "Iteration 1254, loss = 0.67534486\n",
      "Iteration 1255, loss = 0.67533552\n",
      "Iteration 1256, loss = 0.67532616\n",
      "Iteration 1257, loss = 0.67531678\n",
      "Iteration 1258, loss = 0.67530737\n",
      "Iteration 1259, loss = 0.67529794\n",
      "Iteration 1260, loss = 0.67528848\n",
      "Iteration 1261, loss = 0.67527900\n",
      "Iteration 1262, loss = 0.67526950\n",
      "Iteration 1263, loss = 0.67525997\n",
      "Iteration 1264, loss = 0.67525042\n",
      "Iteration 1265, loss = 0.67524085\n",
      "Iteration 1266, loss = 0.67523125\n",
      "Iteration 1267, loss = 0.67522163\n",
      "Iteration 1268, loss = 0.67521198\n",
      "Iteration 1269, loss = 0.67520231\n",
      "Iteration 1270, loss = 0.67519262\n",
      "Iteration 1271, loss = 0.67518290\n",
      "Iteration 1272, loss = 0.67517315\n",
      "Iteration 1273, loss = 0.67516338\n",
      "Iteration 1274, loss = 0.67515359\n",
      "Iteration 1275, loss = 0.67514377\n",
      "Iteration 1276, loss = 0.67513393\n",
      "Iteration 1277, loss = 0.67512406\n",
      "Iteration 1278, loss = 0.67511417\n",
      "Iteration 1279, loss = 0.67510426\n",
      "Iteration 1280, loss = 0.67509432\n",
      "Iteration 1281, loss = 0.67508435\n",
      "Iteration 1282, loss = 0.67507436\n",
      "Iteration 1283, loss = 0.67506434\n",
      "Iteration 1284, loss = 0.67505430\n",
      "Iteration 1285, loss = 0.67504423\n",
      "Iteration 1286, loss = 0.67503414\n",
      "Iteration 1287, loss = 0.67502402\n",
      "Iteration 1288, loss = 0.67501388\n",
      "Iteration 1289, loss = 0.67500371\n",
      "Iteration 1290, loss = 0.67499352\n",
      "Iteration 1291, loss = 0.67498330\n",
      "Iteration 1292, loss = 0.67497305\n",
      "Iteration 1293, loss = 0.67496278\n",
      "Iteration 1294, loss = 0.67495248\n",
      "Iteration 1295, loss = 0.67494216\n",
      "Iteration 1296, loss = 0.67493181\n",
      "Iteration 1297, loss = 0.67492144\n",
      "Iteration 1298, loss = 0.67491104\n",
      "Iteration 1299, loss = 0.67490061\n",
      "Iteration 1300, loss = 0.67489015\n",
      "Iteration 1301, loss = 0.67487967\n",
      "Iteration 1302, loss = 0.67486917\n",
      "Iteration 1303, loss = 0.67485864\n",
      "Iteration 1304, loss = 0.67484808\n",
      "Iteration 1305, loss = 0.67483749\n",
      "Iteration 1306, loss = 0.67482688\n",
      "Iteration 1307, loss = 0.67481624\n",
      "Iteration 1308, loss = 0.67480557\n",
      "Iteration 1309, loss = 0.67479488\n",
      "Iteration 1310, loss = 0.67478416\n",
      "Iteration 1311, loss = 0.67477341\n",
      "Iteration 1312, loss = 0.67476264\n",
      "Iteration 1313, loss = 0.67475184\n",
      "Iteration 1314, loss = 0.67474101\n",
      "Iteration 1315, loss = 0.67473015\n",
      "Iteration 1316, loss = 0.67471927\n",
      "Iteration 1317, loss = 0.67470836\n",
      "Iteration 1318, loss = 0.67469742\n",
      "Iteration 1319, loss = 0.67468646\n",
      "Iteration 1320, loss = 0.67467546\n",
      "Iteration 1321, loss = 0.67466444\n",
      "Iteration 1322, loss = 0.67465339\n",
      "Iteration 1323, loss = 0.67464232\n",
      "Iteration 1324, loss = 0.67463121\n",
      "Iteration 1325, loss = 0.67462008\n",
      "Iteration 1326, loss = 0.67460892\n",
      "Iteration 1327, loss = 0.67459773\n",
      "Iteration 1328, loss = 0.67458651\n",
      "Iteration 1329, loss = 0.67457527\n",
      "Iteration 1330, loss = 0.67456399\n",
      "Iteration 1331, loss = 0.67455269\n",
      "Iteration 1332, loss = 0.67454136\n",
      "Iteration 1333, loss = 0.67453000\n",
      "Iteration 1334, loss = 0.67451861\n",
      "Iteration 1335, loss = 0.67450720\n",
      "Iteration 1336, loss = 0.67449575\n",
      "Iteration 1337, loss = 0.67448428\n",
      "Iteration 1338, loss = 0.67447278\n",
      "Iteration 1339, loss = 0.67446124\n",
      "Iteration 1340, loss = 0.67444968\n",
      "Iteration 1341, loss = 0.67443809\n",
      "Iteration 1342, loss = 0.67442647\n",
      "Iteration 1343, loss = 0.67441482\n",
      "Iteration 1344, loss = 0.67440314\n",
      "Iteration 1345, loss = 0.67439144\n",
      "Iteration 1346, loss = 0.67437970\n",
      "Iteration 1347, loss = 0.67436793\n",
      "Iteration 1348, loss = 0.67435613\n",
      "Iteration 1349, loss = 0.67434431\n",
      "Iteration 1350, loss = 0.67433245\n",
      "Iteration 1351, loss = 0.67432056\n",
      "Iteration 1352, loss = 0.67430865\n",
      "Iteration 1353, loss = 0.67429670\n",
      "Iteration 1354, loss = 0.67428472\n",
      "Iteration 1355, loss = 0.67427272\n",
      "Iteration 1356, loss = 0.67426068\n",
      "Iteration 1357, loss = 0.67424861\n",
      "Iteration 1358, loss = 0.67423651\n",
      "Iteration 1359, loss = 0.67422438\n",
      "Iteration 1360, loss = 0.67421222\n",
      "Iteration 1361, loss = 0.67420003\n",
      "Iteration 1362, loss = 0.67418781\n",
      "Iteration 1363, loss = 0.67417556\n",
      "Iteration 1364, loss = 0.67416328\n",
      "Iteration 1365, loss = 0.67415096\n",
      "Iteration 1366, loss = 0.67413862\n",
      "Iteration 1367, loss = 0.67412624\n",
      "Iteration 1368, loss = 0.67411384\n",
      "Iteration 1369, loss = 0.67410140\n",
      "Iteration 1370, loss = 0.67408893\n",
      "Iteration 1371, loss = 0.67407643\n",
      "Iteration 1372, loss = 0.67406389\n",
      "Iteration 1373, loss = 0.67405133\n",
      "Iteration 1374, loss = 0.67403873\n",
      "Iteration 1375, loss = 0.67402610\n",
      "Iteration 1376, loss = 0.67401344\n",
      "Iteration 1377, loss = 0.67400075\n",
      "Iteration 1378, loss = 0.67398803\n",
      "Iteration 1379, loss = 0.67397527\n",
      "Iteration 1380, loss = 0.67396248\n",
      "Iteration 1381, loss = 0.67394966\n",
      "Iteration 1382, loss = 0.67393681\n",
      "Iteration 1383, loss = 0.67392392\n",
      "Iteration 1384, loss = 0.67391100\n",
      "Iteration 1385, loss = 0.67389805\n",
      "Iteration 1386, loss = 0.67388507\n",
      "Iteration 1387, loss = 0.67387205\n",
      "Iteration 1388, loss = 0.67385900\n",
      "Iteration 1389, loss = 0.67384592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1390, loss = 0.67383281\n",
      "Iteration 1391, loss = 0.67381966\n",
      "Iteration 1392, loss = 0.67380647\n",
      "Iteration 1393, loss = 0.67379326\n",
      "Iteration 1394, loss = 0.67378001\n",
      "Iteration 1395, loss = 0.67376673\n",
      "Iteration 1396, loss = 0.67375341\n",
      "Iteration 1397, loss = 0.67374007\n",
      "Iteration 1398, loss = 0.67372668\n",
      "Iteration 1399, loss = 0.67371327\n",
      "Iteration 1400, loss = 0.67369982\n",
      "Iteration 1401, loss = 0.67368633\n",
      "Iteration 1402, loss = 0.67367281\n",
      "Iteration 1403, loss = 0.67365926\n",
      "Iteration 1404, loss = 0.67364568\n",
      "Iteration 1405, loss = 0.67363205\n",
      "Iteration 1406, loss = 0.67361840\n",
      "Iteration 1407, loss = 0.67360471\n",
      "Iteration 1408, loss = 0.67359098\n",
      "Iteration 1409, loss = 0.67357723\n",
      "Iteration 1410, loss = 0.67356343\n",
      "Iteration 1411, loss = 0.67354960\n",
      "Iteration 1412, loss = 0.67353574\n",
      "Iteration 1413, loss = 0.67352184\n",
      "Iteration 1414, loss = 0.67350791\n",
      "Iteration 1415, loss = 0.67349394\n",
      "Iteration 1416, loss = 0.67347994\n",
      "Iteration 1417, loss = 0.67346590\n",
      "Iteration 1418, loss = 0.67345182\n",
      "Iteration 1419, loss = 0.67343771\n",
      "Iteration 1420, loss = 0.67342357\n",
      "Iteration 1421, loss = 0.67340939\n",
      "Iteration 1422, loss = 0.67339517\n",
      "Iteration 1423, loss = 0.67338092\n",
      "Iteration 1424, loss = 0.67336663\n",
      "Iteration 1425, loss = 0.67335230\n",
      "Iteration 1426, loss = 0.67333794\n",
      "Iteration 1427, loss = 0.67332354\n",
      "Iteration 1428, loss = 0.67330911\n",
      "Iteration 1429, loss = 0.67329464\n",
      "Iteration 1430, loss = 0.67328013\n",
      "Iteration 1431, loss = 0.67326559\n",
      "Iteration 1432, loss = 0.67325101\n",
      "Iteration 1433, loss = 0.67323639\n",
      "Iteration 1434, loss = 0.67322174\n",
      "Iteration 1435, loss = 0.67320705\n",
      "Iteration 1436, loss = 0.67319232\n",
      "Iteration 1437, loss = 0.67317756\n",
      "Iteration 1438, loss = 0.67316276\n",
      "Iteration 1439, loss = 0.67314792\n",
      "Iteration 1440, loss = 0.67313304\n",
      "Iteration 1441, loss = 0.67311813\n",
      "Iteration 1442, loss = 0.67310318\n",
      "Iteration 1443, loss = 0.67308819\n",
      "Iteration 1444, loss = 0.67307316\n",
      "Iteration 1445, loss = 0.67305810\n",
      "Iteration 1446, loss = 0.67304300\n",
      "Iteration 1447, loss = 0.67302786\n",
      "Iteration 1448, loss = 0.67301268\n",
      "Iteration 1449, loss = 0.67299746\n",
      "Iteration 1450, loss = 0.67298221\n",
      "Iteration 1451, loss = 0.67296691\n",
      "Iteration 1452, loss = 0.67295158\n",
      "Iteration 1453, loss = 0.67293621\n",
      "Iteration 1454, loss = 0.67292080\n",
      "Iteration 1455, loss = 0.67290535\n",
      "Iteration 1456, loss = 0.67288987\n",
      "Iteration 1457, loss = 0.67287434\n",
      "Iteration 1458, loss = 0.67285878\n",
      "Iteration 1459, loss = 0.67284317\n",
      "Iteration 1460, loss = 0.67282753\n",
      "Iteration 1461, loss = 0.67281185\n",
      "Iteration 1462, loss = 0.67279613\n",
      "Iteration 1463, loss = 0.67278036\n",
      "Iteration 1464, loss = 0.67276456\n",
      "Iteration 1465, loss = 0.67274872\n",
      "Iteration 1466, loss = 0.67273284\n",
      "Iteration 1467, loss = 0.67271692\n",
      "Iteration 1468, loss = 0.67270096\n",
      "Iteration 1469, loss = 0.67268496\n",
      "Iteration 1470, loss = 0.67266892\n",
      "Iteration 1471, loss = 0.67265284\n",
      "Iteration 1472, loss = 0.67263672\n",
      "Iteration 1473, loss = 0.67262056\n",
      "Iteration 1474, loss = 0.67260435\n",
      "Iteration 1475, loss = 0.67258811\n",
      "Iteration 1476, loss = 0.67257183\n",
      "Iteration 1477, loss = 0.67255550\n",
      "Iteration 1478, loss = 0.67253914\n",
      "Iteration 1479, loss = 0.67252273\n",
      "Iteration 1480, loss = 0.67250628\n",
      "Iteration 1481, loss = 0.67248979\n",
      "Iteration 1482, loss = 0.67247326\n",
      "Iteration 1483, loss = 0.67245669\n",
      "Iteration 1484, loss = 0.67244008\n",
      "Iteration 1485, loss = 0.67242342\n",
      "Iteration 1486, loss = 0.67240672\n",
      "Iteration 1487, loss = 0.67238998\n",
      "Iteration 1488, loss = 0.67237320\n",
      "Iteration 1489, loss = 0.67235638\n",
      "Iteration 1490, loss = 0.67233952\n",
      "Iteration 1491, loss = 0.67232261\n",
      "Iteration 1492, loss = 0.67230566\n",
      "Iteration 1493, loss = 0.67228867\n",
      "Iteration 1494, loss = 0.67227163\n",
      "Iteration 1495, loss = 0.67225456\n",
      "Iteration 1496, loss = 0.67223744\n",
      "Iteration 1497, loss = 0.67222027\n",
      "Iteration 1498, loss = 0.67220307\n",
      "Iteration 1499, loss = 0.67218582\n",
      "Iteration 1500, loss = 0.67216853\n",
      "Iteration 1501, loss = 0.67215119\n",
      "Iteration 1502, loss = 0.67213381\n",
      "Iteration 1503, loss = 0.67211639\n",
      "Iteration 1504, loss = 0.67209892\n",
      "Iteration 1505, loss = 0.67208142\n",
      "Iteration 1506, loss = 0.67206386\n",
      "Iteration 1507, loss = 0.67204627\n",
      "Iteration 1508, loss = 0.67202862\n",
      "Iteration 1509, loss = 0.67201094\n",
      "Iteration 1510, loss = 0.67199321\n",
      "Iteration 1511, loss = 0.67197544\n",
      "Iteration 1512, loss = 0.67195762\n",
      "Iteration 1513, loss = 0.67193976\n",
      "Iteration 1514, loss = 0.67192185\n",
      "Iteration 1515, loss = 0.67190390\n",
      "Iteration 1516, loss = 0.67188590\n",
      "Iteration 1517, loss = 0.67186786\n",
      "Iteration 1518, loss = 0.67184977\n",
      "Iteration 1519, loss = 0.67183164\n",
      "Iteration 1520, loss = 0.67181346\n",
      "Iteration 1521, loss = 0.67179524\n",
      "Iteration 1522, loss = 0.67177697\n",
      "Iteration 1523, loss = 0.67175866\n",
      "Iteration 1524, loss = 0.67174030\n",
      "Iteration 1525, loss = 0.67172190\n",
      "Iteration 1526, loss = 0.67170345\n",
      "Iteration 1527, loss = 0.67168495\n",
      "Iteration 1528, loss = 0.67166641\n",
      "Iteration 1529, loss = 0.67164782\n",
      "Iteration 1530, loss = 0.67162919\n",
      "Iteration 1531, loss = 0.67161050\n",
      "Iteration 1532, loss = 0.67159178\n",
      "Iteration 1533, loss = 0.67157300\n",
      "Iteration 1534, loss = 0.67155418\n",
      "Iteration 1535, loss = 0.67153531\n",
      "Iteration 1536, loss = 0.67151640\n",
      "Iteration 1537, loss = 0.67149744\n",
      "Iteration 1538, loss = 0.67147843\n",
      "Iteration 1539, loss = 0.67145937\n",
      "Iteration 1540, loss = 0.67144027\n",
      "Iteration 1541, loss = 0.67142111\n",
      "Iteration 1542, loss = 0.67140192\n",
      "Iteration 1543, loss = 0.67138267\n",
      "Iteration 1544, loss = 0.67136337\n",
      "Iteration 1545, loss = 0.67134403\n",
      "Iteration 1546, loss = 0.67132464\n",
      "Iteration 1547, loss = 0.67130520\n",
      "Iteration 1548, loss = 0.67128572\n",
      "Iteration 1549, loss = 0.67126618\n",
      "Iteration 1550, loss = 0.67124660\n",
      "Iteration 1551, loss = 0.67122697\n",
      "Iteration 1552, loss = 0.67120728\n",
      "Iteration 1553, loss = 0.67118756\n",
      "Iteration 1554, loss = 0.67116778\n",
      "Iteration 1555, loss = 0.67114795\n",
      "Iteration 1556, loss = 0.67112807\n",
      "Iteration 1557, loss = 0.67110815\n",
      "Iteration 1558, loss = 0.67108817\n",
      "Iteration 1559, loss = 0.67106815\n",
      "Iteration 1560, loss = 0.67104807\n",
      "Iteration 1561, loss = 0.67102795\n",
      "Iteration 1562, loss = 0.67100777\n",
      "Iteration 1563, loss = 0.67098755\n",
      "Iteration 1564, loss = 0.67096728\n",
      "Iteration 1565, loss = 0.67094695\n",
      "Iteration 1566, loss = 0.67092658\n",
      "Iteration 1567, loss = 0.67090615\n",
      "Iteration 1568, loss = 0.67088568\n",
      "Iteration 1569, loss = 0.67086515\n",
      "Iteration 1570, loss = 0.67084457\n",
      "Iteration 1571, loss = 0.67082395\n",
      "Iteration 1572, loss = 0.67080327\n",
      "Iteration 1573, loss = 0.67078254\n",
      "Iteration 1574, loss = 0.67076176\n",
      "Iteration 1575, loss = 0.67074093\n",
      "Iteration 1576, loss = 0.67072004\n",
      "Iteration 1577, loss = 0.67069911\n",
      "Iteration 1578, loss = 0.67067812\n",
      "Iteration 1579, loss = 0.67065708\n",
      "Iteration 1580, loss = 0.67063599\n",
      "Iteration 1581, loss = 0.67061485\n",
      "Iteration 1582, loss = 0.67059365\n",
      "Iteration 1583, loss = 0.67057241\n",
      "Iteration 1584, loss = 0.67055111\n",
      "Iteration 1585, loss = 0.67052976\n",
      "Iteration 1586, loss = 0.67050835\n",
      "Iteration 1587, loss = 0.67048690\n",
      "Iteration 1588, loss = 0.67046539\n",
      "Iteration 1589, loss = 0.67044382\n",
      "Iteration 1590, loss = 0.67042221\n",
      "Iteration 1591, loss = 0.67040054\n",
      "Iteration 1592, loss = 0.67037882\n",
      "Iteration 1593, loss = 0.67035704\n",
      "Iteration 1594, loss = 0.67033521\n",
      "Iteration 1595, loss = 0.67031333\n",
      "Iteration 1596, loss = 0.67029139\n",
      "Iteration 1597, loss = 0.67026940\n",
      "Iteration 1598, loss = 0.67024736\n",
      "Iteration 1599, loss = 0.67022526\n",
      "Iteration 1600, loss = 0.67020310\n",
      "Iteration 1601, loss = 0.67018090\n",
      "Iteration 1602, loss = 0.67015864\n",
      "Iteration 1603, loss = 0.67013632\n",
      "Iteration 1604, loss = 0.67011395\n",
      "Iteration 1605, loss = 0.67009152\n",
      "Iteration 1606, loss = 0.67006904\n",
      "Iteration 1607, loss = 0.67004650\n",
      "Iteration 1608, loss = 0.67002391\n",
      "Iteration 1609, loss = 0.67000126\n",
      "Iteration 1610, loss = 0.66997856\n",
      "Iteration 1611, loss = 0.66995580\n",
      "Iteration 1612, loss = 0.66993299\n",
      "Iteration 1613, loss = 0.66991012\n",
      "Iteration 1614, loss = 0.66988719\n",
      "Iteration 1615, loss = 0.66986421\n",
      "Iteration 1616, loss = 0.66984117\n",
      "Iteration 1617, loss = 0.66981808\n",
      "Iteration 1618, loss = 0.66979493\n",
      "Iteration 1619, loss = 0.66977172\n",
      "Iteration 1620, loss = 0.66974846\n",
      "Iteration 1621, loss = 0.66972514\n",
      "Iteration 1622, loss = 0.66970176\n",
      "Iteration 1623, loss = 0.66967832\n",
      "Iteration 1624, loss = 0.66965483\n",
      "Iteration 1625, loss = 0.66963128\n",
      "Iteration 1626, loss = 0.66960767\n",
      "Iteration 1627, loss = 0.66958401\n",
      "Iteration 1628, loss = 0.66956029\n",
      "Iteration 1629, loss = 0.66953651\n",
      "Iteration 1630, loss = 0.66951267\n",
      "Iteration 1631, loss = 0.66948877\n",
      "Iteration 1632, loss = 0.66946482\n",
      "Iteration 1633, loss = 0.66944080\n",
      "Iteration 1634, loss = 0.66941673\n",
      "Iteration 1635, loss = 0.66939260\n",
      "Iteration 1636, loss = 0.66936841\n",
      "Iteration 1637, loss = 0.66934416\n",
      "Iteration 1638, loss = 0.66931986\n",
      "Iteration 1639, loss = 0.66929549\n",
      "Iteration 1640, loss = 0.66927106\n",
      "Iteration 1641, loss = 0.66924658\n",
      "Iteration 1642, loss = 0.66922203\n",
      "Iteration 1643, loss = 0.66919743\n",
      "Iteration 1644, loss = 0.66917277\n",
      "Iteration 1645, loss = 0.66914804\n",
      "Iteration 1646, loss = 0.66912326\n",
      "Iteration 1647, loss = 0.66909841\n",
      "Iteration 1648, loss = 0.66907351\n",
      "Iteration 1649, loss = 0.66904854\n",
      "Iteration 1650, loss = 0.66902352\n",
      "Iteration 1651, loss = 0.66899843\n",
      "Iteration 1652, loss = 0.66897328\n",
      "Iteration 1653, loss = 0.66894808\n",
      "Iteration 1654, loss = 0.66892281\n",
      "Iteration 1655, loss = 0.66889748\n",
      "Iteration 1656, loss = 0.66887208\n",
      "Iteration 1657, loss = 0.66884663\n",
      "Iteration 1658, loss = 0.66882112\n",
      "Iteration 1659, loss = 0.66879554\n",
      "Iteration 1660, loss = 0.66876990\n",
      "Iteration 1661, loss = 0.66874420\n",
      "Iteration 1662, loss = 0.66871844\n",
      "Iteration 1663, loss = 0.66869261\n",
      "Iteration 1664, loss = 0.66866672\n",
      "Iteration 1665, loss = 0.66864077\n",
      "Iteration 1666, loss = 0.66861476\n",
      "Iteration 1667, loss = 0.66858868\n",
      "Iteration 1668, loss = 0.66856254\n",
      "Iteration 1669, loss = 0.66853634\n",
      "Iteration 1670, loss = 0.66851007\n",
      "Iteration 1671, loss = 0.66848375\n",
      "Iteration 1672, loss = 0.66845735\n",
      "Iteration 1673, loss = 0.66843090\n",
      "Iteration 1674, loss = 0.66840438\n",
      "Iteration 1675, loss = 0.66837779\n",
      "Iteration 1676, loss = 0.66835114\n",
      "Iteration 1677, loss = 0.66832443\n",
      "Iteration 1678, loss = 0.66829765\n",
      "Iteration 1679, loss = 0.66827081\n",
      "Iteration 1680, loss = 0.66824391\n",
      "Iteration 1681, loss = 0.66821694\n",
      "Iteration 1682, loss = 0.66818990\n",
      "Iteration 1683, loss = 0.66816280\n",
      "Iteration 1684, loss = 0.66813563\n",
      "Iteration 1685, loss = 0.66810840\n",
      "Iteration 1686, loss = 0.66808110\n",
      "Iteration 1687, loss = 0.66805374\n",
      "Iteration 1688, loss = 0.66802631\n",
      "Iteration 1689, loss = 0.66799882\n",
      "Iteration 1690, loss = 0.66797126\n",
      "Iteration 1691, loss = 0.66794363\n",
      "Iteration 1692, loss = 0.66791594\n",
      "Iteration 1693, loss = 0.66788818\n",
      "Iteration 1694, loss = 0.66786035\n",
      "Iteration 1695, loss = 0.66783246\n",
      "Iteration 1696, loss = 0.66780450\n",
      "Iteration 1697, loss = 0.66777647\n",
      "Iteration 1698, loss = 0.66774838\n",
      "Iteration 1699, loss = 0.66772022\n",
      "Iteration 1700, loss = 0.66769199\n",
      "Iteration 1701, loss = 0.66766369\n",
      "Iteration 1702, loss = 0.66763533\n",
      "Iteration 1703, loss = 0.66760689\n",
      "Iteration 1704, loss = 0.66757839\n",
      "Iteration 1705, loss = 0.66754982\n",
      "Iteration 1706, loss = 0.66752119\n",
      "Iteration 1707, loss = 0.66749248\n",
      "Iteration 1708, loss = 0.66746371\n",
      "Iteration 1709, loss = 0.66743486\n",
      "Iteration 1710, loss = 0.66740595\n",
      "Iteration 1711, loss = 0.66737697\n",
      "Iteration 1712, loss = 0.66734792\n",
      "Iteration 1713, loss = 0.66731880\n",
      "Iteration 1714, loss = 0.66728961\n",
      "Iteration 1715, loss = 0.66726035\n",
      "Iteration 1716, loss = 0.66723102\n",
      "Iteration 1717, loss = 0.66720162\n",
      "Iteration 1718, loss = 0.66717216\n",
      "Iteration 1719, loss = 0.66714262\n",
      "Iteration 1720, loss = 0.66711301\n",
      "Iteration 1721, loss = 0.66708333\n",
      "Iteration 1722, loss = 0.66705357\n",
      "Iteration 1723, loss = 0.66702375\n",
      "Iteration 1724, loss = 0.66699386\n",
      "Iteration 1725, loss = 0.66696389\n",
      "Iteration 1726, loss = 0.66693386\n",
      "Iteration 1727, loss = 0.66690375\n",
      "Iteration 1728, loss = 0.66687357\n",
      "Iteration 1729, loss = 0.66684332\n",
      "Iteration 1730, loss = 0.66681300\n",
      "Iteration 1731, loss = 0.66678260\n",
      "Iteration 1732, loss = 0.66675214\n",
      "Iteration 1733, loss = 0.66672160\n",
      "Iteration 1734, loss = 0.66669099\n",
      "Iteration 1735, loss = 0.66666030\n",
      "Iteration 1736, loss = 0.66662954\n",
      "Iteration 1737, loss = 0.66659871\n",
      "Iteration 1738, loss = 0.66656781\n",
      "Iteration 1739, loss = 0.66653683\n",
      "Iteration 1740, loss = 0.66650578\n",
      "Iteration 1741, loss = 0.66647466\n",
      "Iteration 1742, loss = 0.66644346\n",
      "Iteration 1743, loss = 0.66641219\n",
      "Iteration 1744, loss = 0.66638084\n",
      "Iteration 1745, loss = 0.66634942\n",
      "Iteration 1746, loss = 0.66631792\n",
      "Iteration 1747, loss = 0.66628635\n",
      "Iteration 1748, loss = 0.66625471\n",
      "Iteration 1749, loss = 0.66622299\n",
      "Iteration 1750, loss = 0.66619120\n",
      "Iteration 1751, loss = 0.66615933\n",
      "Iteration 1752, loss = 0.66612738\n",
      "Iteration 1753, loss = 0.66609536\n",
      "Iteration 1754, loss = 0.66606326\n",
      "Iteration 1755, loss = 0.66603109\n",
      "Iteration 1756, loss = 0.66599884\n",
      "Iteration 1757, loss = 0.66596652\n",
      "Iteration 1758, loss = 0.66593412\n",
      "Iteration 1759, loss = 0.66590164\n",
      "Iteration 1760, loss = 0.66586909\n",
      "Iteration 1761, loss = 0.66583646\n",
      "Iteration 1762, loss = 0.66580375\n",
      "Iteration 1763, loss = 0.66577096\n",
      "Iteration 1764, loss = 0.66573810\n",
      "Iteration 1765, loss = 0.66570516\n",
      "Iteration 1766, loss = 0.66567215\n",
      "Iteration 1767, loss = 0.66563905\n",
      "Iteration 1768, loss = 0.66560588\n",
      "Iteration 1769, loss = 0.66557263\n",
      "Iteration 1770, loss = 0.66553930\n",
      "Iteration 1771, loss = 0.66550589\n",
      "Iteration 1772, loss = 0.66547240\n",
      "Iteration 1773, loss = 0.66543884\n",
      "Iteration 1774, loss = 0.66540519\n",
      "Iteration 1775, loss = 0.66537147\n",
      "Iteration 1776, loss = 0.66533767\n",
      "Iteration 1777, loss = 0.66530378\n",
      "Iteration 1778, loss = 0.66526982\n",
      "Iteration 1779, loss = 0.66523578\n",
      "Iteration 1780, loss = 0.66520166\n",
      "Iteration 1781, loss = 0.66516746\n",
      "Iteration 1782, loss = 0.66513317\n",
      "Iteration 1783, loss = 0.66509881\n",
      "Iteration 1784, loss = 0.66506437\n",
      "Iteration 1785, loss = 0.66502985\n",
      "Iteration 1786, loss = 0.66499524\n",
      "Iteration 1787, loss = 0.66496055\n",
      "Iteration 1788, loss = 0.66492579\n",
      "Iteration 1789, loss = 0.66489094\n",
      "Iteration 1790, loss = 0.66485601\n",
      "Iteration 1791, loss = 0.66482100\n",
      "Iteration 1792, loss = 0.66478590\n",
      "Iteration 1793, loss = 0.66475072\n",
      "Iteration 1794, loss = 0.66471547\n",
      "Iteration 1795, loss = 0.66468012\n",
      "Iteration 1796, loss = 0.66464470\n",
      "Iteration 1797, loss = 0.66460919\n",
      "Iteration 1798, loss = 0.66457360\n",
      "Iteration 1799, loss = 0.66453793\n",
      "Iteration 1800, loss = 0.66450217\n",
      "Iteration 1801, loss = 0.66446633\n",
      "Iteration 1802, loss = 0.66443041\n",
      "Iteration 1803, loss = 0.66439440\n",
      "Iteration 1804, loss = 0.66435831\n",
      "Iteration 1805, loss = 0.66432213\n",
      "Iteration 1806, loss = 0.66428587\n",
      "Iteration 1807, loss = 0.66424953\n",
      "Iteration 1808, loss = 0.66421310\n",
      "Iteration 1809, loss = 0.66417658\n",
      "Iteration 1810, loss = 0.66413998\n",
      "Iteration 1811, loss = 0.66410330\n",
      "Iteration 1812, loss = 0.66406653\n",
      "Iteration 1813, loss = 0.66402967\n",
      "Iteration 1814, loss = 0.66399273\n",
      "Iteration 1815, loss = 0.66395570\n",
      "Iteration 1816, loss = 0.66391859\n",
      "Iteration 1817, loss = 0.66388139\n",
      "Iteration 1818, loss = 0.66384410\n",
      "Iteration 1819, loss = 0.66380673\n",
      "Iteration 1820, loss = 0.66376926\n",
      "Iteration 1821, loss = 0.66373172\n",
      "Iteration 1822, loss = 0.66369408\n",
      "Iteration 1823, loss = 0.66365636\n",
      "Iteration 1824, loss = 0.66361855\n",
      "Iteration 1825, loss = 0.66358065\n",
      "Iteration 1826, loss = 0.66354266\n",
      "Iteration 1827, loss = 0.66350459\n",
      "Iteration 1828, loss = 0.66346643\n",
      "Iteration 1829, loss = 0.66342817\n",
      "Iteration 1830, loss = 0.66338983\n",
      "Iteration 1831, loss = 0.66335140\n",
      "Iteration 1832, loss = 0.66331289\n",
      "Iteration 1833, loss = 0.66327428\n",
      "Iteration 1834, loss = 0.66323558\n",
      "Iteration 1835, loss = 0.66319680\n",
      "Iteration 1836, loss = 0.66315792\n",
      "Iteration 1837, loss = 0.66311895\n",
      "Iteration 1838, loss = 0.66307990\n",
      "Iteration 1839, loss = 0.66304075\n",
      "Iteration 1840, loss = 0.66300151\n",
      "Iteration 1841, loss = 0.66296218\n",
      "Iteration 1842, loss = 0.66292276\n",
      "Iteration 1843, loss = 0.66288325\n",
      "Iteration 1844, loss = 0.66284365\n",
      "Iteration 1845, loss = 0.66280396\n",
      "Iteration 1846, loss = 0.66276417\n",
      "Iteration 1847, loss = 0.66272429\n",
      "Iteration 1848, loss = 0.66268432\n",
      "Iteration 1849, loss = 0.66264426\n",
      "Iteration 1850, loss = 0.66260411\n",
      "Iteration 1851, loss = 0.66256386\n",
      "Iteration 1852, loss = 0.66252352\n",
      "Iteration 1853, loss = 0.66248309\n",
      "Iteration 1854, loss = 0.66244256\n",
      "Iteration 1855, loss = 0.66240194\n",
      "Iteration 1856, loss = 0.66236123\n",
      "Iteration 1857, loss = 0.66232042\n",
      "Iteration 1858, loss = 0.66227952\n",
      "Iteration 1859, loss = 0.66223853\n",
      "Iteration 1860, loss = 0.66219744\n",
      "Iteration 1861, loss = 0.66215625\n",
      "Iteration 1862, loss = 0.66211497\n",
      "Iteration 1863, loss = 0.66207360\n",
      "Iteration 1864, loss = 0.66203213\n",
      "Iteration 1865, loss = 0.66199057\n",
      "Iteration 1866, loss = 0.66194891\n",
      "Iteration 1867, loss = 0.66190715\n",
      "Iteration 1868, loss = 0.66186530\n",
      "Iteration 1869, loss = 0.66182335\n",
      "Iteration 1870, loss = 0.66178131\n",
      "Iteration 1871, loss = 0.66173916\n",
      "Iteration 1872, loss = 0.66169693\n",
      "Iteration 1873, loss = 0.66165459\n",
      "Iteration 1874, loss = 0.66161216\n",
      "Iteration 1875, loss = 0.66156963\n",
      "Iteration 1876, loss = 0.66152700\n",
      "Iteration 1877, loss = 0.66148428\n",
      "Iteration 1878, loss = 0.66144146\n",
      "Iteration 1879, loss = 0.66139853\n",
      "Iteration 1880, loss = 0.66135552\n",
      "Iteration 1881, loss = 0.66131240\n",
      "Iteration 1882, loss = 0.66126918\n",
      "Iteration 1883, loss = 0.66122587\n",
      "Iteration 1884, loss = 0.66118245\n",
      "Iteration 1885, loss = 0.66113894\n",
      "Iteration 1886, loss = 0.66109532\n",
      "Iteration 1887, loss = 0.66105161\n",
      "Iteration 1888, loss = 0.66100780\n",
      "Iteration 1889, loss = 0.66096388\n",
      "Iteration 1890, loss = 0.66091987\n",
      "Iteration 1891, loss = 0.66087575\n",
      "Iteration 1892, loss = 0.66083154\n",
      "Iteration 1893, loss = 0.66078722\n",
      "Iteration 1894, loss = 0.66074280\n",
      "Iteration 1895, loss = 0.66069828\n",
      "Iteration 1896, loss = 0.66065366\n",
      "Iteration 1897, loss = 0.66060894\n",
      "Iteration 1898, loss = 0.66056412\n",
      "Iteration 1899, loss = 0.66051919\n",
      "Iteration 1900, loss = 0.66047416\n",
      "Iteration 1901, loss = 0.66042903\n",
      "Iteration 1902, loss = 0.66038379\n",
      "Iteration 1903, loss = 0.66033845\n",
      "Iteration 1904, loss = 0.66029301\n",
      "Iteration 1905, loss = 0.66024747\n",
      "Iteration 1906, loss = 0.66020182\n",
      "Iteration 1907, loss = 0.66015607\n",
      "Iteration 1908, loss = 0.66011021\n",
      "Iteration 1909, loss = 0.66006425\n",
      "Iteration 1910, loss = 0.66001818\n",
      "Iteration 1911, loss = 0.65997201\n",
      "Iteration 1912, loss = 0.65992574\n",
      "Iteration 1913, loss = 0.65987935\n",
      "Iteration 1914, loss = 0.65983287\n",
      "Iteration 1915, loss = 0.65978628\n",
      "Iteration 1916, loss = 0.65973958\n",
      "Iteration 1917, loss = 0.65969278\n",
      "Iteration 1918, loss = 0.65964587\n",
      "Iteration 1919, loss = 0.65959885\n",
      "Iteration 1920, loss = 0.65955173\n",
      "Iteration 1921, loss = 0.65950450\n",
      "Iteration 1922, loss = 0.65945716\n",
      "Iteration 1923, loss = 0.65940971\n",
      "Iteration 1924, loss = 0.65936216\n",
      "Iteration 1925, loss = 0.65931450\n",
      "Iteration 1926, loss = 0.65926673\n",
      "Iteration 1927, loss = 0.65921886\n",
      "Iteration 1928, loss = 0.65917087\n",
      "Iteration 1929, loss = 0.65912278\n",
      "Iteration 1930, loss = 0.65907458\n",
      "Iteration 1931, loss = 0.65902626\n",
      "Iteration 1932, loss = 0.65897784\n",
      "Iteration 1933, loss = 0.65892931\n",
      "Iteration 1934, loss = 0.65888067\n",
      "Iteration 1935, loss = 0.65883192\n",
      "Iteration 1936, loss = 0.65878306\n",
      "Iteration 1937, loss = 0.65873409\n",
      "Iteration 1938, loss = 0.65868501\n",
      "Iteration 1939, loss = 0.65863582\n",
      "Iteration 1940, loss = 0.65858651\n",
      "Iteration 1941, loss = 0.65853710\n",
      "Iteration 1942, loss = 0.65848757\n",
      "Iteration 1943, loss = 0.65843794\n",
      "Iteration 1944, loss = 0.65838819\n",
      "Iteration 1945, loss = 0.65833832\n",
      "Iteration 1946, loss = 0.65828835\n",
      "Iteration 1947, loss = 0.65823826\n",
      "Iteration 1948, loss = 0.65818806\n",
      "Iteration 1949, loss = 0.65813775\n",
      "Iteration 1950, loss = 0.65808732\n",
      "Iteration 1951, loss = 0.65803678\n",
      "Iteration 1952, loss = 0.65798613\n",
      "Iteration 1953, loss = 0.65793536\n",
      "Iteration 1954, loss = 0.65788448\n",
      "Iteration 1955, loss = 0.65783348\n",
      "Iteration 1956, loss = 0.65778237\n",
      "Iteration 1957, loss = 0.65773114\n",
      "Iteration 1958, loss = 0.65767980\n",
      "Iteration 1959, loss = 0.65762834\n",
      "Iteration 1960, loss = 0.65757677\n",
      "Iteration 1961, loss = 0.65752508\n",
      "Iteration 1962, loss = 0.65747328\n",
      "Iteration 1963, loss = 0.65742135\n",
      "Iteration 1964, loss = 0.65736932\n",
      "Iteration 1965, loss = 0.65731716\n",
      "Iteration 1966, loss = 0.65726489\n",
      "Iteration 1967, loss = 0.65721250\n",
      "Iteration 1968, loss = 0.65716000\n",
      "Iteration 1969, loss = 0.65710737\n",
      "Iteration 1970, loss = 0.65705463\n",
      "Iteration 1971, loss = 0.65700177\n",
      "Iteration 1972, loss = 0.65694879\n",
      "Iteration 1973, loss = 0.65689570\n",
      "Iteration 1974, loss = 0.65684248\n",
      "Iteration 1975, loss = 0.65678914\n",
      "Iteration 1976, loss = 0.65673569\n",
      "Iteration 1977, loss = 0.65668212\n",
      "Iteration 1978, loss = 0.65662842\n",
      "Iteration 1979, loss = 0.65657461\n",
      "Iteration 1980, loss = 0.65652067\n",
      "Iteration 1981, loss = 0.65646662\n",
      "Iteration 1982, loss = 0.65641244\n",
      "Iteration 1983, loss = 0.65635815\n",
      "Iteration 1984, loss = 0.65630373\n",
      "Iteration 1985, loss = 0.65624919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1986, loss = 0.65619453\n",
      "Iteration 1987, loss = 0.65613975\n",
      "Iteration 1988, loss = 0.65608484\n",
      "Iteration 1989, loss = 0.65602981\n",
      "Iteration 1990, loss = 0.65597466\n",
      "Iteration 1991, loss = 0.65591939\n",
      "Iteration 1992, loss = 0.65586399\n",
      "Iteration 1993, loss = 0.65580847\n",
      "Iteration 1994, loss = 0.65575283\n",
      "Iteration 1995, loss = 0.65569706\n",
      "Iteration 1996, loss = 0.65564117\n",
      "Iteration 1997, loss = 0.65558516\n",
      "Iteration 1998, loss = 0.65552901\n",
      "Iteration 1999, loss = 0.65547275\n",
      "Iteration 2000, loss = 0.65541636\n",
      "Iteration 2001, loss = 0.65535984\n",
      "Iteration 2002, loss = 0.65530320\n",
      "Iteration 2003, loss = 0.65524643\n",
      "Iteration 2004, loss = 0.65518954\n",
      "Iteration 2005, loss = 0.65513252\n",
      "Iteration 2006, loss = 0.65507538\n",
      "Iteration 2007, loss = 0.65501810\n",
      "Iteration 2008, loss = 0.65496070\n",
      "Iteration 2009, loss = 0.65490317\n",
      "Iteration 2010, loss = 0.65484552\n",
      "Iteration 2011, loss = 0.65478774\n",
      "Iteration 2012, loss = 0.65472983\n",
      "Iteration 2013, loss = 0.65467179\n",
      "Iteration 2014, loss = 0.65461362\n",
      "Iteration 2015, loss = 0.65455532\n",
      "Iteration 2016, loss = 0.65449690\n",
      "Iteration 2017, loss = 0.65443834\n",
      "Iteration 2018, loss = 0.65437966\n",
      "Iteration 2019, loss = 0.65432084\n",
      "Iteration 2020, loss = 0.65426190\n",
      "Iteration 2021, loss = 0.65420282\n",
      "Iteration 2022, loss = 0.65414362\n",
      "Iteration 2023, loss = 0.65408428\n",
      "Iteration 2024, loss = 0.65402482\n",
      "Iteration 2025, loss = 0.65396522\n",
      "Iteration 2026, loss = 0.65390549\n",
      "Iteration 2027, loss = 0.65384563\n",
      "Iteration 2028, loss = 0.65378563\n",
      "Iteration 2029, loss = 0.65372551\n",
      "Iteration 2030, loss = 0.65366525\n",
      "Iteration 2031, loss = 0.65360486\n",
      "Iteration 2032, loss = 0.65354433\n",
      "Iteration 2033, loss = 0.65348367\n",
      "Iteration 2034, loss = 0.65342288\n",
      "Iteration 2035, loss = 0.65336196\n",
      "Iteration 2036, loss = 0.65330090\n",
      "Iteration 2037, loss = 0.65323971\n",
      "Iteration 2038, loss = 0.65317838\n",
      "Iteration 2039, loss = 0.65311691\n",
      "Iteration 2040, loss = 0.65305532\n",
      "Iteration 2041, loss = 0.65299358\n",
      "Iteration 2042, loss = 0.65293171\n",
      "Iteration 2043, loss = 0.65286971\n",
      "Iteration 2044, loss = 0.65280757\n",
      "Iteration 2045, loss = 0.65274529\n",
      "Iteration 2046, loss = 0.65268287\n",
      "Iteration 2047, loss = 0.65262032\n",
      "Iteration 2048, loss = 0.65255763\n",
      "Iteration 2049, loss = 0.65249481\n",
      "Iteration 2050, loss = 0.65243184\n",
      "Iteration 2051, loss = 0.65236874\n",
      "Iteration 2052, loss = 0.65230550\n",
      "Iteration 2053, loss = 0.65224213\n",
      "Iteration 2054, loss = 0.65217861\n",
      "Iteration 2055, loss = 0.65211495\n",
      "Iteration 2056, loss = 0.65205116\n",
      "Iteration 2057, loss = 0.65198722\n",
      "Iteration 2058, loss = 0.65192315\n",
      "Iteration 2059, loss = 0.65185893\n",
      "Iteration 2060, loss = 0.65179458\n",
      "Iteration 2061, loss = 0.65173008\n",
      "Iteration 2062, loss = 0.65166545\n",
      "Iteration 2063, loss = 0.65160067\n",
      "Iteration 2064, loss = 0.65153575\n",
      "Iteration 2065, loss = 0.65147069\n",
      "Iteration 2066, loss = 0.65140549\n",
      "Iteration 2067, loss = 0.65134014\n",
      "Iteration 2068, loss = 0.65127466\n",
      "Iteration 2069, loss = 0.65120903\n",
      "Iteration 2070, loss = 0.65114325\n",
      "Iteration 2071, loss = 0.65107734\n",
      "Iteration 2072, loss = 0.65101128\n",
      "Iteration 2073, loss = 0.65094508\n",
      "Iteration 2074, loss = 0.65087873\n",
      "Iteration 2075, loss = 0.65081224\n",
      "Iteration 2076, loss = 0.65074560\n",
      "Iteration 2077, loss = 0.65067882\n",
      "Iteration 2078, loss = 0.65061190\n",
      "Iteration 2079, loss = 0.65054483\n",
      "Iteration 2080, loss = 0.65047761\n",
      "Iteration 2081, loss = 0.65041025\n",
      "Iteration 2082, loss = 0.65034274\n",
      "Iteration 2083, loss = 0.65027508\n",
      "Iteration 2084, loss = 0.65020728\n",
      "Iteration 2085, loss = 0.65013933\n",
      "Iteration 2086, loss = 0.65007124\n",
      "Iteration 2087, loss = 0.65000299\n",
      "Iteration 2088, loss = 0.64993460\n",
      "Iteration 2089, loss = 0.64986606\n",
      "Iteration 2090, loss = 0.64979738\n",
      "Iteration 2091, loss = 0.64972854\n",
      "Iteration 2092, loss = 0.64965956\n",
      "Iteration 2093, loss = 0.64959042\n",
      "Iteration 2094, loss = 0.64952114\n",
      "Iteration 2095, loss = 0.64945171\n",
      "Iteration 2096, loss = 0.64938212\n",
      "Iteration 2097, loss = 0.64931239\n",
      "Iteration 2098, loss = 0.64924251\n",
      "Iteration 2099, loss = 0.64917247\n",
      "Iteration 2100, loss = 0.64910229\n",
      "Iteration 2101, loss = 0.64903195\n",
      "Iteration 2102, loss = 0.64896146\n",
      "Iteration 2103, loss = 0.64889082\n",
      "Iteration 2104, loss = 0.64882003\n",
      "Iteration 2105, loss = 0.64874909\n",
      "Iteration 2106, loss = 0.64867799\n",
      "Iteration 2107, loss = 0.64860674\n",
      "Iteration 2108, loss = 0.64853533\n",
      "Iteration 2109, loss = 0.64846378\n",
      "Iteration 2110, loss = 0.64839207\n",
      "Iteration 2111, loss = 0.64832020\n",
      "Iteration 2112, loss = 0.64824818\n",
      "Iteration 2113, loss = 0.64817601\n",
      "Iteration 2114, loss = 0.64810368\n",
      "Iteration 2115, loss = 0.64803120\n",
      "Iteration 2116, loss = 0.64795856\n",
      "Iteration 2117, loss = 0.64788577\n",
      "Iteration 2118, loss = 0.64781281\n",
      "Iteration 2119, loss = 0.64773971\n",
      "Iteration 2120, loss = 0.64766644\n",
      "Iteration 2121, loss = 0.64759302\n",
      "Iteration 2122, loss = 0.64751945\n",
      "Iteration 2123, loss = 0.64744571\n",
      "Iteration 2124, loss = 0.64737182\n",
      "Iteration 2125, loss = 0.64729777\n",
      "Iteration 2126, loss = 0.64722356\n",
      "Iteration 2127, loss = 0.64714920\n",
      "Iteration 2128, loss = 0.64707467\n",
      "Iteration 2129, loss = 0.64699999\n",
      "Iteration 2130, loss = 0.64692514\n",
      "Iteration 2131, loss = 0.64685014\n",
      "Iteration 2132, loss = 0.64677497\n",
      "Iteration 2133, loss = 0.64669965\n",
      "Iteration 2134, loss = 0.64662417\n",
      "Iteration 2135, loss = 0.64654852\n",
      "Iteration 2136, loss = 0.64647272\n",
      "Iteration 2137, loss = 0.64639675\n",
      "Iteration 2138, loss = 0.64632062\n",
      "Iteration 2139, loss = 0.64624433\n",
      "Iteration 2140, loss = 0.64616788\n",
      "Iteration 2141, loss = 0.64609126\n",
      "Iteration 2142, loss = 0.64601449\n",
      "Iteration 2143, loss = 0.64593755\n",
      "Iteration 2144, loss = 0.64586044\n",
      "Iteration 2145, loss = 0.64578318\n",
      "Iteration 2146, loss = 0.64570575\n",
      "Iteration 2147, loss = 0.64562815\n",
      "Iteration 2148, loss = 0.64555039\n",
      "Iteration 2149, loss = 0.64547247\n",
      "Iteration 2150, loss = 0.64539438\n",
      "Iteration 2151, loss = 0.64531612\n",
      "Iteration 2152, loss = 0.64523770\n",
      "Iteration 2153, loss = 0.64515912\n",
      "Iteration 2154, loss = 0.64508037\n",
      "Iteration 2155, loss = 0.64500145\n",
      "Iteration 2156, loss = 0.64492237\n",
      "Iteration 2157, loss = 0.64484311\n",
      "Iteration 2158, loss = 0.64476370\n",
      "Iteration 2159, loss = 0.64468411\n",
      "Iteration 2160, loss = 0.64460436\n",
      "Iteration 2161, loss = 0.64452443\n",
      "Iteration 2162, loss = 0.64444434\n",
      "Iteration 2163, loss = 0.64436409\n",
      "Iteration 2164, loss = 0.64428366\n",
      "Iteration 2165, loss = 0.64420306\n",
      "Iteration 2166, loss = 0.64412229\n",
      "Iteration 2167, loss = 0.64404136\n",
      "Iteration 2168, loss = 0.64396025\n",
      "Iteration 2169, loss = 0.64387898\n",
      "Iteration 2170, loss = 0.64379753\n",
      "Iteration 2171, loss = 0.64371591\n",
      "Iteration 2172, loss = 0.64363412\n",
      "Iteration 2173, loss = 0.64355216\n",
      "Iteration 2174, loss = 0.64347003\n",
      "Iteration 2175, loss = 0.64338772\n",
      "Iteration 2176, loss = 0.64330525\n",
      "Iteration 2177, loss = 0.64322260\n",
      "Iteration 2178, loss = 0.64313977\n",
      "Iteration 2179, loss = 0.64305678\n",
      "Iteration 2180, loss = 0.64297361\n",
      "Iteration 2181, loss = 0.64289027\n",
      "Iteration 2182, loss = 0.64280675\n",
      "Iteration 2183, loss = 0.64272306\n",
      "Iteration 2184, loss = 0.64263919\n",
      "Iteration 2185, loss = 0.64255515\n",
      "Iteration 2186, loss = 0.64247094\n",
      "Iteration 2187, loss = 0.64238654\n",
      "Iteration 2188, loss = 0.64230198\n",
      "Iteration 2189, loss = 0.64221723\n",
      "Iteration 2190, loss = 0.64213231\n",
      "Iteration 2191, loss = 0.64204722\n",
      "Iteration 2192, loss = 0.64196194\n",
      "Iteration 2193, loss = 0.64187649\n",
      "Iteration 2194, loss = 0.64179086\n",
      "Iteration 2195, loss = 0.64170506\n",
      "Iteration 2196, loss = 0.64161907\n",
      "Iteration 2197, loss = 0.64153291\n",
      "Iteration 2198, loss = 0.64144657\n",
      "Iteration 2199, loss = 0.64136005\n",
      "Iteration 2200, loss = 0.64127335\n",
      "Iteration 2201, loss = 0.64118647\n",
      "Iteration 2202, loss = 0.64109941\n",
      "Iteration 2203, loss = 0.64101218\n",
      "Iteration 2204, loss = 0.64092476\n",
      "Iteration 2205, loss = 0.64083716\n",
      "Iteration 2206, loss = 0.64074938\n",
      "Iteration 2207, loss = 0.64066141\n",
      "Iteration 2208, loss = 0.64057327\n",
      "Iteration 2209, loss = 0.64048494\n",
      "Iteration 2210, loss = 0.64039644\n",
      "Iteration 2211, loss = 0.64030775\n",
      "Iteration 2212, loss = 0.64021887\n",
      "Iteration 2213, loss = 0.64012982\n",
      "Iteration 2214, loss = 0.64004058\n",
      "Iteration 2215, loss = 0.63995116\n",
      "Iteration 2216, loss = 0.63986155\n",
      "Iteration 2217, loss = 0.63977176\n",
      "Iteration 2218, loss = 0.63968178\n",
      "Iteration 2219, loss = 0.63959162\n",
      "Iteration 2220, loss = 0.63950128\n",
      "Iteration 2221, loss = 0.63941075\n",
      "Iteration 2222, loss = 0.63932003\n",
      "Iteration 2223, loss = 0.63922913\n",
      "Iteration 2224, loss = 0.63913804\n",
      "Iteration 2225, loss = 0.63904677\n",
      "Iteration 2226, loss = 0.63895531\n",
      "Iteration 2227, loss = 0.63886366\n",
      "Iteration 2228, loss = 0.63877182\n",
      "Iteration 2229, loss = 0.63867980\n",
      "Iteration 2230, loss = 0.63858759\n",
      "Iteration 2231, loss = 0.63849519\n",
      "Iteration 2232, loss = 0.63840260\n",
      "Iteration 2233, loss = 0.63830982\n",
      "Iteration 2234, loss = 0.63821686\n",
      "Iteration 2235, loss = 0.63812370\n",
      "Iteration 2236, loss = 0.63803036\n",
      "Iteration 2237, loss = 0.63793682\n",
      "Iteration 2238, loss = 0.63784310\n",
      "Iteration 2239, loss = 0.63774918\n",
      "Iteration 2240, loss = 0.63765507\n",
      "Iteration 2241, loss = 0.63756078\n",
      "Iteration 2242, loss = 0.63746629\n",
      "Iteration 2243, loss = 0.63737161\n",
      "Iteration 2244, loss = 0.63727673\n",
      "Iteration 2245, loss = 0.63718167\n",
      "Iteration 2246, loss = 0.63708641\n",
      "Iteration 2247, loss = 0.63699096\n",
      "Iteration 2248, loss = 0.63689532\n",
      "Iteration 2249, loss = 0.63679948\n",
      "Iteration 2250, loss = 0.63670345\n",
      "Iteration 2251, loss = 0.63660722\n",
      "Iteration 2252, loss = 0.63651081\n",
      "Iteration 2253, loss = 0.63641419\n",
      "Iteration 2254, loss = 0.63631738\n",
      "Iteration 2255, loss = 0.63622038\n",
      "Iteration 2256, loss = 0.63612318\n",
      "Iteration 2257, loss = 0.63602578\n",
      "Iteration 2258, loss = 0.63592819\n",
      "Iteration 2259, loss = 0.63583041\n",
      "Iteration 2260, loss = 0.63573242\n",
      "Iteration 2261, loss = 0.63563424\n",
      "Iteration 2262, loss = 0.63553586\n",
      "Iteration 2263, loss = 0.63543729\n",
      "Iteration 2264, loss = 0.63533851\n",
      "Iteration 2265, loss = 0.63523954\n",
      "Iteration 2266, loss = 0.63514037\n",
      "Iteration 2267, loss = 0.63504100\n",
      "Iteration 2268, loss = 0.63494144\n",
      "Iteration 2269, loss = 0.63484167\n",
      "Iteration 2270, loss = 0.63474170\n",
      "Iteration 2271, loss = 0.63464154\n",
      "Iteration 2272, loss = 0.63454117\n",
      "Iteration 2273, loss = 0.63444060\n",
      "Iteration 2274, loss = 0.63433984\n",
      "Iteration 2275, loss = 0.63423887\n",
      "Iteration 2276, loss = 0.63413770\n",
      "Iteration 2277, loss = 0.63403633\n",
      "Iteration 2278, loss = 0.63393476\n",
      "Iteration 2279, loss = 0.63383298\n",
      "Iteration 2280, loss = 0.63373100\n",
      "Iteration 2281, loss = 0.63362883\n",
      "Iteration 2282, loss = 0.63352644\n",
      "Iteration 2283, loss = 0.63342386\n",
      "Iteration 2284, loss = 0.63332107\n",
      "Iteration 2285, loss = 0.63321807\n",
      "Iteration 2286, loss = 0.63311488\n",
      "Iteration 2287, loss = 0.63301147\n",
      "Iteration 2288, loss = 0.63290787\n",
      "Iteration 2289, loss = 0.63280406\n",
      "Iteration 2290, loss = 0.63270004\n",
      "Iteration 2291, loss = 0.63259582\n",
      "Iteration 2292, loss = 0.63249139\n",
      "Iteration 2293, loss = 0.63238676\n",
      "Iteration 2294, loss = 0.63228191\n",
      "Iteration 2295, loss = 0.63217687\n",
      "Iteration 2296, loss = 0.63207161\n",
      "Iteration 2297, loss = 0.63196615\n",
      "Iteration 2298, loss = 0.63186048\n",
      "Iteration 2299, loss = 0.63175461\n",
      "Iteration 2300, loss = 0.63164852\n",
      "Iteration 2301, loss = 0.63154223\n",
      "Iteration 2302, loss = 0.63143573\n",
      "Iteration 2303, loss = 0.63132902\n",
      "Iteration 2304, loss = 0.63122210\n",
      "Iteration 2305, loss = 0.63111497\n",
      "Iteration 2306, loss = 0.63100763\n",
      "Iteration 2307, loss = 0.63090008\n",
      "Iteration 2308, loss = 0.63079232\n",
      "Iteration 2309, loss = 0.63068435\n",
      "Iteration 2310, loss = 0.63057617\n",
      "Iteration 2311, loss = 0.63046777\n",
      "Iteration 2312, loss = 0.63035917\n",
      "Iteration 2313, loss = 0.63025035\n",
      "Iteration 2314, loss = 0.63014133\n",
      "Iteration 2315, loss = 0.63003209\n",
      "Iteration 2316, loss = 0.62992263\n",
      "Iteration 2317, loss = 0.62981297\n",
      "Iteration 2318, loss = 0.62970309\n",
      "Iteration 2319, loss = 0.62959300\n",
      "Iteration 2320, loss = 0.62948269\n",
      "Iteration 2321, loss = 0.62937217\n",
      "Iteration 2322, loss = 0.62926144\n",
      "Iteration 2323, loss = 0.62915049\n",
      "Iteration 2324, loss = 0.62903933\n",
      "Iteration 2325, loss = 0.62892795\n",
      "Iteration 2326, loss = 0.62881635\n",
      "Iteration 2327, loss = 0.62870454\n",
      "Iteration 2328, loss = 0.62859252\n",
      "Iteration 2329, loss = 0.62848028\n",
      "Iteration 2330, loss = 0.62836782\n",
      "Iteration 2331, loss = 0.62825514\n",
      "Iteration 2332, loss = 0.62814225\n",
      "Iteration 2333, loss = 0.62802914\n",
      "Iteration 2334, loss = 0.62791581\n",
      "Iteration 2335, loss = 0.62780227\n",
      "Iteration 2336, loss = 0.62768851\n",
      "Iteration 2337, loss = 0.62757452\n",
      "Iteration 2338, loss = 0.62746032\n",
      "Iteration 2339, loss = 0.62734591\n",
      "Iteration 2340, loss = 0.62723127\n",
      "Iteration 2341, loss = 0.62711641\n",
      "Iteration 2342, loss = 0.62700133\n",
      "Iteration 2343, loss = 0.62688603\n",
      "Iteration 2344, loss = 0.62677051\n",
      "Iteration 2345, loss = 0.62665478\n",
      "Iteration 2346, loss = 0.62653882\n",
      "Iteration 2347, loss = 0.62642264\n",
      "Iteration 2348, loss = 0.62630623\n",
      "Iteration 2349, loss = 0.62618961\n",
      "Iteration 2350, loss = 0.62607276\n",
      "Iteration 2351, loss = 0.62595569\n",
      "Iteration 2352, loss = 0.62583840\n",
      "Iteration 2353, loss = 0.62572089\n",
      "Iteration 2354, loss = 0.62560315\n",
      "Iteration 2355, loss = 0.62548519\n",
      "Iteration 2356, loss = 0.62536701\n",
      "Iteration 2357, loss = 0.62524860\n",
      "Iteration 2358, loss = 0.62512997\n",
      "Iteration 2359, loss = 0.62501112\n",
      "Iteration 2360, loss = 0.62489204\n",
      "Iteration 2361, loss = 0.62477273\n",
      "Iteration 2362, loss = 0.62465320\n",
      "Iteration 2363, loss = 0.62453344\n",
      "Iteration 2364, loss = 0.62441346\n",
      "Iteration 2365, loss = 0.62429325\n",
      "Iteration 2366, loss = 0.62417282\n",
      "Iteration 2367, loss = 0.62405216\n",
      "Iteration 2368, loss = 0.62393127\n",
      "Iteration 2369, loss = 0.62381016\n",
      "Iteration 2370, loss = 0.62368882\n",
      "Iteration 2371, loss = 0.62356725\n",
      "Iteration 2372, loss = 0.62344545\n",
      "Iteration 2373, loss = 0.62332343\n",
      "Iteration 2374, loss = 0.62320117\n",
      "Iteration 2375, loss = 0.62307869\n",
      "Iteration 2376, loss = 0.62295598\n",
      "Iteration 2377, loss = 0.62283304\n",
      "Iteration 2378, loss = 0.62270987\n",
      "Iteration 2379, loss = 0.62258647\n",
      "Iteration 2380, loss = 0.62246284\n",
      "Iteration 2381, loss = 0.62233899\n",
      "Iteration 2382, loss = 0.62221490\n",
      "Iteration 2383, loss = 0.62209058\n",
      "Iteration 2384, loss = 0.62196602\n",
      "Iteration 2385, loss = 0.62184124\n",
      "Iteration 2386, loss = 0.62171623\n",
      "Iteration 2387, loss = 0.62159098\n",
      "Iteration 2388, loss = 0.62146551\n",
      "Iteration 2389, loss = 0.62133980\n",
      "Iteration 2390, loss = 0.62121386\n",
      "Iteration 2391, loss = 0.62108768\n",
      "Iteration 2392, loss = 0.62096127\n",
      "Iteration 2393, loss = 0.62083463\n",
      "Iteration 2394, loss = 0.62070776\n",
      "Iteration 2395, loss = 0.62058065\n",
      "Iteration 2396, loss = 0.62045331\n",
      "Iteration 2397, loss = 0.62032573\n",
      "Iteration 2398, loss = 0.62019792\n",
      "Iteration 2399, loss = 0.62006988\n",
      "Iteration 2400, loss = 0.61994160\n",
      "Iteration 2401, loss = 0.61981308\n",
      "Iteration 2402, loss = 0.61968433\n",
      "Iteration 2403, loss = 0.61955535\n",
      "Iteration 2404, loss = 0.61942612\n",
      "Iteration 2405, loss = 0.61929666\n",
      "Iteration 2406, loss = 0.61916697\n",
      "Iteration 2407, loss = 0.61903704\n",
      "Iteration 2408, loss = 0.61890687\n",
      "Iteration 2409, loss = 0.61877646\n",
      "Iteration 2410, loss = 0.61864582\n",
      "Iteration 2411, loss = 0.61851494\n",
      "Iteration 2412, loss = 0.61838382\n",
      "Iteration 2413, loss = 0.61825246\n",
      "Iteration 2414, loss = 0.61812087\n",
      "Iteration 2415, loss = 0.61798904\n",
      "Iteration 2416, loss = 0.61785696\n",
      "Iteration 2417, loss = 0.61772465\n",
      "Iteration 2418, loss = 0.61759210\n",
      "Iteration 2419, loss = 0.61745931\n",
      "Iteration 2420, loss = 0.61732628\n",
      "Iteration 2421, loss = 0.61719301\n",
      "Iteration 2422, loss = 0.61705950\n",
      "Iteration 2423, loss = 0.61692575\n",
      "Iteration 2424, loss = 0.61679176\n",
      "Iteration 2425, loss = 0.61665752\n",
      "Iteration 2426, loss = 0.61652305\n",
      "Iteration 2427, loss = 0.61638834\n",
      "Iteration 2428, loss = 0.61625338\n",
      "Iteration 2429, loss = 0.61611818\n",
      "Iteration 2430, loss = 0.61598274\n",
      "Iteration 2431, loss = 0.61584706\n",
      "Iteration 2432, loss = 0.61571113\n",
      "Iteration 2433, loss = 0.61557496\n",
      "Iteration 2434, loss = 0.61543855\n",
      "Iteration 2435, loss = 0.61530189\n",
      "Iteration 2436, loss = 0.61516500\n",
      "Iteration 2437, loss = 0.61502785\n",
      "Iteration 2438, loss = 0.61489047\n",
      "Iteration 2439, loss = 0.61475284\n",
      "Iteration 2440, loss = 0.61461496\n",
      "Iteration 2441, loss = 0.61447684\n",
      "Iteration 2442, loss = 0.61433848\n",
      "Iteration 2443, loss = 0.61419987\n",
      "Iteration 2444, loss = 0.61406102\n",
      "Iteration 2445, loss = 0.61392192\n",
      "Iteration 2446, loss = 0.61378257\n",
      "Iteration 2447, loss = 0.61364298\n",
      "Iteration 2448, loss = 0.61350314\n",
      "Iteration 2449, loss = 0.61336306\n",
      "Iteration 2450, loss = 0.61322273\n",
      "Iteration 2451, loss = 0.61308215\n",
      "Iteration 2452, loss = 0.61294133\n",
      "Iteration 2453, loss = 0.61280026\n",
      "Iteration 2454, loss = 0.61265894\n",
      "Iteration 2455, loss = 0.61251737\n",
      "Iteration 2456, loss = 0.61237556\n",
      "Iteration 2457, loss = 0.61223350\n",
      "Iteration 2458, loss = 0.61209118\n",
      "Iteration 2459, loss = 0.61194863\n",
      "Iteration 2460, loss = 0.61180582\n",
      "Iteration 2461, loss = 0.61166276\n",
      "Iteration 2462, loss = 0.61151946\n",
      "Iteration 2463, loss = 0.61137590\n",
      "Iteration 2464, loss = 0.61123210\n",
      "Iteration 2465, loss = 0.61108804\n",
      "Iteration 2466, loss = 0.61094374\n",
      "Iteration 2467, loss = 0.61079919\n",
      "Iteration 2468, loss = 0.61065438\n",
      "Iteration 2469, loss = 0.61050933\n",
      "Iteration 2470, loss = 0.61036402\n",
      "Iteration 2471, loss = 0.61021847\n",
      "Iteration 2472, loss = 0.61007266\n",
      "Iteration 2473, loss = 0.60992660\n",
      "Iteration 2474, loss = 0.60978029\n",
      "Iteration 2475, loss = 0.60963373\n",
      "Iteration 2476, loss = 0.60948691\n",
      "Iteration 2477, loss = 0.60933985\n",
      "Iteration 2478, loss = 0.60919253\n",
      "Iteration 2479, loss = 0.60904496\n",
      "Iteration 2480, loss = 0.60889714\n",
      "Iteration 2481, loss = 0.60874906\n",
      "Iteration 2482, loss = 0.60860073\n",
      "Iteration 2483, loss = 0.60845215\n",
      "Iteration 2484, loss = 0.60830331\n",
      "Iteration 2485, loss = 0.60815422\n",
      "Iteration 2486, loss = 0.60800488\n",
      "Iteration 2487, loss = 0.60785528\n",
      "Iteration 2488, loss = 0.60770543\n",
      "Iteration 2489, loss = 0.60755533\n",
      "Iteration 2490, loss = 0.60740497\n",
      "Iteration 2491, loss = 0.60725435\n",
      "Iteration 2492, loss = 0.60710348\n",
      "Iteration 2493, loss = 0.60695236\n",
      "Iteration 2494, loss = 0.60680098\n",
      "Iteration 2495, loss = 0.60664935\n",
      "Iteration 2496, loss = 0.60649746\n",
      "Iteration 2497, loss = 0.60634531\n",
      "Iteration 2498, loss = 0.60619291\n",
      "Iteration 2499, loss = 0.60604025\n",
      "Iteration 2500, loss = 0.60588733\n",
      "Iteration 2501, loss = 0.60573416\n",
      "Iteration 2502, loss = 0.60558074\n",
      "Iteration 2503, loss = 0.60542705\n",
      "Iteration 2504, loss = 0.60527311\n",
      "Iteration 2505, loss = 0.60511891\n",
      "Iteration 2506, loss = 0.60496446\n",
      "Iteration 2507, loss = 0.60480974\n",
      "Iteration 2508, loss = 0.60465477\n",
      "Iteration 2509, loss = 0.60449954\n",
      "Iteration 2510, loss = 0.60434406\n",
      "Iteration 2511, loss = 0.60418831\n",
      "Iteration 2512, loss = 0.60403231\n",
      "Iteration 2513, loss = 0.60387605\n",
      "Iteration 2514, loss = 0.60371953\n",
      "Iteration 2515, loss = 0.60356275\n",
      "Iteration 2516, loss = 0.60340572\n",
      "Iteration 2517, loss = 0.60324842\n",
      "Iteration 2518, loss = 0.60309086\n",
      "Iteration 2519, loss = 0.60293305\n",
      "Iteration 2520, loss = 0.60277498\n",
      "Iteration 2521, loss = 0.60261664\n",
      "Iteration 2522, loss = 0.60245805\n",
      "Iteration 2523, loss = 0.60229919\n",
      "Iteration 2524, loss = 0.60214008\n",
      "Iteration 2525, loss = 0.60198071\n",
      "Iteration 2526, loss = 0.60182107\n",
      "Iteration 2527, loss = 0.60166118\n",
      "Iteration 2528, loss = 0.60150102\n",
      "Iteration 2529, loss = 0.60134061\n",
      "Iteration 2530, loss = 0.60117993\n",
      "Iteration 2531, loss = 0.60101899\n",
      "Iteration 2532, loss = 0.60085779\n",
      "Iteration 2533, loss = 0.60069633\n",
      "Iteration 2534, loss = 0.60053461\n",
      "Iteration 2535, loss = 0.60037263\n",
      "Iteration 2536, loss = 0.60021038\n",
      "Iteration 2537, loss = 0.60004788\n",
      "Iteration 2538, loss = 0.59988511\n",
      "Iteration 2539, loss = 0.59972207\n",
      "Iteration 2540, loss = 0.59955878\n",
      "Iteration 2541, loss = 0.59939523\n",
      "Iteration 2542, loss = 0.59923141\n",
      "Iteration 2543, loss = 0.59906733\n",
      "Iteration 2544, loss = 0.59890298\n",
      "Iteration 2545, loss = 0.59873838\n",
      "Iteration 2546, loss = 0.59857351\n",
      "Iteration 2547, loss = 0.59840837\n",
      "Iteration 2548, loss = 0.59824298\n",
      "Iteration 2549, loss = 0.59807732\n",
      "Iteration 2550, loss = 0.59791139\n",
      "Iteration 2551, loss = 0.59774521\n",
      "Iteration 2552, loss = 0.59757876\n",
      "Iteration 2553, loss = 0.59741204\n",
      "Iteration 2554, loss = 0.59724507\n",
      "Iteration 2555, loss = 0.59707782\n",
      "Iteration 2556, loss = 0.59691032\n",
      "Iteration 2557, loss = 0.59674255\n",
      "Iteration 2558, loss = 0.59657451\n",
      "Iteration 2559, loss = 0.59640621\n",
      "Iteration 2560, loss = 0.59623765\n",
      "Iteration 2561, loss = 0.59606882\n",
      "Iteration 2562, loss = 0.59589973\n",
      "Iteration 2563, loss = 0.59573037\n",
      "Iteration 2564, loss = 0.59556075\n",
      "Iteration 2565, loss = 0.59539086\n",
      "Iteration 2566, loss = 0.59522070\n",
      "Iteration 2567, loss = 0.59505029\n",
      "Iteration 2568, loss = 0.59487960\n",
      "Iteration 2569, loss = 0.59470865\n",
      "Iteration 2570, loss = 0.59453744\n",
      "Iteration 2571, loss = 0.59436596\n",
      "Iteration 2572, loss = 0.59419421\n",
      "Iteration 2573, loss = 0.59402220\n",
      "Iteration 2574, loss = 0.59384992\n",
      "Iteration 2575, loss = 0.59367738\n",
      "Iteration 2576, loss = 0.59350457\n",
      "Iteration 2577, loss = 0.59333149\n",
      "Iteration 2578, loss = 0.59315815\n",
      "Iteration 2579, loss = 0.59298454\n",
      "Iteration 2580, loss = 0.59281066\n",
      "Iteration 2581, loss = 0.59263652\n",
      "Iteration 2582, loss = 0.59246211\n",
      "Iteration 2583, loss = 0.59228744\n",
      "Iteration 2584, loss = 0.59211250\n",
      "Iteration 2585, loss = 0.59193729\n",
      "Iteration 2586, loss = 0.59176181\n",
      "Iteration 2587, loss = 0.59158607\n",
      "Iteration 2588, loss = 0.59141006\n",
      "Iteration 2589, loss = 0.59123379\n",
      "Iteration 2590, loss = 0.59105724\n",
      "Iteration 2591, loss = 0.59088043\n",
      "Iteration 2592, loss = 0.59070336\n",
      "Iteration 2593, loss = 0.59052601\n",
      "Iteration 2594, loss = 0.59034840\n",
      "Iteration 2595, loss = 0.59017052\n",
      "Iteration 2596, loss = 0.58999238\n",
      "Iteration 2597, loss = 0.58981396\n",
      "Iteration 2598, loss = 0.58963528\n",
      "Iteration 2599, loss = 0.58945633\n",
      "Iteration 2600, loss = 0.58927711\n",
      "Iteration 2601, loss = 0.58909763\n",
      "Iteration 2602, loss = 0.58891788\n",
      "Iteration 2603, loss = 0.58873786\n",
      "Iteration 2604, loss = 0.58855757\n",
      "Iteration 2605, loss = 0.58837701\n",
      "Iteration 2606, loss = 0.58819619\n",
      "Iteration 2607, loss = 0.58801510\n",
      "Iteration 2608, loss = 0.58783374\n",
      "Iteration 2609, loss = 0.58765211\n",
      "Iteration 2610, loss = 0.58747021\n",
      "Iteration 2611, loss = 0.58728805\n",
      "Iteration 2612, loss = 0.58710562\n",
      "Iteration 2613, loss = 0.58692292\n",
      "Iteration 2614, loss = 0.58673995\n",
      "Iteration 2615, loss = 0.58655671\n",
      "Iteration 2616, loss = 0.58637321\n",
      "Iteration 2617, loss = 0.58618943\n",
      "Iteration 2618, loss = 0.58600539\n",
      "Iteration 2619, loss = 0.58582108\n",
      "Iteration 2620, loss = 0.58563651\n",
      "Iteration 2621, loss = 0.58545166\n",
      "Iteration 2622, loss = 0.58526654\n",
      "Iteration 2623, loss = 0.58508116\n",
      "Iteration 2624, loss = 0.58489551\n",
      "Iteration 2625, loss = 0.58470959\n",
      "Iteration 2626, loss = 0.58452340\n",
      "Iteration 2627, loss = 0.58433695\n",
      "Iteration 2628, loss = 0.58415022\n",
      "Iteration 2629, loss = 0.58396323\n",
      "Iteration 2630, loss = 0.58377596\n",
      "Iteration 2631, loss = 0.58358843\n",
      "Iteration 2632, loss = 0.58340064\n",
      "Iteration 2633, loss = 0.58321257\n",
      "Iteration 2634, loss = 0.58302423\n",
      "Iteration 2635, loss = 0.58283563\n",
      "Iteration 2636, loss = 0.58264676\n",
      "Iteration 2637, loss = 0.58245761\n",
      "Iteration 2638, loss = 0.58226820\n",
      "Iteration 2639, loss = 0.58207853\n",
      "Iteration 2640, loss = 0.58188858\n",
      "Iteration 2641, loss = 0.58169837\n",
      "Iteration 2642, loss = 0.58150788\n",
      "Iteration 2643, loss = 0.58131713\n",
      "Iteration 2644, loss = 0.58112611\n",
      "Iteration 2645, loss = 0.58093482\n",
      "Iteration 2646, loss = 0.58074327\n",
      "Iteration 2647, loss = 0.58055144\n",
      "Iteration 2648, loss = 0.58035935\n",
      "Iteration 2649, loss = 0.58016699\n",
      "Iteration 2650, loss = 0.57997436\n",
      "Iteration 2651, loss = 0.57978146\n",
      "Iteration 2652, loss = 0.57958830\n",
      "Iteration 2653, loss = 0.57939486\n",
      "Iteration 2654, loss = 0.57920116\n",
      "Iteration 2655, loss = 0.57900719\n",
      "Iteration 2656, loss = 0.57881295\n",
      "Iteration 2657, loss = 0.57861845\n",
      "Iteration 2658, loss = 0.57842367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2659, loss = 0.57822863\n",
      "Iteration 2660, loss = 0.57803332\n",
      "Iteration 2661, loss = 0.57783775\n",
      "Iteration 2662, loss = 0.57764190\n",
      "Iteration 2663, loss = 0.57744579\n",
      "Iteration 2664, loss = 0.57724941\n",
      "Iteration 2665, loss = 0.57705276\n",
      "Iteration 2666, loss = 0.57685585\n",
      "Iteration 2667, loss = 0.57665866\n",
      "Iteration 2668, loss = 0.57646121\n",
      "Iteration 2669, loss = 0.57626350\n",
      "Iteration 2670, loss = 0.57606551\n",
      "Iteration 2671, loss = 0.57586726\n",
      "Iteration 2672, loss = 0.57566874\n",
      "Iteration 2673, loss = 0.57546995\n",
      "Iteration 2674, loss = 0.57527090\n",
      "Iteration 2675, loss = 0.57507158\n",
      "Iteration 2676, loss = 0.57487199\n",
      "Iteration 2677, loss = 0.57467214\n",
      "Iteration 2678, loss = 0.57447202\n",
      "Iteration 2679, loss = 0.57427163\n",
      "Iteration 2680, loss = 0.57407098\n",
      "Iteration 2681, loss = 0.57387006\n",
      "Iteration 2682, loss = 0.57366887\n",
      "Iteration 2683, loss = 0.57346742\n",
      "Iteration 2684, loss = 0.57326570\n",
      "Iteration 2685, loss = 0.57306371\n",
      "Iteration 2686, loss = 0.57286146\n",
      "Iteration 2687, loss = 0.57265894\n",
      "Iteration 2688, loss = 0.57245616\n",
      "Iteration 2689, loss = 0.57225311\n",
      "Iteration 2690, loss = 0.57204980\n",
      "Iteration 2691, loss = 0.57184622\n",
      "Iteration 2692, loss = 0.57164237\n",
      "Iteration 2693, loss = 0.57143826\n",
      "Iteration 2694, loss = 0.57123388\n",
      "Iteration 2695, loss = 0.57102924\n",
      "Iteration 2696, loss = 0.57082433\n",
      "Iteration 2697, loss = 0.57061916\n",
      "Iteration 2698, loss = 0.57041373\n",
      "Iteration 2699, loss = 0.57020802\n",
      "Iteration 2700, loss = 0.57000206\n",
      "Iteration 2701, loss = 0.56979583\n",
      "Iteration 2702, loss = 0.56958933\n",
      "Iteration 2703, loss = 0.56938257\n",
      "Iteration 2704, loss = 0.56917555\n",
      "Iteration 2705, loss = 0.56896826\n",
      "Iteration 2706, loss = 0.56876071\n",
      "Iteration 2707, loss = 0.56855290\n",
      "Iteration 2708, loss = 0.56834482\n",
      "Iteration 2709, loss = 0.56813648\n",
      "Iteration 2710, loss = 0.56792787\n",
      "Iteration 2711, loss = 0.56771900\n",
      "Iteration 2712, loss = 0.56750987\n",
      "Iteration 2713, loss = 0.56730047\n",
      "Iteration 2714, loss = 0.56709082\n",
      "Iteration 2715, loss = 0.56688090\n",
      "Iteration 2716, loss = 0.56667071\n",
      "Iteration 2717, loss = 0.56646027\n",
      "Iteration 2718, loss = 0.56624956\n",
      "Iteration 2719, loss = 0.56603859\n",
      "Iteration 2720, loss = 0.56582736\n",
      "Iteration 2721, loss = 0.56561587\n",
      "Iteration 2722, loss = 0.56540411\n",
      "Iteration 2723, loss = 0.56519209\n",
      "Iteration 2724, loss = 0.56497982\n",
      "Iteration 2725, loss = 0.56476728\n",
      "Iteration 2726, loss = 0.56455448\n",
      "Iteration 2727, loss = 0.56434142\n",
      "Iteration 2728, loss = 0.56412809\n",
      "Iteration 2729, loss = 0.56391451\n",
      "Iteration 2730, loss = 0.56370067\n",
      "Iteration 2731, loss = 0.56348657\n",
      "Iteration 2732, loss = 0.56327220\n",
      "Iteration 2733, loss = 0.56305758\n",
      "Iteration 2734, loss = 0.56284270\n",
      "Iteration 2735, loss = 0.56262756\n",
      "Iteration 2736, loss = 0.56241216\n",
      "Iteration 2737, loss = 0.56219650\n",
      "Iteration 2738, loss = 0.56198058\n",
      "Iteration 2739, loss = 0.56176440\n",
      "Iteration 2740, loss = 0.56154796\n",
      "Iteration 2741, loss = 0.56133127\n",
      "Iteration 2742, loss = 0.56111432\n",
      "Iteration 2743, loss = 0.56089711\n",
      "Iteration 2744, loss = 0.56067964\n",
      "Iteration 2745, loss = 0.56046191\n",
      "Iteration 2746, loss = 0.56024393\n",
      "Iteration 2747, loss = 0.56002569\n",
      "Iteration 2748, loss = 0.55980719\n",
      "Iteration 2749, loss = 0.55958844\n",
      "Iteration 2750, loss = 0.55936943\n",
      "Iteration 2751, loss = 0.55915016\n",
      "Iteration 2752, loss = 0.55893064\n",
      "Iteration 2753, loss = 0.55871086\n",
      "Iteration 2754, loss = 0.55849082\n",
      "Iteration 2755, loss = 0.55827053\n",
      "Iteration 2756, loss = 0.55804999\n",
      "Iteration 2757, loss = 0.55782918\n",
      "Iteration 2758, loss = 0.55760813\n",
      "Iteration 2759, loss = 0.55738682\n",
      "Iteration 2760, loss = 0.55716525\n",
      "Iteration 2761, loss = 0.55694344\n",
      "Iteration 2762, loss = 0.55672136\n",
      "Iteration 2763, loss = 0.55649904\n",
      "Iteration 2764, loss = 0.55627646\n",
      "Iteration 2765, loss = 0.55605363\n",
      "Iteration 2766, loss = 0.55583054\n",
      "Iteration 2767, loss = 0.55560720\n",
      "Iteration 2768, loss = 0.55538361\n",
      "Iteration 2769, loss = 0.55515977\n",
      "Iteration 2770, loss = 0.55493567\n",
      "Iteration 2771, loss = 0.55471132\n",
      "Iteration 2772, loss = 0.55448673\n",
      "Iteration 2773, loss = 0.55426188\n",
      "Iteration 2774, loss = 0.55403678\n",
      "Iteration 2775, loss = 0.55381142\n",
      "Iteration 2776, loss = 0.55358582\n",
      "Iteration 2777, loss = 0.55335997\n",
      "Iteration 2778, loss = 0.55313387\n",
      "Iteration 2779, loss = 0.55290752\n",
      "Iteration 2780, loss = 0.55268091\n",
      "Iteration 2781, loss = 0.55245406\n",
      "Iteration 2782, loss = 0.55222697\n",
      "Iteration 2783, loss = 0.55199962\n",
      "Iteration 2784, loss = 0.55177202\n",
      "Iteration 2785, loss = 0.55154418\n",
      "Iteration 2786, loss = 0.55131608\n",
      "Iteration 2787, loss = 0.55108774\n",
      "Iteration 2788, loss = 0.55085916\n",
      "Iteration 2789, loss = 0.55063032\n",
      "Iteration 2790, loss = 0.55040124\n",
      "Iteration 2791, loss = 0.55017192\n",
      "Iteration 2792, loss = 0.54994234\n",
      "Iteration 2793, loss = 0.54971252\n",
      "Iteration 2794, loss = 0.54948246\n",
      "Iteration 2795, loss = 0.54925215\n",
      "Iteration 2796, loss = 0.54902160\n",
      "Iteration 2797, loss = 0.54879080\n",
      "Iteration 2798, loss = 0.54855975\n",
      "Iteration 2799, loss = 0.54832847\n",
      "Iteration 2800, loss = 0.54809693\n",
      "Iteration 2801, loss = 0.54786516\n",
      "Iteration 2802, loss = 0.54763314\n",
      "Iteration 2803, loss = 0.54740088\n",
      "Iteration 2804, loss = 0.54716838\n",
      "Iteration 2805, loss = 0.54693563\n",
      "Iteration 2806, loss = 0.54670265\n",
      "Iteration 2807, loss = 0.54646942\n",
      "Iteration 2808, loss = 0.54623595\n",
      "Iteration 2809, loss = 0.54600224\n",
      "Iteration 2810, loss = 0.54576829\n",
      "Iteration 2811, loss = 0.54553409\n",
      "Iteration 2812, loss = 0.54529966\n",
      "Iteration 2813, loss = 0.54506499\n",
      "Iteration 2814, loss = 0.54483008\n",
      "Iteration 2815, loss = 0.54459493\n",
      "Iteration 2816, loss = 0.54435954\n",
      "Iteration 2817, loss = 0.54412392\n",
      "Iteration 2818, loss = 0.54388805\n",
      "Iteration 2819, loss = 0.54365195\n",
      "Iteration 2820, loss = 0.54341561\n",
      "Iteration 2821, loss = 0.54317904\n",
      "Iteration 2822, loss = 0.54294223\n",
      "Iteration 2823, loss = 0.54270518\n",
      "Iteration 2824, loss = 0.54246789\n",
      "Iteration 2825, loss = 0.54223037\n",
      "Iteration 2826, loss = 0.54199262\n",
      "Iteration 2827, loss = 0.54175463\n",
      "Iteration 2828, loss = 0.54151640\n",
      "Iteration 2829, loss = 0.54127795\n",
      "Iteration 2830, loss = 0.54103925\n",
      "Iteration 2831, loss = 0.54080033\n",
      "Iteration 2832, loss = 0.54056117\n",
      "Iteration 2833, loss = 0.54032178\n",
      "Iteration 2834, loss = 0.54008216\n",
      "Iteration 2835, loss = 0.53984230\n",
      "Iteration 2836, loss = 0.53960221\n",
      "Iteration 2837, loss = 0.53936190\n",
      "Iteration 2838, loss = 0.53912135\n",
      "Iteration 2839, loss = 0.53888057\n",
      "Iteration 2840, loss = 0.53863956\n",
      "Iteration 2841, loss = 0.53839832\n",
      "Iteration 2842, loss = 0.53815685\n",
      "Iteration 2843, loss = 0.53791516\n",
      "Iteration 2844, loss = 0.53767323\n",
      "Iteration 2845, loss = 0.53743108\n",
      "Iteration 2846, loss = 0.53718870\n",
      "Iteration 2847, loss = 0.53694609\n",
      "Iteration 2848, loss = 0.53670326\n",
      "Iteration 2849, loss = 0.53646019\n",
      "Iteration 2850, loss = 0.53621691\n",
      "Iteration 2851, loss = 0.53597339\n",
      "Iteration 2852, loss = 0.53572965\n",
      "Iteration 2853, loss = 0.53548569\n",
      "Iteration 2854, loss = 0.53524150\n",
      "Iteration 2855, loss = 0.53499709\n",
      "Iteration 2856, loss = 0.53475246\n",
      "Iteration 2857, loss = 0.53450760\n",
      "Iteration 2858, loss = 0.53426252\n",
      "Iteration 2859, loss = 0.53401721\n",
      "Iteration 2860, loss = 0.53377169\n",
      "Iteration 2861, loss = 0.53352594\n",
      "Iteration 2862, loss = 0.53327997\n",
      "Iteration 2863, loss = 0.53303378\n",
      "Iteration 2864, loss = 0.53278737\n",
      "Iteration 2865, loss = 0.53254074\n",
      "Iteration 2866, loss = 0.53229389\n",
      "Iteration 2867, loss = 0.53204682\n",
      "Iteration 2868, loss = 0.53179954\n",
      "Iteration 2869, loss = 0.53155203\n",
      "Iteration 2870, loss = 0.53130431\n",
      "Iteration 2871, loss = 0.53105637\n",
      "Iteration 2872, loss = 0.53080821\n",
      "Iteration 2873, loss = 0.53055984\n",
      "Iteration 2874, loss = 0.53031125\n",
      "Iteration 2875, loss = 0.53006244\n",
      "Iteration 2876, loss = 0.52981342\n",
      "Iteration 2877, loss = 0.52956419\n",
      "Iteration 2878, loss = 0.52931474\n",
      "Iteration 2879, loss = 0.52906508\n",
      "Iteration 2880, loss = 0.52881520\n",
      "Iteration 2881, loss = 0.52856512\n",
      "Iteration 2882, loss = 0.52831481\n",
      "Iteration 2883, loss = 0.52806430\n",
      "Iteration 2884, loss = 0.52781358\n",
      "Iteration 2885, loss = 0.52756264\n",
      "Iteration 2886, loss = 0.52731150\n",
      "Iteration 2887, loss = 0.52706014\n",
      "Iteration 2888, loss = 0.52680858\n",
      "Iteration 2889, loss = 0.52655680\n",
      "Iteration 2890, loss = 0.52630482\n",
      "Iteration 2891, loss = 0.52605263\n",
      "Iteration 2892, loss = 0.52580023\n",
      "Iteration 2893, loss = 0.52554762\n",
      "Iteration 2894, loss = 0.52529481\n",
      "Iteration 2895, loss = 0.52504179\n",
      "Iteration 2896, loss = 0.52478857\n",
      "Iteration 2897, loss = 0.52453514\n",
      "Iteration 2898, loss = 0.52428150\n",
      "Iteration 2899, loss = 0.52402766\n",
      "Iteration 2900, loss = 0.52377362\n",
      "Iteration 2901, loss = 0.52351937\n",
      "Iteration 2902, loss = 0.52326493\n",
      "Iteration 2903, loss = 0.52301027\n",
      "Iteration 2904, loss = 0.52275542\n",
      "Iteration 2905, loss = 0.52250037\n",
      "Iteration 2906, loss = 0.52224511\n",
      "Iteration 2907, loss = 0.52198966\n",
      "Iteration 2908, loss = 0.52173400\n",
      "Iteration 2909, loss = 0.52147814\n",
      "Iteration 2910, loss = 0.52122209\n",
      "Iteration 2911, loss = 0.52096584\n",
      "Iteration 2912, loss = 0.52070939\n",
      "Iteration 2913, loss = 0.52045274\n",
      "Iteration 2914, loss = 0.52019590\n",
      "Iteration 2915, loss = 0.51993886\n",
      "Iteration 2916, loss = 0.51968162\n",
      "Iteration 2917, loss = 0.51942419\n",
      "Iteration 2918, loss = 0.51916656\n",
      "Iteration 2919, loss = 0.51890874\n",
      "Iteration 2920, loss = 0.51865073\n",
      "Iteration 2921, loss = 0.51839252\n",
      "Iteration 2922, loss = 0.51813412\n",
      "Iteration 2923, loss = 0.51787553\n",
      "Iteration 2924, loss = 0.51761674\n",
      "Iteration 2925, loss = 0.51735777\n",
      "Iteration 2926, loss = 0.51709860\n",
      "Iteration 2927, loss = 0.51683925\n",
      "Iteration 2928, loss = 0.51657970\n",
      "Iteration 2929, loss = 0.51631997\n",
      "Iteration 2930, loss = 0.51606005\n",
      "Iteration 2931, loss = 0.51579994\n",
      "Iteration 2932, loss = 0.51553964\n",
      "Iteration 2933, loss = 0.51527916\n",
      "Iteration 2934, loss = 0.51501849\n",
      "Iteration 2935, loss = 0.51475763\n",
      "Iteration 2936, loss = 0.51449659\n",
      "Iteration 2937, loss = 0.51423536\n",
      "Iteration 2938, loss = 0.51397395\n",
      "Iteration 2939, loss = 0.51371236\n",
      "Iteration 2940, loss = 0.51345058\n",
      "Iteration 2941, loss = 0.51318862\n",
      "Iteration 2942, loss = 0.51292648\n",
      "Iteration 2943, loss = 0.51266416\n",
      "Iteration 2944, loss = 0.51240166\n",
      "Iteration 2945, loss = 0.51213898\n",
      "Iteration 2946, loss = 0.51187611\n",
      "Iteration 2947, loss = 0.51161307\n",
      "Iteration 2948, loss = 0.51134985\n",
      "Iteration 2949, loss = 0.51108646\n",
      "Iteration 2950, loss = 0.51082288\n",
      "Iteration 2951, loss = 0.51055913\n",
      "Iteration 2952, loss = 0.51029520\n",
      "Iteration 2953, loss = 0.51003110\n",
      "Iteration 2954, loss = 0.50976682\n",
      "Iteration 2955, loss = 0.50950237\n",
      "Iteration 2956, loss = 0.50923774\n",
      "Iteration 2957, loss = 0.50897294\n",
      "Iteration 2958, loss = 0.50870797\n",
      "Iteration 2959, loss = 0.50844282\n",
      "Iteration 2960, loss = 0.50817750\n",
      "Iteration 2961, loss = 0.50791202\n",
      "Iteration 2962, loss = 0.50764636\n",
      "Iteration 2963, loss = 0.50738053\n",
      "Iteration 2964, loss = 0.50711453\n",
      "Iteration 2965, loss = 0.50684837\n",
      "Iteration 2966, loss = 0.50658203\n",
      "Iteration 2967, loss = 0.50631553\n",
      "Iteration 2968, loss = 0.50604886\n",
      "Iteration 2969, loss = 0.50578203\n",
      "Iteration 2970, loss = 0.50551503\n",
      "Iteration 2971, loss = 0.50524786\n",
      "Iteration 2972, loss = 0.50498053\n",
      "Iteration 2973, loss = 0.50471304\n",
      "Iteration 2974, loss = 0.50444538\n",
      "Iteration 2975, loss = 0.50417756\n",
      "Iteration 2976, loss = 0.50390958\n",
      "Iteration 2977, loss = 0.50364144\n",
      "Iteration 2978, loss = 0.50337313\n",
      "Iteration 2979, loss = 0.50310467\n",
      "Iteration 2980, loss = 0.50283605\n",
      "Iteration 2981, loss = 0.50256726\n",
      "Iteration 2982, loss = 0.50229832\n",
      "Iteration 2983, loss = 0.50202922\n",
      "Iteration 2984, loss = 0.50175997\n",
      "Iteration 2985, loss = 0.50149055\n",
      "Iteration 2986, loss = 0.50122098\n",
      "Iteration 2987, loss = 0.50095126\n",
      "Iteration 2988, loss = 0.50068138\n",
      "Iteration 2989, loss = 0.50041135\n",
      "Iteration 2990, loss = 0.50014116\n",
      "Iteration 2991, loss = 0.49987082\n",
      "Iteration 2992, loss = 0.49960033\n",
      "Iteration 2993, loss = 0.49932968\n",
      "Iteration 2994, loss = 0.49905889\n",
      "Iteration 2995, loss = 0.49878794\n",
      "Iteration 2996, loss = 0.49851685\n",
      "Iteration 2997, loss = 0.49824560\n",
      "Iteration 2998, loss = 0.49797421\n",
      "Iteration 2999, loss = 0.49770267\n",
      "Iteration 3000, loss = 0.49743098\n",
      "Iteration 3001, loss = 0.49715915\n",
      "Iteration 3002, loss = 0.49688717\n",
      "Iteration 3003, loss = 0.49661504\n",
      "Iteration 3004, loss = 0.49634277\n",
      "Iteration 3005, loss = 0.49607036\n",
      "Iteration 3006, loss = 0.49579780\n",
      "Iteration 3007, loss = 0.49552510\n",
      "Iteration 3008, loss = 0.49525226\n",
      "Iteration 3009, loss = 0.49497928\n",
      "Iteration 3010, loss = 0.49470615\n",
      "Iteration 3011, loss = 0.49443288\n",
      "Iteration 3012, loss = 0.49415948\n",
      "Iteration 3013, loss = 0.49388594\n",
      "Iteration 3014, loss = 0.49361225\n",
      "Iteration 3015, loss = 0.49333843\n",
      "Iteration 3016, loss = 0.49306448\n",
      "Iteration 3017, loss = 0.49279039\n",
      "Iteration 3018, loss = 0.49251616\n",
      "Iteration 3019, loss = 0.49224179\n",
      "Iteration 3020, loss = 0.49196729\n",
      "Iteration 3021, loss = 0.49169266\n",
      "Iteration 3022, loss = 0.49141790\n",
      "Iteration 3023, loss = 0.49114300\n",
      "Iteration 3024, loss = 0.49086797\n",
      "Iteration 3025, loss = 0.49059281\n",
      "Iteration 3026, loss = 0.49031752\n",
      "Iteration 3027, loss = 0.49004210\n",
      "Iteration 3028, loss = 0.48976655\n",
      "Iteration 3029, loss = 0.48949087\n",
      "Iteration 3030, loss = 0.48921507\n",
      "Iteration 3031, loss = 0.48893913\n",
      "Iteration 3032, loss = 0.48866308\n",
      "Iteration 3033, loss = 0.48838689\n",
      "Iteration 3034, loss = 0.48811058\n",
      "Iteration 3035, loss = 0.48783414\n",
      "Iteration 3036, loss = 0.48755759\n",
      "Iteration 3037, loss = 0.48728090\n",
      "Iteration 3038, loss = 0.48700410\n",
      "Iteration 3039, loss = 0.48672717\n",
      "Iteration 3040, loss = 0.48645013\n",
      "Iteration 3041, loss = 0.48617296\n",
      "Iteration 3042, loss = 0.48589567\n",
      "Iteration 3043, loss = 0.48561827\n",
      "Iteration 3044, loss = 0.48534074\n",
      "Iteration 3045, loss = 0.48506310\n",
      "Iteration 3046, loss = 0.48478534\n",
      "Iteration 3047, loss = 0.48450746\n",
      "Iteration 3048, loss = 0.48422947\n",
      "Iteration 3049, loss = 0.48395136\n",
      "Iteration 3050, loss = 0.48367314\n",
      "Iteration 3051, loss = 0.48339481\n",
      "Iteration 3052, loss = 0.48311636\n",
      "Iteration 3053, loss = 0.48283780\n",
      "Iteration 3054, loss = 0.48255912\n",
      "Iteration 3055, loss = 0.48228034\n",
      "Iteration 3056, loss = 0.48200145\n",
      "Iteration 3057, loss = 0.48172244\n",
      "Iteration 3058, loss = 0.48144333\n",
      "Iteration 3059, loss = 0.48116411\n",
      "Iteration 3060, loss = 0.48088478\n",
      "Iteration 3061, loss = 0.48060535\n",
      "Iteration 3062, loss = 0.48032581\n",
      "Iteration 3063, loss = 0.48004616\n",
      "Iteration 3064, loss = 0.47976641\n",
      "Iteration 3065, loss = 0.47948655\n",
      "Iteration 3066, loss = 0.47920660\n",
      "Iteration 3067, loss = 0.47892653\n",
      "Iteration 3068, loss = 0.47864637\n",
      "Iteration 3069, loss = 0.47836611\n",
      "Iteration 3070, loss = 0.47808574\n",
      "Iteration 3071, loss = 0.47780527\n",
      "Iteration 3072, loss = 0.47752471\n",
      "Iteration 3073, loss = 0.47724404\n",
      "Iteration 3074, loss = 0.47696328\n",
      "Iteration 3075, loss = 0.47668242\n",
      "Iteration 3076, loss = 0.47640147\n",
      "Iteration 3077, loss = 0.47612042\n",
      "Iteration 3078, loss = 0.47583927\n",
      "Iteration 3079, loss = 0.47555803\n",
      "Iteration 3080, loss = 0.47527669\n",
      "Iteration 3081, loss = 0.47499527\n",
      "Iteration 3082, loss = 0.47471375\n",
      "Iteration 3083, loss = 0.47443213\n",
      "Iteration 3084, loss = 0.47415043\n",
      "Iteration 3085, loss = 0.47386864\n",
      "Iteration 3086, loss = 0.47358676\n",
      "Iteration 3087, loss = 0.47330479\n",
      "Iteration 3088, loss = 0.47302273\n",
      "Iteration 3089, loss = 0.47274058\n",
      "Iteration 3090, loss = 0.47245834\n",
      "Iteration 3091, loss = 0.47217602\n",
      "Iteration 3092, loss = 0.47189362\n",
      "Iteration 3093, loss = 0.47161113\n",
      "Iteration 3094, loss = 0.47132855\n",
      "Iteration 3095, loss = 0.47104590\n",
      "Iteration 3096, loss = 0.47076316\n",
      "Iteration 3097, loss = 0.47048034\n",
      "Iteration 3098, loss = 0.47019743\n",
      "Iteration 3099, loss = 0.46991445\n",
      "Iteration 3100, loss = 0.46963139\n",
      "Iteration 3101, loss = 0.46934824\n",
      "Iteration 3102, loss = 0.46906502\n",
      "Iteration 3103, loss = 0.46878173\n",
      "Iteration 3104, loss = 0.46849835\n",
      "Iteration 3105, loss = 0.46821490\n",
      "Iteration 3106, loss = 0.46793137\n",
      "Iteration 3107, loss = 0.46764777\n",
      "Iteration 3108, loss = 0.46736409\n",
      "Iteration 3109, loss = 0.46708035\n",
      "Iteration 3110, loss = 0.46679652\n",
      "Iteration 3111, loss = 0.46651263\n",
      "Iteration 3112, loss = 0.46622866\n",
      "Iteration 3113, loss = 0.46594463\n",
      "Iteration 3114, loss = 0.46566052\n",
      "Iteration 3115, loss = 0.46537635\n",
      "Iteration 3116, loss = 0.46509210\n",
      "Iteration 3117, loss = 0.46480779\n",
      "Iteration 3118, loss = 0.46452342\n",
      "Iteration 3119, loss = 0.46423897\n",
      "Iteration 3120, loss = 0.46395446\n",
      "Iteration 3121, loss = 0.46366989\n",
      "Iteration 3122, loss = 0.46338525\n",
      "Iteration 3123, loss = 0.46310054\n",
      "Iteration 3124, loss = 0.46281578\n",
      "Iteration 3125, loss = 0.46253095\n",
      "Iteration 3126, loss = 0.46224606\n",
      "Iteration 3127, loss = 0.46196111\n",
      "Iteration 3128, loss = 0.46167610\n",
      "Iteration 3129, loss = 0.46139103\n",
      "Iteration 3130, loss = 0.46110590\n",
      "Iteration 3131, loss = 0.46082072\n",
      "Iteration 3132, loss = 0.46053548\n",
      "Iteration 3133, loss = 0.46025018\n",
      "Iteration 3134, loss = 0.45996482\n",
      "Iteration 3135, loss = 0.45967941\n",
      "Iteration 3136, loss = 0.45939395\n",
      "Iteration 3137, loss = 0.45910843\n",
      "Iteration 3138, loss = 0.45882286\n",
      "Iteration 3139, loss = 0.45853724\n",
      "Iteration 3140, loss = 0.45825156\n",
      "Iteration 3141, loss = 0.45796584\n",
      "Iteration 3142, loss = 0.45768006\n",
      "Iteration 3143, loss = 0.45739424\n",
      "Iteration 3144, loss = 0.45710836\n",
      "Iteration 3145, loss = 0.45682244\n",
      "Iteration 3146, loss = 0.45653647\n",
      "Iteration 3147, loss = 0.45625046\n",
      "Iteration 3148, loss = 0.45596440\n",
      "Iteration 3149, loss = 0.45567829\n",
      "Iteration 3150, loss = 0.45539214\n",
      "Iteration 3151, loss = 0.45510595\n",
      "Iteration 3152, loss = 0.45481971\n",
      "Iteration 3153, loss = 0.45453343\n",
      "Iteration 3154, loss = 0.45424711\n",
      "Iteration 3155, loss = 0.45396075\n",
      "Iteration 3156, loss = 0.45367434\n",
      "Iteration 3157, loss = 0.45338790\n",
      "Iteration 3158, loss = 0.45310142\n",
      "Iteration 3159, loss = 0.45281490\n",
      "Iteration 3160, loss = 0.45252835\n",
      "Iteration 3161, loss = 0.45224176\n",
      "Iteration 3162, loss = 0.45195513\n",
      "Iteration 3163, loss = 0.45166846\n",
      "Iteration 3164, loss = 0.45138177\n",
      "Iteration 3165, loss = 0.45109503\n",
      "Iteration 3166, loss = 0.45080827\n",
      "Iteration 3167, loss = 0.45052147\n",
      "Iteration 3168, loss = 0.45023464\n",
      "Iteration 3169, loss = 0.44994778\n",
      "Iteration 3170, loss = 0.44966089\n",
      "Iteration 3171, loss = 0.44937397\n",
      "Iteration 3172, loss = 0.44908702\n",
      "Iteration 3173, loss = 0.44880005\n",
      "Iteration 3174, loss = 0.44851304\n",
      "Iteration 3175, loss = 0.44822601\n",
      "Iteration 3176, loss = 0.44793895\n",
      "Iteration 3177, loss = 0.44765187\n",
      "Iteration 3178, loss = 0.44736476\n",
      "Iteration 3179, loss = 0.44707763\n",
      "Iteration 3180, loss = 0.44679047\n",
      "Iteration 3181, loss = 0.44650330\n",
      "Iteration 3182, loss = 0.44621610\n",
      "Iteration 3183, loss = 0.44592888\n",
      "Iteration 3184, loss = 0.44564163\n",
      "Iteration 3185, loss = 0.44535437\n",
      "Iteration 3186, loss = 0.44506709\n",
      "Iteration 3187, loss = 0.44477979\n",
      "Iteration 3188, loss = 0.44449248\n",
      "Iteration 3189, loss = 0.44420514\n",
      "Iteration 3190, loss = 0.44391779\n",
      "Iteration 3191, loss = 0.44363043\n",
      "Iteration 3192, loss = 0.44334305\n",
      "Iteration 3193, loss = 0.44305565\n",
      "Iteration 3194, loss = 0.44276824\n",
      "Iteration 3195, loss = 0.44248082\n",
      "Iteration 3196, loss = 0.44219339\n",
      "Iteration 3197, loss = 0.44190594\n",
      "Iteration 3198, loss = 0.44161849\n",
      "Iteration 3199, loss = 0.44133102\n",
      "Iteration 3200, loss = 0.44104355\n",
      "Iteration 3201, loss = 0.44075606\n",
      "Iteration 3202, loss = 0.44046857\n",
      "Iteration 3203, loss = 0.44018107\n",
      "Iteration 3204, loss = 0.43989357\n",
      "Iteration 3205, loss = 0.43960605\n",
      "Iteration 3206, loss = 0.43931854\n",
      "Iteration 3207, loss = 0.43903101\n",
      "Iteration 3208, loss = 0.43874349\n",
      "Iteration 3209, loss = 0.43845596\n",
      "Iteration 3210, loss = 0.43816843\n",
      "Iteration 3211, loss = 0.43788089\n",
      "Iteration 3212, loss = 0.43759336\n",
      "Iteration 3213, loss = 0.43730582\n",
      "Iteration 3214, loss = 0.43701829\n",
      "Iteration 3215, loss = 0.43673075\n",
      "Iteration 3216, loss = 0.43644322\n",
      "Iteration 3217, loss = 0.43615569\n",
      "Iteration 3218, loss = 0.43586816\n",
      "Iteration 3219, loss = 0.43558064\n",
      "Iteration 3220, loss = 0.43529312\n",
      "Iteration 3221, loss = 0.43500560\n",
      "Iteration 3222, loss = 0.43471810\n",
      "Iteration 3223, loss = 0.43443059\n",
      "Iteration 3224, loss = 0.43414310\n",
      "Iteration 3225, loss = 0.43385561\n",
      "Iteration 3226, loss = 0.43356813\n",
      "Iteration 3227, loss = 0.43328066\n",
      "Iteration 3228, loss = 0.43299319\n",
      "Iteration 3229, loss = 0.43270574\n",
      "Iteration 3230, loss = 0.43241830\n",
      "Iteration 3231, loss = 0.43213087\n",
      "Iteration 3232, loss = 0.43184346\n",
      "Iteration 3233, loss = 0.43155605\n",
      "Iteration 3234, loss = 0.43126866\n",
      "Iteration 3235, loss = 0.43098129\n",
      "Iteration 3236, loss = 0.43069393\n",
      "Iteration 3237, loss = 0.43040658\n",
      "Iteration 3238, loss = 0.43011925\n",
      "Iteration 3239, loss = 0.42983194\n",
      "Iteration 3240, loss = 0.42954465\n",
      "Iteration 3241, loss = 0.42925737\n",
      "Iteration 3242, loss = 0.42897011\n",
      "Iteration 3243, loss = 0.42868288\n",
      "Iteration 3244, loss = 0.42839566\n",
      "Iteration 3245, loss = 0.42810846\n",
      "Iteration 3246, loss = 0.42782129\n",
      "Iteration 3247, loss = 0.42753414\n",
      "Iteration 3248, loss = 0.42724701\n",
      "Iteration 3249, loss = 0.42695990\n",
      "Iteration 3250, loss = 0.42667282\n",
      "Iteration 3251, loss = 0.42638576\n",
      "Iteration 3252, loss = 0.42609873\n",
      "Iteration 3253, loss = 0.42581173\n",
      "Iteration 3254, loss = 0.42552475\n",
      "Iteration 3255, loss = 0.42523780\n",
      "Iteration 3256, loss = 0.42495087\n",
      "Iteration 3257, loss = 0.42466398\n",
      "Iteration 3258, loss = 0.42437711\n",
      "Iteration 3259, loss = 0.42409028\n",
      "Iteration 3260, loss = 0.42380348\n",
      "Iteration 3261, loss = 0.42351670\n",
      "Iteration 3262, loss = 0.42322996\n",
      "Iteration 3263, loss = 0.42294325\n",
      "Iteration 3264, loss = 0.42265658\n",
      "Iteration 3265, loss = 0.42236994\n",
      "Iteration 3266, loss = 0.42208333\n",
      "Iteration 3267, loss = 0.42179676\n",
      "Iteration 3268, loss = 0.42151022\n",
      "Iteration 3269, loss = 0.42122372\n",
      "Iteration 3270, loss = 0.42093726\n",
      "Iteration 3271, loss = 0.42065084\n",
      "Iteration 3272, loss = 0.42036445\n",
      "Iteration 3273, loss = 0.42007810\n",
      "Iteration 3274, loss = 0.41979179\n",
      "Iteration 3275, loss = 0.41950553\n",
      "Iteration 3276, loss = 0.41921930\n",
      "Iteration 3277, loss = 0.41893312\n",
      "Iteration 3278, loss = 0.41864697\n",
      "Iteration 3279, loss = 0.41836087\n",
      "Iteration 3280, loss = 0.41807481\n",
      "Iteration 3281, loss = 0.41778880\n",
      "Iteration 3282, loss = 0.41750283\n",
      "Iteration 3283, loss = 0.41721691\n",
      "Iteration 3284, loss = 0.41693103\n",
      "Iteration 3285, loss = 0.41664520\n",
      "Iteration 3286, loss = 0.41635942\n",
      "Iteration 3287, loss = 0.41607368\n",
      "Iteration 3288, loss = 0.41578799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3289, loss = 0.41550235\n",
      "Iteration 3290, loss = 0.41521676\n",
      "Iteration 3291, loss = 0.41493122\n",
      "Iteration 3292, loss = 0.41464573\n",
      "Iteration 3293, loss = 0.41436029\n",
      "Iteration 3294, loss = 0.41407491\n",
      "Iteration 3295, loss = 0.41378958\n",
      "Iteration 3296, loss = 0.41350430\n",
      "Iteration 3297, loss = 0.41321907\n",
      "Iteration 3298, loss = 0.41293390\n",
      "Iteration 3299, loss = 0.41264878\n",
      "Iteration 3300, loss = 0.41236372\n",
      "Iteration 3301, loss = 0.41207871\n",
      "Iteration 3302, loss = 0.41179376\n",
      "Iteration 3303, loss = 0.41150887\n",
      "Iteration 3304, loss = 0.41122404\n",
      "Iteration 3305, loss = 0.41093927\n",
      "Iteration 3306, loss = 0.41065455\n",
      "Iteration 3307, loss = 0.41036989\n",
      "Iteration 3308, loss = 0.41008530\n",
      "Iteration 3309, loss = 0.40980076\n",
      "Iteration 3310, loss = 0.40951629\n",
      "Iteration 3311, loss = 0.40923188\n",
      "Iteration 3312, loss = 0.40894753\n",
      "Iteration 3313, loss = 0.40866325\n",
      "Iteration 3314, loss = 0.40837902\n",
      "Iteration 3315, loss = 0.40809487\n",
      "Iteration 3316, loss = 0.40781078\n",
      "Iteration 3317, loss = 0.40752675\n",
      "Iteration 3318, loss = 0.40724279\n",
      "Iteration 3319, loss = 0.40695889\n",
      "Iteration 3320, loss = 0.40667507\n",
      "Iteration 3321, loss = 0.40639131\n",
      "Iteration 3322, loss = 0.40610762\n",
      "Iteration 3323, loss = 0.40582400\n",
      "Iteration 3324, loss = 0.40554045\n",
      "Iteration 3325, loss = 0.40525697\n",
      "Iteration 3326, loss = 0.40497356\n",
      "Iteration 3327, loss = 0.40469022\n",
      "Iteration 3328, loss = 0.40440695\n",
      "Iteration 3329, loss = 0.40412376\n",
      "Iteration 3330, loss = 0.40384063\n",
      "Iteration 3331, loss = 0.40355759\n",
      "Iteration 3332, loss = 0.40327461\n",
      "Iteration 3333, loss = 0.40299171\n",
      "Iteration 3334, loss = 0.40270889\n",
      "Iteration 3335, loss = 0.40242614\n",
      "Iteration 3336, loss = 0.40214347\n",
      "Iteration 3337, loss = 0.40186087\n",
      "Iteration 3338, loss = 0.40157836\n",
      "Iteration 3339, loss = 0.40129592\n",
      "Iteration 3340, loss = 0.40101355\n",
      "Iteration 3341, loss = 0.40073127\n",
      "Iteration 3342, loss = 0.40044907\n",
      "Iteration 3343, loss = 0.40016695\n",
      "Iteration 3344, loss = 0.39988491\n",
      "Iteration 3345, loss = 0.39960295\n",
      "Iteration 3346, loss = 0.39932107\n",
      "Iteration 3347, loss = 0.39903927\n",
      "Iteration 3348, loss = 0.39875756\n",
      "Iteration 3349, loss = 0.39847593\n",
      "Iteration 3350, loss = 0.39819438\n",
      "Iteration 3351, loss = 0.39791292\n",
      "Iteration 3352, loss = 0.39763155\n",
      "Iteration 3353, loss = 0.39735025\n",
      "Iteration 3354, loss = 0.39706905\n",
      "Iteration 3355, loss = 0.39678793\n",
      "Iteration 3356, loss = 0.39650690\n",
      "Iteration 3357, loss = 0.39622596\n",
      "Iteration 3358, loss = 0.39594510\n",
      "Iteration 3359, loss = 0.39566434\n",
      "Iteration 3360, loss = 0.39538366\n",
      "Iteration 3361, loss = 0.39510307\n",
      "Iteration 3362, loss = 0.39482258\n",
      "Iteration 3363, loss = 0.39454217\n",
      "Iteration 3364, loss = 0.39426186\n",
      "Iteration 3365, loss = 0.39398163\n",
      "Iteration 3366, loss = 0.39370150\n",
      "Iteration 3367, loss = 0.39342146\n",
      "Iteration 3368, loss = 0.39314152\n",
      "Iteration 3369, loss = 0.39286167\n",
      "Iteration 3370, loss = 0.39258191\n",
      "Iteration 3371, loss = 0.39230225\n",
      "Iteration 3372, loss = 0.39202269\n",
      "Iteration 3373, loss = 0.39174322\n",
      "Iteration 3374, loss = 0.39146384\n",
      "Iteration 3375, loss = 0.39118457\n",
      "Iteration 3376, loss = 0.39090539\n",
      "Iteration 3377, loss = 0.39062631\n",
      "Iteration 3378, loss = 0.39034732\n",
      "Iteration 3379, loss = 0.39006844\n",
      "Iteration 3380, loss = 0.38978965\n",
      "Iteration 3381, loss = 0.38951097\n",
      "Iteration 3382, loss = 0.38923239\n",
      "Iteration 3383, loss = 0.38895390\n",
      "Iteration 3384, loss = 0.38867552\n",
      "Iteration 3385, loss = 0.38839724\n",
      "Iteration 3386, loss = 0.38811906\n",
      "Iteration 3387, loss = 0.38784099\n",
      "Iteration 3388, loss = 0.38756301\n",
      "Iteration 3389, loss = 0.38728515\n",
      "Iteration 3390, loss = 0.38700738\n",
      "Iteration 3391, loss = 0.38672972\n",
      "Iteration 3392, loss = 0.38645217\n",
      "Iteration 3393, loss = 0.38617472\n",
      "Iteration 3394, loss = 0.38589738\n",
      "Iteration 3395, loss = 0.38562014\n",
      "Iteration 3396, loss = 0.38534301\n",
      "Iteration 3397, loss = 0.38506599\n",
      "Iteration 3398, loss = 0.38478908\n",
      "Iteration 3399, loss = 0.38451227\n",
      "Iteration 3400, loss = 0.38423558\n",
      "Iteration 3401, loss = 0.38395899\n",
      "Iteration 3402, loss = 0.38368251\n",
      "Iteration 3403, loss = 0.38340615\n",
      "Iteration 3404, loss = 0.38312989\n",
      "Iteration 3405, loss = 0.38285375\n",
      "Iteration 3406, loss = 0.38257772\n",
      "Iteration 3407, loss = 0.38230180\n",
      "Iteration 3408, loss = 0.38202599\n",
      "Iteration 3409, loss = 0.38175029\n",
      "Iteration 3410, loss = 0.38147471\n",
      "Iteration 3411, loss = 0.38119924\n",
      "Iteration 3412, loss = 0.38092389\n",
      "Iteration 3413, loss = 0.38064865\n",
      "Iteration 3414, loss = 0.38037353\n",
      "Iteration 3415, loss = 0.38009852\n",
      "Iteration 3416, loss = 0.37982363\n",
      "Iteration 3417, loss = 0.37954886\n",
      "Iteration 3418, loss = 0.37927420\n",
      "Iteration 3419, loss = 0.37899966\n",
      "Iteration 3420, loss = 0.37872524\n",
      "Iteration 3421, loss = 0.37845093\n",
      "Iteration 3422, loss = 0.37817675\n",
      "Iteration 3423, loss = 0.37790268\n",
      "Iteration 3424, loss = 0.37762873\n",
      "Iteration 3425, loss = 0.37735491\n",
      "Iteration 3426, loss = 0.37708120\n",
      "Iteration 3427, loss = 0.37680762\n",
      "Iteration 3428, loss = 0.37653415\n",
      "Iteration 3429, loss = 0.37626081\n",
      "Iteration 3430, loss = 0.37598759\n",
      "Iteration 3431, loss = 0.37571449\n",
      "Iteration 3432, loss = 0.37544152\n",
      "Iteration 3433, loss = 0.37516866\n",
      "Iteration 3434, loss = 0.37489594\n",
      "Iteration 3435, loss = 0.37462333\n",
      "Iteration 3436, loss = 0.37435085\n",
      "Iteration 3437, loss = 0.37407850\n",
      "Iteration 3438, loss = 0.37380627\n",
      "Iteration 3439, loss = 0.37353417\n",
      "Iteration 3440, loss = 0.37326219\n",
      "Iteration 3441, loss = 0.37299034\n",
      "Iteration 3442, loss = 0.37271862\n",
      "Iteration 3443, loss = 0.37244703\n",
      "Iteration 3444, loss = 0.37217556\n",
      "Iteration 3445, loss = 0.37190422\n",
      "Iteration 3446, loss = 0.37163301\n",
      "Iteration 3447, loss = 0.37136193\n",
      "Iteration 3448, loss = 0.37109097\n",
      "Iteration 3449, loss = 0.37082015\n",
      "Iteration 3450, loss = 0.37054946\n",
      "Iteration 3451, loss = 0.37027889\n",
      "Iteration 3452, loss = 0.37000846\n",
      "Iteration 3453, loss = 0.36973816\n",
      "Iteration 3454, loss = 0.36946800\n",
      "Iteration 3455, loss = 0.36919796\n",
      "Iteration 3456, loss = 0.36892806\n",
      "Iteration 3457, loss = 0.36865828\n",
      "Iteration 3458, loss = 0.36838865\n",
      "Iteration 3459, loss = 0.36811914\n",
      "Iteration 3460, loss = 0.36784977\n",
      "Iteration 3461, loss = 0.36758054\n",
      "Iteration 3462, loss = 0.36731143\n",
      "Iteration 3463, loss = 0.36704247\n",
      "Iteration 3464, loss = 0.36677364\n",
      "Iteration 3465, loss = 0.36650494\n",
      "Iteration 3466, loss = 0.36623638\n",
      "Iteration 3467, loss = 0.36596796\n",
      "Iteration 3468, loss = 0.36569967\n",
      "Iteration 3469, loss = 0.36543153\n",
      "Iteration 3470, loss = 0.36516351\n",
      "Iteration 3471, loss = 0.36489564\n",
      "Iteration 3472, loss = 0.36462791\n",
      "Iteration 3473, loss = 0.36436031\n",
      "Iteration 3474, loss = 0.36409285\n",
      "Iteration 3475, loss = 0.36382553\n",
      "Iteration 3476, loss = 0.36355836\n",
      "Iteration 3477, loss = 0.36329132\n",
      "Iteration 3478, loss = 0.36302442\n",
      "Iteration 3479, loss = 0.36275766\n",
      "Iteration 3480, loss = 0.36249105\n",
      "Iteration 3481, loss = 0.36222457\n",
      "Iteration 3482, loss = 0.36195824\n",
      "Iteration 3483, loss = 0.36169205\n",
      "Iteration 3484, loss = 0.36142600\n",
      "Iteration 3485, loss = 0.36116009\n",
      "Iteration 3486, loss = 0.36089433\n",
      "Iteration 3487, loss = 0.36062871\n",
      "Iteration 3488, loss = 0.36036323\n",
      "Iteration 3489, loss = 0.36009790\n",
      "Iteration 3490, loss = 0.35983271\n",
      "Iteration 3491, loss = 0.35956767\n",
      "Iteration 3492, loss = 0.35930277\n",
      "Iteration 3493, loss = 0.35903802\n",
      "Iteration 3494, loss = 0.35877341\n",
      "Iteration 3495, loss = 0.35850895\n",
      "Iteration 3496, loss = 0.35824464\n",
      "Iteration 3497, loss = 0.35798047\n",
      "Iteration 3498, loss = 0.35771645\n",
      "Iteration 3499, loss = 0.35745257\n",
      "Iteration 3500, loss = 0.35718884\n",
      "Iteration 3501, loss = 0.35692527\n",
      "Iteration 3502, loss = 0.35666183\n",
      "Iteration 3503, loss = 0.35639855\n",
      "Iteration 3504, loss = 0.35613542\n",
      "Iteration 3505, loss = 0.35587243\n",
      "Iteration 3506, loss = 0.35560960\n",
      "Iteration 3507, loss = 0.35534691\n",
      "Iteration 3508, loss = 0.35508438\n",
      "Iteration 3509, loss = 0.35482199\n",
      "Iteration 3510, loss = 0.35455976\n",
      "Iteration 3511, loss = 0.35429767\n",
      "Iteration 3512, loss = 0.35403574\n",
      "Iteration 3513, loss = 0.35377396\n",
      "Iteration 3514, loss = 0.35351233\n",
      "Iteration 3515, loss = 0.35325085\n",
      "Iteration 3516, loss = 0.35298952\n",
      "Iteration 3517, loss = 0.35272835\n",
      "Iteration 3518, loss = 0.35246733\n",
      "Iteration 3519, loss = 0.35220646\n",
      "Iteration 3520, loss = 0.35194575\n",
      "Iteration 3521, loss = 0.35168519\n",
      "Iteration 3522, loss = 0.35142478\n",
      "Iteration 3523, loss = 0.35116453\n",
      "Iteration 3524, loss = 0.35090444\n",
      "Iteration 3525, loss = 0.35064449\n",
      "Iteration 3526, loss = 0.35038470\n",
      "Iteration 3527, loss = 0.35012507\n",
      "Iteration 3528, loss = 0.34986560\n",
      "Iteration 3529, loss = 0.34960628\n",
      "Iteration 3530, loss = 0.34934711\n",
      "Iteration 3531, loss = 0.34908810\n",
      "Iteration 3532, loss = 0.34882925\n",
      "Iteration 3533, loss = 0.34857056\n",
      "Iteration 3534, loss = 0.34831202\n",
      "Iteration 3535, loss = 0.34805364\n",
      "Iteration 3536, loss = 0.34779542\n",
      "Iteration 3537, loss = 0.34753735\n",
      "Iteration 3538, loss = 0.34727944\n",
      "Iteration 3539, loss = 0.34702170\n",
      "Iteration 3540, loss = 0.34676411\n",
      "Iteration 3541, loss = 0.34650668\n",
      "Iteration 3542, loss = 0.34624941\n",
      "Iteration 3543, loss = 0.34599229\n",
      "Iteration 3544, loss = 0.34573534\n",
      "Iteration 3545, loss = 0.34547855\n",
      "Iteration 3546, loss = 0.34522192\n",
      "Iteration 3547, loss = 0.34496545\n",
      "Iteration 3548, loss = 0.34470914\n",
      "Iteration 3549, loss = 0.34445299\n",
      "Iteration 3550, loss = 0.34419700\n",
      "Iteration 3551, loss = 0.34394117\n",
      "Iteration 3552, loss = 0.34368550\n",
      "Iteration 3553, loss = 0.34343000\n",
      "Iteration 3554, loss = 0.34317466\n",
      "Iteration 3555, loss = 0.34291948\n",
      "Iteration 3556, loss = 0.34266446\n",
      "Iteration 3557, loss = 0.34240961\n",
      "Iteration 3558, loss = 0.34215491\n",
      "Iteration 3559, loss = 0.34190039\n",
      "Iteration 3560, loss = 0.34164602\n",
      "Iteration 3561, loss = 0.34139182\n",
      "Iteration 3562, loss = 0.34113778\n",
      "Iteration 3563, loss = 0.34088391\n",
      "Iteration 3564, loss = 0.34063020\n",
      "Iteration 3565, loss = 0.34037665\n",
      "Iteration 3566, loss = 0.34012327\n",
      "Iteration 3567, loss = 0.33987006\n",
      "Iteration 3568, loss = 0.33961701\n",
      "Iteration 3569, loss = 0.33936412\n",
      "Iteration 3570, loss = 0.33911141\n",
      "Iteration 3571, loss = 0.33885885\n",
      "Iteration 3572, loss = 0.33860646\n",
      "Iteration 3573, loss = 0.33835424\n",
      "Iteration 3574, loss = 0.33810219\n",
      "Iteration 3575, loss = 0.33785030\n",
      "Iteration 3576, loss = 0.33759858\n",
      "Iteration 3577, loss = 0.33734702\n",
      "Iteration 3578, loss = 0.33709564\n",
      "Iteration 3579, loss = 0.33684442\n",
      "Iteration 3580, loss = 0.33659336\n",
      "Iteration 3581, loss = 0.33634248\n",
      "Iteration 3582, loss = 0.33609176\n",
      "Iteration 3583, loss = 0.33584121\n",
      "Iteration 3584, loss = 0.33559083\n",
      "Iteration 3585, loss = 0.33534062\n",
      "Iteration 3586, loss = 0.33509058\n",
      "Iteration 3587, loss = 0.33484071\n",
      "Iteration 3588, loss = 0.33459100\n",
      "Iteration 3589, loss = 0.33434146\n",
      "Iteration 3590, loss = 0.33409210\n",
      "Iteration 3591, loss = 0.33384290\n",
      "Iteration 3592, loss = 0.33359387\n",
      "Iteration 3593, loss = 0.33334502\n",
      "Iteration 3594, loss = 0.33309633\n",
      "Iteration 3595, loss = 0.33284781\n",
      "Iteration 3596, loss = 0.33259947\n",
      "Iteration 3597, loss = 0.33235129\n",
      "Iteration 3598, loss = 0.33210329\n",
      "Iteration 3599, loss = 0.33185545\n",
      "Iteration 3600, loss = 0.33160779\n",
      "Iteration 3601, loss = 0.33136030\n",
      "Iteration 3602, loss = 0.33111298\n",
      "Iteration 3603, loss = 0.33086583\n",
      "Iteration 3604, loss = 0.33061886\n",
      "Iteration 3605, loss = 0.33037205\n",
      "Iteration 3606, loss = 0.33012542\n",
      "Iteration 3607, loss = 0.32987896\n",
      "Iteration 3608, loss = 0.32963267\n",
      "Iteration 3609, loss = 0.32938656\n",
      "Iteration 3610, loss = 0.32914062\n",
      "Iteration 3611, loss = 0.32889485\n",
      "Iteration 3612, loss = 0.32864925\n",
      "Iteration 3613, loss = 0.32840383\n",
      "Iteration 3614, loss = 0.32815858\n",
      "Iteration 3615, loss = 0.32791351\n",
      "Iteration 3616, loss = 0.32766860\n",
      "Iteration 3617, loss = 0.32742388\n",
      "Iteration 3618, loss = 0.32717932\n",
      "Iteration 3619, loss = 0.32693494\n",
      "Iteration 3620, loss = 0.32669074\n",
      "Iteration 3621, loss = 0.32644670\n",
      "Iteration 3622, loss = 0.32620285\n",
      "Iteration 3623, loss = 0.32595916\n",
      "Iteration 3624, loss = 0.32571566\n",
      "Iteration 3625, loss = 0.32547233\n",
      "Iteration 3626, loss = 0.32522917\n",
      "Iteration 3627, loss = 0.32498619\n",
      "Iteration 3628, loss = 0.32474338\n",
      "Iteration 3629, loss = 0.32450075\n",
      "Iteration 3630, loss = 0.32425829\n",
      "Iteration 3631, loss = 0.32401601\n",
      "Iteration 3632, loss = 0.32377391\n",
      "Iteration 3633, loss = 0.32353198\n",
      "Iteration 3634, loss = 0.32329023\n",
      "Iteration 3635, loss = 0.32304865\n",
      "Iteration 3636, loss = 0.32280725\n",
      "Iteration 3637, loss = 0.32256603\n",
      "Iteration 3638, loss = 0.32232498\n",
      "Iteration 3639, loss = 0.32208412\n",
      "Iteration 3640, loss = 0.32184342\n",
      "Iteration 3641, loss = 0.32160291\n",
      "Iteration 3642, loss = 0.32136257\n",
      "Iteration 3643, loss = 0.32112241\n",
      "Iteration 3644, loss = 0.32088242\n",
      "Iteration 3645, loss = 0.32064262\n",
      "Iteration 3646, loss = 0.32040299\n",
      "Iteration 3647, loss = 0.32016354\n",
      "Iteration 3648, loss = 0.31992426\n",
      "Iteration 3649, loss = 0.31968517\n",
      "Iteration 3650, loss = 0.31944625\n",
      "Iteration 3651, loss = 0.31920751\n",
      "Iteration 3652, loss = 0.31896895\n",
      "Iteration 3653, loss = 0.31873056\n",
      "Iteration 3654, loss = 0.31849236\n",
      "Iteration 3655, loss = 0.31825433\n",
      "Iteration 3656, loss = 0.31801649\n",
      "Iteration 3657, loss = 0.31777882\n",
      "Iteration 3658, loss = 0.31754133\n",
      "Iteration 3659, loss = 0.31730401\n",
      "Iteration 3660, loss = 0.31706688\n",
      "Iteration 3661, loss = 0.31682993\n",
      "Iteration 3662, loss = 0.31659315\n",
      "Iteration 3663, loss = 0.31635656\n",
      "Iteration 3664, loss = 0.31612014\n",
      "Iteration 3665, loss = 0.31588391\n",
      "Iteration 3666, loss = 0.31564785\n",
      "Iteration 3667, loss = 0.31541197\n",
      "Iteration 3668, loss = 0.31517628\n",
      "Iteration 3669, loss = 0.31494076\n",
      "Iteration 3670, loss = 0.31470542\n",
      "Iteration 3671, loss = 0.31447026\n",
      "Iteration 3672, loss = 0.31423528\n",
      "Iteration 3673, loss = 0.31400049\n",
      "Iteration 3674, loss = 0.31376587\n",
      "Iteration 3675, loss = 0.31353143\n",
      "Iteration 3676, loss = 0.31329717\n",
      "Iteration 3677, loss = 0.31306310\n",
      "Iteration 3678, loss = 0.31282920\n",
      "Iteration 3679, loss = 0.31259549\n",
      "Iteration 3680, loss = 0.31236195\n",
      "Iteration 3681, loss = 0.31212860\n",
      "Iteration 3682, loss = 0.31189542\n",
      "Iteration 3683, loss = 0.31166243\n",
      "Iteration 3684, loss = 0.31142962\n",
      "Iteration 3685, loss = 0.31119699\n",
      "Iteration 3686, loss = 0.31096454\n",
      "Iteration 3687, loss = 0.31073227\n",
      "Iteration 3688, loss = 0.31050018\n",
      "Iteration 3689, loss = 0.31026828\n",
      "Iteration 3690, loss = 0.31003655\n",
      "Iteration 3691, loss = 0.30980501\n",
      "Iteration 3692, loss = 0.30957365\n",
      "Iteration 3693, loss = 0.30934247\n",
      "Iteration 3694, loss = 0.30911147\n",
      "Iteration 3695, loss = 0.30888065\n",
      "Iteration 3696, loss = 0.30865002\n",
      "Iteration 3697, loss = 0.30841956\n",
      "Iteration 3698, loss = 0.30818929\n",
      "Iteration 3699, loss = 0.30795920\n",
      "Iteration 3700, loss = 0.30772929\n",
      "Iteration 3701, loss = 0.30749957\n",
      "Iteration 3702, loss = 0.30727002\n",
      "Iteration 3703, loss = 0.30704066\n",
      "Iteration 3704, loss = 0.30681148\n",
      "Iteration 3705, loss = 0.30658248\n",
      "Iteration 3706, loss = 0.30635366\n",
      "Iteration 3707, loss = 0.30612503\n",
      "Iteration 3708, loss = 0.30589658\n",
      "Iteration 3709, loss = 0.30566831\n",
      "Iteration 3710, loss = 0.30544022\n",
      "Iteration 3711, loss = 0.30521232\n",
      "Iteration 3712, loss = 0.30498460\n",
      "Iteration 3713, loss = 0.30475706\n",
      "Iteration 3714, loss = 0.30452970\n",
      "Iteration 3715, loss = 0.30430253\n",
      "Iteration 3716, loss = 0.30407553\n",
      "Iteration 3717, loss = 0.30384872\n",
      "Iteration 3718, loss = 0.30362210\n",
      "Iteration 3719, loss = 0.30339565\n",
      "Iteration 3720, loss = 0.30316939\n",
      "Iteration 3721, loss = 0.30294331\n",
      "Iteration 3722, loss = 0.30271742\n",
      "Iteration 3723, loss = 0.30249171\n",
      "Iteration 3724, loss = 0.30226618\n",
      "Iteration 3725, loss = 0.30204083\n",
      "Iteration 3726, loss = 0.30181567\n",
      "Iteration 3727, loss = 0.30159068\n",
      "Iteration 3728, loss = 0.30136589\n",
      "Iteration 3729, loss = 0.30114127\n",
      "Iteration 3730, loss = 0.30091684\n",
      "Iteration 3731, loss = 0.30069259\n",
      "Iteration 3732, loss = 0.30046852\n",
      "Iteration 3733, loss = 0.30024464\n",
      "Iteration 3734, loss = 0.30002094\n",
      "Iteration 3735, loss = 0.29979742\n",
      "Iteration 3736, loss = 0.29957409\n",
      "Iteration 3737, loss = 0.29935094\n",
      "Iteration 3738, loss = 0.29912797\n",
      "Iteration 3739, loss = 0.29890519\n",
      "Iteration 3740, loss = 0.29868259\n",
      "Iteration 3741, loss = 0.29846017\n",
      "Iteration 3742, loss = 0.29823794\n",
      "Iteration 3743, loss = 0.29801588\n",
      "Iteration 3744, loss = 0.29779402\n",
      "Iteration 3745, loss = 0.29757233\n",
      "Iteration 3746, loss = 0.29735083\n",
      "Iteration 3747, loss = 0.29712951\n",
      "Iteration 3748, loss = 0.29690838\n",
      "Iteration 3749, loss = 0.29668743\n",
      "Iteration 3750, loss = 0.29646666\n",
      "Iteration 3751, loss = 0.29624607\n",
      "Iteration 3752, loss = 0.29602567\n",
      "Iteration 3753, loss = 0.29580545\n",
      "Iteration 3754, loss = 0.29558542\n",
      "Iteration 3755, loss = 0.29536557\n",
      "Iteration 3756, loss = 0.29514590\n",
      "Iteration 3757, loss = 0.29492641\n",
      "Iteration 3758, loss = 0.29470711\n",
      "Iteration 3759, loss = 0.29448799\n",
      "Iteration 3760, loss = 0.29426906\n",
      "Iteration 3761, loss = 0.29405031\n",
      "Iteration 3762, loss = 0.29383174\n",
      "Iteration 3763, loss = 0.29361335\n",
      "Iteration 3764, loss = 0.29339515\n",
      "Iteration 3765, loss = 0.29317713\n",
      "Iteration 3766, loss = 0.29295930\n",
      "Iteration 3767, loss = 0.29274165\n",
      "Iteration 3768, loss = 0.29252418\n",
      "Iteration 3769, loss = 0.29230689\n",
      "Iteration 3770, loss = 0.29208979\n",
      "Iteration 3771, loss = 0.29187287\n",
      "Iteration 3772, loss = 0.29165614\n",
      "Iteration 3773, loss = 0.29143958\n",
      "Iteration 3774, loss = 0.29122322\n",
      "Iteration 3775, loss = 0.29100703\n",
      "Iteration 3776, loss = 0.29079103\n",
      "Iteration 3777, loss = 0.29057521\n",
      "Iteration 3778, loss = 0.29035957\n",
      "Iteration 3779, loss = 0.29014412\n",
      "Iteration 3780, loss = 0.28992885\n",
      "Iteration 3781, loss = 0.28971376\n",
      "Iteration 3782, loss = 0.28949886\n",
      "Iteration 3783, loss = 0.28928414\n",
      "Iteration 3784, loss = 0.28906960\n",
      "Iteration 3785, loss = 0.28885525\n",
      "Iteration 3786, loss = 0.28864107\n",
      "Iteration 3787, loss = 0.28842709\n",
      "Iteration 3788, loss = 0.28821328\n",
      "Iteration 3789, loss = 0.28799966\n",
      "Iteration 3790, loss = 0.28778622\n",
      "Iteration 3791, loss = 0.28757296\n",
      "Iteration 3792, loss = 0.28735989\n",
      "Iteration 3793, loss = 0.28714700\n",
      "Iteration 3794, loss = 0.28693429\n",
      "Iteration 3795, loss = 0.28672176\n",
      "Iteration 3796, loss = 0.28650942\n",
      "Iteration 3797, loss = 0.28629726\n",
      "Iteration 3798, loss = 0.28608528\n",
      "Iteration 3799, loss = 0.28587349\n",
      "Iteration 3800, loss = 0.28566188\n",
      "Iteration 3801, loss = 0.28545045\n",
      "Iteration 3802, loss = 0.28523920\n",
      "Iteration 3803, loss = 0.28502814\n",
      "Iteration 3804, loss = 0.28481726\n",
      "Iteration 3805, loss = 0.28460656\n",
      "Iteration 3806, loss = 0.28439604\n",
      "Iteration 3807, loss = 0.28418571\n",
      "Iteration 3808, loss = 0.28397555\n",
      "Iteration 3809, loss = 0.28376558\n",
      "Iteration 3810, loss = 0.28355580\n",
      "Iteration 3811, loss = 0.28334619\n",
      "Iteration 3812, loss = 0.28313677\n",
      "Iteration 3813, loss = 0.28292753\n",
      "Iteration 3814, loss = 0.28271847\n",
      "Iteration 3815, loss = 0.28250959\n",
      "Iteration 3816, loss = 0.28230090\n",
      "Iteration 3817, loss = 0.28209239\n",
      "Iteration 3818, loss = 0.28188406\n",
      "Iteration 3819, loss = 0.28167591\n",
      "Iteration 3820, loss = 0.28146794\n",
      "Iteration 3821, loss = 0.28126016\n",
      "Iteration 3822, loss = 0.28105255\n",
      "Iteration 3823, loss = 0.28084513\n",
      "Iteration 3824, loss = 0.28063789\n",
      "Iteration 3825, loss = 0.28043083\n",
      "Iteration 3826, loss = 0.28022396\n",
      "Iteration 3827, loss = 0.28001726\n",
      "Iteration 3828, loss = 0.27981075\n",
      "Iteration 3829, loss = 0.27960442\n",
      "Iteration 3830, loss = 0.27939827\n",
      "Iteration 3831, loss = 0.27919230\n",
      "Iteration 3832, loss = 0.27898651\n",
      "Iteration 3833, loss = 0.27878090\n",
      "Iteration 3834, loss = 0.27857548\n",
      "Iteration 3835, loss = 0.27837023\n",
      "Iteration 3836, loss = 0.27816517\n",
      "Iteration 3837, loss = 0.27796029\n",
      "Iteration 3838, loss = 0.27775559\n",
      "Iteration 3839, loss = 0.27755107\n",
      "Iteration 3840, loss = 0.27734673\n",
      "Iteration 3841, loss = 0.27714257\n",
      "Iteration 3842, loss = 0.27693859\n",
      "Iteration 3843, loss = 0.27673479\n",
      "Iteration 3844, loss = 0.27653118\n",
      "Iteration 3845, loss = 0.27632774\n",
      "Iteration 3846, loss = 0.27612449\n",
      "Iteration 3847, loss = 0.27592141\n",
      "Iteration 3848, loss = 0.27571852\n",
      "Iteration 3849, loss = 0.27551580\n",
      "Iteration 3850, loss = 0.27531327\n",
      "Iteration 3851, loss = 0.27511091\n",
      "Iteration 3852, loss = 0.27490874\n",
      "Iteration 3853, loss = 0.27470675\n",
      "Iteration 3854, loss = 0.27450493\n",
      "Iteration 3855, loss = 0.27430330\n",
      "Iteration 3856, loss = 0.27410184\n",
      "Iteration 3857, loss = 0.27390057\n",
      "Iteration 3858, loss = 0.27369948\n",
      "Iteration 3859, loss = 0.27349856\n",
      "Iteration 3860, loss = 0.27329783\n",
      "Iteration 3861, loss = 0.27309727\n",
      "Iteration 3862, loss = 0.27289690\n",
      "Iteration 3863, loss = 0.27269670\n",
      "Iteration 3864, loss = 0.27249668\n",
      "Iteration 3865, loss = 0.27229684\n",
      "Iteration 3866, loss = 0.27209718\n",
      "Iteration 3867, loss = 0.27189771\n",
      "Iteration 3868, loss = 0.27169840\n",
      "Iteration 3869, loss = 0.27149928\n",
      "Iteration 3870, loss = 0.27130034\n",
      "Iteration 3871, loss = 0.27110158\n",
      "Iteration 3872, loss = 0.27090299\n",
      "Iteration 3873, loss = 0.27070458\n",
      "Iteration 3874, loss = 0.27050636\n",
      "Iteration 3875, loss = 0.27030831\n",
      "Iteration 3876, loss = 0.27011044\n",
      "Iteration 3877, loss = 0.26991274\n",
      "Iteration 3878, loss = 0.26971523\n",
      "Iteration 3879, loss = 0.26951789\n",
      "Iteration 3880, loss = 0.26932074\n",
      "Iteration 3881, loss = 0.26912376\n",
      "Iteration 3882, loss = 0.26892695\n",
      "Iteration 3883, loss = 0.26873033\n",
      "Iteration 3884, loss = 0.26853389\n",
      "Iteration 3885, loss = 0.26833762\n",
      "Iteration 3886, loss = 0.26814153\n",
      "Iteration 3887, loss = 0.26794561\n",
      "Iteration 3888, loss = 0.26774988\n",
      "Iteration 3889, loss = 0.26755432\n",
      "Iteration 3890, loss = 0.26735894\n",
      "Iteration 3891, loss = 0.26716374\n",
      "Iteration 3892, loss = 0.26696871\n",
      "Iteration 3893, loss = 0.26677386\n",
      "Iteration 3894, loss = 0.26657919\n",
      "Iteration 3895, loss = 0.26638469\n",
      "Iteration 3896, loss = 0.26619038\n",
      "Iteration 3897, loss = 0.26599624\n",
      "Iteration 3898, loss = 0.26580227\n",
      "Iteration 3899, loss = 0.26560848\n",
      "Iteration 3900, loss = 0.26541487\n",
      "Iteration 3901, loss = 0.26522144\n",
      "Iteration 3902, loss = 0.26502818\n",
      "Iteration 3903, loss = 0.26483509\n",
      "Iteration 3904, loss = 0.26464219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3905, loss = 0.26444946\n",
      "Iteration 3906, loss = 0.26425690\n",
      "Iteration 3907, loss = 0.26406453\n",
      "Iteration 3908, loss = 0.26387232\n",
      "Iteration 3909, loss = 0.26368030\n",
      "Iteration 3910, loss = 0.26348844\n",
      "Iteration 3911, loss = 0.26329677\n",
      "Iteration 3912, loss = 0.26310527\n",
      "Iteration 3913, loss = 0.26291394\n",
      "Iteration 3914, loss = 0.26272279\n",
      "Iteration 3915, loss = 0.26253182\n",
      "Iteration 3916, loss = 0.26234102\n",
      "Iteration 3917, loss = 0.26215040\n",
      "Iteration 3918, loss = 0.26195995\n",
      "Iteration 3919, loss = 0.26176967\n",
      "Iteration 3920, loss = 0.26157957\n",
      "Iteration 3921, loss = 0.26138965\n",
      "Iteration 3922, loss = 0.26119990\n",
      "Iteration 3923, loss = 0.26101032\n",
      "Iteration 3924, loss = 0.26082092\n",
      "Iteration 3925, loss = 0.26063169\n",
      "Iteration 3926, loss = 0.26044264\n",
      "Iteration 3927, loss = 0.26025376\n",
      "Iteration 3928, loss = 0.26006505\n",
      "Iteration 3929, loss = 0.25987652\n",
      "Iteration 3930, loss = 0.25968816\n",
      "Iteration 3931, loss = 0.25949998\n",
      "Iteration 3932, loss = 0.25931197\n",
      "Iteration 3933, loss = 0.25912413\n",
      "Iteration 3934, loss = 0.25893646\n",
      "Iteration 3935, loss = 0.25874897\n",
      "Iteration 3936, loss = 0.25856166\n",
      "Iteration 3937, loss = 0.25837451\n",
      "Iteration 3938, loss = 0.25818754\n",
      "Iteration 3939, loss = 0.25800074\n",
      "Iteration 3940, loss = 0.25781411\n",
      "Iteration 3941, loss = 0.25762766\n",
      "Iteration 3942, loss = 0.25744138\n",
      "Iteration 3943, loss = 0.25725527\n",
      "Iteration 3944, loss = 0.25706934\n",
      "Iteration 3945, loss = 0.25688357\n",
      "Iteration 3946, loss = 0.25669798\n",
      "Iteration 3947, loss = 0.25651256\n",
      "Iteration 3948, loss = 0.25632731\n",
      "Iteration 3949, loss = 0.25614224\n",
      "Iteration 3950, loss = 0.25595733\n",
      "Iteration 3951, loss = 0.25577260\n",
      "Iteration 3952, loss = 0.25558804\n",
      "Iteration 3953, loss = 0.25540365\n",
      "Iteration 3954, loss = 0.25521943\n",
      "Iteration 3955, loss = 0.25503538\n",
      "Iteration 3956, loss = 0.25485151\n",
      "Iteration 3957, loss = 0.25466780\n",
      "Iteration 3958, loss = 0.25448427\n",
      "Iteration 3959, loss = 0.25430091\n",
      "Iteration 3960, loss = 0.25411771\n",
      "Iteration 3961, loss = 0.25393469\n",
      "Iteration 3962, loss = 0.25375184\n",
      "Iteration 3963, loss = 0.25356916\n",
      "Iteration 3964, loss = 0.25338665\n",
      "Iteration 3965, loss = 0.25320431\n",
      "Iteration 3966, loss = 0.25302213\n",
      "Iteration 3967, loss = 0.25284013\n",
      "Iteration 3968, loss = 0.25265830\n",
      "Iteration 3969, loss = 0.25247664\n",
      "Iteration 3970, loss = 0.25229515\n",
      "Iteration 3971, loss = 0.25211383\n",
      "Iteration 3972, loss = 0.25193267\n",
      "Iteration 3973, loss = 0.25175169\n",
      "Iteration 3974, loss = 0.25157087\n",
      "Iteration 3975, loss = 0.25139023\n",
      "Iteration 3976, loss = 0.25120975\n",
      "Iteration 3977, loss = 0.25102944\n",
      "Iteration 3978, loss = 0.25084930\n",
      "Iteration 3979, loss = 0.25066933\n",
      "Iteration 3980, loss = 0.25048953\n",
      "Iteration 3981, loss = 0.25030989\n",
      "Iteration 3982, loss = 0.25013043\n",
      "Iteration 3983, loss = 0.24995113\n",
      "Iteration 3984, loss = 0.24977200\n",
      "Iteration 3985, loss = 0.24959304\n",
      "Iteration 3986, loss = 0.24941425\n",
      "Iteration 3987, loss = 0.24923562\n",
      "Iteration 3988, loss = 0.24905716\n",
      "Iteration 3989, loss = 0.24887887\n",
      "Iteration 3990, loss = 0.24870074\n",
      "Iteration 3991, loss = 0.24852279\n",
      "Iteration 3992, loss = 0.24834500\n",
      "Iteration 3993, loss = 0.24816737\n",
      "Iteration 3994, loss = 0.24798992\n",
      "Iteration 3995, loss = 0.24781263\n",
      "Iteration 3996, loss = 0.24763551\n",
      "Iteration 3997, loss = 0.24745855\n",
      "Iteration 3998, loss = 0.24728176\n",
      "Iteration 3999, loss = 0.24710514\n",
      "Iteration 4000, loss = 0.24692868\n",
      "Iteration 4001, loss = 0.24675239\n",
      "Iteration 4002, loss = 0.24657626\n",
      "Iteration 4003, loss = 0.24640030\n",
      "Iteration 4004, loss = 0.24622451\n",
      "Iteration 4005, loss = 0.24604888\n",
      "Iteration 4006, loss = 0.24587342\n",
      "Iteration 4007, loss = 0.24569812\n",
      "Iteration 4008, loss = 0.24552299\n",
      "Iteration 4009, loss = 0.24534802\n",
      "Iteration 4010, loss = 0.24517322\n",
      "Iteration 4011, loss = 0.24499858\n",
      "Iteration 4012, loss = 0.24482411\n",
      "Iteration 4013, loss = 0.24464980\n",
      "Iteration 4014, loss = 0.24447566\n",
      "Iteration 4015, loss = 0.24430168\n",
      "Iteration 4016, loss = 0.24412786\n",
      "Iteration 4017, loss = 0.24395421\n",
      "Iteration 4018, loss = 0.24378072\n",
      "Iteration 4019, loss = 0.24360740\n",
      "Iteration 4020, loss = 0.24343424\n",
      "Iteration 4021, loss = 0.24326124\n",
      "Iteration 4022, loss = 0.24308841\n",
      "Iteration 4023, loss = 0.24291574\n",
      "Iteration 4024, loss = 0.24274324\n",
      "Iteration 4025, loss = 0.24257090\n",
      "Iteration 4026, loss = 0.24239872\n",
      "Iteration 4027, loss = 0.24222670\n",
      "Iteration 4028, loss = 0.24205485\n",
      "Iteration 4029, loss = 0.24188315\n",
      "Iteration 4030, loss = 0.24171163\n",
      "Iteration 4031, loss = 0.24154026\n",
      "Iteration 4032, loss = 0.24136906\n",
      "Iteration 4033, loss = 0.24119801\n",
      "Iteration 4034, loss = 0.24102713\n",
      "Iteration 4035, loss = 0.24085642\n",
      "Iteration 4036, loss = 0.24068586\n",
      "Iteration 4037, loss = 0.24051547\n",
      "Iteration 4038, loss = 0.24034523\n",
      "Iteration 4039, loss = 0.24017516\n",
      "Iteration 4040, loss = 0.24000525\n",
      "Iteration 4041, loss = 0.23983550\n",
      "Iteration 4042, loss = 0.23966592\n",
      "Iteration 4043, loss = 0.23949649\n",
      "Iteration 4044, loss = 0.23932722\n",
      "Iteration 4045, loss = 0.23915812\n",
      "Iteration 4046, loss = 0.23898917\n",
      "Iteration 4047, loss = 0.23882039\n",
      "Iteration 4048, loss = 0.23865176\n",
      "Iteration 4049, loss = 0.23848330\n",
      "Iteration 4050, loss = 0.23831500\n",
      "Iteration 4051, loss = 0.23814685\n",
      "Iteration 4052, loss = 0.23797887\n",
      "Iteration 4053, loss = 0.23781104\n",
      "Iteration 4054, loss = 0.23764338\n",
      "Iteration 4055, loss = 0.23747587\n",
      "Iteration 4056, loss = 0.23730853\n",
      "Iteration 4057, loss = 0.23714134\n",
      "Iteration 4058, loss = 0.23697431\n",
      "Iteration 4059, loss = 0.23680744\n",
      "Iteration 4060, loss = 0.23664073\n",
      "Iteration 4061, loss = 0.23647418\n",
      "Iteration 4062, loss = 0.23630778\n",
      "Iteration 4063, loss = 0.23614155\n",
      "Iteration 4064, loss = 0.23597547\n",
      "Iteration 4065, loss = 0.23580955\n",
      "Iteration 4066, loss = 0.23564379\n",
      "Iteration 4067, loss = 0.23547819\n",
      "Iteration 4068, loss = 0.23531274\n",
      "Iteration 4069, loss = 0.23514746\n",
      "Iteration 4070, loss = 0.23498233\n",
      "Iteration 4071, loss = 0.23481735\n",
      "Iteration 4072, loss = 0.23465254\n",
      "Iteration 4073, loss = 0.23448788\n",
      "Iteration 4074, loss = 0.23432338\n",
      "Iteration 4075, loss = 0.23415903\n",
      "Iteration 4076, loss = 0.23399485\n",
      "Iteration 4077, loss = 0.23383081\n",
      "Iteration 4078, loss = 0.23366694\n",
      "Iteration 4079, loss = 0.23350322\n",
      "Iteration 4080, loss = 0.23333966\n",
      "Iteration 4081, loss = 0.23317625\n",
      "Iteration 4082, loss = 0.23301300\n",
      "Iteration 4083, loss = 0.23284991\n",
      "Iteration 4084, loss = 0.23268697\n",
      "Iteration 4085, loss = 0.23252418\n",
      "Iteration 4086, loss = 0.23236156\n",
      "Iteration 4087, loss = 0.23219908\n",
      "Iteration 4088, loss = 0.23203676\n",
      "Iteration 4089, loss = 0.23187460\n",
      "Iteration 4090, loss = 0.23171259\n",
      "Iteration 4091, loss = 0.23155074\n",
      "Iteration 4092, loss = 0.23138904\n",
      "Iteration 4093, loss = 0.23122750\n",
      "Iteration 4094, loss = 0.23106611\n",
      "Iteration 4095, loss = 0.23090487\n",
      "Iteration 4096, loss = 0.23074379\n",
      "Iteration 4097, loss = 0.23058286\n",
      "Iteration 4098, loss = 0.23042209\n",
      "Iteration 4099, loss = 0.23026147\n",
      "Iteration 4100, loss = 0.23010101\n",
      "Iteration 4101, loss = 0.22994069\n",
      "Iteration 4102, loss = 0.22978053\n",
      "Iteration 4103, loss = 0.22962053\n",
      "Iteration 4104, loss = 0.22946067\n",
      "Iteration 4105, loss = 0.22930097\n",
      "Iteration 4106, loss = 0.22914142\n",
      "Iteration 4107, loss = 0.22898203\n",
      "Iteration 4108, loss = 0.22882279\n",
      "Iteration 4109, loss = 0.22866370\n",
      "Iteration 4110, loss = 0.22850476\n",
      "Iteration 4111, loss = 0.22834597\n",
      "Iteration 4112, loss = 0.22818734\n",
      "Iteration 4113, loss = 0.22802886\n",
      "Iteration 4114, loss = 0.22787053\n",
      "Iteration 4115, loss = 0.22771235\n",
      "Iteration 4116, loss = 0.22755432\n",
      "Iteration 4117, loss = 0.22739645\n",
      "Iteration 4118, loss = 0.22723872\n",
      "Iteration 4119, loss = 0.22708115\n",
      "Iteration 4120, loss = 0.22692372\n",
      "Iteration 4121, loss = 0.22676645\n",
      "Iteration 4122, loss = 0.22660933\n",
      "Iteration 4123, loss = 0.22645236\n",
      "Iteration 4124, loss = 0.22629554\n",
      "Iteration 4125, loss = 0.22613887\n",
      "Iteration 4126, loss = 0.22598235\n",
      "Iteration 4127, loss = 0.22582598\n",
      "Iteration 4128, loss = 0.22566976\n",
      "Iteration 4129, loss = 0.22551369\n",
      "Iteration 4130, loss = 0.22535777\n",
      "Iteration 4131, loss = 0.22520199\n",
      "Iteration 4132, loss = 0.22504637\n",
      "Iteration 4133, loss = 0.22489090\n",
      "Iteration 4134, loss = 0.22473557\n",
      "Iteration 4135, loss = 0.22458040\n",
      "Iteration 4136, loss = 0.22442537\n",
      "Iteration 4137, loss = 0.22427050\n",
      "Iteration 4138, loss = 0.22411577\n",
      "Iteration 4139, loss = 0.22396119\n",
      "Iteration 4140, loss = 0.22380675\n",
      "Iteration 4141, loss = 0.22365247\n",
      "Iteration 4142, loss = 0.22349833\n",
      "Iteration 4143, loss = 0.22334434\n",
      "Iteration 4144, loss = 0.22319050\n",
      "Iteration 4145, loss = 0.22303681\n",
      "Iteration 4146, loss = 0.22288326\n",
      "Iteration 4147, loss = 0.22272986\n",
      "Iteration 4148, loss = 0.22257661\n",
      "Iteration 4149, loss = 0.22242351\n",
      "Iteration 4150, loss = 0.22227055\n",
      "Iteration 4151, loss = 0.22211774\n",
      "Iteration 4152, loss = 0.22196507\n",
      "Iteration 4153, loss = 0.22181256\n",
      "Iteration 4154, loss = 0.22166019\n",
      "Iteration 4155, loss = 0.22150796\n",
      "Iteration 4156, loss = 0.22135588\n",
      "Iteration 4157, loss = 0.22120395\n",
      "Iteration 4158, loss = 0.22105216\n",
      "Iteration 4159, loss = 0.22090052\n",
      "Iteration 4160, loss = 0.22074902\n",
      "Iteration 4161, loss = 0.22059767\n",
      "Iteration 4162, loss = 0.22044647\n",
      "Iteration 4163, loss = 0.22029541\n",
      "Iteration 4164, loss = 0.22014449\n",
      "Iteration 4165, loss = 0.21999372\n",
      "Iteration 4166, loss = 0.21984310\n",
      "Iteration 4167, loss = 0.21969261\n",
      "Iteration 4168, loss = 0.21954228\n",
      "Iteration 4169, loss = 0.21939209\n",
      "Iteration 4170, loss = 0.21924204\n",
      "Iteration 4171, loss = 0.21909213\n",
      "Iteration 4172, loss = 0.21894237\n",
      "Iteration 4173, loss = 0.21879276\n",
      "Iteration 4174, loss = 0.21864329\n",
      "Iteration 4175, loss = 0.21849396\n",
      "Iteration 4176, loss = 0.21834477\n",
      "Iteration 4177, loss = 0.21819573\n",
      "Iteration 4178, loss = 0.21804683\n",
      "Iteration 4179, loss = 0.21789807\n",
      "Iteration 4180, loss = 0.21774946\n",
      "Iteration 4181, loss = 0.21760099\n",
      "Iteration 4182, loss = 0.21745266\n",
      "Iteration 4183, loss = 0.21730448\n",
      "Iteration 4184, loss = 0.21715643\n",
      "Iteration 4185, loss = 0.21700853\n",
      "Iteration 4186, loss = 0.21686077\n",
      "Iteration 4187, loss = 0.21671315\n",
      "Iteration 4188, loss = 0.21656568\n",
      "Iteration 4189, loss = 0.21641835\n",
      "Iteration 4190, loss = 0.21627115\n",
      "Iteration 4191, loss = 0.21612410\n",
      "Iteration 4192, loss = 0.21597719\n",
      "Iteration 4193, loss = 0.21583042\n",
      "Iteration 4194, loss = 0.21568380\n",
      "Iteration 4195, loss = 0.21553731\n",
      "Iteration 4196, loss = 0.21539096\n",
      "Iteration 4197, loss = 0.21524476\n",
      "Iteration 4198, loss = 0.21509869\n",
      "Iteration 4199, loss = 0.21495277\n",
      "Iteration 4200, loss = 0.21480698\n",
      "Iteration 4201, loss = 0.21466134\n",
      "Iteration 4202, loss = 0.21451584\n",
      "Iteration 4203, loss = 0.21437047\n",
      "Iteration 4204, loss = 0.21422525\n",
      "Iteration 4205, loss = 0.21408016\n",
      "Iteration 4206, loss = 0.21393521\n",
      "Iteration 4207, loss = 0.21379041\n",
      "Iteration 4208, loss = 0.21364574\n",
      "Iteration 4209, loss = 0.21350121\n",
      "Iteration 4210, loss = 0.21335682\n",
      "Iteration 4211, loss = 0.21321257\n",
      "Iteration 4212, loss = 0.21306845\n",
      "Iteration 4213, loss = 0.21292448\n",
      "Iteration 4214, loss = 0.21278064\n",
      "Iteration 4215, loss = 0.21263694\n",
      "Iteration 4216, loss = 0.21249338\n",
      "Iteration 4217, loss = 0.21234996\n",
      "Iteration 4218, loss = 0.21220668\n",
      "Iteration 4219, loss = 0.21206353\n",
      "Iteration 4220, loss = 0.21192052\n",
      "Iteration 4221, loss = 0.21177765\n",
      "Iteration 4222, loss = 0.21163491\n",
      "Iteration 4223, loss = 0.21149232\n",
      "Iteration 4224, loss = 0.21134985\n",
      "Iteration 4225, loss = 0.21120753\n",
      "Iteration 4226, loss = 0.21106534\n",
      "Iteration 4227, loss = 0.21092329\n",
      "Iteration 4228, loss = 0.21078138\n",
      "Iteration 4229, loss = 0.21063960\n",
      "Iteration 4230, loss = 0.21049796\n",
      "Iteration 4231, loss = 0.21035645\n",
      "Iteration 4232, loss = 0.21021508\n",
      "Iteration 4233, loss = 0.21007385\n",
      "Iteration 4234, loss = 0.20993275\n",
      "Iteration 4235, loss = 0.20979178\n",
      "Iteration 4236, loss = 0.20965095\n",
      "Iteration 4237, loss = 0.20951026\n",
      "Iteration 4238, loss = 0.20936970\n",
      "Iteration 4239, loss = 0.20922928\n",
      "Iteration 4240, loss = 0.20908899\n",
      "Iteration 4241, loss = 0.20894884\n",
      "Iteration 4242, loss = 0.20880882\n",
      "Iteration 4243, loss = 0.20866893\n",
      "Iteration 4244, loss = 0.20852918\n",
      "Iteration 4245, loss = 0.20838957\n",
      "Iteration 4246, loss = 0.20825009\n",
      "Iteration 4247, loss = 0.20811074\n",
      "Iteration 4248, loss = 0.20797152\n",
      "Iteration 4249, loss = 0.20783244\n",
      "Iteration 4250, loss = 0.20769349\n",
      "Iteration 4251, loss = 0.20755468\n",
      "Iteration 4252, loss = 0.20741600\n",
      "Iteration 4253, loss = 0.20727745\n",
      "Iteration 4254, loss = 0.20713903\n",
      "Iteration 4255, loss = 0.20700075\n",
      "Iteration 4256, loss = 0.20686260\n",
      "Iteration 4257, loss = 0.20672459\n",
      "Iteration 4258, loss = 0.20658670\n",
      "Iteration 4259, loss = 0.20644895\n",
      "Iteration 4260, loss = 0.20631133\n",
      "Iteration 4261, loss = 0.20617384\n",
      "Iteration 4262, loss = 0.20603648\n",
      "Iteration 4263, loss = 0.20589926\n",
      "Iteration 4264, loss = 0.20576216\n",
      "Iteration 4265, loss = 0.20562520\n",
      "Iteration 4266, loss = 0.20548837\n",
      "Iteration 4267, loss = 0.20535167\n",
      "Iteration 4268, loss = 0.20521510\n",
      "Iteration 4269, loss = 0.20507867\n",
      "Iteration 4270, loss = 0.20494236\n",
      "Iteration 4271, loss = 0.20480618\n",
      "Iteration 4272, loss = 0.20467014\n",
      "Iteration 4273, loss = 0.20453422\n",
      "Iteration 4274, loss = 0.20439844\n",
      "Iteration 4275, loss = 0.20426279\n",
      "Iteration 4276, loss = 0.20412726\n",
      "Iteration 4277, loss = 0.20399187\n",
      "Iteration 4278, loss = 0.20385660\n",
      "Iteration 4279, loss = 0.20372147\n",
      "Iteration 4280, loss = 0.20358646\n",
      "Iteration 4281, loss = 0.20345158\n",
      "Iteration 4282, loss = 0.20331684\n",
      "Iteration 4283, loss = 0.20318222\n",
      "Iteration 4284, loss = 0.20304773\n",
      "Iteration 4285, loss = 0.20291337\n",
      "Iteration 4286, loss = 0.20277914\n",
      "Iteration 4287, loss = 0.20264504\n",
      "Iteration 4288, loss = 0.20251106\n",
      "Iteration 4289, loss = 0.20237721\n",
      "Iteration 4290, loss = 0.20224350\n",
      "Iteration 4291, loss = 0.20210991\n",
      "Iteration 4292, loss = 0.20197644\n",
      "Iteration 4293, loss = 0.20184311\n",
      "Iteration 4294, loss = 0.20170990\n",
      "Iteration 4295, loss = 0.20157682\n",
      "Iteration 4296, loss = 0.20144387\n",
      "Iteration 4297, loss = 0.20131105\n",
      "Iteration 4298, loss = 0.20117835\n",
      "Iteration 4299, loss = 0.20104578\n",
      "Iteration 4300, loss = 0.20091334\n",
      "Iteration 4301, loss = 0.20078102\n",
      "Iteration 4302, loss = 0.20064883\n",
      "Iteration 4303, loss = 0.20051676\n",
      "Iteration 4304, loss = 0.20038483\n",
      "Iteration 4305, loss = 0.20025302\n",
      "Iteration 4306, loss = 0.20012133\n",
      "Iteration 4307, loss = 0.19998977\n",
      "Iteration 4308, loss = 0.19985834\n",
      "Iteration 4309, loss = 0.19972703\n",
      "Iteration 4310, loss = 0.19959585\n",
      "Iteration 4311, loss = 0.19946479\n",
      "Iteration 4312, loss = 0.19933386\n",
      "Iteration 4313, loss = 0.19920305\n",
      "Iteration 4314, loss = 0.19907237\n",
      "Iteration 4315, loss = 0.19894182\n",
      "Iteration 4316, loss = 0.19881138\n",
      "Iteration 4317, loss = 0.19868108\n",
      "Iteration 4318, loss = 0.19855089\n",
      "Iteration 4319, loss = 0.19842084\n",
      "Iteration 4320, loss = 0.19829090\n",
      "Iteration 4321, loss = 0.19816109\n",
      "Iteration 4322, loss = 0.19803141\n",
      "Iteration 4323, loss = 0.19790185\n",
      "Iteration 4324, loss = 0.19777241\n",
      "Iteration 4325, loss = 0.19764309\n",
      "Iteration 4326, loss = 0.19751390\n",
      "Iteration 4327, loss = 0.19738484\n",
      "Iteration 4328, loss = 0.19725589\n",
      "Iteration 4329, loss = 0.19712707\n",
      "Iteration 4330, loss = 0.19699837\n",
      "Iteration 4331, loss = 0.19686980\n",
      "Iteration 4332, loss = 0.19674134\n",
      "Iteration 4333, loss = 0.19661301\n",
      "Iteration 4334, loss = 0.19648481\n",
      "Iteration 4335, loss = 0.19635672\n",
      "Iteration 4336, loss = 0.19622876\n",
      "Iteration 4337, loss = 0.19610092\n",
      "Iteration 4338, loss = 0.19597320\n",
      "Iteration 4339, loss = 0.19584560\n",
      "Iteration 4340, loss = 0.19571813\n",
      "Iteration 4341, loss = 0.19559077\n",
      "Iteration 4342, loss = 0.19546354\n",
      "Iteration 4343, loss = 0.19533643\n",
      "Iteration 4344, loss = 0.19520944\n",
      "Iteration 4345, loss = 0.19508257\n",
      "Iteration 4346, loss = 0.19495582\n",
      "Iteration 4347, loss = 0.19482919\n",
      "Iteration 4348, loss = 0.19470269\n",
      "Iteration 4349, loss = 0.19457630\n",
      "Iteration 4350, loss = 0.19445004\n",
      "Iteration 4351, loss = 0.19432389\n",
      "Iteration 4352, loss = 0.19419787\n",
      "Iteration 4353, loss = 0.19407196\n",
      "Iteration 4354, loss = 0.19394618\n",
      "Iteration 4355, loss = 0.19382051\n",
      "Iteration 4356, loss = 0.19369497\n",
      "Iteration 4357, loss = 0.19356954\n",
      "Iteration 4358, loss = 0.19344423\n",
      "Iteration 4359, loss = 0.19331905\n",
      "Iteration 4360, loss = 0.19319398\n",
      "Iteration 4361, loss = 0.19306903\n",
      "Iteration 4362, loss = 0.19294420\n",
      "Iteration 4363, loss = 0.19281949\n",
      "Iteration 4364, loss = 0.19269490\n",
      "Iteration 4365, loss = 0.19257042\n",
      "Iteration 4366, loss = 0.19244607\n",
      "Iteration 4367, loss = 0.19232183\n",
      "Iteration 4368, loss = 0.19219771\n",
      "Iteration 4369, loss = 0.19207371\n",
      "Iteration 4370, loss = 0.19194982\n",
      "Iteration 4371, loss = 0.19182606\n",
      "Iteration 4372, loss = 0.19170241\n",
      "Iteration 4373, loss = 0.19157888\n",
      "Iteration 4374, loss = 0.19145547\n",
      "Iteration 4375, loss = 0.19133217\n",
      "Iteration 4376, loss = 0.19120899\n",
      "Iteration 4377, loss = 0.19108593\n",
      "Iteration 4378, loss = 0.19096299\n",
      "Iteration 4379, loss = 0.19084016\n",
      "Iteration 4380, loss = 0.19071745\n",
      "Iteration 4381, loss = 0.19059485\n",
      "Iteration 4382, loss = 0.19047238\n",
      "Iteration 4383, loss = 0.19035001\n",
      "Iteration 4384, loss = 0.19022777\n",
      "Iteration 4385, loss = 0.19010564\n",
      "Iteration 4386, loss = 0.18998363\n",
      "Iteration 4387, loss = 0.18986173\n",
      "Iteration 4388, loss = 0.18973994\n",
      "Iteration 4389, loss = 0.18961828\n",
      "Iteration 4390, loss = 0.18949673\n",
      "Iteration 4391, loss = 0.18937529\n",
      "Iteration 4392, loss = 0.18925397\n",
      "Iteration 4393, loss = 0.18913276\n",
      "Iteration 4394, loss = 0.18901167\n",
      "Iteration 4395, loss = 0.18889070\n",
      "Iteration 4396, loss = 0.18876984\n",
      "Iteration 4397, loss = 0.18864909\n",
      "Iteration 4398, loss = 0.18852846\n",
      "Iteration 4399, loss = 0.18840794\n",
      "Iteration 4400, loss = 0.18828754\n",
      "Iteration 4401, loss = 0.18816725\n",
      "Iteration 4402, loss = 0.18804707\n",
      "Iteration 4403, loss = 0.18792701\n",
      "Iteration 4404, loss = 0.18780706\n",
      "Iteration 4405, loss = 0.18768723\n",
      "Iteration 4406, loss = 0.18756751\n",
      "Iteration 4407, loss = 0.18744790\n",
      "Iteration 4408, loss = 0.18732840\n",
      "Iteration 4409, loss = 0.18720902\n",
      "Iteration 4410, loss = 0.18708975\n",
      "Iteration 4411, loss = 0.18697060\n",
      "Iteration 4412, loss = 0.18685156\n",
      "Iteration 4413, loss = 0.18673262\n",
      "Iteration 4414, loss = 0.18661381\n",
      "Iteration 4415, loss = 0.18649510\n",
      "Iteration 4416, loss = 0.18637651\n",
      "Iteration 4417, loss = 0.18625803\n",
      "Iteration 4418, loss = 0.18613966\n",
      "Iteration 4419, loss = 0.18602140\n",
      "Iteration 4420, loss = 0.18590326\n",
      "Iteration 4421, loss = 0.18578522\n",
      "Iteration 4422, loss = 0.18566730\n",
      "Iteration 4423, loss = 0.18554949\n",
      "Iteration 4424, loss = 0.18543179\n",
      "Iteration 4425, loss = 0.18531420\n",
      "Iteration 4426, loss = 0.18519673\n",
      "Iteration 4427, loss = 0.18507936\n",
      "Iteration 4428, loss = 0.18496210\n",
      "Iteration 4429, loss = 0.18484496\n",
      "Iteration 4430, loss = 0.18472793\n",
      "Iteration 4431, loss = 0.18461100\n",
      "Iteration 4432, loss = 0.18449419\n",
      "Iteration 4433, loss = 0.18437749\n",
      "Iteration 4434, loss = 0.18426089\n",
      "Iteration 4435, loss = 0.18414441\n",
      "Iteration 4436, loss = 0.18402804\n",
      "Iteration 4437, loss = 0.18391177\n",
      "Iteration 4438, loss = 0.18379562\n",
      "Iteration 4439, loss = 0.18367958\n",
      "Iteration 4440, loss = 0.18356364\n",
      "Iteration 4441, loss = 0.18344782\n",
      "Iteration 4442, loss = 0.18333210\n",
      "Iteration 4443, loss = 0.18321649\n",
      "Iteration 4444, loss = 0.18310099\n",
      "Iteration 4445, loss = 0.18298560\n",
      "Iteration 4446, loss = 0.18287032\n",
      "Iteration 4447, loss = 0.18275515\n",
      "Iteration 4448, loss = 0.18264009\n",
      "Iteration 4449, loss = 0.18252513\n",
      "Iteration 4450, loss = 0.18241028\n",
      "Iteration 4451, loss = 0.18229554\n",
      "Iteration 4452, loss = 0.18218091\n",
      "Iteration 4453, loss = 0.18206639\n",
      "Iteration 4454, loss = 0.18195197\n",
      "Iteration 4455, loss = 0.18183766\n",
      "Iteration 4456, loss = 0.18172346\n",
      "Iteration 4457, loss = 0.18160937\n",
      "Iteration 4458, loss = 0.18149538\n",
      "Iteration 4459, loss = 0.18138151\n",
      "Iteration 4460, loss = 0.18126773\n",
      "Iteration 4461, loss = 0.18115407\n",
      "Iteration 4462, loss = 0.18104051\n",
      "Iteration 4463, loss = 0.18092706\n",
      "Iteration 4464, loss = 0.18081372\n",
      "Iteration 4465, loss = 0.18070048\n",
      "Iteration 4466, loss = 0.18058734\n",
      "Iteration 4467, loss = 0.18047432\n",
      "Iteration 4468, loss = 0.18036140\n",
      "Iteration 4469, loss = 0.18024859\n",
      "Iteration 4470, loss = 0.18013588\n",
      "Iteration 4471, loss = 0.18002328\n",
      "Iteration 4472, loss = 0.17991078\n",
      "Iteration 4473, loss = 0.17979839\n",
      "Iteration 4474, loss = 0.17968610\n",
      "Iteration 4475, loss = 0.17957392\n",
      "Iteration 4476, loss = 0.17946185\n",
      "Iteration 4477, loss = 0.17934988\n",
      "Iteration 4478, loss = 0.17923801\n",
      "Iteration 4479, loss = 0.17912625\n",
      "Iteration 4480, loss = 0.17901460\n",
      "Iteration 4481, loss = 0.17890305\n",
      "Iteration 4482, loss = 0.17879160\n",
      "Iteration 4483, loss = 0.17868026\n",
      "Iteration 4484, loss = 0.17856902\n",
      "Iteration 4485, loss = 0.17845789\n",
      "Iteration 4486, loss = 0.17834686\n",
      "Iteration 4487, loss = 0.17823594\n",
      "Iteration 4488, loss = 0.17812511\n",
      "Iteration 4489, loss = 0.17801440\n",
      "Iteration 4490, loss = 0.17790378\n",
      "Iteration 4491, loss = 0.17779327\n",
      "Iteration 4492, loss = 0.17768286\n",
      "Iteration 4493, loss = 0.17757256\n",
      "Iteration 4494, loss = 0.17746236\n",
      "Iteration 4495, loss = 0.17735226\n",
      "Iteration 4496, loss = 0.17724227\n",
      "Iteration 4497, loss = 0.17713237\n",
      "Iteration 4498, loss = 0.17702258\n",
      "Iteration 4499, loss = 0.17691290\n",
      "Iteration 4500, loss = 0.17680331\n",
      "Iteration 4501, loss = 0.17669383\n",
      "Iteration 4502, loss = 0.17658445\n",
      "Iteration 4503, loss = 0.17647517\n",
      "Iteration 4504, loss = 0.17636600\n",
      "Iteration 4505, loss = 0.17625692\n",
      "Iteration 4506, loss = 0.17614795\n",
      "Iteration 4507, loss = 0.17603908\n",
      "Iteration 4508, loss = 0.17593031\n",
      "Iteration 4509, loss = 0.17582165\n",
      "Iteration 4510, loss = 0.17571308\n",
      "Iteration 4511, loss = 0.17560462\n",
      "Iteration 4512, loss = 0.17549625\n",
      "Iteration 4513, loss = 0.17538799\n",
      "Iteration 4514, loss = 0.17527983\n",
      "Iteration 4515, loss = 0.17517177\n",
      "Iteration 4516, loss = 0.17506381\n",
      "Iteration 4517, loss = 0.17495595\n",
      "Iteration 4518, loss = 0.17484819\n",
      "Iteration 4519, loss = 0.17474053\n",
      "Iteration 4520, loss = 0.17463297\n",
      "Iteration 4521, loss = 0.17452551\n",
      "Iteration 4522, loss = 0.17441816\n",
      "Iteration 4523, loss = 0.17431090\n",
      "Iteration 4524, loss = 0.17420374\n",
      "Iteration 4525, loss = 0.17409668\n",
      "Iteration 4526, loss = 0.17398972\n",
      "Iteration 4527, loss = 0.17388286\n",
      "Iteration 4528, loss = 0.17377610\n",
      "Iteration 4529, loss = 0.17366944\n",
      "Iteration 4530, loss = 0.17356288\n",
      "Iteration 4531, loss = 0.17345642\n",
      "Iteration 4532, loss = 0.17335005\n",
      "Iteration 4533, loss = 0.17324379\n",
      "Iteration 4534, loss = 0.17313762\n",
      "Iteration 4535, loss = 0.17303155\n",
      "Iteration 4536, loss = 0.17292558\n",
      "Iteration 4537, loss = 0.17281971\n",
      "Iteration 4538, loss = 0.17271394\n",
      "Iteration 4539, loss = 0.17260827\n",
      "Iteration 4540, loss = 0.17250269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4541, loss = 0.17239721\n",
      "Iteration 4542, loss = 0.17229183\n",
      "Iteration 4543, loss = 0.17218655\n",
      "Iteration 4544, loss = 0.17208136\n",
      "Iteration 4545, loss = 0.17197627\n",
      "Iteration 4546, loss = 0.17187128\n",
      "Iteration 4547, loss = 0.17176639\n",
      "Iteration 4548, loss = 0.17166160\n",
      "Iteration 4549, loss = 0.17155690\n",
      "Iteration 4550, loss = 0.17145230\n",
      "Iteration 4551, loss = 0.17134779\n",
      "Iteration 4552, loss = 0.17124338\n",
      "Iteration 4553, loss = 0.17113907\n",
      "Iteration 4554, loss = 0.17103486\n",
      "Iteration 4555, loss = 0.17093074\n",
      "Iteration 4556, loss = 0.17082672\n",
      "Iteration 4557, loss = 0.17072279\n",
      "Iteration 4558, loss = 0.17061896\n",
      "Iteration 4559, loss = 0.17051523\n",
      "Iteration 4560, loss = 0.17041159\n",
      "Iteration 4561, loss = 0.17030805\n",
      "Iteration 4562, loss = 0.17020460\n",
      "Iteration 4563, loss = 0.17010125\n",
      "Iteration 4564, loss = 0.16999800\n",
      "Iteration 4565, loss = 0.16989484\n",
      "Iteration 4566, loss = 0.16979178\n",
      "Iteration 4567, loss = 0.16968881\n",
      "Iteration 4568, loss = 0.16958593\n",
      "Iteration 4569, loss = 0.16948315\n",
      "Iteration 4570, loss = 0.16938047\n",
      "Iteration 4571, loss = 0.16927788\n",
      "Iteration 4572, loss = 0.16917539\n",
      "Iteration 4573, loss = 0.16907299\n",
      "Iteration 4574, loss = 0.16897068\n",
      "Iteration 4575, loss = 0.16886847\n",
      "Iteration 4576, loss = 0.16876635\n",
      "Iteration 4577, loss = 0.16866433\n",
      "Iteration 4578, loss = 0.16856240\n",
      "Iteration 4579, loss = 0.16846057\n",
      "Iteration 4580, loss = 0.16835883\n",
      "Iteration 4581, loss = 0.16825718\n",
      "Iteration 4582, loss = 0.16815563\n",
      "Iteration 4583, loss = 0.16805417\n",
      "Iteration 4584, loss = 0.16795280\n",
      "Iteration 4585, loss = 0.16785153\n",
      "Iteration 4586, loss = 0.16775035\n",
      "Iteration 4587, loss = 0.16764926\n",
      "Iteration 4588, loss = 0.16754827\n",
      "Iteration 4589, loss = 0.16744736\n",
      "Iteration 4590, loss = 0.16734656\n",
      "Iteration 4591, loss = 0.16724584\n",
      "Iteration 4592, loss = 0.16714522\n",
      "Iteration 4593, loss = 0.16704469\n",
      "Iteration 4594, loss = 0.16694425\n",
      "Iteration 4595, loss = 0.16684390\n",
      "Iteration 4596, loss = 0.16674365\n",
      "Iteration 4597, loss = 0.16664349\n",
      "Iteration 4598, loss = 0.16654342\n",
      "Iteration 4599, loss = 0.16644344\n",
      "Iteration 4600, loss = 0.16634356\n",
      "Iteration 4601, loss = 0.16624376\n",
      "Iteration 4602, loss = 0.16614406\n",
      "Iteration 4603, loss = 0.16604445\n",
      "Iteration 4604, loss = 0.16594493\n",
      "Iteration 4605, loss = 0.16584550\n",
      "Iteration 4606, loss = 0.16574616\n",
      "Iteration 4607, loss = 0.16564692\n",
      "Iteration 4608, loss = 0.16554776\n",
      "Iteration 4609, loss = 0.16544870\n",
      "Iteration 4610, loss = 0.16534973\n",
      "Iteration 4611, loss = 0.16525084\n",
      "Iteration 4612, loss = 0.16515205\n",
      "Iteration 4613, loss = 0.16505335\n",
      "Iteration 4614, loss = 0.16495474\n",
      "Iteration 4615, loss = 0.16485622\n",
      "Iteration 4616, loss = 0.16475779\n",
      "Iteration 4617, loss = 0.16465945\n",
      "Iteration 4618, loss = 0.16456120\n",
      "Iteration 4619, loss = 0.16446303\n",
      "Iteration 4620, loss = 0.16436496\n",
      "Iteration 4621, loss = 0.16426698\n",
      "Iteration 4622, loss = 0.16416909\n",
      "Iteration 4623, loss = 0.16407129\n",
      "Iteration 4624, loss = 0.16397357\n",
      "Iteration 4625, loss = 0.16387595\n",
      "Iteration 4626, loss = 0.16377842\n",
      "Iteration 4627, loss = 0.16368097\n",
      "Iteration 4628, loss = 0.16358361\n",
      "Iteration 4629, loss = 0.16348635\n",
      "Iteration 4630, loss = 0.16338917\n",
      "Iteration 4631, loss = 0.16329208\n",
      "Iteration 4632, loss = 0.16319508\n",
      "Iteration 4633, loss = 0.16309816\n",
      "Iteration 4634, loss = 0.16300134\n",
      "Iteration 4635, loss = 0.16290460\n",
      "Iteration 4636, loss = 0.16280796\n",
      "Iteration 4637, loss = 0.16271140\n",
      "Iteration 4638, loss = 0.16261492\n",
      "Iteration 4639, loss = 0.16251854\n",
      "Iteration 4640, loss = 0.16242224\n",
      "Iteration 4641, loss = 0.16232604\n",
      "Iteration 4642, loss = 0.16222991\n",
      "Iteration 4643, loss = 0.16213388\n",
      "Iteration 4644, loss = 0.16203794\n",
      "Iteration 4645, loss = 0.16194208\n",
      "Iteration 4646, loss = 0.16184631\n",
      "Iteration 4647, loss = 0.16175062\n",
      "Iteration 4648, loss = 0.16165502\n",
      "Iteration 4649, loss = 0.16155951\n",
      "Iteration 4650, loss = 0.16146409\n",
      "Iteration 4651, loss = 0.16136875\n",
      "Iteration 4652, loss = 0.16127350\n",
      "Iteration 4653, loss = 0.16117834\n",
      "Iteration 4654, loss = 0.16108326\n",
      "Iteration 4655, loss = 0.16098827\n",
      "Iteration 4656, loss = 0.16089337\n",
      "Iteration 4657, loss = 0.16079855\n",
      "Iteration 4658, loss = 0.16070382\n",
      "Iteration 4659, loss = 0.16060917\n",
      "Iteration 4660, loss = 0.16051461\n",
      "Iteration 4661, loss = 0.16042014\n",
      "Iteration 4662, loss = 0.16032575\n",
      "Iteration 4663, loss = 0.16023145\n",
      "Iteration 4664, loss = 0.16013723\n",
      "Iteration 4665, loss = 0.16004310\n",
      "Iteration 4666, loss = 0.15994905\n",
      "Iteration 4667, loss = 0.15985509\n",
      "Iteration 4668, loss = 0.15976121\n",
      "Iteration 4669, loss = 0.15966742\n",
      "Iteration 4670, loss = 0.15957372\n",
      "Iteration 4671, loss = 0.15948009\n",
      "Iteration 4672, loss = 0.15938656\n",
      "Iteration 4673, loss = 0.15929310\n",
      "Iteration 4674, loss = 0.15919974\n",
      "Iteration 4675, loss = 0.15910645\n",
      "Iteration 4676, loss = 0.15901326\n",
      "Iteration 4677, loss = 0.15892014\n",
      "Iteration 4678, loss = 0.15882711\n",
      "Iteration 4679, loss = 0.15873416\n",
      "Iteration 4680, loss = 0.15864130\n",
      "Iteration 4681, loss = 0.15854852\n",
      "Iteration 4682, loss = 0.15845583\n",
      "Iteration 4683, loss = 0.15836322\n",
      "Iteration 4684, loss = 0.15827069\n",
      "Iteration 4685, loss = 0.15817825\n",
      "Iteration 4686, loss = 0.15808589\n",
      "Iteration 4687, loss = 0.15799361\n",
      "Iteration 4688, loss = 0.15790142\n",
      "Iteration 4689, loss = 0.15780931\n",
      "Iteration 4690, loss = 0.15771728\n",
      "Iteration 4691, loss = 0.15762534\n",
      "Iteration 4692, loss = 0.15753348\n",
      "Iteration 4693, loss = 0.15744170\n",
      "Iteration 4694, loss = 0.15735000\n",
      "Iteration 4695, loss = 0.15725839\n",
      "Iteration 4696, loss = 0.15716686\n",
      "Iteration 4697, loss = 0.15707541\n",
      "Iteration 4698, loss = 0.15698405\n",
      "Iteration 4699, loss = 0.15689276\n",
      "Iteration 4700, loss = 0.15680156\n",
      "Iteration 4701, loss = 0.15671044\n",
      "Iteration 4702, loss = 0.15661940\n",
      "Iteration 4703, loss = 0.15652845\n",
      "Iteration 4704, loss = 0.15643758\n",
      "Iteration 4705, loss = 0.15634678\n",
      "Iteration 4706, loss = 0.15625607\n",
      "Iteration 4707, loss = 0.15616545\n",
      "Iteration 4708, loss = 0.15607490\n",
      "Iteration 4709, loss = 0.15598443\n",
      "Iteration 4710, loss = 0.15589405\n",
      "Iteration 4711, loss = 0.15580374\n",
      "Iteration 4712, loss = 0.15571352\n",
      "Iteration 4713, loss = 0.15562338\n",
      "Iteration 4714, loss = 0.15553332\n",
      "Iteration 4715, loss = 0.15544334\n",
      "Iteration 4716, loss = 0.15535344\n",
      "Iteration 4717, loss = 0.15526362\n",
      "Iteration 4718, loss = 0.15517389\n",
      "Iteration 4719, loss = 0.15508423\n",
      "Iteration 4720, loss = 0.15499465\n",
      "Iteration 4721, loss = 0.15490516\n",
      "Iteration 4722, loss = 0.15481574\n",
      "Iteration 4723, loss = 0.15472640\n",
      "Iteration 4724, loss = 0.15463715\n",
      "Iteration 4725, loss = 0.15454797\n",
      "Iteration 4726, loss = 0.15445888\n",
      "Iteration 4727, loss = 0.15436986\n",
      "Iteration 4728, loss = 0.15428092\n",
      "Iteration 4729, loss = 0.15419206\n",
      "Iteration 4730, loss = 0.15410329\n",
      "Iteration 4731, loss = 0.15401459\n",
      "Iteration 4732, loss = 0.15392597\n",
      "Iteration 4733, loss = 0.15383743\n",
      "Iteration 4734, loss = 0.15374897\n",
      "Iteration 4735, loss = 0.15366059\n",
      "Iteration 4736, loss = 0.15357228\n",
      "Iteration 4737, loss = 0.15348406\n",
      "Iteration 4738, loss = 0.15339592\n",
      "Iteration 4739, loss = 0.15330785\n",
      "Iteration 4740, loss = 0.15321986\n",
      "Iteration 4741, loss = 0.15313195\n",
      "Iteration 4742, loss = 0.15304412\n",
      "Iteration 4743, loss = 0.15295637\n",
      "Iteration 4744, loss = 0.15286869\n",
      "Iteration 4745, loss = 0.15278110\n",
      "Iteration 4746, loss = 0.15269358\n",
      "Iteration 4747, loss = 0.15260614\n",
      "Iteration 4748, loss = 0.15251878\n",
      "Iteration 4749, loss = 0.15243149\n",
      "Iteration 4750, loss = 0.15234429\n",
      "Iteration 4751, loss = 0.15225716\n",
      "Iteration 4752, loss = 0.15217011\n",
      "Iteration 4753, loss = 0.15208313\n",
      "Iteration 4754, loss = 0.15199624\n",
      "Iteration 4755, loss = 0.15190942\n",
      "Iteration 4756, loss = 0.15182268\n",
      "Iteration 4757, loss = 0.15173601\n",
      "Iteration 4758, loss = 0.15164943\n",
      "Iteration 4759, loss = 0.15156292\n",
      "Iteration 4760, loss = 0.15147648\n",
      "Iteration 4761, loss = 0.15139012\n",
      "Iteration 4762, loss = 0.15130384\n",
      "Iteration 4763, loss = 0.15121764\n",
      "Iteration 4764, loss = 0.15113151\n",
      "Iteration 4765, loss = 0.15104546\n",
      "Iteration 4766, loss = 0.15095949\n",
      "Iteration 4767, loss = 0.15087359\n",
      "Iteration 4768, loss = 0.15078777\n",
      "Iteration 4769, loss = 0.15070203\n",
      "Iteration 4770, loss = 0.15061636\n",
      "Iteration 4771, loss = 0.15053076\n",
      "Iteration 4772, loss = 0.15044525\n",
      "Iteration 4773, loss = 0.15035980\n",
      "Iteration 4774, loss = 0.15027444\n",
      "Iteration 4775, loss = 0.15018915\n",
      "Iteration 4776, loss = 0.15010393\n",
      "Iteration 4777, loss = 0.15001879\n",
      "Iteration 4778, loss = 0.14993373\n",
      "Iteration 4779, loss = 0.14984874\n",
      "Iteration 4780, loss = 0.14976383\n",
      "Iteration 4781, loss = 0.14967899\n",
      "Iteration 4782, loss = 0.14959423\n",
      "Iteration 4783, loss = 0.14950954\n",
      "Iteration 4784, loss = 0.14942493\n",
      "Iteration 4785, loss = 0.14934039\n",
      "Iteration 4786, loss = 0.14925592\n",
      "Iteration 4787, loss = 0.14917154\n",
      "Iteration 4788, loss = 0.14908722\n",
      "Iteration 4789, loss = 0.14900298\n",
      "Iteration 4790, loss = 0.14891882\n",
      "Iteration 4791, loss = 0.14883472\n",
      "Iteration 4792, loss = 0.14875071\n",
      "Iteration 4793, loss = 0.14866676\n",
      "Iteration 4794, loss = 0.14858290\n",
      "Iteration 4795, loss = 0.14849910\n",
      "Iteration 4796, loss = 0.14841538\n",
      "Iteration 4797, loss = 0.14833173\n",
      "Iteration 4798, loss = 0.14824816\n",
      "Iteration 4799, loss = 0.14816466\n",
      "Iteration 4800, loss = 0.14808123\n",
      "Iteration 4801, loss = 0.14799788\n",
      "Iteration 4802, loss = 0.14791460\n",
      "Iteration 4803, loss = 0.14783139\n",
      "Iteration 4804, loss = 0.14774826\n",
      "Iteration 4805, loss = 0.14766520\n",
      "Iteration 4806, loss = 0.14758222\n",
      "Iteration 4807, loss = 0.14749930\n",
      "Iteration 4808, loss = 0.14741646\n",
      "Iteration 4809, loss = 0.14733369\n",
      "Iteration 4810, loss = 0.14725100\n",
      "Iteration 4811, loss = 0.14716838\n",
      "Iteration 4812, loss = 0.14708583\n",
      "Iteration 4813, loss = 0.14700335\n",
      "Iteration 4814, loss = 0.14692095\n",
      "Iteration 4815, loss = 0.14683862\n",
      "Iteration 4816, loss = 0.14675636\n",
      "Iteration 4817, loss = 0.14667417\n",
      "Iteration 4818, loss = 0.14659205\n",
      "Iteration 4819, loss = 0.14651001\n",
      "Iteration 4820, loss = 0.14642804\n",
      "Iteration 4821, loss = 0.14634614\n",
      "Iteration 4822, loss = 0.14626431\n",
      "Iteration 4823, loss = 0.14618255\n",
      "Iteration 4824, loss = 0.14610087\n",
      "Iteration 4825, loss = 0.14601926\n",
      "Iteration 4826, loss = 0.14593772\n",
      "Iteration 4827, loss = 0.14585625\n",
      "Iteration 4828, loss = 0.14577485\n",
      "Iteration 4829, loss = 0.14569352\n",
      "Iteration 4830, loss = 0.14561226\n",
      "Iteration 4831, loss = 0.14553108\n",
      "Iteration 4832, loss = 0.14544997\n",
      "Iteration 4833, loss = 0.14536892\n",
      "Iteration 4834, loss = 0.14528795\n",
      "Iteration 4835, loss = 0.14520705\n",
      "Iteration 4836, loss = 0.14512622\n",
      "Iteration 4837, loss = 0.14504546\n",
      "Iteration 4838, loss = 0.14496477\n",
      "Iteration 4839, loss = 0.14488415\n",
      "Iteration 4840, loss = 0.14480360\n",
      "Iteration 4841, loss = 0.14472312\n",
      "Iteration 4842, loss = 0.14464272\n",
      "Iteration 4843, loss = 0.14456238\n",
      "Iteration 4844, loss = 0.14448211\n",
      "Iteration 4845, loss = 0.14440191\n",
      "Iteration 4846, loss = 0.14432179\n",
      "Iteration 4847, loss = 0.14424173\n",
      "Iteration 4848, loss = 0.14416174\n",
      "Iteration 4849, loss = 0.14408182\n",
      "Iteration 4850, loss = 0.14400198\n",
      "Iteration 4851, loss = 0.14392220\n",
      "Iteration 4852, loss = 0.14384249\n",
      "Iteration 4853, loss = 0.14376285\n",
      "Iteration 4854, loss = 0.14368328\n",
      "Iteration 4855, loss = 0.14360378\n",
      "Iteration 4856, loss = 0.14352434\n",
      "Iteration 4857, loss = 0.14344498\n",
      "Iteration 4858, loss = 0.14336569\n",
      "Iteration 4859, loss = 0.14328646\n",
      "Iteration 4860, loss = 0.14320730\n",
      "Iteration 4861, loss = 0.14312822\n",
      "Iteration 4862, loss = 0.14304920\n",
      "Iteration 4863, loss = 0.14297025\n",
      "Iteration 4864, loss = 0.14289137\n",
      "Iteration 4865, loss = 0.14281255\n",
      "Iteration 4866, loss = 0.14273381\n",
      "Iteration 4867, loss = 0.14265513\n",
      "Iteration 4868, loss = 0.14257653\n",
      "Iteration 4869, loss = 0.14249799\n",
      "Iteration 4870, loss = 0.14241952\n",
      "Iteration 4871, loss = 0.14234111\n",
      "Iteration 4872, loss = 0.14226278\n",
      "Iteration 4873, loss = 0.14218451\n",
      "Iteration 4874, loss = 0.14210631\n",
      "Iteration 4875, loss = 0.14202818\n",
      "Iteration 4876, loss = 0.14195011\n",
      "Iteration 4877, loss = 0.14187212\n",
      "Iteration 4878, loss = 0.14179419\n",
      "Iteration 4879, loss = 0.14171633\n",
      "Iteration 4880, loss = 0.14163853\n",
      "Iteration 4881, loss = 0.14156081\n",
      "Iteration 4882, loss = 0.14148315\n",
      "Iteration 4883, loss = 0.14140555\n",
      "Iteration 4884, loss = 0.14132803\n",
      "Iteration 4885, loss = 0.14125057\n",
      "Iteration 4886, loss = 0.14117318\n",
      "Iteration 4887, loss = 0.14109586\n",
      "Iteration 4888, loss = 0.14101860\n",
      "Iteration 4889, loss = 0.14094141\n",
      "Iteration 4890, loss = 0.14086428\n",
      "Iteration 4891, loss = 0.14078722\n",
      "Iteration 4892, loss = 0.14071023\n",
      "Iteration 4893, loss = 0.14063331\n",
      "Iteration 4894, loss = 0.14055645\n",
      "Iteration 4895, loss = 0.14047966\n",
      "Iteration 4896, loss = 0.14040293\n",
      "Iteration 4897, loss = 0.14032627\n",
      "Iteration 4898, loss = 0.14024968\n",
      "Iteration 4899, loss = 0.14017315\n",
      "Iteration 4900, loss = 0.14009669\n",
      "Iteration 4901, loss = 0.14002030\n",
      "Iteration 4902, loss = 0.13994397\n",
      "Iteration 4903, loss = 0.13986770\n",
      "Iteration 4904, loss = 0.13979150\n",
      "Iteration 4905, loss = 0.13971537\n",
      "Iteration 4906, loss = 0.13963930\n",
      "Iteration 4907, loss = 0.13956330\n",
      "Iteration 4908, loss = 0.13948736\n",
      "Iteration 4909, loss = 0.13941149\n",
      "Iteration 4910, loss = 0.13933569\n",
      "Iteration 4911, loss = 0.13925995\n",
      "Iteration 4912, loss = 0.13918427\n",
      "Iteration 4913, loss = 0.13910866\n",
      "Iteration 4914, loss = 0.13903311\n",
      "Iteration 4915, loss = 0.13895763\n",
      "Iteration 4916, loss = 0.13888222\n",
      "Iteration 4917, loss = 0.13880686\n",
      "Iteration 4918, loss = 0.13873158\n",
      "Iteration 4919, loss = 0.13865636\n",
      "Iteration 4920, loss = 0.13858120\n",
      "Iteration 4921, loss = 0.13850610\n",
      "Iteration 4922, loss = 0.13843108\n",
      "Iteration 4923, loss = 0.13835611\n",
      "Iteration 4924, loss = 0.13828121\n",
      "Iteration 4925, loss = 0.13820637\n",
      "Iteration 4926, loss = 0.13813160\n",
      "Iteration 4927, loss = 0.13805689\n",
      "Iteration 4928, loss = 0.13798225\n",
      "Iteration 4929, loss = 0.13790767\n",
      "Iteration 4930, loss = 0.13783315\n",
      "Iteration 4931, loss = 0.13775870\n",
      "Iteration 4932, loss = 0.13768431\n",
      "Iteration 4933, loss = 0.13760999\n",
      "Iteration 4934, loss = 0.13753573\n",
      "Iteration 4935, loss = 0.13746153\n",
      "Iteration 4936, loss = 0.13738739\n",
      "Iteration 4937, loss = 0.13731332\n",
      "Iteration 4938, loss = 0.13723931\n",
      "Iteration 4939, loss = 0.13716537\n",
      "Iteration 4940, loss = 0.13709149\n",
      "Iteration 4941, loss = 0.13701767\n",
      "Iteration 4942, loss = 0.13694391\n",
      "Iteration 4943, loss = 0.13687022\n",
      "Iteration 4944, loss = 0.13679659\n",
      "Iteration 4945, loss = 0.13672302\n",
      "Iteration 4946, loss = 0.13664952\n",
      "Iteration 4947, loss = 0.13657607\n",
      "Iteration 4948, loss = 0.13650269\n",
      "Iteration 4949, loss = 0.13642938\n",
      "Iteration 4950, loss = 0.13635612\n",
      "Iteration 4951, loss = 0.13628293\n",
      "Iteration 4952, loss = 0.13620980\n",
      "Iteration 4953, loss = 0.13613673\n",
      "Iteration 4954, loss = 0.13606373\n",
      "Iteration 4955, loss = 0.13599079\n",
      "Iteration 4956, loss = 0.13591790\n",
      "Iteration 4957, loss = 0.13584509\n",
      "Iteration 4958, loss = 0.13577233\n",
      "Iteration 4959, loss = 0.13569963\n",
      "Iteration 4960, loss = 0.13562700\n",
      "Iteration 4961, loss = 0.13555443\n",
      "Iteration 4962, loss = 0.13548192\n",
      "Iteration 4963, loss = 0.13540947\n",
      "Iteration 4964, loss = 0.13533708\n",
      "Iteration 4965, loss = 0.13526476\n",
      "Iteration 4966, loss = 0.13519249\n",
      "Iteration 4967, loss = 0.13512029\n",
      "Iteration 4968, loss = 0.13504815\n",
      "Iteration 4969, loss = 0.13497607\n",
      "Iteration 4970, loss = 0.13490405\n",
      "Iteration 4971, loss = 0.13483209\n",
      "Iteration 4972, loss = 0.13476019\n",
      "Iteration 4973, loss = 0.13468835\n",
      "Iteration 4974, loss = 0.13461658\n",
      "Iteration 4975, loss = 0.13454486\n",
      "Iteration 4976, loss = 0.13447321\n",
      "Iteration 4977, loss = 0.13440162\n",
      "Iteration 4978, loss = 0.13433008\n",
      "Iteration 4979, loss = 0.13425861\n",
      "Iteration 4980, loss = 0.13418720\n",
      "Iteration 4981, loss = 0.13411585\n",
      "Iteration 4982, loss = 0.13404456\n",
      "Iteration 4983, loss = 0.13397333\n",
      "Iteration 4984, loss = 0.13390215\n",
      "Iteration 4985, loss = 0.13383104\n",
      "Iteration 4986, loss = 0.13375999\n",
      "Iteration 4987, loss = 0.13368900\n",
      "Iteration 4988, loss = 0.13361807\n",
      "Iteration 4989, loss = 0.13354720\n",
      "Iteration 4990, loss = 0.13347639\n",
      "Iteration 4991, loss = 0.13340564\n",
      "Iteration 4992, loss = 0.13333495\n",
      "Iteration 4993, loss = 0.13326432\n",
      "Iteration 4994, loss = 0.13319374\n",
      "Iteration 4995, loss = 0.13312323\n",
      "Iteration 4996, loss = 0.13305278\n",
      "Iteration 4997, loss = 0.13298238\n",
      "Iteration 4998, loss = 0.13291205\n",
      "Iteration 4999, loss = 0.13284177\n",
      "Iteration 5000, loss = 0.13277156\n",
      "Iteration 5001, loss = 0.13270140\n",
      "Iteration 5002, loss = 0.13263130\n",
      "Iteration 5003, loss = 0.13256126\n",
      "Iteration 5004, loss = 0.13249128\n",
      "Iteration 5005, loss = 0.13242136\n",
      "Iteration 5006, loss = 0.13235150\n",
      "Iteration 5007, loss = 0.13228169\n",
      "Iteration 5008, loss = 0.13221195\n",
      "Iteration 5009, loss = 0.13214226\n",
      "Iteration 5010, loss = 0.13207263\n",
      "Iteration 5011, loss = 0.13200306\n",
      "Iteration 5012, loss = 0.13193355\n",
      "Iteration 5013, loss = 0.13186410\n",
      "Iteration 5014, loss = 0.13179470\n",
      "Iteration 5015, loss = 0.13172537\n",
      "Iteration 5016, loss = 0.13165609\n",
      "Iteration 5017, loss = 0.13158687\n",
      "Iteration 5018, loss = 0.13151770\n",
      "Iteration 5019, loss = 0.13144860\n",
      "Iteration 5020, loss = 0.13137955\n",
      "Iteration 5021, loss = 0.13131057\n",
      "Iteration 5022, loss = 0.13124163\n",
      "Iteration 5023, loss = 0.13117276\n",
      "Iteration 5024, loss = 0.13110395\n",
      "Iteration 5025, loss = 0.13103519\n",
      "Iteration 5026, loss = 0.13096649\n",
      "Iteration 5027, loss = 0.13089785\n",
      "Iteration 5028, loss = 0.13082926\n",
      "Iteration 5029, loss = 0.13076073\n",
      "Iteration 5030, loss = 0.13069226\n",
      "Iteration 5031, loss = 0.13062385\n",
      "Iteration 5032, loss = 0.13055549\n",
      "Iteration 5033, loss = 0.13048719\n",
      "Iteration 5034, loss = 0.13041895\n",
      "Iteration 5035, loss = 0.13035076\n",
      "Iteration 5036, loss = 0.13028264\n",
      "Iteration 5037, loss = 0.13021456\n",
      "Iteration 5038, loss = 0.13014655\n",
      "Iteration 5039, loss = 0.13007859\n",
      "Iteration 5040, loss = 0.13001069\n",
      "Iteration 5041, loss = 0.12994285\n",
      "Iteration 5042, loss = 0.12987506\n",
      "Iteration 5043, loss = 0.12980733\n",
      "Iteration 5044, loss = 0.12973965\n",
      "Iteration 5045, loss = 0.12967203\n",
      "Iteration 5046, loss = 0.12960447\n",
      "Iteration 5047, loss = 0.12953696\n",
      "Iteration 5048, loss = 0.12946951\n",
      "Iteration 5049, loss = 0.12940212\n",
      "Iteration 5050, loss = 0.12933478\n",
      "Iteration 5051, loss = 0.12926750\n",
      "Iteration 5052, loss = 0.12920028\n",
      "Iteration 5053, loss = 0.12913311\n",
      "Iteration 5054, loss = 0.12906599\n",
      "Iteration 5055, loss = 0.12899893\n",
      "Iteration 5056, loss = 0.12893193\n",
      "Iteration 5057, loss = 0.12886498\n",
      "Iteration 5058, loss = 0.12879809\n",
      "Iteration 5059, loss = 0.12873126\n",
      "Iteration 5060, loss = 0.12866448\n",
      "Iteration 5061, loss = 0.12859775\n",
      "Iteration 5062, loss = 0.12853108\n",
      "Iteration 5063, loss = 0.12846447\n",
      "Iteration 5064, loss = 0.12839791\n",
      "Iteration 5065, loss = 0.12833141\n",
      "Iteration 5066, loss = 0.12826496\n",
      "Iteration 5067, loss = 0.12819856\n",
      "Iteration 5068, loss = 0.12813222\n",
      "Iteration 5069, loss = 0.12806594\n",
      "Iteration 5070, loss = 0.12799971\n",
      "Iteration 5071, loss = 0.12793354\n",
      "Iteration 5072, loss = 0.12786742\n",
      "Iteration 5073, loss = 0.12780135\n",
      "Iteration 5074, loss = 0.12773534\n",
      "Iteration 5075, loss = 0.12766939\n",
      "Iteration 5076, loss = 0.12760349\n",
      "Iteration 5077, loss = 0.12753764\n",
      "Iteration 5078, loss = 0.12747185\n",
      "Iteration 5079, loss = 0.12740611\n",
      "Iteration 5080, loss = 0.12734043\n",
      "Iteration 5081, loss = 0.12727480\n",
      "Iteration 5082, loss = 0.12720923\n",
      "Iteration 5083, loss = 0.12714371\n",
      "Iteration 5084, loss = 0.12707824\n",
      "Iteration 5085, loss = 0.12701283\n",
      "Iteration 5086, loss = 0.12694747\n",
      "Iteration 5087, loss = 0.12688217\n",
      "Iteration 5088, loss = 0.12681691\n",
      "Iteration 5089, loss = 0.12675172\n",
      "Iteration 5090, loss = 0.12668657\n",
      "Iteration 5091, loss = 0.12662149\n",
      "Iteration 5092, loss = 0.12655645\n",
      "Iteration 5093, loss = 0.12649147\n",
      "Iteration 5094, loss = 0.12642654\n",
      "Iteration 5095, loss = 0.12636166\n",
      "Iteration 5096, loss = 0.12629684\n",
      "Iteration 5097, loss = 0.12623207\n",
      "Iteration 5098, loss = 0.12616736\n",
      "Iteration 5099, loss = 0.12610270\n",
      "Iteration 5100, loss = 0.12603809\n",
      "Iteration 5101, loss = 0.12597353\n",
      "Iteration 5102, loss = 0.12590903\n",
      "Iteration 5103, loss = 0.12584458\n",
      "Iteration 5104, loss = 0.12578018\n",
      "Iteration 5105, loss = 0.12571584\n",
      "Iteration 5106, loss = 0.12565155\n",
      "Iteration 5107, loss = 0.12558731\n",
      "Iteration 5108, loss = 0.12552312\n",
      "Iteration 5109, loss = 0.12545899\n",
      "Iteration 5110, loss = 0.12539491\n",
      "Iteration 5111, loss = 0.12533088\n",
      "Iteration 5112, loss = 0.12526691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5113, loss = 0.12520298\n",
      "Iteration 5114, loss = 0.12513911\n",
      "Iteration 5115, loss = 0.12507529\n",
      "Iteration 5116, loss = 0.12501153\n",
      "Iteration 5117, loss = 0.12494782\n",
      "Iteration 5118, loss = 0.12488415\n",
      "Iteration 5119, loss = 0.12482054\n",
      "Iteration 5120, loss = 0.12475699\n",
      "Iteration 5121, loss = 0.12469348\n",
      "Iteration 5122, loss = 0.12463003\n",
      "Iteration 5123, loss = 0.12456663\n",
      "Iteration 5124, loss = 0.12450328\n",
      "Iteration 5125, loss = 0.12443998\n",
      "Iteration 5126, loss = 0.12437673\n",
      "Iteration 5127, loss = 0.12431354\n",
      "Iteration 5128, loss = 0.12425040\n",
      "Iteration 5129, loss = 0.12418731\n",
      "Iteration 5130, loss = 0.12412427\n",
      "Iteration 5131, loss = 0.12406128\n",
      "Iteration 5132, loss = 0.12399834\n",
      "Iteration 5133, loss = 0.12393545\n",
      "Iteration 5134, loss = 0.12387262\n",
      "Iteration 5135, loss = 0.12380984\n",
      "Iteration 5136, loss = 0.12374711\n",
      "Iteration 5137, loss = 0.12368442\n",
      "Iteration 5138, loss = 0.12362179\n",
      "Iteration 5139, loss = 0.12355922\n",
      "Iteration 5140, loss = 0.12349669\n",
      "Iteration 5141, loss = 0.12343421\n",
      "Iteration 5142, loss = 0.12337179\n",
      "Iteration 5143, loss = 0.12330941\n",
      "Iteration 5144, loss = 0.12324709\n",
      "Iteration 5145, loss = 0.12318481\n",
      "Iteration 5146, loss = 0.12312259\n",
      "Iteration 5147, loss = 0.12306042\n",
      "Iteration 5148, loss = 0.12299830\n",
      "Iteration 5149, loss = 0.12293622\n",
      "Iteration 5150, loss = 0.12287420\n",
      "Iteration 5151, loss = 0.12281223\n",
      "Iteration 5152, loss = 0.12275031\n",
      "Iteration 5153, loss = 0.12268844\n",
      "Iteration 5154, loss = 0.12262662\n",
      "Iteration 5155, loss = 0.12256485\n",
      "Iteration 5156, loss = 0.12250313\n",
      "Iteration 5157, loss = 0.12244146\n",
      "Iteration 5158, loss = 0.12237984\n",
      "Iteration 5159, loss = 0.12231827\n",
      "Iteration 5160, loss = 0.12225675\n",
      "Iteration 5161, loss = 0.12219529\n",
      "Iteration 5162, loss = 0.12213387\n",
      "Iteration 5163, loss = 0.12207250\n",
      "Iteration 5164, loss = 0.12201117\n",
      "Iteration 5165, loss = 0.12194990\n",
      "Iteration 5166, loss = 0.12188868\n",
      "Iteration 5167, loss = 0.12182751\n",
      "Iteration 5168, loss = 0.12176639\n",
      "Iteration 5169, loss = 0.12170532\n",
      "Iteration 5170, loss = 0.12164429\n",
      "Iteration 5171, loss = 0.12158332\n",
      "Iteration 5172, loss = 0.12152239\n",
      "Iteration 5173, loss = 0.12146152\n",
      "Iteration 5174, loss = 0.12140069\n",
      "Iteration 5175, loss = 0.12133991\n",
      "Iteration 5176, loss = 0.12127919\n",
      "Iteration 5177, loss = 0.12121851\n",
      "Iteration 5178, loss = 0.12115788\n",
      "Iteration 5179, loss = 0.12109730\n",
      "Iteration 5180, loss = 0.12103676\n",
      "Iteration 5181, loss = 0.12097628\n",
      "Iteration 5182, loss = 0.12091584\n",
      "Iteration 5183, loss = 0.12085546\n",
      "Iteration 5184, loss = 0.12079512\n",
      "Iteration 5185, loss = 0.12073483\n",
      "Iteration 5186, loss = 0.12067459\n",
      "Iteration 5187, loss = 0.12061440\n",
      "Iteration 5188, loss = 0.12055426\n",
      "Iteration 5189, loss = 0.12049416\n",
      "Iteration 5190, loss = 0.12043412\n",
      "Iteration 5191, loss = 0.12037412\n",
      "Iteration 5192, loss = 0.12031417\n",
      "Iteration 5193, loss = 0.12025427\n",
      "Iteration 5194, loss = 0.12019441\n",
      "Iteration 5195, loss = 0.12013461\n",
      "Iteration 5196, loss = 0.12007485\n",
      "Iteration 5197, loss = 0.12001514\n",
      "Iteration 5198, loss = 0.11995548\n",
      "Iteration 5199, loss = 0.11989586\n",
      "Iteration 5200, loss = 0.11983630\n",
      "Iteration 5201, loss = 0.11977678\n",
      "Iteration 5202, loss = 0.11971731\n",
      "Iteration 5203, loss = 0.11965789\n",
      "Iteration 5204, loss = 0.11959851\n",
      "Iteration 5205, loss = 0.11953919\n",
      "Iteration 5206, loss = 0.11947991\n",
      "Iteration 5207, loss = 0.11942067\n",
      "Iteration 5208, loss = 0.11936149\n",
      "Iteration 5209, loss = 0.11930235\n",
      "Iteration 5210, loss = 0.11924326\n",
      "Iteration 5211, loss = 0.11918422\n",
      "Iteration 5212, loss = 0.11912523\n",
      "Iteration 5213, loss = 0.11906628\n",
      "Iteration 5214, loss = 0.11900738\n",
      "Iteration 5215, loss = 0.11894852\n",
      "Iteration 5216, loss = 0.11888972\n",
      "Iteration 5217, loss = 0.11883096\n",
      "Iteration 5218, loss = 0.11877224\n",
      "Iteration 5219, loss = 0.11871358\n",
      "Iteration 5220, loss = 0.11865496\n",
      "Iteration 5221, loss = 0.11859639\n",
      "Iteration 5222, loss = 0.11853786\n",
      "Iteration 5223, loss = 0.11847938\n",
      "Iteration 5224, loss = 0.11842095\n",
      "Iteration 5225, loss = 0.11836257\n",
      "Iteration 5226, loss = 0.11830423\n",
      "Iteration 5227, loss = 0.11824593\n",
      "Iteration 5228, loss = 0.11818769\n",
      "Iteration 5229, loss = 0.11812949\n",
      "Iteration 5230, loss = 0.11807134\n",
      "Iteration 5231, loss = 0.11801323\n",
      "Iteration 5232, loss = 0.11795517\n",
      "Iteration 5233, loss = 0.11789716\n",
      "Iteration 5234, loss = 0.11783919\n",
      "Iteration 5235, loss = 0.11778127\n",
      "Iteration 5236, loss = 0.11772339\n",
      "Iteration 5237, loss = 0.11766556\n",
      "Iteration 5238, loss = 0.11760778\n",
      "Iteration 5239, loss = 0.11755004\n",
      "Iteration 5240, loss = 0.11749235\n",
      "Iteration 5241, loss = 0.11743470\n",
      "Iteration 5242, loss = 0.11737710\n",
      "Iteration 5243, loss = 0.11731955\n",
      "Iteration 5244, loss = 0.11726204\n",
      "Iteration 5245, loss = 0.11720458\n",
      "Iteration 5246, loss = 0.11714716\n",
      "Iteration 5247, loss = 0.11708979\n",
      "Iteration 5248, loss = 0.11703246\n",
      "Iteration 5249, loss = 0.11697518\n",
      "Iteration 5250, loss = 0.11691795\n",
      "Iteration 5251, loss = 0.11686076\n",
      "Iteration 5252, loss = 0.11680361\n",
      "Iteration 5253, loss = 0.11674651\n",
      "Iteration 5254, loss = 0.11668946\n",
      "Iteration 5255, loss = 0.11663245\n",
      "Iteration 5256, loss = 0.11657549\n",
      "Iteration 5257, loss = 0.11651857\n",
      "Iteration 5258, loss = 0.11646169\n",
      "Iteration 5259, loss = 0.11640487\n",
      "Iteration 5260, loss = 0.11634808\n",
      "Iteration 5261, loss = 0.11629134\n",
      "Iteration 5262, loss = 0.11623465\n",
      "Iteration 5263, loss = 0.11617800\n",
      "Iteration 5264, loss = 0.11612140\n",
      "Iteration 5265, loss = 0.11606484\n",
      "Iteration 5266, loss = 0.11600832\n",
      "Iteration 5267, loss = 0.11595185\n",
      "Iteration 5268, loss = 0.11589543\n",
      "Iteration 5269, loss = 0.11583905\n",
      "Iteration 5270, loss = 0.11578271\n",
      "Iteration 5271, loss = 0.11572642\n",
      "Iteration 5272, loss = 0.11567017\n",
      "Iteration 5273, loss = 0.11561397\n",
      "Iteration 5274, loss = 0.11555781\n",
      "Iteration 5275, loss = 0.11550170\n",
      "Iteration 5276, loss = 0.11544563\n",
      "Iteration 5277, loss = 0.11538960\n",
      "Iteration 5278, loss = 0.11533362\n",
      "Iteration 5279, loss = 0.11527768\n",
      "Iteration 5280, loss = 0.11522179\n",
      "Iteration 5281, loss = 0.11516594\n",
      "Iteration 5282, loss = 0.11511013\n",
      "Iteration 5283, loss = 0.11505437\n",
      "Iteration 5284, loss = 0.11499865\n",
      "Iteration 5285, loss = 0.11494298\n",
      "Iteration 5286, loss = 0.11488735\n",
      "Iteration 5287, loss = 0.11483176\n",
      "Iteration 5288, loss = 0.11477622\n",
      "Iteration 5289, loss = 0.11472072\n",
      "Iteration 5290, loss = 0.11466526\n",
      "Iteration 5291, loss = 0.11460985\n",
      "Iteration 5292, loss = 0.11455448\n",
      "Iteration 5293, loss = 0.11449916\n",
      "Iteration 5294, loss = 0.11444388\n",
      "Iteration 5295, loss = 0.11438864\n",
      "Iteration 5296, loss = 0.11433344\n",
      "Iteration 5297, loss = 0.11427829\n",
      "Iteration 5298, loss = 0.11422318\n",
      "Iteration 5299, loss = 0.11416812\n",
      "Iteration 5300, loss = 0.11411310\n",
      "Iteration 5301, loss = 0.11405812\n",
      "Iteration 5302, loss = 0.11400318\n",
      "Iteration 5303, loss = 0.11394829\n",
      "Iteration 5304, loss = 0.11389344\n",
      "Iteration 5305, loss = 0.11383863\n",
      "Iteration 5306, loss = 0.11378387\n",
      "Iteration 5307, loss = 0.11372915\n",
      "Iteration 5308, loss = 0.11367447\n",
      "Iteration 5309, loss = 0.11361983\n",
      "Iteration 5310, loss = 0.11356524\n",
      "Iteration 5311, loss = 0.11351069\n",
      "Iteration 5312, loss = 0.11345618\n",
      "Iteration 5313, loss = 0.11340172\n",
      "Iteration 5314, loss = 0.11334729\n",
      "Iteration 5315, loss = 0.11329291\n",
      "Iteration 5316, loss = 0.11323858\n",
      "Iteration 5317, loss = 0.11318428\n",
      "Iteration 5318, loss = 0.11313003\n",
      "Iteration 5319, loss = 0.11307582\n",
      "Iteration 5320, loss = 0.11302165\n",
      "Iteration 5321, loss = 0.11296752\n",
      "Iteration 5322, loss = 0.11291344\n",
      "Iteration 5323, loss = 0.11285940\n",
      "Iteration 5324, loss = 0.11280540\n",
      "Iteration 5325, loss = 0.11275144\n",
      "Iteration 5326, loss = 0.11269753\n",
      "Iteration 5327, loss = 0.11264365\n",
      "Iteration 5328, loss = 0.11258982\n",
      "Iteration 5329, loss = 0.11253603\n",
      "Iteration 5330, loss = 0.11248229\n",
      "Iteration 5331, loss = 0.11242858\n",
      "Iteration 5332, loss = 0.11237492\n",
      "Iteration 5333, loss = 0.11232129\n",
      "Iteration 5334, loss = 0.11226771\n",
      "Iteration 5335, loss = 0.11221417\n",
      "Iteration 5336, loss = 0.11216068\n",
      "Iteration 5337, loss = 0.11210722\n",
      "Iteration 5338, loss = 0.11205381\n",
      "Iteration 5339, loss = 0.11200043\n",
      "Iteration 5340, loss = 0.11194710\n",
      "Iteration 5341, loss = 0.11189381\n",
      "Iteration 5342, loss = 0.11184056\n",
      "Iteration 5343, loss = 0.11178736\n",
      "Iteration 5344, loss = 0.11173419\n",
      "Iteration 5345, loss = 0.11168106\n",
      "Iteration 5346, loss = 0.11162798\n",
      "Iteration 5347, loss = 0.11157494\n",
      "Iteration 5348, loss = 0.11152194\n",
      "Iteration 5349, loss = 0.11146898\n",
      "Iteration 5350, loss = 0.11141606\n",
      "Iteration 5351, loss = 0.11136318\n",
      "Iteration 5352, loss = 0.11131034\n",
      "Iteration 5353, loss = 0.11125754\n",
      "Iteration 5354, loss = 0.11120479\n",
      "Iteration 5355, loss = 0.11115207\n",
      "Iteration 5356, loss = 0.11109940\n",
      "Iteration 5357, loss = 0.11104677\n",
      "Iteration 5358, loss = 0.11099417\n",
      "Iteration 5359, loss = 0.11094162\n",
      "Iteration 5360, loss = 0.11088911\n",
      "Iteration 5361, loss = 0.11083664\n",
      "Iteration 5362, loss = 0.11078421\n",
      "Iteration 5363, loss = 0.11073182\n",
      "Iteration 5364, loss = 0.11067947\n",
      "Iteration 5365, loss = 0.11062716\n",
      "Iteration 5366, loss = 0.11057489\n",
      "Iteration 5367, loss = 0.11052266\n",
      "Iteration 5368, loss = 0.11047047\n",
      "Iteration 5369, loss = 0.11041832\n",
      "Iteration 5370, loss = 0.11036622\n",
      "Iteration 5371, loss = 0.11031415\n",
      "Iteration 5372, loss = 0.11026212\n",
      "Iteration 5373, loss = 0.11021013\n",
      "Iteration 5374, loss = 0.11015819\n",
      "Iteration 5375, loss = 0.11010628\n",
      "Iteration 5376, loss = 0.11005441\n",
      "Iteration 5377, loss = 0.11000258\n",
      "Iteration 5378, loss = 0.10995080\n",
      "Iteration 5379, loss = 0.10989905\n",
      "Iteration 5380, loss = 0.10984734\n",
      "Iteration 5381, loss = 0.10979567\n",
      "Iteration 5382, loss = 0.10974404\n",
      "Iteration 5383, loss = 0.10969245\n",
      "Iteration 5384, loss = 0.10964090\n",
      "Iteration 5385, loss = 0.10958939\n",
      "Iteration 5386, loss = 0.10953792\n",
      "Iteration 5387, loss = 0.10948649\n",
      "Iteration 5388, loss = 0.10943510\n",
      "Iteration 5389, loss = 0.10938375\n",
      "Iteration 5390, loss = 0.10933244\n",
      "Iteration 5391, loss = 0.10928116\n",
      "Iteration 5392, loss = 0.10922993\n",
      "Iteration 5393, loss = 0.10917874\n",
      "Iteration 5394, loss = 0.10912758\n",
      "Iteration 5395, loss = 0.10907646\n",
      "Iteration 5396, loss = 0.10902539\n",
      "Iteration 5397, loss = 0.10897435\n",
      "Iteration 5398, loss = 0.10892335\n",
      "Iteration 5399, loss = 0.10887239\n",
      "Iteration 5400, loss = 0.10882147\n",
      "Iteration 5401, loss = 0.10877059\n",
      "Iteration 5402, loss = 0.10871974\n",
      "Iteration 5403, loss = 0.10866894\n",
      "Iteration 5404, loss = 0.10861817\n",
      "Iteration 5405, loss = 0.10856745\n",
      "Iteration 5406, loss = 0.10851676\n",
      "Iteration 5407, loss = 0.10846611\n",
      "Iteration 5408, loss = 0.10841550\n",
      "Iteration 5409, loss = 0.10836493\n",
      "Iteration 5410, loss = 0.10831440\n",
      "Iteration 5411, loss = 0.10826390\n",
      "Iteration 5412, loss = 0.10821345\n",
      "Iteration 5413, loss = 0.10816303\n",
      "Iteration 5414, loss = 0.10811265\n",
      "Iteration 5415, loss = 0.10806231\n",
      "Iteration 5416, loss = 0.10801201\n",
      "Iteration 5417, loss = 0.10796174\n",
      "Iteration 5418, loss = 0.10791152\n",
      "Iteration 5419, loss = 0.10786133\n",
      "Iteration 5420, loss = 0.10781118\n",
      "Iteration 5421, loss = 0.10776107\n",
      "Iteration 5422, loss = 0.10771100\n",
      "Iteration 5423, loss = 0.10766096\n",
      "Iteration 5424, loss = 0.10761096\n",
      "Iteration 5425, loss = 0.10756101\n",
      "Iteration 5426, loss = 0.10751109\n",
      "Iteration 5427, loss = 0.10746120\n",
      "Iteration 5428, loss = 0.10741136\n",
      "Iteration 5429, loss = 0.10736155\n",
      "Iteration 5430, loss = 0.10731178\n",
      "Iteration 5431, loss = 0.10726205\n",
      "Iteration 5432, loss = 0.10721236\n",
      "Iteration 5433, loss = 0.10716270\n",
      "Iteration 5434, loss = 0.10711308\n",
      "Iteration 5435, loss = 0.10706350\n",
      "Iteration 5436, loss = 0.10701396\n",
      "Iteration 5437, loss = 0.10696445\n",
      "Iteration 5438, loss = 0.10691499\n",
      "Iteration 5439, loss = 0.10686556\n",
      "Iteration 5440, loss = 0.10681616\n",
      "Iteration 5441, loss = 0.10676681\n",
      "Iteration 5442, loss = 0.10671749\n",
      "Iteration 5443, loss = 0.10666821\n",
      "Iteration 5444, loss = 0.10661897\n",
      "Iteration 5445, loss = 0.10656976\n",
      "Iteration 5446, loss = 0.10652059\n",
      "Iteration 5447, loss = 0.10647146\n",
      "Iteration 5448, loss = 0.10642237\n",
      "Iteration 5449, loss = 0.10637331\n",
      "Iteration 5450, loss = 0.10632429\n",
      "Iteration 5451, loss = 0.10627530\n",
      "Iteration 5452, loss = 0.10622636\n",
      "Iteration 5453, loss = 0.10617745\n",
      "Iteration 5454, loss = 0.10612858\n",
      "Iteration 5455, loss = 0.10607974\n",
      "Iteration 5456, loss = 0.10603094\n",
      "Iteration 5457, loss = 0.10598218\n",
      "Iteration 5458, loss = 0.10593346\n",
      "Iteration 5459, loss = 0.10588477\n",
      "Iteration 5460, loss = 0.10583612\n",
      "Iteration 5461, loss = 0.10578750\n",
      "Iteration 5462, loss = 0.10573893\n",
      "Iteration 5463, loss = 0.10569038\n",
      "Iteration 5464, loss = 0.10564188\n",
      "Iteration 5465, loss = 0.10559341\n",
      "Iteration 5466, loss = 0.10554498\n",
      "Iteration 5467, loss = 0.10549658\n",
      "Iteration 5468, loss = 0.10544822\n",
      "Iteration 5469, loss = 0.10539990\n",
      "Iteration 5470, loss = 0.10535162\n",
      "Iteration 5471, loss = 0.10530337\n",
      "Iteration 5472, loss = 0.10525515\n",
      "Iteration 5473, loss = 0.10520698\n",
      "Iteration 5474, loss = 0.10515883\n",
      "Iteration 5475, loss = 0.10511073\n",
      "Iteration 5476, loss = 0.10506266\n",
      "Iteration 5477, loss = 0.10501463\n",
      "Iteration 5478, loss = 0.10496663\n",
      "Iteration 5479, loss = 0.10491867\n",
      "Iteration 5480, loss = 0.10487075\n",
      "Iteration 5481, loss = 0.10482286\n",
      "Iteration 5482, loss = 0.10477501\n",
      "Iteration 5483, loss = 0.10472719\n",
      "Iteration 5484, loss = 0.10467941\n",
      "Iteration 5485, loss = 0.10463166\n",
      "Iteration 5486, loss = 0.10458395\n",
      "Iteration 5487, loss = 0.10453628\n",
      "Iteration 5488, loss = 0.10448864\n",
      "Iteration 5489, loss = 0.10444104\n",
      "Iteration 5490, loss = 0.10439347\n",
      "Iteration 5491, loss = 0.10434594\n",
      "Iteration 5492, loss = 0.10429845\n",
      "Iteration 5493, loss = 0.10425099\n",
      "Iteration 5494, loss = 0.10420356\n",
      "Iteration 5495, loss = 0.10415618\n",
      "Iteration 5496, loss = 0.10410882\n",
      "Iteration 5497, loss = 0.10406150\n",
      "Iteration 5498, loss = 0.10401422\n",
      "Iteration 5499, loss = 0.10396697\n",
      "Iteration 5500, loss = 0.10391976\n",
      "Iteration 5501, loss = 0.10387259\n",
      "Iteration 5502, loss = 0.10382544\n",
      "Iteration 5503, loss = 0.10377834\n",
      "Iteration 5504, loss = 0.10373127\n",
      "Iteration 5505, loss = 0.10368423\n",
      "Iteration 5506, loss = 0.10363723\n",
      "Iteration 5507, loss = 0.10359026\n",
      "Iteration 5508, loss = 0.10354333\n",
      "Iteration 5509, loss = 0.10349644\n",
      "Iteration 5510, loss = 0.10344958\n",
      "Iteration 5511, loss = 0.10340275\n",
      "Iteration 5512, loss = 0.10335596\n",
      "Iteration 5513, loss = 0.10330920\n",
      "Iteration 5514, loss = 0.10326248\n",
      "Iteration 5515, loss = 0.10321580\n",
      "Iteration 5516, loss = 0.10316914\n",
      "Iteration 5517, loss = 0.10312253\n",
      "Iteration 5518, loss = 0.10307594\n",
      "Iteration 5519, loss = 0.10302940\n",
      "Iteration 5520, loss = 0.10298288\n",
      "Iteration 5521, loss = 0.10293641\n",
      "Iteration 5522, loss = 0.10288996\n",
      "Iteration 5523, loss = 0.10284355\n",
      "Iteration 5524, loss = 0.10279718\n",
      "Iteration 5525, loss = 0.10275084\n",
      "Iteration 5526, loss = 0.10270453\n",
      "Iteration 5527, loss = 0.10265826\n",
      "Iteration 5528, loss = 0.10261202\n",
      "Iteration 5529, loss = 0.10256582\n",
      "Iteration 5530, loss = 0.10251965\n",
      "Iteration 5531, loss = 0.10247351\n",
      "Iteration 5532, loss = 0.10242741\n",
      "Iteration 5533, loss = 0.10238135\n",
      "Iteration 5534, loss = 0.10233531\n",
      "Iteration 5535, loss = 0.10228932\n",
      "Iteration 5536, loss = 0.10224335\n",
      "Iteration 5537, loss = 0.10219742\n",
      "Iteration 5538, loss = 0.10215153\n",
      "Iteration 5539, loss = 0.10210566\n",
      "Iteration 5540, loss = 0.10205984\n",
      "Iteration 5541, loss = 0.10201404\n",
      "Iteration 5542, loss = 0.10196828\n",
      "Iteration 5543, loss = 0.10192255\n",
      "Iteration 5544, loss = 0.10187686\n",
      "Iteration 5545, loss = 0.10183120\n",
      "Iteration 5546, loss = 0.10178558\n",
      "Iteration 5547, loss = 0.10173999\n",
      "Iteration 5548, loss = 0.10169443\n",
      "Iteration 5549, loss = 0.10164890\n",
      "Iteration 5550, loss = 0.10160341\n",
      "Iteration 5551, loss = 0.10155796\n",
      "Iteration 5552, loss = 0.10151253\n",
      "Iteration 5553, loss = 0.10146714\n",
      "Iteration 5554, loss = 0.10142179\n",
      "Iteration 5555, loss = 0.10137646\n",
      "Iteration 5556, loss = 0.10133118\n",
      "Iteration 5557, loss = 0.10128592\n",
      "Iteration 5558, loss = 0.10124070\n",
      "Iteration 5559, loss = 0.10119551\n",
      "Iteration 5560, loss = 0.10115035\n",
      "Iteration 5561, loss = 0.10110523\n",
      "Iteration 5562, loss = 0.10106014\n",
      "Iteration 5563, loss = 0.10101508\n",
      "Iteration 5564, loss = 0.10097006\n",
      "Iteration 5565, loss = 0.10092507\n",
      "Iteration 5566, loss = 0.10088011\n",
      "Iteration 5567, loss = 0.10083519\n",
      "Iteration 5568, loss = 0.10079029\n",
      "Iteration 5569, loss = 0.10074544\n",
      "Iteration 5570, loss = 0.10070061\n",
      "Iteration 5571, loss = 0.10065582\n",
      "Iteration 5572, loss = 0.10061106\n",
      "Iteration 5573, loss = 0.10056633\n",
      "Iteration 5574, loss = 0.10052164\n",
      "Iteration 5575, loss = 0.10047698\n",
      "Iteration 5576, loss = 0.10043235\n",
      "Iteration 5577, loss = 0.10038776\n",
      "Iteration 5578, loss = 0.10034319\n",
      "Iteration 5579, loss = 0.10029866\n",
      "Iteration 5580, loss = 0.10025417\n",
      "Iteration 5581, loss = 0.10020970\n",
      "Iteration 5582, loss = 0.10016527\n",
      "Iteration 5583, loss = 0.10012087\n",
      "Iteration 5584, loss = 0.10007650\n",
      "Iteration 5585, loss = 0.10003217\n",
      "Iteration 5586, loss = 0.09998786\n",
      "Iteration 5587, loss = 0.09994359\n",
      "Iteration 5588, loss = 0.09989936\n",
      "Iteration 5589, loss = 0.09985515\n",
      "Iteration 5590, loss = 0.09981098\n",
      "Iteration 5591, loss = 0.09976684\n",
      "Iteration 5592, loss = 0.09972273\n",
      "Iteration 5593, loss = 0.09967866\n",
      "Iteration 5594, loss = 0.09963461\n",
      "Iteration 5595, loss = 0.09959060\n",
      "Iteration 5596, loss = 0.09954662\n",
      "Iteration 5597, loss = 0.09950267\n",
      "Iteration 5598, loss = 0.09945876\n",
      "Iteration 5599, loss = 0.09941488\n",
      "Iteration 5600, loss = 0.09937102\n",
      "Iteration 5601, loss = 0.09932720\n",
      "Iteration 5602, loss = 0.09928342\n",
      "Iteration 5603, loss = 0.09923966\n",
      "Iteration 5604, loss = 0.09919594\n",
      "Iteration 5605, loss = 0.09915225\n",
      "Iteration 5606, loss = 0.09910859\n",
      "Iteration 5607, loss = 0.09906496\n",
      "Iteration 5608, loss = 0.09902136\n",
      "Iteration 5609, loss = 0.09897780\n",
      "Iteration 5610, loss = 0.09893427\n",
      "Iteration 5611, loss = 0.09889077\n",
      "Iteration 5612, loss = 0.09884730\n",
      "Iteration 5613, loss = 0.09880386\n",
      "Iteration 5614, loss = 0.09876045\n",
      "Iteration 5615, loss = 0.09871708\n",
      "Iteration 5616, loss = 0.09867373\n",
      "Iteration 5617, loss = 0.09863042\n",
      "Iteration 5618, loss = 0.09858714\n",
      "Iteration 5619, loss = 0.09854389\n",
      "Iteration 5620, loss = 0.09850068\n",
      "Iteration 5621, loss = 0.09845749\n",
      "Iteration 5622, loss = 0.09841434\n",
      "Iteration 5623, loss = 0.09837121\n",
      "Iteration 5624, loss = 0.09832812\n",
      "Iteration 5625, loss = 0.09828506\n",
      "Iteration 5626, loss = 0.09824203\n",
      "Iteration 5627, loss = 0.09819903\n",
      "Iteration 5628, loss = 0.09815607\n",
      "Iteration 5629, loss = 0.09811313\n",
      "Iteration 5630, loss = 0.09807022\n",
      "Iteration 5631, loss = 0.09802735\n",
      "Iteration 5632, loss = 0.09798451\n",
      "Iteration 5633, loss = 0.09794170\n",
      "Iteration 5634, loss = 0.09789892\n",
      "Iteration 5635, loss = 0.09785617\n",
      "Iteration 5636, loss = 0.09781345\n",
      "Iteration 5637, loss = 0.09777076\n",
      "Iteration 5638, loss = 0.09772810\n",
      "Iteration 5639, loss = 0.09768548\n",
      "Iteration 5640, loss = 0.09764288\n",
      "Iteration 5641, loss = 0.09760032\n",
      "Iteration 5642, loss = 0.09755778\n",
      "Iteration 5643, loss = 0.09751528\n",
      "Iteration 5644, loss = 0.09747281\n",
      "Iteration 5645, loss = 0.09743036\n",
      "Iteration 5646, loss = 0.09738795\n",
      "Iteration 5647, loss = 0.09734557\n",
      "Iteration 5648, loss = 0.09730322\n",
      "Iteration 5649, loss = 0.09726090\n",
      "Iteration 5650, loss = 0.09721861\n",
      "Iteration 5651, loss = 0.09717635\n",
      "Iteration 5652, loss = 0.09713413\n",
      "Iteration 5653, loss = 0.09709193\n",
      "Iteration 5654, loss = 0.09704976\n",
      "Iteration 5655, loss = 0.09700762\n",
      "Iteration 5656, loss = 0.09696552\n",
      "Iteration 5657, loss = 0.09692344\n",
      "Iteration 5658, loss = 0.09688139\n",
      "Iteration 5659, loss = 0.09683938\n",
      "Iteration 5660, loss = 0.09679739\n",
      "Iteration 5661, loss = 0.09675544\n",
      "Iteration 5662, loss = 0.09671351\n",
      "Iteration 5663, loss = 0.09667162\n",
      "Iteration 5664, loss = 0.09662975\n",
      "Iteration 5665, loss = 0.09658792\n",
      "Iteration 5666, loss = 0.09654611\n",
      "Iteration 5667, loss = 0.09650434\n",
      "Iteration 5668, loss = 0.09646259\n",
      "Iteration 5669, loss = 0.09642088\n",
      "Iteration 5670, loss = 0.09637919\n",
      "Iteration 5671, loss = 0.09633754\n",
      "Iteration 5672, loss = 0.09629591\n",
      "Iteration 5673, loss = 0.09625432\n",
      "Iteration 5674, loss = 0.09621275\n",
      "Iteration 5675, loss = 0.09617122\n",
      "Iteration 5676, loss = 0.09612971\n",
      "Iteration 5677, loss = 0.09608824\n",
      "Iteration 5678, loss = 0.09604679\n",
      "Iteration 5679, loss = 0.09600537\n",
      "Iteration 5680, loss = 0.09596398\n",
      "Iteration 5681, loss = 0.09592263\n",
      "Iteration 5682, loss = 0.09588130\n",
      "Iteration 5683, loss = 0.09584000\n",
      "Iteration 5684, loss = 0.09579873\n",
      "Iteration 5685, loss = 0.09575749\n",
      "Iteration 5686, loss = 0.09571628\n",
      "Iteration 5687, loss = 0.09567510\n",
      "Iteration 5688, loss = 0.09563395\n",
      "Iteration 5689, loss = 0.09559283\n",
      "Iteration 5690, loss = 0.09555174\n",
      "Iteration 5691, loss = 0.09551068\n",
      "Iteration 5692, loss = 0.09546964\n",
      "Iteration 5693, loss = 0.09542864\n",
      "Iteration 5694, loss = 0.09538766\n",
      "Iteration 5695, loss = 0.09534672\n",
      "Iteration 5696, loss = 0.09530580\n",
      "Iteration 5697, loss = 0.09526491\n",
      "Iteration 5698, loss = 0.09522406\n",
      "Iteration 5699, loss = 0.09518323\n",
      "Iteration 5700, loss = 0.09514243\n",
      "Iteration 5701, loss = 0.09510166\n",
      "Iteration 5702, loss = 0.09506091\n",
      "Iteration 5703, loss = 0.09502020\n",
      "Iteration 5704, loss = 0.09497952\n",
      "Iteration 5705, loss = 0.09493886\n",
      "Iteration 5706, loss = 0.09489824\n",
      "Iteration 5707, loss = 0.09485764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5708, loss = 0.09481707\n",
      "Iteration 5709, loss = 0.09477653\n",
      "Iteration 5710, loss = 0.09473602\n",
      "Iteration 5711, loss = 0.09469554\n",
      "Iteration 5712, loss = 0.09465509\n",
      "Iteration 5713, loss = 0.09461466\n",
      "Iteration 5714, loss = 0.09457427\n",
      "Iteration 5715, loss = 0.09453390\n",
      "Iteration 5716, loss = 0.09449356\n",
      "Iteration 5717, loss = 0.09445325\n",
      "Iteration 5718, loss = 0.09441297\n",
      "Iteration 5719, loss = 0.09437272\n",
      "Iteration 5720, loss = 0.09433249\n",
      "Iteration 5721, loss = 0.09429230\n",
      "Iteration 5722, loss = 0.09425213\n",
      "Iteration 5723, loss = 0.09421199\n",
      "Iteration 5724, loss = 0.09417188\n",
      "Iteration 5725, loss = 0.09413180\n",
      "Iteration 5726, loss = 0.09409175\n",
      "Iteration 5727, loss = 0.09405172\n",
      "Iteration 5728, loss = 0.09401173\n",
      "Iteration 5729, loss = 0.09397176\n",
      "Iteration 5730, loss = 0.09393182\n",
      "Iteration 5731, loss = 0.09389191\n",
      "Iteration 5732, loss = 0.09385203\n",
      "Iteration 5733, loss = 0.09381217\n",
      "Iteration 5734, loss = 0.09377234\n",
      "Iteration 5735, loss = 0.09373254\n",
      "Iteration 5736, loss = 0.09369277\n",
      "Iteration 5737, loss = 0.09365303\n",
      "Iteration 5738, loss = 0.09361332\n",
      "Iteration 5739, loss = 0.09357363\n",
      "Iteration 5740, loss = 0.09353397\n",
      "Iteration 5741, loss = 0.09349434\n",
      "Iteration 5742, loss = 0.09345474\n",
      "Iteration 5743, loss = 0.09341516\n",
      "Iteration 5744, loss = 0.09337562\n",
      "Iteration 5745, loss = 0.09333610\n",
      "Iteration 5746, loss = 0.09329661\n",
      "Iteration 5747, loss = 0.09325714\n",
      "Iteration 5748, loss = 0.09321771\n",
      "Iteration 5749, loss = 0.09317830\n",
      "Iteration 5750, loss = 0.09313892\n",
      "Iteration 5751, loss = 0.09309957\n",
      "Iteration 5752, loss = 0.09306025\n",
      "Iteration 5753, loss = 0.09302095\n",
      "Iteration 5754, loss = 0.09298168\n",
      "Iteration 5755, loss = 0.09294244\n",
      "Iteration 5756, loss = 0.09290323\n",
      "Iteration 5757, loss = 0.09286404\n",
      "Iteration 5758, loss = 0.09282488\n",
      "Iteration 5759, loss = 0.09278575\n",
      "Iteration 5760, loss = 0.09274665\n",
      "Iteration 5761, loss = 0.09270757\n",
      "Iteration 5762, loss = 0.09266852\n",
      "Iteration 5763, loss = 0.09262950\n",
      "Iteration 5764, loss = 0.09259051\n",
      "Iteration 5765, loss = 0.09255154\n",
      "Iteration 5766, loss = 0.09251260\n",
      "Iteration 5767, loss = 0.09247369\n",
      "Iteration 5768, loss = 0.09243480\n",
      "Iteration 5769, loss = 0.09239595\n",
      "Iteration 5770, loss = 0.09235712\n",
      "Iteration 5771, loss = 0.09231831\n",
      "Iteration 5772, loss = 0.09227954\n",
      "Iteration 5773, loss = 0.09224079\n",
      "Iteration 5774, loss = 0.09220207\n",
      "Iteration 5775, loss = 0.09216337\n",
      "Iteration 5776, loss = 0.09212471\n",
      "Iteration 5777, loss = 0.09208607\n",
      "Iteration 5778, loss = 0.09204745\n",
      "Iteration 5779, loss = 0.09200887\n",
      "Iteration 5780, loss = 0.09197031\n",
      "Iteration 5781, loss = 0.09193178\n",
      "Iteration 5782, loss = 0.09189327\n",
      "Iteration 5783, loss = 0.09185479\n",
      "Iteration 5784, loss = 0.09181634\n",
      "Iteration 5785, loss = 0.09177792\n",
      "Iteration 5786, loss = 0.09173952\n",
      "Iteration 5787, loss = 0.09170115\n",
      "Iteration 5788, loss = 0.09166281\n",
      "Iteration 5789, loss = 0.09162449\n",
      "Iteration 5790, loss = 0.09158620\n",
      "Iteration 5791, loss = 0.09154793\n",
      "Iteration 5792, loss = 0.09150970\n",
      "Iteration 5793, loss = 0.09147149\n",
      "Iteration 5794, loss = 0.09143330\n",
      "Iteration 5795, loss = 0.09139514\n",
      "Iteration 5796, loss = 0.09135701\n",
      "Iteration 5797, loss = 0.09131891\n",
      "Iteration 5798, loss = 0.09128083\n",
      "Iteration 5799, loss = 0.09124278\n",
      "Iteration 5800, loss = 0.09120476\n",
      "Iteration 5801, loss = 0.09116676\n",
      "Iteration 5802, loss = 0.09112879\n",
      "Iteration 5803, loss = 0.09109084\n",
      "Iteration 5804, loss = 0.09105292\n",
      "Iteration 5805, loss = 0.09101503\n",
      "Iteration 5806, loss = 0.09097716\n",
      "Iteration 5807, loss = 0.09093933\n",
      "Iteration 5808, loss = 0.09090151\n",
      "Iteration 5809, loss = 0.09086372\n",
      "Iteration 5810, loss = 0.09082596\n",
      "Iteration 5811, loss = 0.09078823\n",
      "Iteration 5812, loss = 0.09075052\n",
      "Iteration 5813, loss = 0.09071284\n",
      "Iteration 5814, loss = 0.09067518\n",
      "Iteration 5815, loss = 0.09063755\n",
      "Iteration 5816, loss = 0.09059995\n",
      "Iteration 5817, loss = 0.09056237\n",
      "Iteration 5818, loss = 0.09052482\n",
      "Iteration 5819, loss = 0.09048729\n",
      "Iteration 5820, loss = 0.09044979\n",
      "Iteration 5821, loss = 0.09041232\n",
      "Iteration 5822, loss = 0.09037487\n",
      "Iteration 5823, loss = 0.09033745\n",
      "Iteration 5824, loss = 0.09030005\n",
      "Iteration 5825, loss = 0.09026268\n",
      "Iteration 5826, loss = 0.09022534\n",
      "Iteration 5827, loss = 0.09018802\n",
      "Iteration 5828, loss = 0.09015073\n",
      "Iteration 5829, loss = 0.09011346\n",
      "Iteration 5830, loss = 0.09007622\n",
      "Iteration 5831, loss = 0.09003900\n",
      "Iteration 5832, loss = 0.09000181\n",
      "Iteration 5833, loss = 0.08996465\n",
      "Iteration 5834, loss = 0.08992751\n",
      "Iteration 5835, loss = 0.08989040\n",
      "Iteration 5836, loss = 0.08985331\n",
      "Iteration 5837, loss = 0.08981625\n",
      "Iteration 5838, loss = 0.08977921\n",
      "Iteration 5839, loss = 0.08974220\n",
      "Iteration 5840, loss = 0.08970522\n",
      "Iteration 5841, loss = 0.08966826\n",
      "Iteration 5842, loss = 0.08963133\n",
      "Iteration 5843, loss = 0.08959442\n",
      "Iteration 5844, loss = 0.08955754\n",
      "Iteration 5845, loss = 0.08952068\n",
      "Iteration 5846, loss = 0.08948385\n",
      "Iteration 5847, loss = 0.08944704\n",
      "Iteration 5848, loss = 0.08941026\n",
      "Iteration 5849, loss = 0.08937350\n",
      "Iteration 5850, loss = 0.08933677\n",
      "Iteration 5851, loss = 0.08930007\n",
      "Iteration 5852, loss = 0.08926339\n",
      "Iteration 5853, loss = 0.08922673\n",
      "Iteration 5854, loss = 0.08919010\n",
      "Iteration 5855, loss = 0.08915350\n",
      "Iteration 5856, loss = 0.08911692\n",
      "Iteration 5857, loss = 0.08908036\n",
      "Iteration 5858, loss = 0.08904383\n",
      "Iteration 5859, loss = 0.08900733\n",
      "Iteration 5860, loss = 0.08897085\n",
      "Iteration 5861, loss = 0.08893440\n",
      "Iteration 5862, loss = 0.08889797\n",
      "Iteration 5863, loss = 0.08886156\n",
      "Iteration 5864, loss = 0.08882518\n",
      "Iteration 5865, loss = 0.08878883\n",
      "Iteration 5866, loss = 0.08875250\n",
      "Iteration 5867, loss = 0.08871620\n",
      "Iteration 5868, loss = 0.08867992\n",
      "Iteration 5869, loss = 0.08864366\n",
      "Iteration 5870, loss = 0.08860743\n",
      "Iteration 5871, loss = 0.08857123\n",
      "Iteration 5872, loss = 0.08853505\n",
      "Iteration 5873, loss = 0.08849889\n",
      "Iteration 5874, loss = 0.08846276\n",
      "Iteration 5875, loss = 0.08842666\n",
      "Iteration 5876, loss = 0.08839057\n",
      "Iteration 5877, loss = 0.08835452\n",
      "Iteration 5878, loss = 0.08831849\n",
      "Iteration 5879, loss = 0.08828248\n",
      "Iteration 5880, loss = 0.08824650\n",
      "Iteration 5881, loss = 0.08821054\n",
      "Iteration 5882, loss = 0.08817461\n",
      "Iteration 5883, loss = 0.08813870\n",
      "Iteration 5884, loss = 0.08810281\n",
      "Iteration 5885, loss = 0.08806695\n",
      "Iteration 5886, loss = 0.08803112\n",
      "Iteration 5887, loss = 0.08799531\n",
      "Iteration 5888, loss = 0.08795952\n",
      "Iteration 5889, loss = 0.08792376\n",
      "Iteration 5890, loss = 0.08788802\n",
      "Iteration 5891, loss = 0.08785231\n",
      "Iteration 5892, loss = 0.08781662\n",
      "Iteration 5893, loss = 0.08778095\n",
      "Iteration 5894, loss = 0.08774531\n",
      "Iteration 5895, loss = 0.08770970\n",
      "Iteration 5896, loss = 0.08767410\n",
      "Iteration 5897, loss = 0.08763854\n",
      "Iteration 5898, loss = 0.08760299\n",
      "Iteration 5899, loss = 0.08756747\n",
      "Iteration 5900, loss = 0.08753198\n",
      "Iteration 5901, loss = 0.08749651\n",
      "Iteration 5902, loss = 0.08746106\n",
      "Iteration 5903, loss = 0.08742564\n",
      "Iteration 5904, loss = 0.08739024\n",
      "Iteration 5905, loss = 0.08735487\n",
      "Iteration 5906, loss = 0.08731951\n",
      "Iteration 5907, loss = 0.08728419\n",
      "Iteration 5908, loss = 0.08724889\n",
      "Iteration 5909, loss = 0.08721361\n",
      "Iteration 5910, loss = 0.08717835\n",
      "Iteration 5911, loss = 0.08714312\n",
      "Iteration 5912, loss = 0.08710792\n",
      "Iteration 5913, loss = 0.08707273\n",
      "Iteration 5914, loss = 0.08703757\n",
      "Iteration 5915, loss = 0.08700244\n",
      "Iteration 5916, loss = 0.08696733\n",
      "Iteration 5917, loss = 0.08693224\n",
      "Iteration 5918, loss = 0.08689718\n",
      "Iteration 5919, loss = 0.08686214\n",
      "Iteration 5920, loss = 0.08682712\n",
      "Iteration 5921, loss = 0.08679213\n",
      "Iteration 5922, loss = 0.08675716\n",
      "Iteration 5923, loss = 0.08672221\n",
      "Iteration 5924, loss = 0.08668729\n",
      "Iteration 5925, loss = 0.08665239\n",
      "Iteration 5926, loss = 0.08661752\n",
      "Iteration 5927, loss = 0.08658267\n",
      "Iteration 5928, loss = 0.08654784\n",
      "Iteration 5929, loss = 0.08651304\n",
      "Iteration 5930, loss = 0.08647826\n",
      "Iteration 5931, loss = 0.08644350\n",
      "Iteration 5932, loss = 0.08640877\n",
      "Iteration 5933, loss = 0.08637406\n",
      "Iteration 5934, loss = 0.08633937\n",
      "Iteration 5935, loss = 0.08630471\n",
      "Iteration 5936, loss = 0.08627007\n",
      "Iteration 5937, loss = 0.08623545\n",
      "Iteration 5938, loss = 0.08620086\n",
      "Iteration 5939, loss = 0.08616629\n",
      "Iteration 5940, loss = 0.08613174\n",
      "Iteration 5941, loss = 0.08609722\n",
      "Iteration 5942, loss = 0.08606272\n",
      "Iteration 5943, loss = 0.08602824\n",
      "Iteration 5944, loss = 0.08599379\n",
      "Iteration 5945, loss = 0.08595936\n",
      "Iteration 5946, loss = 0.08592495\n",
      "Iteration 5947, loss = 0.08589057\n",
      "Iteration 5948, loss = 0.08585621\n",
      "Iteration 5949, loss = 0.08582187\n",
      "Iteration 5950, loss = 0.08578756\n",
      "Iteration 5951, loss = 0.08575327\n",
      "Iteration 5952, loss = 0.08571900\n",
      "Iteration 5953, loss = 0.08568475\n",
      "Iteration 5954, loss = 0.08565053\n",
      "Iteration 5955, loss = 0.08561633\n",
      "Iteration 5956, loss = 0.08558216\n",
      "Iteration 5957, loss = 0.08554800\n",
      "Iteration 5958, loss = 0.08551387\n",
      "Iteration 5959, loss = 0.08547977\n",
      "Iteration 5960, loss = 0.08544568\n",
      "Iteration 5961, loss = 0.08541162\n",
      "Iteration 5962, loss = 0.08537758\n",
      "Iteration 5963, loss = 0.08534357\n",
      "Iteration 5964, loss = 0.08530957\n",
      "Iteration 5965, loss = 0.08527560\n",
      "Iteration 5966, loss = 0.08524165\n",
      "Iteration 5967, loss = 0.08520773\n",
      "Iteration 5968, loss = 0.08517383\n",
      "Iteration 5969, loss = 0.08513995\n",
      "Iteration 5970, loss = 0.08510609\n",
      "Iteration 5971, loss = 0.08507226\n",
      "Iteration 5972, loss = 0.08503845\n",
      "Iteration 5973, loss = 0.08500466\n",
      "Iteration 5974, loss = 0.08497089\n",
      "Iteration 5975, loss = 0.08493715\n",
      "Iteration 5976, loss = 0.08490343\n",
      "Iteration 5977, loss = 0.08486973\n",
      "Iteration 5978, loss = 0.08483605\n",
      "Iteration 5979, loss = 0.08480240\n",
      "Iteration 5980, loss = 0.08476877\n",
      "Iteration 5981, loss = 0.08473516\n",
      "Iteration 5982, loss = 0.08470157\n",
      "Iteration 5983, loss = 0.08466801\n",
      "Iteration 5984, loss = 0.08463447\n",
      "Iteration 5985, loss = 0.08460095\n",
      "Iteration 5986, loss = 0.08456745\n",
      "Iteration 5987, loss = 0.08453398\n",
      "Iteration 5988, loss = 0.08450053\n",
      "Iteration 5989, loss = 0.08446710\n",
      "Iteration 5990, loss = 0.08443369\n",
      "Iteration 5991, loss = 0.08440031\n",
      "Iteration 5992, loss = 0.08436694\n",
      "Iteration 5993, loss = 0.08433360\n",
      "Iteration 5994, loss = 0.08430028\n",
      "Iteration 5995, loss = 0.08426699\n",
      "Iteration 5996, loss = 0.08423371\n",
      "Iteration 5997, loss = 0.08420046\n",
      "Iteration 5998, loss = 0.08416723\n",
      "Iteration 5999, loss = 0.08413403\n",
      "Iteration 6000, loss = 0.08410084\n",
      "Iteration 6001, loss = 0.08406768\n",
      "Iteration 6002, loss = 0.08403453\n",
      "Iteration 6003, loss = 0.08400142\n",
      "Iteration 6004, loss = 0.08396832\n",
      "Iteration 6005, loss = 0.08393524\n",
      "Iteration 6006, loss = 0.08390219\n",
      "Iteration 6007, loss = 0.08386916\n",
      "Iteration 6008, loss = 0.08383615\n",
      "Iteration 6009, loss = 0.08380316\n",
      "Iteration 6010, loss = 0.08377020\n",
      "Iteration 6011, loss = 0.08373725\n",
      "Iteration 6012, loss = 0.08370433\n",
      "Iteration 6013, loss = 0.08367143\n",
      "Iteration 6014, loss = 0.08363855\n",
      "Iteration 6015, loss = 0.08360569\n",
      "Iteration 6016, loss = 0.08357286\n",
      "Iteration 6017, loss = 0.08354005\n",
      "Iteration 6018, loss = 0.08350726\n",
      "Iteration 6019, loss = 0.08347449\n",
      "Iteration 6020, loss = 0.08344174\n",
      "Iteration 6021, loss = 0.08340901\n",
      "Iteration 6022, loss = 0.08337631\n",
      "Iteration 6023, loss = 0.08334362\n",
      "Iteration 6024, loss = 0.08331096\n",
      "Iteration 6025, loss = 0.08327832\n",
      "Iteration 6026, loss = 0.08324571\n",
      "Iteration 6027, loss = 0.08321311\n",
      "Iteration 6028, loss = 0.08318053\n",
      "Iteration 6029, loss = 0.08314798\n",
      "Iteration 6030, loss = 0.08311545\n",
      "Iteration 6031, loss = 0.08308294\n",
      "Iteration 6032, loss = 0.08305045\n",
      "Iteration 6033, loss = 0.08301798\n",
      "Iteration 6034, loss = 0.08298554\n",
      "Iteration 6035, loss = 0.08295311\n",
      "Iteration 6036, loss = 0.08292071\n",
      "Iteration 6037, loss = 0.08288833\n",
      "Iteration 6038, loss = 0.08285596\n",
      "Iteration 6039, loss = 0.08282363\n",
      "Iteration 6040, loss = 0.08279131\n",
      "Iteration 6041, loss = 0.08275901\n",
      "Iteration 6042, loss = 0.08272674\n",
      "Iteration 6043, loss = 0.08269448\n",
      "Iteration 6044, loss = 0.08266225\n",
      "Iteration 6045, loss = 0.08263004\n",
      "Iteration 6046, loss = 0.08259785\n",
      "Iteration 6047, loss = 0.08256568\n",
      "Iteration 6048, loss = 0.08253353\n",
      "Iteration 6049, loss = 0.08250140\n",
      "Iteration 6050, loss = 0.08246930\n",
      "Iteration 6051, loss = 0.08243721\n",
      "Iteration 6052, loss = 0.08240515\n",
      "Iteration 6053, loss = 0.08237311\n",
      "Iteration 6054, loss = 0.08234108\n",
      "Iteration 6055, loss = 0.08230908\n",
      "Iteration 6056, loss = 0.08227710\n",
      "Iteration 6057, loss = 0.08224515\n",
      "Iteration 6058, loss = 0.08221321\n",
      "Iteration 6059, loss = 0.08218129\n",
      "Iteration 6060, loss = 0.08214940\n",
      "Iteration 6061, loss = 0.08211752\n",
      "Iteration 6062, loss = 0.08208567\n",
      "Iteration 6063, loss = 0.08205383\n",
      "Iteration 6064, loss = 0.08202202\n",
      "Iteration 6065, loss = 0.08199023\n",
      "Iteration 6066, loss = 0.08195846\n",
      "Iteration 6067, loss = 0.08192671\n",
      "Iteration 6068, loss = 0.08189498\n",
      "Iteration 6069, loss = 0.08186327\n",
      "Iteration 6070, loss = 0.08183159\n",
      "Iteration 6071, loss = 0.08179992\n",
      "Iteration 6072, loss = 0.08176828\n",
      "Iteration 6073, loss = 0.08173665\n",
      "Iteration 6074, loss = 0.08170505\n",
      "Iteration 6075, loss = 0.08167346\n",
      "Iteration 6076, loss = 0.08164190\n",
      "Iteration 6077, loss = 0.08161036\n",
      "Iteration 6078, loss = 0.08157884\n",
      "Iteration 6079, loss = 0.08154733\n",
      "Iteration 6080, loss = 0.08151585\n",
      "Iteration 6081, loss = 0.08148439\n",
      "Iteration 6082, loss = 0.08145295\n",
      "Iteration 6083, loss = 0.08142154\n",
      "Iteration 6084, loss = 0.08139014\n",
      "Iteration 6085, loss = 0.08135876\n",
      "Iteration 6086, loss = 0.08132740\n",
      "Iteration 6087, loss = 0.08129606\n",
      "Iteration 6088, loss = 0.08126475\n",
      "Iteration 6089, loss = 0.08123345\n",
      "Iteration 6090, loss = 0.08120218\n",
      "Iteration 6091, loss = 0.08117092\n",
      "Iteration 6092, loss = 0.08113969\n",
      "Iteration 6093, loss = 0.08110847\n",
      "Iteration 6094, loss = 0.08107728\n",
      "Iteration 6095, loss = 0.08104610\n",
      "Iteration 6096, loss = 0.08101495\n",
      "Iteration 6097, loss = 0.08098381\n",
      "Iteration 6098, loss = 0.08095270\n",
      "Iteration 6099, loss = 0.08092161\n",
      "Iteration 6100, loss = 0.08089054\n",
      "Iteration 6101, loss = 0.08085948\n",
      "Iteration 6102, loss = 0.08082845\n",
      "Iteration 6103, loss = 0.08079744\n",
      "Iteration 6104, loss = 0.08076645\n",
      "Iteration 6105, loss = 0.08073547\n",
      "Iteration 6106, loss = 0.08070452\n",
      "Iteration 6107, loss = 0.08067359\n",
      "Iteration 6108, loss = 0.08064268\n",
      "Iteration 6109, loss = 0.08061179\n",
      "Iteration 6110, loss = 0.08058092\n",
      "Iteration 6111, loss = 0.08055006\n",
      "Iteration 6112, loss = 0.08051923\n",
      "Iteration 6113, loss = 0.08048842\n",
      "Iteration 6114, loss = 0.08045763\n",
      "Iteration 6115, loss = 0.08042686\n",
      "Iteration 6116, loss = 0.08039611\n",
      "Iteration 6117, loss = 0.08036538\n",
      "Iteration 6118, loss = 0.08033466\n",
      "Iteration 6119, loss = 0.08030397\n",
      "Iteration 6120, loss = 0.08027330\n",
      "Iteration 6121, loss = 0.08024265\n",
      "Iteration 6122, loss = 0.08021202\n",
      "Iteration 6123, loss = 0.08018140\n",
      "Iteration 6124, loss = 0.08015081\n",
      "Iteration 6125, loss = 0.08012024\n",
      "Iteration 6126, loss = 0.08008968\n",
      "Iteration 6127, loss = 0.08005915\n",
      "Iteration 6128, loss = 0.08002864\n",
      "Iteration 6129, loss = 0.07999814\n",
      "Iteration 6130, loss = 0.07996767\n",
      "Iteration 6131, loss = 0.07993721\n",
      "Iteration 6132, loss = 0.07990678\n",
      "Iteration 6133, loss = 0.07987636\n",
      "Iteration 6134, loss = 0.07984597\n",
      "Iteration 6135, loss = 0.07981559\n",
      "Iteration 6136, loss = 0.07978524\n",
      "Iteration 6137, loss = 0.07975490\n",
      "Iteration 6138, loss = 0.07972458\n",
      "Iteration 6139, loss = 0.07969428\n",
      "Iteration 6140, loss = 0.07966401\n",
      "Iteration 6141, loss = 0.07963375\n",
      "Iteration 6142, loss = 0.07960351\n",
      "Iteration 6143, loss = 0.07957329\n",
      "Iteration 6144, loss = 0.07954309\n",
      "Iteration 6145, loss = 0.07951291\n",
      "Iteration 6146, loss = 0.07948274\n",
      "Iteration 6147, loss = 0.07945260\n",
      "Iteration 6148, loss = 0.07942248\n",
      "Iteration 6149, loss = 0.07939238\n",
      "Iteration 6150, loss = 0.07936229\n",
      "Iteration 6151, loss = 0.07933223\n",
      "Iteration 6152, loss = 0.07930218\n",
      "Iteration 6153, loss = 0.07927216\n",
      "Iteration 6154, loss = 0.07924215\n",
      "Iteration 6155, loss = 0.07921216\n",
      "Iteration 6156, loss = 0.07918220\n",
      "Iteration 6157, loss = 0.07915225\n",
      "Iteration 6158, loss = 0.07912232\n",
      "Iteration 6159, loss = 0.07909241\n",
      "Iteration 6160, loss = 0.07906252\n",
      "Iteration 6161, loss = 0.07903264\n",
      "Iteration 6162, loss = 0.07900279\n",
      "Iteration 6163, loss = 0.07897296\n",
      "Iteration 6164, loss = 0.07894314\n",
      "Iteration 6165, loss = 0.07891335\n",
      "Iteration 6166, loss = 0.07888357\n",
      "Iteration 6167, loss = 0.07885381\n",
      "Iteration 6168, loss = 0.07882408\n",
      "Iteration 6169, loss = 0.07879436\n",
      "Iteration 6170, loss = 0.07876466\n",
      "Iteration 6171, loss = 0.07873498\n",
      "Iteration 6172, loss = 0.07870531\n",
      "Iteration 6173, loss = 0.07867567\n",
      "Iteration 6174, loss = 0.07864605\n",
      "Iteration 6175, loss = 0.07861644\n",
      "Iteration 6176, loss = 0.07858685\n",
      "Iteration 6177, loss = 0.07855729\n",
      "Iteration 6178, loss = 0.07852774\n",
      "Iteration 6179, loss = 0.07849821\n",
      "Iteration 6180, loss = 0.07846870\n",
      "Iteration 6181, loss = 0.07843921\n",
      "Iteration 6182, loss = 0.07840973\n",
      "Iteration 6183, loss = 0.07838028\n",
      "Iteration 6184, loss = 0.07835084\n",
      "Iteration 6185, loss = 0.07832143\n",
      "Iteration 6186, loss = 0.07829203\n",
      "Iteration 6187, loss = 0.07826265\n",
      "Iteration 6188, loss = 0.07823329\n",
      "Iteration 6189, loss = 0.07820395\n",
      "Iteration 6190, loss = 0.07817462\n",
      "Iteration 6191, loss = 0.07814532\n",
      "Iteration 6192, loss = 0.07811604\n",
      "Iteration 6193, loss = 0.07808677\n",
      "Iteration 6194, loss = 0.07805752\n",
      "Iteration 6195, loss = 0.07802829\n",
      "Iteration 6196, loss = 0.07799908\n",
      "Iteration 6197, loss = 0.07796989\n",
      "Iteration 6198, loss = 0.07794071\n",
      "Iteration 6199, loss = 0.07791156\n",
      "Iteration 6200, loss = 0.07788242\n",
      "Iteration 6201, loss = 0.07785330\n",
      "Iteration 6202, loss = 0.07782420\n",
      "Iteration 6203, loss = 0.07779512\n",
      "Iteration 6204, loss = 0.07776606\n",
      "Iteration 6205, loss = 0.07773701\n",
      "Iteration 6206, loss = 0.07770799\n",
      "Iteration 6207, loss = 0.07767898\n",
      "Iteration 6208, loss = 0.07764999\n",
      "Iteration 6209, loss = 0.07762102\n",
      "Iteration 6210, loss = 0.07759207\n",
      "Iteration 6211, loss = 0.07756314\n",
      "Iteration 6212, loss = 0.07753422\n",
      "Iteration 6213, loss = 0.07750532\n",
      "Iteration 6214, loss = 0.07747644\n",
      "Iteration 6215, loss = 0.07744758\n",
      "Iteration 6216, loss = 0.07741874\n",
      "Iteration 6217, loss = 0.07738992\n",
      "Iteration 6218, loss = 0.07736111\n",
      "Iteration 6219, loss = 0.07733232\n",
      "Iteration 6220, loss = 0.07730356\n",
      "Iteration 6221, loss = 0.07727480\n",
      "Iteration 6222, loss = 0.07724607\n",
      "Iteration 6223, loss = 0.07721736\n",
      "Iteration 6224, loss = 0.07718866\n",
      "Iteration 6225, loss = 0.07715998\n",
      "Iteration 6226, loss = 0.07713132\n",
      "Iteration 6227, loss = 0.07710268\n",
      "Iteration 6228, loss = 0.07707406\n",
      "Iteration 6229, loss = 0.07704545\n",
      "Iteration 6230, loss = 0.07701686\n",
      "Iteration 6231, loss = 0.07698829\n",
      "Iteration 6232, loss = 0.07695974\n",
      "Iteration 6233, loss = 0.07693121\n",
      "Iteration 6234, loss = 0.07690269\n",
      "Iteration 6235, loss = 0.07687420\n",
      "Iteration 6236, loss = 0.07684572\n",
      "Iteration 6237, loss = 0.07681725\n",
      "Iteration 6238, loss = 0.07678881\n",
      "Iteration 6239, loss = 0.07676039\n",
      "Iteration 6240, loss = 0.07673198\n",
      "Iteration 6241, loss = 0.07670359\n",
      "Iteration 6242, loss = 0.07667522\n",
      "Iteration 6243, loss = 0.07664686\n",
      "Iteration 6244, loss = 0.07661853\n",
      "Iteration 6245, loss = 0.07659021\n",
      "Iteration 6246, loss = 0.07656191\n",
      "Iteration 6247, loss = 0.07653362\n",
      "Iteration 6248, loss = 0.07650536\n",
      "Iteration 6249, loss = 0.07647711\n",
      "Iteration 6250, loss = 0.07644888\n",
      "Iteration 6251, loss = 0.07642067\n",
      "Iteration 6252, loss = 0.07639248\n",
      "Iteration 6253, loss = 0.07636430\n",
      "Iteration 6254, loss = 0.07633614\n",
      "Iteration 6255, loss = 0.07630800\n",
      "Iteration 6256, loss = 0.07627988\n",
      "Iteration 6257, loss = 0.07625177\n",
      "Iteration 6258, loss = 0.07622369\n",
      "Iteration 6259, loss = 0.07619562\n",
      "Iteration 6260, loss = 0.07616756\n",
      "Iteration 6261, loss = 0.07613953\n",
      "Iteration 6262, loss = 0.07611151\n",
      "Iteration 6263, loss = 0.07608351\n",
      "Iteration 6264, loss = 0.07605553\n",
      "Iteration 6265, loss = 0.07602757\n",
      "Iteration 6266, loss = 0.07599962\n",
      "Iteration 6267, loss = 0.07597169\n",
      "Iteration 6268, loss = 0.07594378\n",
      "Iteration 6269, loss = 0.07591588\n",
      "Iteration 6270, loss = 0.07588800\n",
      "Iteration 6271, loss = 0.07586015\n",
      "Iteration 6272, loss = 0.07583230\n",
      "Iteration 6273, loss = 0.07580448\n",
      "Iteration 6274, loss = 0.07577667\n",
      "Iteration 6275, loss = 0.07574888\n",
      "Iteration 6276, loss = 0.07572111\n",
      "Iteration 6277, loss = 0.07569335\n",
      "Iteration 6278, loss = 0.07566562\n",
      "Iteration 6279, loss = 0.07563790\n",
      "Iteration 6280, loss = 0.07561019\n",
      "Iteration 6281, loss = 0.07558251\n",
      "Iteration 6282, loss = 0.07555484\n",
      "Iteration 6283, loss = 0.07552719\n",
      "Iteration 6284, loss = 0.07549955\n",
      "Iteration 6285, loss = 0.07547194\n",
      "Iteration 6286, loss = 0.07544434\n",
      "Iteration 6287, loss = 0.07541676\n",
      "Iteration 6288, loss = 0.07538919\n",
      "Iteration 6289, loss = 0.07536164\n",
      "Iteration 6290, loss = 0.07533411\n",
      "Iteration 6291, loss = 0.07530660\n",
      "Iteration 6292, loss = 0.07527910\n",
      "Iteration 6293, loss = 0.07525162\n",
      "Iteration 6294, loss = 0.07522416\n",
      "Iteration 6295, loss = 0.07519672\n",
      "Iteration 6296, loss = 0.07516929\n",
      "Iteration 6297, loss = 0.07514188\n",
      "Iteration 6298, loss = 0.07511448\n",
      "Iteration 6299, loss = 0.07508711\n",
      "Iteration 6300, loss = 0.07505975\n",
      "Iteration 6301, loss = 0.07503241\n",
      "Iteration 6302, loss = 0.07500508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6303, loss = 0.07497777\n",
      "Iteration 6304, loss = 0.07495048\n",
      "Iteration 6305, loss = 0.07492320\n",
      "Iteration 6306, loss = 0.07489595\n",
      "Iteration 6307, loss = 0.07486871\n",
      "Iteration 6308, loss = 0.07484148\n",
      "Iteration 6309, loss = 0.07481428\n",
      "Iteration 6310, loss = 0.07478709\n",
      "Iteration 6311, loss = 0.07475991\n",
      "Iteration 6312, loss = 0.07473276\n",
      "Iteration 6313, loss = 0.07470562\n",
      "Iteration 6314, loss = 0.07467849\n",
      "Iteration 6315, loss = 0.07465139\n",
      "Iteration 6316, loss = 0.07462430\n",
      "Iteration 6317, loss = 0.07459723\n",
      "Iteration 6318, loss = 0.07457017\n",
      "Iteration 6319, loss = 0.07454313\n",
      "Iteration 6320, loss = 0.07451611\n",
      "Iteration 6321, loss = 0.07448911\n",
      "Iteration 6322, loss = 0.07446212\n",
      "Iteration 6323, loss = 0.07443515\n",
      "Iteration 6324, loss = 0.07440819\n",
      "Iteration 6325, loss = 0.07438125\n",
      "Iteration 6326, loss = 0.07435433\n",
      "Iteration 6327, loss = 0.07432743\n",
      "Iteration 6328, loss = 0.07430054\n",
      "Iteration 6329, loss = 0.07427367\n",
      "Iteration 6330, loss = 0.07424681\n",
      "Iteration 6331, loss = 0.07421998\n",
      "Iteration 6332, loss = 0.07419315\n",
      "Iteration 6333, loss = 0.07416635\n",
      "Iteration 6334, loss = 0.07413956\n",
      "Iteration 6335, loss = 0.07411279\n",
      "Iteration 6336, loss = 0.07408603\n",
      "Iteration 6337, loss = 0.07405929\n",
      "Iteration 6338, loss = 0.07403257\n",
      "Iteration 6339, loss = 0.07400586\n",
      "Iteration 6340, loss = 0.07397918\n",
      "Iteration 6341, loss = 0.07395250\n",
      "Iteration 6342, loss = 0.07392585\n",
      "Iteration 6343, loss = 0.07389921\n",
      "Iteration 6344, loss = 0.07387258\n",
      "Iteration 6345, loss = 0.07384597\n",
      "Iteration 6346, loss = 0.07381938\n",
      "Iteration 6347, loss = 0.07379281\n",
      "Iteration 6348, loss = 0.07376625\n",
      "Iteration 6349, loss = 0.07373971\n",
      "Iteration 6350, loss = 0.07371318\n",
      "Iteration 6351, loss = 0.07368667\n",
      "Iteration 6352, loss = 0.07366018\n",
      "Iteration 6353, loss = 0.07363370\n",
      "Iteration 6354, loss = 0.07360724\n",
      "Iteration 6355, loss = 0.07358080\n",
      "Iteration 6356, loss = 0.07355437\n",
      "Iteration 6357, loss = 0.07352796\n",
      "Iteration 6358, loss = 0.07350157\n",
      "Iteration 6359, loss = 0.07347519\n",
      "Iteration 6360, loss = 0.07344882\n",
      "Iteration 6361, loss = 0.07342248\n",
      "Iteration 6362, loss = 0.07339615\n",
      "Iteration 6363, loss = 0.07336983\n",
      "Iteration 6364, loss = 0.07334353\n",
      "Iteration 6365, loss = 0.07331725\n",
      "Iteration 6366, loss = 0.07329099\n",
      "Iteration 6367, loss = 0.07326474\n",
      "Iteration 6368, loss = 0.07323850\n",
      "Iteration 6369, loss = 0.07321229\n",
      "Iteration 6370, loss = 0.07318608\n",
      "Iteration 6371, loss = 0.07315990\n",
      "Iteration 6372, loss = 0.07313373\n",
      "Iteration 6373, loss = 0.07310758\n",
      "Iteration 6374, loss = 0.07308144\n",
      "Iteration 6375, loss = 0.07305532\n",
      "Iteration 6376, loss = 0.07302921\n",
      "Iteration 6377, loss = 0.07300312\n",
      "Iteration 6378, loss = 0.07297705\n",
      "Iteration 6379, loss = 0.07295099\n",
      "Iteration 6380, loss = 0.07292495\n",
      "Iteration 6381, loss = 0.07289892\n",
      "Iteration 6382, loss = 0.07287292\n",
      "Iteration 6383, loss = 0.07284692\n",
      "Iteration 6384, loss = 0.07282094\n",
      "Iteration 6385, loss = 0.07279498\n",
      "Iteration 6386, loss = 0.07276904\n",
      "Iteration 6387, loss = 0.07274311\n",
      "Iteration 6388, loss = 0.07271719\n",
      "Iteration 6389, loss = 0.07269129\n",
      "Iteration 6390, loss = 0.07266541\n",
      "Iteration 6391, loss = 0.07263954\n",
      "Iteration 6392, loss = 0.07261369\n",
      "Iteration 6393, loss = 0.07258786\n",
      "Iteration 6394, loss = 0.07256204\n",
      "Iteration 6395, loss = 0.07253623\n",
      "Iteration 6396, loss = 0.07251044\n",
      "Iteration 6397, loss = 0.07248467\n",
      "Iteration 6398, loss = 0.07245892\n",
      "Iteration 6399, loss = 0.07243317\n",
      "Iteration 6400, loss = 0.07240745\n",
      "Iteration 6401, loss = 0.07238174\n",
      "Iteration 6402, loss = 0.07235605\n",
      "Iteration 6403, loss = 0.07233037\n",
      "Iteration 6404, loss = 0.07230470\n",
      "Iteration 6405, loss = 0.07227906\n",
      "Iteration 6406, loss = 0.07225343\n",
      "Iteration 6407, loss = 0.07222781\n",
      "Iteration 6408, loss = 0.07220221\n",
      "Iteration 6409, loss = 0.07217662\n",
      "Iteration 6410, loss = 0.07215105\n",
      "Iteration 6411, loss = 0.07212550\n",
      "Iteration 6412, loss = 0.07209996\n",
      "Iteration 6413, loss = 0.07207444\n",
      "Iteration 6414, loss = 0.07204893\n",
      "Iteration 6415, loss = 0.07202344\n",
      "Iteration 6416, loss = 0.07199796\n",
      "Iteration 6417, loss = 0.07197250\n",
      "Iteration 6418, loss = 0.07194706\n",
      "Iteration 6419, loss = 0.07192163\n",
      "Iteration 6420, loss = 0.07189621\n",
      "Iteration 6421, loss = 0.07187082\n",
      "Iteration 6422, loss = 0.07184543\n",
      "Iteration 6423, loss = 0.07182006\n",
      "Iteration 6424, loss = 0.07179471\n",
      "Iteration 6425, loss = 0.07176937\n",
      "Iteration 6426, loss = 0.07174405\n",
      "Iteration 6427, loss = 0.07171874\n",
      "Iteration 6428, loss = 0.07169345\n",
      "Iteration 6429, loss = 0.07166818\n",
      "Iteration 6430, loss = 0.07164292\n",
      "Iteration 6431, loss = 0.07161767\n",
      "Iteration 6432, loss = 0.07159244\n",
      "Iteration 6433, loss = 0.07156722\n",
      "Iteration 6434, loss = 0.07154202\n",
      "Iteration 6435, loss = 0.07151684\n",
      "Iteration 6436, loss = 0.07149167\n",
      "Iteration 6437, loss = 0.07146652\n",
      "Iteration 6438, loss = 0.07144138\n",
      "Iteration 6439, loss = 0.07141625\n",
      "Iteration 6440, loss = 0.07139115\n",
      "Iteration 6441, loss = 0.07136605\n",
      "Iteration 6442, loss = 0.07134097\n",
      "Iteration 6443, loss = 0.07131591\n",
      "Iteration 6444, loss = 0.07129086\n",
      "Iteration 6445, loss = 0.07126583\n",
      "Iteration 6446, loss = 0.07124081\n",
      "Iteration 6447, loss = 0.07121581\n",
      "Iteration 6448, loss = 0.07119082\n",
      "Iteration 6449, loss = 0.07116585\n",
      "Iteration 6450, loss = 0.07114089\n",
      "Iteration 6451, loss = 0.07111595\n",
      "Iteration 6452, loss = 0.07109102\n",
      "Iteration 6453, loss = 0.07106611\n",
      "Iteration 6454, loss = 0.07104121\n",
      "Iteration 6455, loss = 0.07101633\n",
      "Iteration 6456, loss = 0.07099146\n",
      "Iteration 6457, loss = 0.07096661\n",
      "Iteration 6458, loss = 0.07094177\n",
      "Iteration 6459, loss = 0.07091695\n",
      "Iteration 6460, loss = 0.07089214\n",
      "Iteration 6461, loss = 0.07086735\n",
      "Iteration 6462, loss = 0.07084257\n",
      "Iteration 6463, loss = 0.07081781\n",
      "Iteration 6464, loss = 0.07079306\n",
      "Iteration 6465, loss = 0.07076833\n",
      "Iteration 6466, loss = 0.07074361\n",
      "Iteration 6467, loss = 0.07071891\n",
      "Iteration 6468, loss = 0.07069422\n",
      "Iteration 6469, loss = 0.07066954\n",
      "Iteration 6470, loss = 0.07064488\n",
      "Iteration 6471, loss = 0.07062024\n",
      "Iteration 6472, loss = 0.07059561\n",
      "Iteration 6473, loss = 0.07057100\n",
      "Iteration 6474, loss = 0.07054640\n",
      "Iteration 6475, loss = 0.07052181\n",
      "Iteration 6476, loss = 0.07049724\n",
      "Iteration 6477, loss = 0.07047268\n",
      "Iteration 6478, loss = 0.07044814\n",
      "Iteration 6479, loss = 0.07042362\n",
      "Iteration 6480, loss = 0.07039911\n",
      "Iteration 6481, loss = 0.07037461\n",
      "Iteration 6482, loss = 0.07035013\n",
      "Iteration 6483, loss = 0.07032566\n",
      "Iteration 6484, loss = 0.07030121\n",
      "Iteration 6485, loss = 0.07027677\n",
      "Iteration 6486, loss = 0.07025234\n",
      "Iteration 6487, loss = 0.07022794\n",
      "Iteration 6488, loss = 0.07020354\n",
      "Iteration 6489, loss = 0.07017916\n",
      "Iteration 6490, loss = 0.07015480\n",
      "Iteration 6491, loss = 0.07013045\n",
      "Iteration 6492, loss = 0.07010611\n",
      "Iteration 6493, loss = 0.07008179\n",
      "Iteration 6494, loss = 0.07005748\n",
      "Iteration 6495, loss = 0.07003319\n",
      "Iteration 6496, loss = 0.07000891\n",
      "Iteration 6497, loss = 0.06998465\n",
      "Iteration 6498, loss = 0.06996040\n",
      "Iteration 6499, loss = 0.06993616\n",
      "Iteration 6500, loss = 0.06991194\n",
      "Iteration 6501, loss = 0.06988774\n",
      "Iteration 6502, loss = 0.06986354\n",
      "Iteration 6503, loss = 0.06983937\n",
      "Iteration 6504, loss = 0.06981520\n",
      "Iteration 6505, loss = 0.06979106\n",
      "Iteration 6506, loss = 0.06976692\n",
      "Iteration 6507, loss = 0.06974280\n",
      "Iteration 6508, loss = 0.06971870\n",
      "Iteration 6509, loss = 0.06969461\n",
      "Iteration 6510, loss = 0.06967053\n",
      "Iteration 6511, loss = 0.06964647\n",
      "Iteration 6512, loss = 0.06962242\n",
      "Iteration 6513, loss = 0.06959839\n",
      "Iteration 6514, loss = 0.06957437\n",
      "Iteration 6515, loss = 0.06955036\n",
      "Iteration 6516, loss = 0.06952637\n",
      "Iteration 6517, loss = 0.06950240\n",
      "Iteration 6518, loss = 0.06947844\n",
      "Iteration 6519, loss = 0.06945449\n",
      "Iteration 6520, loss = 0.06943055\n",
      "Iteration 6521, loss = 0.06940663\n",
      "Iteration 6522, loss = 0.06938273\n",
      "Iteration 6523, loss = 0.06935884\n",
      "Iteration 6524, loss = 0.06933496\n",
      "Iteration 6525, loss = 0.06931110\n",
      "Iteration 6526, loss = 0.06928725\n",
      "Iteration 6527, loss = 0.06926342\n",
      "Iteration 6528, loss = 0.06923960\n",
      "Iteration 6529, loss = 0.06921579\n",
      "Iteration 6530, loss = 0.06919200\n",
      "Iteration 6531, loss = 0.06916822\n",
      "Iteration 6532, loss = 0.06914446\n",
      "Iteration 6533, loss = 0.06912071\n",
      "Iteration 6534, loss = 0.06909697\n",
      "Iteration 6535, loss = 0.06907325\n",
      "Iteration 6536, loss = 0.06904954\n",
      "Iteration 6537, loss = 0.06902585\n",
      "Iteration 6538, loss = 0.06900217\n",
      "Iteration 6539, loss = 0.06897850\n",
      "Iteration 6540, loss = 0.06895485\n",
      "Iteration 6541, loss = 0.06893121\n",
      "Iteration 6542, loss = 0.06890759\n",
      "Iteration 6543, loss = 0.06888398\n",
      "Iteration 6544, loss = 0.06886039\n",
      "Iteration 6545, loss = 0.06883680\n",
      "Iteration 6546, loss = 0.06881324\n",
      "Iteration 6547, loss = 0.06878968\n",
      "Iteration 6548, loss = 0.06876614\n",
      "Iteration 6549, loss = 0.06874262\n",
      "Iteration 6550, loss = 0.06871910\n",
      "Iteration 6551, loss = 0.06869561\n",
      "Iteration 6552, loss = 0.06867212\n",
      "Iteration 6553, loss = 0.06864865\n",
      "Iteration 6554, loss = 0.06862519\n",
      "Iteration 6555, loss = 0.06860175\n",
      "Iteration 6556, loss = 0.06857832\n",
      "Iteration 6557, loss = 0.06855491\n",
      "Iteration 6558, loss = 0.06853151\n",
      "Iteration 6559, loss = 0.06850812\n",
      "Iteration 6560, loss = 0.06848474\n",
      "Iteration 6561, loss = 0.06846138\n",
      "Iteration 6562, loss = 0.06843804\n",
      "Iteration 6563, loss = 0.06841471\n",
      "Iteration 6564, loss = 0.06839139\n",
      "Iteration 6565, loss = 0.06836808\n",
      "Iteration 6566, loss = 0.06834479\n",
      "Iteration 6567, loss = 0.06832151\n",
      "Iteration 6568, loss = 0.06829825\n",
      "Iteration 6569, loss = 0.06827500\n",
      "Iteration 6570, loss = 0.06825176\n",
      "Iteration 6571, loss = 0.06822854\n",
      "Iteration 6572, loss = 0.06820533\n",
      "Iteration 6573, loss = 0.06818213\n",
      "Iteration 6574, loss = 0.06815895\n",
      "Iteration 6575, loss = 0.06813578\n",
      "Iteration 6576, loss = 0.06811263\n",
      "Iteration 6577, loss = 0.06808949\n",
      "Iteration 6578, loss = 0.06806636\n",
      "Iteration 6579, loss = 0.06804324\n",
      "Iteration 6580, loss = 0.06802014\n",
      "Iteration 6581, loss = 0.06799706\n",
      "Iteration 6582, loss = 0.06797398\n",
      "Iteration 6583, loss = 0.06795092\n",
      "Iteration 6584, loss = 0.06792788\n",
      "Iteration 6585, loss = 0.06790484\n",
      "Iteration 6586, loss = 0.06788182\n",
      "Iteration 6587, loss = 0.06785882\n",
      "Iteration 6588, loss = 0.06783582\n",
      "Iteration 6589, loss = 0.06781284\n",
      "Iteration 6590, loss = 0.06778988\n",
      "Iteration 6591, loss = 0.06776693\n",
      "Iteration 6592, loss = 0.06774399\n",
      "Iteration 6593, loss = 0.06772106\n",
      "Iteration 6594, loss = 0.06769815\n",
      "Iteration 6595, loss = 0.06767525\n",
      "Iteration 6596, loss = 0.06765237\n",
      "Iteration 6597, loss = 0.06762950\n",
      "Iteration 6598, loss = 0.06760664\n",
      "Iteration 6599, loss = 0.06758379\n",
      "Iteration 6600, loss = 0.06756096\n",
      "Iteration 6601, loss = 0.06753814\n",
      "Iteration 6602, loss = 0.06751534\n",
      "Iteration 6603, loss = 0.06749254\n",
      "Iteration 6604, loss = 0.06746977\n",
      "Iteration 6605, loss = 0.06744700\n",
      "Iteration 6606, loss = 0.06742425\n",
      "Iteration 6607, loss = 0.06740151\n",
      "Iteration 6608, loss = 0.06737878\n",
      "Iteration 6609, loss = 0.06735607\n",
      "Iteration 6610, loss = 0.06733337\n",
      "Iteration 6611, loss = 0.06731069\n",
      "Iteration 6612, loss = 0.06728802\n",
      "Iteration 6613, loss = 0.06726536\n",
      "Iteration 6614, loss = 0.06724271\n",
      "Iteration 6615, loss = 0.06722008\n",
      "Iteration 6616, loss = 0.06719746\n",
      "Iteration 6617, loss = 0.06717485\n",
      "Iteration 6618, loss = 0.06715226\n",
      "Iteration 6619, loss = 0.06712968\n",
      "Iteration 6620, loss = 0.06710711\n",
      "Iteration 6621, loss = 0.06708456\n",
      "Iteration 6622, loss = 0.06706202\n",
      "Iteration 6623, loss = 0.06703949\n",
      "Iteration 6624, loss = 0.06701697\n",
      "Iteration 6625, loss = 0.06699447\n",
      "Iteration 6626, loss = 0.06697198\n",
      "Iteration 6627, loss = 0.06694951\n",
      "Iteration 6628, loss = 0.06692705\n",
      "Iteration 6629, loss = 0.06690460\n",
      "Iteration 6630, loss = 0.06688216\n",
      "Iteration 6631, loss = 0.06685974\n",
      "Iteration 6632, loss = 0.06683733\n",
      "Iteration 6633, loss = 0.06681493\n",
      "Iteration 6634, loss = 0.06679255\n",
      "Iteration 6635, loss = 0.06677017\n",
      "Iteration 6636, loss = 0.06674782\n",
      "Iteration 6637, loss = 0.06672547\n",
      "Iteration 6638, loss = 0.06670314\n",
      "Iteration 6639, loss = 0.06668082\n",
      "Iteration 6640, loss = 0.06665851\n",
      "Iteration 6641, loss = 0.06663622\n",
      "Iteration 6642, loss = 0.06661394\n",
      "Iteration 6643, loss = 0.06659167\n",
      "Iteration 6644, loss = 0.06656942\n",
      "Iteration 6645, loss = 0.06654717\n",
      "Iteration 6646, loss = 0.06652494\n",
      "Iteration 6647, loss = 0.06650273\n",
      "Iteration 6648, loss = 0.06648052\n",
      "Iteration 6649, loss = 0.06645833\n",
      "Iteration 6650, loss = 0.06643616\n",
      "Iteration 6651, loss = 0.06641399\n",
      "Iteration 6652, loss = 0.06639184\n",
      "Iteration 6653, loss = 0.06636970\n",
      "Iteration 6654, loss = 0.06634757\n",
      "Iteration 6655, loss = 0.06632546\n",
      "Iteration 6656, loss = 0.06630336\n",
      "Iteration 6657, loss = 0.06628127\n",
      "Iteration 6658, loss = 0.06625919\n",
      "Iteration 6659, loss = 0.06623713\n",
      "Iteration 6660, loss = 0.06621508\n",
      "Iteration 6661, loss = 0.06619304\n",
      "Iteration 6662, loss = 0.06617102\n",
      "Iteration 6663, loss = 0.06614901\n",
      "Iteration 6664, loss = 0.06612701\n",
      "Iteration 6665, loss = 0.06610502\n",
      "Iteration 6666, loss = 0.06608305\n",
      "Iteration 6667, loss = 0.06606109\n",
      "Iteration 6668, loss = 0.06603914\n",
      "Iteration 6669, loss = 0.06601720\n",
      "Iteration 6670, loss = 0.06599528\n",
      "Iteration 6671, loss = 0.06597337\n",
      "Iteration 6672, loss = 0.06595147\n",
      "Iteration 6673, loss = 0.06592959\n",
      "Iteration 6674, loss = 0.06590771\n",
      "Iteration 6675, loss = 0.06588585\n",
      "Iteration 6676, loss = 0.06586400\n",
      "Iteration 6677, loss = 0.06584217\n",
      "Iteration 6678, loss = 0.06582035\n",
      "Iteration 6679, loss = 0.06579854\n",
      "Iteration 6680, loss = 0.06577674\n",
      "Iteration 6681, loss = 0.06575495\n",
      "Iteration 6682, loss = 0.06573318\n",
      "Iteration 6683, loss = 0.06571142\n",
      "Iteration 6684, loss = 0.06568967\n",
      "Iteration 6685, loss = 0.06566794\n",
      "Iteration 6686, loss = 0.06564622\n",
      "Iteration 6687, loss = 0.06562451\n",
      "Iteration 6688, loss = 0.06560281\n",
      "Iteration 6689, loss = 0.06558112\n",
      "Iteration 6690, loss = 0.06555945\n",
      "Iteration 6691, loss = 0.06553779\n",
      "Iteration 6692, loss = 0.06551614\n",
      "Iteration 6693, loss = 0.06549451\n",
      "Iteration 6694, loss = 0.06547288\n",
      "Iteration 6695, loss = 0.06545127\n",
      "Iteration 6696, loss = 0.06542968\n",
      "Iteration 6697, loss = 0.06540809\n",
      "Iteration 6698, loss = 0.06538652\n",
      "Iteration 6699, loss = 0.06536495\n",
      "Iteration 6700, loss = 0.06534341\n",
      "Iteration 6701, loss = 0.06532187\n",
      "Iteration 6702, loss = 0.06530034\n",
      "Iteration 6703, loss = 0.06527883\n",
      "Iteration 6704, loss = 0.06525733\n",
      "Iteration 6705, loss = 0.06523585\n",
      "Iteration 6706, loss = 0.06521437\n",
      "Iteration 6707, loss = 0.06519291\n",
      "Iteration 6708, loss = 0.06517146\n",
      "Iteration 6709, loss = 0.06515002\n",
      "Iteration 6710, loss = 0.06512859\n",
      "Iteration 6711, loss = 0.06510718\n",
      "Iteration 6712, loss = 0.06508578\n",
      "Iteration 6713, loss = 0.06506439\n",
      "Iteration 6714, loss = 0.06504301\n",
      "Iteration 6715, loss = 0.06502165\n",
      "Iteration 6716, loss = 0.06500029\n",
      "Iteration 6717, loss = 0.06497895\n",
      "Iteration 6718, loss = 0.06495762\n",
      "Iteration 6719, loss = 0.06493631\n",
      "Iteration 6720, loss = 0.06491500\n",
      "Iteration 6721, loss = 0.06489371\n",
      "Iteration 6722, loss = 0.06487243\n",
      "Iteration 6723, loss = 0.06485116\n",
      "Iteration 6724, loss = 0.06482991\n",
      "Iteration 6725, loss = 0.06480867\n",
      "Iteration 6726, loss = 0.06478743\n",
      "Iteration 6727, loss = 0.06476621\n",
      "Iteration 6728, loss = 0.06474501\n",
      "Iteration 6729, loss = 0.06472381\n",
      "Iteration 6730, loss = 0.06470263\n",
      "Iteration 6731, loss = 0.06468146\n",
      "Iteration 6732, loss = 0.06466030\n",
      "Iteration 6733, loss = 0.06463915\n",
      "Iteration 6734, loss = 0.06461802\n",
      "Iteration 6735, loss = 0.06459689\n",
      "Iteration 6736, loss = 0.06457578\n",
      "Iteration 6737, loss = 0.06455468\n",
      "Iteration 6738, loss = 0.06453360\n",
      "Iteration 6739, loss = 0.06451252\n",
      "Iteration 6740, loss = 0.06449146\n",
      "Iteration 6741, loss = 0.06447041\n",
      "Iteration 6742, loss = 0.06444937\n",
      "Iteration 6743, loss = 0.06442834\n",
      "Iteration 6744, loss = 0.06440733\n",
      "Iteration 6745, loss = 0.06438632\n",
      "Iteration 6746, loss = 0.06436533\n",
      "Iteration 6747, loss = 0.06434435\n",
      "Iteration 6748, loss = 0.06432338\n",
      "Iteration 6749, loss = 0.06430243\n",
      "Iteration 6750, loss = 0.06428148\n",
      "Iteration 6751, loss = 0.06426055\n",
      "Iteration 6752, loss = 0.06423963\n",
      "Iteration 6753, loss = 0.06421872\n",
      "Iteration 6754, loss = 0.06419783\n",
      "Iteration 6755, loss = 0.06417694\n",
      "Iteration 6756, loss = 0.06415607\n",
      "Iteration 6757, loss = 0.06413521\n",
      "Iteration 6758, loss = 0.06411436\n",
      "Iteration 6759, loss = 0.06409352\n",
      "Iteration 6760, loss = 0.06407269\n",
      "Iteration 6761, loss = 0.06405188\n",
      "Iteration 6762, loss = 0.06403108\n",
      "Iteration 6763, loss = 0.06401029\n",
      "Iteration 6764, loss = 0.06398951\n",
      "Iteration 6765, loss = 0.06396874\n",
      "Iteration 6766, loss = 0.06394799\n",
      "Iteration 6767, loss = 0.06392724\n",
      "Iteration 6768, loss = 0.06390651\n",
      "Iteration 6769, loss = 0.06388579\n",
      "Iteration 6770, loss = 0.06386508\n",
      "Iteration 6771, loss = 0.06384439\n",
      "Iteration 6772, loss = 0.06382370\n",
      "Iteration 6773, loss = 0.06380303\n",
      "Iteration 6774, loss = 0.06378237\n",
      "Iteration 6775, loss = 0.06376172\n",
      "Iteration 6776, loss = 0.06374108\n",
      "Iteration 6777, loss = 0.06372045\n",
      "Iteration 6778, loss = 0.06369984\n",
      "Iteration 6779, loss = 0.06367923\n",
      "Iteration 6780, loss = 0.06365864\n",
      "Iteration 6781, loss = 0.06363806\n",
      "Iteration 6782, loss = 0.06361749\n",
      "Iteration 6783, loss = 0.06359693\n",
      "Iteration 6784, loss = 0.06357639\n",
      "Iteration 6785, loss = 0.06355585\n",
      "Iteration 6786, loss = 0.06353533\n",
      "Iteration 6787, loss = 0.06351482\n",
      "Iteration 6788, loss = 0.06349432\n",
      "Iteration 6789, loss = 0.06347383\n",
      "Iteration 6790, loss = 0.06345336\n",
      "Iteration 6791, loss = 0.06343289\n",
      "Iteration 6792, loss = 0.06341244\n",
      "Iteration 6793, loss = 0.06339199\n",
      "Iteration 6794, loss = 0.06337156\n",
      "Iteration 6795, loss = 0.06335115\n",
      "Iteration 6796, loss = 0.06333074\n",
      "Iteration 6797, loss = 0.06331034\n",
      "Iteration 6798, loss = 0.06328996\n",
      "Iteration 6799, loss = 0.06326958\n",
      "Iteration 6800, loss = 0.06324922\n",
      "Iteration 6801, loss = 0.06322887\n",
      "Iteration 6802, loss = 0.06320853\n",
      "Iteration 6803, loss = 0.06318820\n",
      "Iteration 6804, loss = 0.06316789\n",
      "Iteration 6805, loss = 0.06314758\n",
      "Iteration 6806, loss = 0.06312729\n",
      "Iteration 6807, loss = 0.06310701\n",
      "Iteration 6808, loss = 0.06308674\n",
      "Iteration 6809, loss = 0.06306648\n",
      "Iteration 6810, loss = 0.06304623\n",
      "Iteration 6811, loss = 0.06302599\n",
      "Iteration 6812, loss = 0.06300577\n",
      "Iteration 6813, loss = 0.06298555\n",
      "Iteration 6814, loss = 0.06296535\n",
      "Iteration 6815, loss = 0.06294516\n",
      "Iteration 6816, loss = 0.06292498\n",
      "Iteration 6817, loss = 0.06290481\n",
      "Iteration 6818, loss = 0.06288465\n",
      "Iteration 6819, loss = 0.06286451\n",
      "Iteration 6820, loss = 0.06284437\n",
      "Iteration 6821, loss = 0.06282425\n",
      "Iteration 6822, loss = 0.06280413\n",
      "Iteration 6823, loss = 0.06278403\n",
      "Iteration 6824, loss = 0.06276394\n",
      "Iteration 6825, loss = 0.06274386\n",
      "Iteration 6826, loss = 0.06272380\n",
      "Iteration 6827, loss = 0.06270374\n",
      "Iteration 6828, loss = 0.06268369\n",
      "Iteration 6829, loss = 0.06266366\n",
      "Iteration 6830, loss = 0.06264364\n",
      "Iteration 6831, loss = 0.06262362\n",
      "Iteration 6832, loss = 0.06260362\n",
      "Iteration 6833, loss = 0.06258363\n",
      "Iteration 6834, loss = 0.06256365\n",
      "Iteration 6835, loss = 0.06254369\n",
      "Iteration 6836, loss = 0.06252373\n",
      "Iteration 6837, loss = 0.06250379\n",
      "Iteration 6838, loss = 0.06248385\n",
      "Iteration 6839, loss = 0.06246393\n",
      "Iteration 6840, loss = 0.06244402\n",
      "Iteration 6841, loss = 0.06242412\n",
      "Iteration 6842, loss = 0.06240423\n",
      "Iteration 6843, loss = 0.06238435\n",
      "Iteration 6844, loss = 0.06236448\n",
      "Iteration 6845, loss = 0.06234462\n",
      "Iteration 6846, loss = 0.06232478\n",
      "Iteration 6847, loss = 0.06230494\n",
      "Iteration 6848, loss = 0.06228512\n",
      "Iteration 6849, loss = 0.06226531\n",
      "Iteration 6850, loss = 0.06224551\n",
      "Iteration 6851, loss = 0.06222572\n",
      "Iteration 6852, loss = 0.06220594\n",
      "Iteration 6853, loss = 0.06218617\n",
      "Iteration 6854, loss = 0.06216641\n",
      "Iteration 6855, loss = 0.06214666\n",
      "Iteration 6856, loss = 0.06212693\n",
      "Iteration 6857, loss = 0.06210720\n",
      "Iteration 6858, loss = 0.06208749\n",
      "Iteration 6859, loss = 0.06206779\n",
      "Iteration 6860, loss = 0.06204809\n",
      "Iteration 6861, loss = 0.06202841\n",
      "Iteration 6862, loss = 0.06200874\n",
      "Iteration 6863, loss = 0.06198908\n",
      "Iteration 6864, loss = 0.06196944\n",
      "Iteration 6865, loss = 0.06194980\n",
      "Iteration 6866, loss = 0.06193017\n",
      "Iteration 6867, loss = 0.06191056\n",
      "Iteration 6868, loss = 0.06189095\n",
      "Iteration 6869, loss = 0.06187136\n",
      "Iteration 6870, loss = 0.06185177\n",
      "Iteration 6871, loss = 0.06183220\n",
      "Iteration 6872, loss = 0.06181264\n",
      "Iteration 6873, loss = 0.06179309\n",
      "Iteration 6874, loss = 0.06177355\n",
      "Iteration 6875, loss = 0.06175402\n",
      "Iteration 6876, loss = 0.06173450\n",
      "Iteration 6877, loss = 0.06171499\n",
      "Iteration 6878, loss = 0.06169550\n",
      "Iteration 6879, loss = 0.06167601\n",
      "Iteration 6880, loss = 0.06165654\n",
      "Iteration 6881, loss = 0.06163707\n",
      "Iteration 6882, loss = 0.06161762\n",
      "Iteration 6883, loss = 0.06159817\n",
      "Iteration 6884, loss = 0.06157874\n",
      "Iteration 6885, loss = 0.06155932\n",
      "Iteration 6886, loss = 0.06153991\n",
      "Iteration 6887, loss = 0.06152051\n",
      "Iteration 6888, loss = 0.06150112\n",
      "Iteration 6889, loss = 0.06148174\n",
      "Iteration 6890, loss = 0.06146237\n",
      "Iteration 6891, loss = 0.06144301\n",
      "Iteration 6892, loss = 0.06142367\n",
      "Iteration 6893, loss = 0.06140433\n",
      "Iteration 6894, loss = 0.06138501\n",
      "Iteration 6895, loss = 0.06136569\n",
      "Iteration 6896, loss = 0.06134639\n",
      "Iteration 6897, loss = 0.06132709\n",
      "Iteration 6898, loss = 0.06130781\n",
      "Iteration 6899, loss = 0.06128854\n",
      "Iteration 6900, loss = 0.06126928\n",
      "Iteration 6901, loss = 0.06125002\n",
      "Iteration 6902, loss = 0.06123078\n",
      "Iteration 6903, loss = 0.06121155\n",
      "Iteration 6904, loss = 0.06119233\n",
      "Iteration 6905, loss = 0.06117312\n",
      "Iteration 6906, loss = 0.06115393\n",
      "Iteration 6907, loss = 0.06113474\n",
      "Iteration 6908, loss = 0.06111556\n",
      "Iteration 6909, loss = 0.06109639\n",
      "Iteration 6910, loss = 0.06107724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6911, loss = 0.06105809\n",
      "Iteration 6912, loss = 0.06103896\n",
      "Iteration 6913, loss = 0.06101983\n",
      "Iteration 6914, loss = 0.06100072\n",
      "Iteration 6915, loss = 0.06098161\n",
      "Iteration 6916, loss = 0.06096252\n",
      "Iteration 6917, loss = 0.06094344\n",
      "Iteration 6918, loss = 0.06092436\n",
      "Iteration 6919, loss = 0.06090530\n",
      "Iteration 6920, loss = 0.06088625\n",
      "Iteration 6921, loss = 0.06086721\n",
      "Iteration 6922, loss = 0.06084818\n",
      "Iteration 6923, loss = 0.06082916\n",
      "Iteration 6924, loss = 0.06081015\n",
      "Iteration 6925, loss = 0.06079115\n",
      "Iteration 6926, loss = 0.06077216\n",
      "Iteration 6927, loss = 0.06075318\n",
      "Iteration 6928, loss = 0.06073421\n",
      "Iteration 6929, loss = 0.06071525\n",
      "Iteration 6930, loss = 0.06069631\n",
      "Iteration 6931, loss = 0.06067737\n",
      "Iteration 6932, loss = 0.06065844\n",
      "Iteration 6933, loss = 0.06063953\n",
      "Iteration 6934, loss = 0.06062062\n",
      "Iteration 6935, loss = 0.06060172\n",
      "Iteration 6936, loss = 0.06058284\n",
      "Iteration 6937, loss = 0.06056396\n",
      "Iteration 6938, loss = 0.06054510\n",
      "Iteration 6939, loss = 0.06052624\n",
      "Iteration 6940, loss = 0.06050740\n",
      "Iteration 6941, loss = 0.06048856\n",
      "Iteration 6942, loss = 0.06046974\n",
      "Iteration 6943, loss = 0.06045093\n",
      "Iteration 6944, loss = 0.06043212\n",
      "Iteration 6945, loss = 0.06041333\n",
      "Iteration 6946, loss = 0.06039455\n",
      "Iteration 6947, loss = 0.06037577\n",
      "Iteration 6948, loss = 0.06035701\n",
      "Iteration 6949, loss = 0.06033826\n",
      "Iteration 6950, loss = 0.06031952\n",
      "Iteration 6951, loss = 0.06030078\n",
      "Iteration 6952, loss = 0.06028206\n",
      "Iteration 6953, loss = 0.06026335\n",
      "Iteration 6954, loss = 0.06024465\n",
      "Iteration 6955, loss = 0.06022596\n",
      "Iteration 6956, loss = 0.06020728\n",
      "Iteration 6957, loss = 0.06018861\n",
      "Iteration 6958, loss = 0.06016995\n",
      "Iteration 6959, loss = 0.06015130\n",
      "Iteration 6960, loss = 0.06013266\n",
      "Iteration 6961, loss = 0.06011403\n",
      "Iteration 6962, loss = 0.06009541\n",
      "Iteration 6963, loss = 0.06007680\n",
      "Iteration 6964, loss = 0.06005820\n",
      "Iteration 6965, loss = 0.06003961\n",
      "Iteration 6966, loss = 0.06002103\n",
      "Iteration 6967, loss = 0.06000246\n",
      "Iteration 6968, loss = 0.05998390\n",
      "Iteration 6969, loss = 0.05996535\n",
      "Iteration 6970, loss = 0.05994681\n",
      "Iteration 6971, loss = 0.05992828\n",
      "Iteration 6972, loss = 0.05990976\n",
      "Iteration 6973, loss = 0.05989125\n",
      "Iteration 6974, loss = 0.05987275\n",
      "Iteration 6975, loss = 0.05985426\n",
      "Iteration 6976, loss = 0.05983578\n",
      "Iteration 6977, loss = 0.05981731\n",
      "Iteration 6978, loss = 0.05979885\n",
      "Iteration 6979, loss = 0.05978040\n",
      "Iteration 6980, loss = 0.05976196\n",
      "Iteration 6981, loss = 0.05974353\n",
      "Iteration 6982, loss = 0.05972512\n",
      "Iteration 6983, loss = 0.05970671\n",
      "Iteration 6984, loss = 0.05968831\n",
      "Iteration 6985, loss = 0.05966992\n",
      "Iteration 6986, loss = 0.05965154\n",
      "Iteration 6987, loss = 0.05963317\n",
      "Iteration 6988, loss = 0.05961481\n",
      "Iteration 6989, loss = 0.05959646\n",
      "Iteration 6990, loss = 0.05957812\n",
      "Iteration 6991, loss = 0.05955979\n",
      "Iteration 6992, loss = 0.05954147\n",
      "Iteration 6993, loss = 0.05952316\n",
      "Iteration 6994, loss = 0.05950486\n",
      "Iteration 6995, loss = 0.05948657\n",
      "Iteration 6996, loss = 0.05946829\n",
      "Iteration 6997, loss = 0.05945002\n",
      "Iteration 6998, loss = 0.05943176\n",
      "Iteration 6999, loss = 0.05941350\n",
      "Iteration 7000, loss = 0.05939526\n",
      "Iteration 7001, loss = 0.05937703\n",
      "Iteration 7002, loss = 0.05935881\n",
      "Iteration 7003, loss = 0.05934060\n",
      "Iteration 7004, loss = 0.05932240\n",
      "Iteration 7005, loss = 0.05930421\n",
      "Iteration 7006, loss = 0.05928602\n",
      "Iteration 7007, loss = 0.05926785\n",
      "Iteration 7008, loss = 0.05924969\n",
      "Iteration 7009, loss = 0.05923153\n",
      "Iteration 7010, loss = 0.05921339\n",
      "Iteration 7011, loss = 0.05919526\n",
      "Iteration 7012, loss = 0.05917713\n",
      "Iteration 7013, loss = 0.05915902\n",
      "Iteration 7014, loss = 0.05914092\n",
      "Iteration 7015, loss = 0.05912282\n",
      "Iteration 7016, loss = 0.05910474\n",
      "Iteration 7017, loss = 0.05908666\n",
      "Iteration 7018, loss = 0.05906860\n",
      "Iteration 7019, loss = 0.05905054\n",
      "Iteration 7020, loss = 0.05903250\n",
      "Iteration 7021, loss = 0.05901446\n",
      "Iteration 7022, loss = 0.05899643\n",
      "Iteration 7023, loss = 0.05897842\n",
      "Iteration 7024, loss = 0.05896041\n",
      "Iteration 7025, loss = 0.05894241\n",
      "Iteration 7026, loss = 0.05892442\n",
      "Iteration 7027, loss = 0.05890645\n",
      "Iteration 7028, loss = 0.05888848\n",
      "Iteration 7029, loss = 0.05887052\n",
      "Iteration 7030, loss = 0.05885257\n",
      "Iteration 7031, loss = 0.05883463\n",
      "Iteration 7032, loss = 0.05881670\n",
      "Iteration 7033, loss = 0.05879878\n",
      "Iteration 7034, loss = 0.05878087\n",
      "Iteration 7035, loss = 0.05876297\n",
      "Iteration 7036, loss = 0.05874507\n",
      "Iteration 7037, loss = 0.05872719\n",
      "Iteration 7038, loss = 0.05870932\n",
      "Iteration 7039, loss = 0.05869146\n",
      "Iteration 7040, loss = 0.05867360\n",
      "Iteration 7041, loss = 0.05865576\n",
      "Iteration 7042, loss = 0.05863792\n",
      "Iteration 7043, loss = 0.05862010\n",
      "Iteration 7044, loss = 0.05860228\n",
      "Iteration 7045, loss = 0.05858448\n",
      "Iteration 7046, loss = 0.05856668\n",
      "Iteration 7047, loss = 0.05854890\n",
      "Iteration 7048, loss = 0.05853112\n",
      "Iteration 7049, loss = 0.05851335\n",
      "Iteration 7050, loss = 0.05849559\n",
      "Iteration 7051, loss = 0.05847784\n",
      "Iteration 7052, loss = 0.05846010\n",
      "Iteration 7053, loss = 0.05844237\n",
      "Iteration 7054, loss = 0.05842465\n",
      "Iteration 7055, loss = 0.05840694\n",
      "Iteration 7056, loss = 0.05838924\n",
      "Iteration 7057, loss = 0.05837155\n",
      "Iteration 7058, loss = 0.05835387\n",
      "Iteration 7059, loss = 0.05833619\n",
      "Iteration 7060, loss = 0.05831853\n",
      "Iteration 7061, loss = 0.05830088\n",
      "Iteration 7062, loss = 0.05828323\n",
      "Iteration 7063, loss = 0.05826559\n",
      "Iteration 7064, loss = 0.05824797\n",
      "Iteration 7065, loss = 0.05823035\n",
      "Iteration 7066, loss = 0.05821274\n",
      "Iteration 7067, loss = 0.05819515\n",
      "Iteration 7068, loss = 0.05817756\n",
      "Iteration 7069, loss = 0.05815998\n",
      "Iteration 7070, loss = 0.05814241\n",
      "Iteration 7071, loss = 0.05812485\n",
      "Iteration 7072, loss = 0.05810730\n",
      "Iteration 7073, loss = 0.05808975\n",
      "Iteration 7074, loss = 0.05807222\n",
      "Iteration 7075, loss = 0.05805470\n",
      "Iteration 7076, loss = 0.05803718\n",
      "Iteration 7077, loss = 0.05801968\n",
      "Iteration 7078, loss = 0.05800218\n",
      "Iteration 7079, loss = 0.05798470\n",
      "Iteration 7080, loss = 0.05796722\n",
      "Iteration 7081, loss = 0.05794975\n",
      "Iteration 7082, loss = 0.05793229\n",
      "Iteration 7083, loss = 0.05791485\n",
      "Iteration 7084, loss = 0.05789741\n",
      "Iteration 7085, loss = 0.05787997\n",
      "Iteration 7086, loss = 0.05786255\n",
      "Iteration 7087, loss = 0.05784514\n",
      "Iteration 7088, loss = 0.05782774\n",
      "Iteration 7089, loss = 0.05781034\n",
      "Iteration 7090, loss = 0.05779296\n",
      "Iteration 7091, loss = 0.05777559\n",
      "Iteration 7092, loss = 0.05775822\n",
      "Iteration 7093, loss = 0.05774086\n",
      "Iteration 7094, loss = 0.05772351\n",
      "Iteration 7095, loss = 0.05770618\n",
      "Iteration 7096, loss = 0.05768885\n",
      "Iteration 7097, loss = 0.05767153\n",
      "Iteration 7098, loss = 0.05765422\n",
      "Iteration 7099, loss = 0.05763691\n",
      "Iteration 7100, loss = 0.05761962\n",
      "Iteration 7101, loss = 0.05760234\n",
      "Iteration 7102, loss = 0.05758506\n",
      "Iteration 7103, loss = 0.05756780\n",
      "Iteration 7104, loss = 0.05755054\n",
      "Iteration 7105, loss = 0.05753329\n",
      "Iteration 7106, loss = 0.05751606\n",
      "Iteration 7107, loss = 0.05749883\n",
      "Iteration 7108, loss = 0.05748161\n",
      "Iteration 7109, loss = 0.05746440\n",
      "Iteration 7110, loss = 0.05744720\n",
      "Iteration 7111, loss = 0.05743000\n",
      "Iteration 7112, loss = 0.05741282\n",
      "Iteration 7113, loss = 0.05739565\n",
      "Iteration 7114, loss = 0.05737848\n",
      "Iteration 7115, loss = 0.05736132\n",
      "Iteration 7116, loss = 0.05734418\n",
      "Iteration 7117, loss = 0.05732704\n",
      "Iteration 7118, loss = 0.05730991\n",
      "Iteration 7119, loss = 0.05729279\n",
      "Iteration 7120, loss = 0.05727568\n",
      "Iteration 7121, loss = 0.05725858\n",
      "Iteration 7122, loss = 0.05724148\n",
      "Iteration 7123, loss = 0.05722440\n",
      "Iteration 7124, loss = 0.05720732\n",
      "Iteration 7125, loss = 0.05719026\n",
      "Iteration 7126, loss = 0.05717320\n",
      "Iteration 7127, loss = 0.05715615\n",
      "Iteration 7128, loss = 0.05713911\n",
      "Iteration 7129, loss = 0.05712208\n",
      "Iteration 7130, loss = 0.05710506\n",
      "Iteration 7131, loss = 0.05708805\n",
      "Iteration 7132, loss = 0.05707105\n",
      "Iteration 7133, loss = 0.05705405\n",
      "Iteration 7134, loss = 0.05703707\n",
      "Iteration 7135, loss = 0.05702009\n",
      "Iteration 7136, loss = 0.05700312\n",
      "Iteration 7137, loss = 0.05698616\n",
      "Iteration 7138, loss = 0.05696921\n",
      "Iteration 7139, loss = 0.05695227\n",
      "Iteration 7140, loss = 0.05693534\n",
      "Iteration 7141, loss = 0.05691842\n",
      "Iteration 7142, loss = 0.05690150\n",
      "Iteration 7143, loss = 0.05688460\n",
      "Iteration 7144, loss = 0.05686770\n",
      "Iteration 7145, loss = 0.05685081\n",
      "Iteration 7146, loss = 0.05683394\n",
      "Iteration 7147, loss = 0.05681707\n",
      "Iteration 7148, loss = 0.05680020\n",
      "Iteration 7149, loss = 0.05678335\n",
      "Iteration 7150, loss = 0.05676651\n",
      "Iteration 7151, loss = 0.05674967\n",
      "Iteration 7152, loss = 0.05673285\n",
      "Iteration 7153, loss = 0.05671603\n",
      "Iteration 7154, loss = 0.05669922\n",
      "Iteration 7155, loss = 0.05668242\n",
      "Iteration 7156, loss = 0.05666563\n",
      "Iteration 7157, loss = 0.05664885\n",
      "Iteration 7158, loss = 0.05663208\n",
      "Iteration 7159, loss = 0.05661531\n",
      "Iteration 7160, loss = 0.05659856\n",
      "Iteration 7161, loss = 0.05658181\n",
      "Iteration 7162, loss = 0.05656507\n",
      "Iteration 7163, loss = 0.05654834\n",
      "Iteration 7164, loss = 0.05653162\n",
      "Iteration 7165, loss = 0.05651491\n",
      "Iteration 7166, loss = 0.05649821\n",
      "Iteration 7167, loss = 0.05648151\n",
      "Iteration 7168, loss = 0.05646483\n",
      "Iteration 7169, loss = 0.05644815\n",
      "Iteration 7170, loss = 0.05643148\n",
      "Iteration 7171, loss = 0.05641482\n",
      "Iteration 7172, loss = 0.05639817\n",
      "Iteration 7173, loss = 0.05638153\n",
      "Iteration 7174, loss = 0.05636490\n",
      "Iteration 7175, loss = 0.05634827\n",
      "Iteration 7176, loss = 0.05633165\n",
      "Iteration 7177, loss = 0.05631505\n",
      "Iteration 7178, loss = 0.05629845\n",
      "Iteration 7179, loss = 0.05628186\n",
      "Iteration 7180, loss = 0.05626528\n",
      "Iteration 7181, loss = 0.05624870\n",
      "Iteration 7182, loss = 0.05623214\n",
      "Iteration 7183, loss = 0.05621558\n",
      "Iteration 7184, loss = 0.05619904\n",
      "Iteration 7185, loss = 0.05618250\n",
      "Iteration 7186, loss = 0.05616597\n",
      "Iteration 7187, loss = 0.05614945\n",
      "Iteration 7188, loss = 0.05613293\n",
      "Iteration 7189, loss = 0.05611643\n",
      "Iteration 7190, loss = 0.05609993\n",
      "Iteration 7191, loss = 0.05608345\n",
      "Iteration 7192, loss = 0.05606697\n",
      "Iteration 7193, loss = 0.05605050\n",
      "Iteration 7194, loss = 0.05603404\n",
      "Iteration 7195, loss = 0.05601758\n",
      "Iteration 7196, loss = 0.05600114\n",
      "Iteration 7197, loss = 0.05598470\n",
      "Iteration 7198, loss = 0.05596828\n",
      "Iteration 7199, loss = 0.05595186\n",
      "Iteration 7200, loss = 0.05593545\n",
      "Iteration 7201, loss = 0.05591905\n",
      "Iteration 7202, loss = 0.05590265\n",
      "Iteration 7203, loss = 0.05588627\n",
      "Iteration 7204, loss = 0.05586989\n",
      "Iteration 7205, loss = 0.05585352\n",
      "Iteration 7206, loss = 0.05583717\n",
      "Iteration 7207, loss = 0.05582081\n",
      "Iteration 7208, loss = 0.05580447\n",
      "Iteration 7209, loss = 0.05578814\n",
      "Iteration 7210, loss = 0.05577181\n",
      "Iteration 7211, loss = 0.05575550\n",
      "Iteration 7212, loss = 0.05573919\n",
      "Iteration 7213, loss = 0.05572289\n",
      "Iteration 7214, loss = 0.05570660\n",
      "Iteration 7215, loss = 0.05569031\n",
      "Iteration 7216, loss = 0.05567404\n",
      "Iteration 7217, loss = 0.05565777\n",
      "Iteration 7218, loss = 0.05564151\n",
      "Iteration 7219, loss = 0.05562527\n",
      "Iteration 7220, loss = 0.05560902\n",
      "Iteration 7221, loss = 0.05559279\n",
      "Iteration 7222, loss = 0.05557657\n",
      "Iteration 7223, loss = 0.05556035\n",
      "Iteration 7224, loss = 0.05554414\n",
      "Iteration 7225, loss = 0.05552794\n",
      "Iteration 7226, loss = 0.05551175\n",
      "Iteration 7227, loss = 0.05549557\n",
      "Iteration 7228, loss = 0.05547940\n",
      "Iteration 7229, loss = 0.05546323\n",
      "Iteration 7230, loss = 0.05544707\n",
      "Iteration 7231, loss = 0.05543093\n",
      "Iteration 7232, loss = 0.05541478\n",
      "Iteration 7233, loss = 0.05539865\n",
      "Iteration 7234, loss = 0.05538253\n",
      "Iteration 7235, loss = 0.05536641\n",
      "Iteration 7236, loss = 0.05535030\n",
      "Iteration 7237, loss = 0.05533421\n",
      "Iteration 7238, loss = 0.05531811\n",
      "Iteration 7239, loss = 0.05530203\n",
      "Iteration 7240, loss = 0.05528596\n",
      "Iteration 7241, loss = 0.05526989\n",
      "Iteration 7242, loss = 0.05525383\n",
      "Iteration 7243, loss = 0.05523778\n",
      "Iteration 7244, loss = 0.05522174\n",
      "Iteration 7245, loss = 0.05520571\n",
      "Iteration 7246, loss = 0.05518969\n",
      "Iteration 7247, loss = 0.05517367\n",
      "Iteration 7248, loss = 0.05515766\n",
      "Iteration 7249, loss = 0.05514166\n",
      "Iteration 7250, loss = 0.05512567\n",
      "Iteration 7251, loss = 0.05510969\n",
      "Iteration 7252, loss = 0.05509371\n",
      "Iteration 7253, loss = 0.05507774\n",
      "Iteration 7254, loss = 0.05506178\n",
      "Iteration 7255, loss = 0.05504583\n",
      "Iteration 7256, loss = 0.05502989\n",
      "Iteration 7257, loss = 0.05501396\n",
      "Iteration 7258, loss = 0.05499803\n",
      "Iteration 7259, loss = 0.05498211\n",
      "Iteration 7260, loss = 0.05496620\n",
      "Iteration 7261, loss = 0.05495030\n",
      "Iteration 7262, loss = 0.05493441\n",
      "Iteration 7263, loss = 0.05491852\n",
      "Iteration 7264, loss = 0.05490264\n",
      "Iteration 7265, loss = 0.05488677\n",
      "Iteration 7266, loss = 0.05487091\n",
      "Iteration 7267, loss = 0.05485506\n",
      "Iteration 7268, loss = 0.05483922\n",
      "Iteration 7269, loss = 0.05482338\n",
      "Iteration 7270, loss = 0.05480755\n",
      "Iteration 7271, loss = 0.05479173\n",
      "Iteration 7272, loss = 0.05477592\n",
      "Iteration 7273, loss = 0.05476011\n",
      "Iteration 7274, loss = 0.05474432\n",
      "Iteration 7275, loss = 0.05472853\n",
      "Iteration 7276, loss = 0.05471275\n",
      "Iteration 7277, loss = 0.05469698\n",
      "Iteration 7278, loss = 0.05468121\n",
      "Iteration 7279, loss = 0.05466546\n",
      "Iteration 7280, loss = 0.05464971\n",
      "Iteration 7281, loss = 0.05463397\n",
      "Iteration 7282, loss = 0.05461824\n",
      "Iteration 7283, loss = 0.05460251\n",
      "Iteration 7284, loss = 0.05458680\n",
      "Iteration 7285, loss = 0.05457109\n",
      "Iteration 7286, loss = 0.05455539\n",
      "Iteration 7287, loss = 0.05453970\n",
      "Iteration 7288, loss = 0.05452402\n",
      "Iteration 7289, loss = 0.05450834\n",
      "Iteration 7290, loss = 0.05449267\n",
      "Iteration 7291, loss = 0.05447701\n",
      "Iteration 7292, loss = 0.05446136\n",
      "Iteration 7293, loss = 0.05444572\n",
      "Iteration 7294, loss = 0.05443008\n",
      "Iteration 7295, loss = 0.05441445\n",
      "Iteration 7296, loss = 0.05439883\n",
      "Iteration 7297, loss = 0.05438322\n",
      "Iteration 7298, loss = 0.05436762\n",
      "Iteration 7299, loss = 0.05435202\n",
      "Iteration 7300, loss = 0.05433644\n",
      "Iteration 7301, loss = 0.05432086\n",
      "Iteration 7302, loss = 0.05430528\n",
      "Iteration 7303, loss = 0.05428972\n",
      "Iteration 7304, loss = 0.05427416\n",
      "Iteration 7305, loss = 0.05425861\n",
      "Iteration 7306, loss = 0.05424307\n",
      "Iteration 7307, loss = 0.05422754\n",
      "Iteration 7308, loss = 0.05421202\n",
      "Iteration 7309, loss = 0.05419650\n",
      "Iteration 7310, loss = 0.05418099\n",
      "Iteration 7311, loss = 0.05416549\n",
      "Iteration 7312, loss = 0.05415000\n",
      "Iteration 7313, loss = 0.05413451\n",
      "Iteration 7314, loss = 0.05411904\n",
      "Iteration 7315, loss = 0.05410357\n",
      "Iteration 7316, loss = 0.05408810\n",
      "Iteration 7317, loss = 0.05407265\n",
      "Iteration 7318, loss = 0.05405720\n",
      "Iteration 7319, loss = 0.05404177\n",
      "Iteration 7320, loss = 0.05402634\n",
      "Iteration 7321, loss = 0.05401091\n",
      "Iteration 7322, loss = 0.05399550\n",
      "Iteration 7323, loss = 0.05398009\n",
      "Iteration 7324, loss = 0.05396469\n",
      "Iteration 7325, loss = 0.05394930\n",
      "Iteration 7326, loss = 0.05393392\n",
      "Iteration 7327, loss = 0.05391854\n",
      "Iteration 7328, loss = 0.05390317\n",
      "Iteration 7329, loss = 0.05388781\n",
      "Iteration 7330, loss = 0.05387246\n",
      "Iteration 7331, loss = 0.05385712\n",
      "Iteration 7332, loss = 0.05384178\n",
      "Iteration 7333, loss = 0.05382645\n",
      "Iteration 7334, loss = 0.05381113\n",
      "Iteration 7335, loss = 0.05379582\n",
      "Iteration 7336, loss = 0.05378051\n",
      "Iteration 7337, loss = 0.05376521\n",
      "Iteration 7338, loss = 0.05374992\n",
      "Iteration 7339, loss = 0.05373464\n",
      "Iteration 7340, loss = 0.05371937\n",
      "Iteration 7341, loss = 0.05370410\n",
      "Iteration 7342, loss = 0.05368884\n",
      "Iteration 7343, loss = 0.05367359\n",
      "Iteration 7344, loss = 0.05365834\n",
      "Iteration 7345, loss = 0.05364311\n",
      "Iteration 7346, loss = 0.05362788\n",
      "Iteration 7347, loss = 0.05361266\n",
      "Iteration 7348, loss = 0.05359745\n",
      "Iteration 7349, loss = 0.05358224\n",
      "Iteration 7350, loss = 0.05356704\n",
      "Iteration 7351, loss = 0.05355185\n",
      "Iteration 7352, loss = 0.05353667\n",
      "Iteration 7353, loss = 0.05352149\n",
      "Iteration 7354, loss = 0.05350633\n",
      "Iteration 7355, loss = 0.05349117\n",
      "Iteration 7356, loss = 0.05347602\n",
      "Iteration 7357, loss = 0.05346087\n",
      "Iteration 7358, loss = 0.05344574\n",
      "Iteration 7359, loss = 0.05343061\n",
      "Iteration 7360, loss = 0.05341548\n",
      "Iteration 7361, loss = 0.05340037\n",
      "Iteration 7362, loss = 0.05338526\n",
      "Iteration 7363, loss = 0.05337017\n",
      "Iteration 7364, loss = 0.05335507\n",
      "Iteration 7365, loss = 0.05333999\n",
      "Iteration 7366, loss = 0.05332492\n",
      "Iteration 7367, loss = 0.05330985\n",
      "Iteration 7368, loss = 0.05329479\n",
      "Iteration 7369, loss = 0.05327973\n",
      "Iteration 7370, loss = 0.05326469\n",
      "Iteration 7371, loss = 0.05324965\n",
      "Iteration 7372, loss = 0.05323462\n",
      "Iteration 7373, loss = 0.05321960\n",
      "Iteration 7374, loss = 0.05320458\n",
      "Iteration 7375, loss = 0.05318957\n",
      "Iteration 7376, loss = 0.05317457\n",
      "Iteration 7377, loss = 0.05315958\n",
      "Iteration 7378, loss = 0.05314460\n",
      "Iteration 7379, loss = 0.05312962\n",
      "Iteration 7380, loss = 0.05311465\n",
      "Iteration 7381, loss = 0.05309968\n",
      "Iteration 7382, loss = 0.05308473\n",
      "Iteration 7383, loss = 0.05306978\n",
      "Iteration 7384, loss = 0.05305484\n",
      "Iteration 7385, loss = 0.05303991\n",
      "Iteration 7386, loss = 0.05302498\n",
      "Iteration 7387, loss = 0.05301007\n",
      "Iteration 7388, loss = 0.05299516\n",
      "Iteration 7389, loss = 0.05298025\n",
      "Iteration 7390, loss = 0.05296536\n",
      "Iteration 7391, loss = 0.05295047\n",
      "Iteration 7392, loss = 0.05293559\n",
      "Iteration 7393, loss = 0.05292072\n",
      "Iteration 7394, loss = 0.05290585\n",
      "Iteration 7395, loss = 0.05289099\n",
      "Iteration 7396, loss = 0.05287614\n",
      "Iteration 7397, loss = 0.05286130\n",
      "Iteration 7398, loss = 0.05284646\n",
      "Iteration 7399, loss = 0.05283163\n",
      "Iteration 7400, loss = 0.05281681\n",
      "Iteration 7401, loss = 0.05280200\n",
      "Iteration 7402, loss = 0.05278719\n",
      "Iteration 7403, loss = 0.05277240\n",
      "Iteration 7404, loss = 0.05275760\n",
      "Iteration 7405, loss = 0.05274282\n",
      "Iteration 7406, loss = 0.05272804\n",
      "Iteration 7407, loss = 0.05271327\n",
      "Iteration 7408, loss = 0.05269851\n",
      "Iteration 7409, loss = 0.05268376\n",
      "Iteration 7410, loss = 0.05266901\n",
      "Iteration 7411, loss = 0.05265427\n",
      "Iteration 7412, loss = 0.05263954\n",
      "Iteration 7413, loss = 0.05262481\n",
      "Iteration 7414, loss = 0.05261010\n",
      "Iteration 7415, loss = 0.05259539\n",
      "Iteration 7416, loss = 0.05258068\n",
      "Iteration 7417, loss = 0.05256599\n",
      "Iteration 7418, loss = 0.05255130\n",
      "Iteration 7419, loss = 0.05253662\n",
      "Iteration 7420, loss = 0.05252194\n",
      "Iteration 7421, loss = 0.05250728\n",
      "Iteration 7422, loss = 0.05249262\n",
      "Iteration 7423, loss = 0.05247797\n",
      "Iteration 7424, loss = 0.05246332\n",
      "Iteration 7425, loss = 0.05244868\n",
      "Iteration 7426, loss = 0.05243405\n",
      "Iteration 7427, loss = 0.05241943\n",
      "Iteration 7428, loss = 0.05240482\n",
      "Iteration 7429, loss = 0.05239021\n",
      "Iteration 7430, loss = 0.05237561\n",
      "Iteration 7431, loss = 0.05236101\n",
      "Iteration 7432, loss = 0.05234643\n",
      "Iteration 7433, loss = 0.05233185\n",
      "Iteration 7434, loss = 0.05231727\n",
      "Iteration 7435, loss = 0.05230271\n",
      "Iteration 7436, loss = 0.05228815\n",
      "Iteration 7437, loss = 0.05227360\n",
      "Iteration 7438, loss = 0.05225906\n",
      "Iteration 7439, loss = 0.05224452\n",
      "Iteration 7440, loss = 0.05222999\n",
      "Iteration 7441, loss = 0.05221547\n",
      "Iteration 7442, loss = 0.05220096\n",
      "Iteration 7443, loss = 0.05218645\n",
      "Iteration 7444, loss = 0.05217195\n",
      "Iteration 7445, loss = 0.05215746\n",
      "Iteration 7446, loss = 0.05214297\n",
      "Iteration 7447, loss = 0.05212850\n",
      "Iteration 7448, loss = 0.05211402\n",
      "Iteration 7449, loss = 0.05209956\n",
      "Iteration 7450, loss = 0.05208510\n",
      "Iteration 7451, loss = 0.05207065\n",
      "Iteration 7452, loss = 0.05205621\n",
      "Iteration 7453, loss = 0.05204178\n",
      "Iteration 7454, loss = 0.05202735\n",
      "Iteration 7455, loss = 0.05201293\n",
      "Iteration 7456, loss = 0.05199851\n",
      "Iteration 7457, loss = 0.05198411\n",
      "Iteration 7458, loss = 0.05196971\n",
      "Iteration 7459, loss = 0.05195531\n",
      "Iteration 7460, loss = 0.05194093\n",
      "Iteration 7461, loss = 0.05192655\n",
      "Iteration 7462, loss = 0.05191218\n",
      "Iteration 7463, loss = 0.05189782\n",
      "Iteration 7464, loss = 0.05188346\n",
      "Iteration 7465, loss = 0.05186911\n",
      "Iteration 7466, loss = 0.05185477\n",
      "Iteration 7467, loss = 0.05184043\n",
      "Iteration 7468, loss = 0.05182610\n",
      "Iteration 7469, loss = 0.05181178\n",
      "Iteration 7470, loss = 0.05179747\n",
      "Iteration 7471, loss = 0.05178316\n",
      "Iteration 7472, loss = 0.05176886\n",
      "Iteration 7473, loss = 0.05175456\n",
      "Iteration 7474, loss = 0.05174028\n",
      "Iteration 7475, loss = 0.05172600\n",
      "Iteration 7476, loss = 0.05171173\n",
      "Iteration 7477, loss = 0.05169746\n",
      "Iteration 7478, loss = 0.05168320\n",
      "Iteration 7479, loss = 0.05166895\n",
      "Iteration 7480, loss = 0.05165471\n",
      "Iteration 7481, loss = 0.05164047\n",
      "Iteration 7482, loss = 0.05162624\n",
      "Iteration 7483, loss = 0.05161202\n",
      "Iteration 7484, loss = 0.05159780\n",
      "Iteration 7485, loss = 0.05158359\n",
      "Iteration 7486, loss = 0.05156939\n",
      "Iteration 7487, loss = 0.05155520\n",
      "Iteration 7488, loss = 0.05154101\n",
      "Iteration 7489, loss = 0.05152683\n",
      "Iteration 7490, loss = 0.05151265\n",
      "Iteration 7491, loss = 0.05149849\n",
      "Iteration 7492, loss = 0.05148432\n",
      "Iteration 7493, loss = 0.05147017\n",
      "Iteration 7494, loss = 0.05145603\n",
      "Iteration 7495, loss = 0.05144189\n",
      "Iteration 7496, loss = 0.05142775\n",
      "Iteration 7497, loss = 0.05141363\n",
      "Iteration 7498, loss = 0.05139951\n",
      "Iteration 7499, loss = 0.05138540\n",
      "Iteration 7500, loss = 0.05137129\n",
      "Iteration 7501, loss = 0.05135720\n",
      "Iteration 7502, loss = 0.05134310\n",
      "Iteration 7503, loss = 0.05132902\n",
      "Iteration 7504, loss = 0.05131494\n",
      "Iteration 7505, loss = 0.05130087\n",
      "Iteration 7506, loss = 0.05128681\n",
      "Iteration 7507, loss = 0.05127275\n",
      "Iteration 7508, loss = 0.05125870\n",
      "Iteration 7509, loss = 0.05124466\n",
      "Iteration 7510, loss = 0.05123063\n",
      "Iteration 7511, loss = 0.05121660\n",
      "Iteration 7512, loss = 0.05120258\n",
      "Iteration 7513, loss = 0.05118856\n",
      "Iteration 7514, loss = 0.05117455\n",
      "Iteration 7515, loss = 0.05116055\n",
      "Iteration 7516, loss = 0.05114656\n",
      "Iteration 7517, loss = 0.05113257\n",
      "Iteration 7518, loss = 0.05111859\n",
      "Iteration 7519, loss = 0.05110461\n",
      "Iteration 7520, loss = 0.05109065\n",
      "Iteration 7521, loss = 0.05107669\n",
      "Iteration 7522, loss = 0.05106273\n",
      "Iteration 7523, loss = 0.05104879\n",
      "Iteration 7524, loss = 0.05103485\n",
      "Iteration 7525, loss = 0.05102091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7526, loss = 0.05100699\n",
      "Iteration 7527, loss = 0.05099307\n",
      "Iteration 7528, loss = 0.05097916\n",
      "Iteration 7529, loss = 0.05096525\n",
      "Iteration 7530, loss = 0.05095135\n",
      "Iteration 7531, loss = 0.05093746\n",
      "Iteration 7532, loss = 0.05092357\n",
      "Iteration 7533, loss = 0.05090969\n",
      "Iteration 7534, loss = 0.05089582\n",
      "Iteration 7535, loss = 0.05088196\n",
      "Iteration 7536, loss = 0.05086810\n",
      "Iteration 7537, loss = 0.05085425\n",
      "Iteration 7538, loss = 0.05084040\n",
      "Iteration 7539, loss = 0.05082656\n",
      "Iteration 7540, loss = 0.05081273\n",
      "Iteration 7541, loss = 0.05079891\n",
      "Iteration 7542, loss = 0.05078509\n",
      "Iteration 7543, loss = 0.05077128\n",
      "Iteration 7544, loss = 0.05075747\n",
      "Iteration 7545, loss = 0.05074368\n",
      "Iteration 7546, loss = 0.05072988\n",
      "Iteration 7547, loss = 0.05071610\n",
      "Iteration 7548, loss = 0.05070232\n",
      "Iteration 7549, loss = 0.05068855\n",
      "Iteration 7550, loss = 0.05067479\n",
      "Iteration 7551, loss = 0.05066103\n",
      "Iteration 7552, loss = 0.05064728\n",
      "Iteration 7553, loss = 0.05063354\n",
      "Iteration 7554, loss = 0.05061980\n",
      "Iteration 7555, loss = 0.05060607\n",
      "Iteration 7556, loss = 0.05059234\n",
      "Iteration 7557, loss = 0.05057863\n",
      "Iteration 7558, loss = 0.05056491\n",
      "Iteration 7559, loss = 0.05055121\n",
      "Iteration 7560, loss = 0.05053751\n",
      "Iteration 7561, loss = 0.05052382\n",
      "Iteration 7562, loss = 0.05051014\n",
      "Iteration 7563, loss = 0.05049646\n",
      "Iteration 7564, loss = 0.05048279\n",
      "Iteration 7565, loss = 0.05046912\n",
      "Iteration 7566, loss = 0.05045547\n",
      "Iteration 7567, loss = 0.05044182\n",
      "Iteration 7568, loss = 0.05042817\n",
      "Iteration 7569, loss = 0.05041453\n",
      "Iteration 7570, loss = 0.05040090\n",
      "Iteration 7571, loss = 0.05038728\n",
      "Iteration 7572, loss = 0.05037366\n",
      "Iteration 7573, loss = 0.05036005\n",
      "Iteration 7574, loss = 0.05034644\n",
      "Iteration 7575, loss = 0.05033285\n",
      "Iteration 7576, loss = 0.05031925\n",
      "Iteration 7577, loss = 0.05030567\n",
      "Iteration 7578, loss = 0.05029209\n",
      "Iteration 7579, loss = 0.05027852\n",
      "Iteration 7580, loss = 0.05026495\n",
      "Iteration 7581, loss = 0.05025139\n",
      "Iteration 7582, loss = 0.05023784\n",
      "Iteration 7583, loss = 0.05022430\n",
      "Iteration 7584, loss = 0.05021076\n",
      "Iteration 7585, loss = 0.05019722\n",
      "Iteration 7586, loss = 0.05018370\n",
      "Iteration 7587, loss = 0.05017018\n",
      "Iteration 7588, loss = 0.05015667\n",
      "Iteration 7589, loss = 0.05014316\n",
      "Iteration 7590, loss = 0.05012966\n",
      "Iteration 7591, loss = 0.05011617\n",
      "Iteration 7592, loss = 0.05010268\n",
      "Iteration 7593, loss = 0.05008920\n",
      "Iteration 7594, loss = 0.05007573\n",
      "Iteration 7595, loss = 0.05006226\n",
      "Iteration 7596, loss = 0.05004880\n",
      "Iteration 7597, loss = 0.05003534\n",
      "Iteration 7598, loss = 0.05002189\n",
      "Iteration 7599, loss = 0.05000845\n",
      "Iteration 7600, loss = 0.04999502\n",
      "Iteration 7601, loss = 0.04998159\n",
      "Iteration 7602, loss = 0.04996817\n",
      "Iteration 7603, loss = 0.04995475\n",
      "Iteration 7604, loss = 0.04994134\n",
      "Iteration 7605, loss = 0.04992794\n",
      "Iteration 7606, loss = 0.04991454\n",
      "Iteration 7607, loss = 0.04990115\n",
      "Iteration 7608, loss = 0.04988777\n",
      "Iteration 7609, loss = 0.04987439\n",
      "Iteration 7610, loss = 0.04986102\n",
      "Iteration 7611, loss = 0.04984766\n",
      "Iteration 7612, loss = 0.04983430\n",
      "Iteration 7613, loss = 0.04982095\n",
      "Iteration 7614, loss = 0.04980761\n",
      "Iteration 7615, loss = 0.04979427\n",
      "Iteration 7616, loss = 0.04978094\n",
      "Iteration 7617, loss = 0.04976761\n",
      "Iteration 7618, loss = 0.04975429\n",
      "Iteration 7619, loss = 0.04974098\n",
      "Iteration 7620, loss = 0.04972767\n",
      "Iteration 7621, loss = 0.04971437\n",
      "Iteration 7622, loss = 0.04970108\n",
      "Iteration 7623, loss = 0.04968779\n",
      "Iteration 7624, loss = 0.04967451\n",
      "Iteration 7625, loss = 0.04966124\n",
      "Iteration 7626, loss = 0.04964797\n",
      "Iteration 7627, loss = 0.04963471\n",
      "Iteration 7628, loss = 0.04962145\n",
      "Iteration 7629, loss = 0.04960820\n",
      "Iteration 7630, loss = 0.04959496\n",
      "Iteration 7631, loss = 0.04958173\n",
      "Iteration 7632, loss = 0.04956850\n",
      "Iteration 7633, loss = 0.04955527\n",
      "Iteration 7634, loss = 0.04954206\n",
      "Iteration 7635, loss = 0.04952884\n",
      "Iteration 7636, loss = 0.04951564\n",
      "Iteration 7637, loss = 0.04950244\n",
      "Iteration 7638, loss = 0.04948925\n",
      "Iteration 7639, loss = 0.04947606\n",
      "Iteration 7640, loss = 0.04946289\n",
      "Iteration 7641, loss = 0.04944971\n",
      "Iteration 7642, loss = 0.04943655\n",
      "Iteration 7643, loss = 0.04942339\n",
      "Iteration 7644, loss = 0.04941023\n",
      "Iteration 7645, loss = 0.04939708\n",
      "Iteration 7646, loss = 0.04938394\n",
      "Iteration 7647, loss = 0.04937081\n",
      "Iteration 7648, loss = 0.04935768\n",
      "Iteration 7649, loss = 0.04934456\n",
      "Iteration 7650, loss = 0.04933144\n",
      "Iteration 7651, loss = 0.04931833\n",
      "Iteration 7652, loss = 0.04930523\n",
      "Iteration 7653, loss = 0.04929213\n",
      "Iteration 7654, loss = 0.04927904\n",
      "Iteration 7655, loss = 0.04926595\n",
      "Iteration 7656, loss = 0.04925287\n",
      "Iteration 7657, loss = 0.04923980\n",
      "Iteration 7658, loss = 0.04922673\n",
      "Iteration 7659, loss = 0.04921367\n",
      "Iteration 7660, loss = 0.04920062\n",
      "Iteration 7661, loss = 0.04918757\n",
      "Iteration 7662, loss = 0.04917453\n",
      "Iteration 7663, loss = 0.04916150\n",
      "Iteration 7664, loss = 0.04914847\n",
      "Iteration 7665, loss = 0.04913544\n",
      "Iteration 7666, loss = 0.04912243\n",
      "Iteration 7667, loss = 0.04910942\n",
      "Iteration 7668, loss = 0.04909641\n",
      "Iteration 7669, loss = 0.04908341\n",
      "Iteration 7670, loss = 0.04907042\n",
      "Iteration 7671, loss = 0.04905744\n",
      "Iteration 7672, loss = 0.04904446\n",
      "Iteration 7673, loss = 0.04903148\n",
      "Iteration 7674, loss = 0.04901852\n",
      "Iteration 7675, loss = 0.04900556\n",
      "Iteration 7676, loss = 0.04899260\n",
      "Iteration 7677, loss = 0.04897965\n",
      "Iteration 7678, loss = 0.04896671\n",
      "Iteration 7679, loss = 0.04895377\n",
      "Iteration 7680, loss = 0.04894084\n",
      "Iteration 7681, loss = 0.04892792\n",
      "Iteration 7682, loss = 0.04891500\n",
      "Iteration 7683, loss = 0.04890209\n",
      "Iteration 7684, loss = 0.04888919\n",
      "Iteration 7685, loss = 0.04887629\n",
      "Iteration 7686, loss = 0.04886339\n",
      "Iteration 7687, loss = 0.04885051\n",
      "Iteration 7688, loss = 0.04883762\n",
      "Iteration 7689, loss = 0.04882475\n",
      "Iteration 7690, loss = 0.04881188\n",
      "Iteration 7691, loss = 0.04879902\n",
      "Iteration 7692, loss = 0.04878616\n",
      "Iteration 7693, loss = 0.04877331\n",
      "Iteration 7694, loss = 0.04876047\n",
      "Iteration 7695, loss = 0.04874763\n",
      "Iteration 7696, loss = 0.04873480\n",
      "Iteration 7697, loss = 0.04872197\n",
      "Iteration 7698, loss = 0.04870915\n",
      "Iteration 7699, loss = 0.04869633\n",
      "Iteration 7700, loss = 0.04868353\n",
      "Iteration 7701, loss = 0.04867072\n",
      "Iteration 7702, loss = 0.04865793\n",
      "Iteration 7703, loss = 0.04864514\n",
      "Iteration 7704, loss = 0.04863236\n",
      "Iteration 7705, loss = 0.04861958\n",
      "Iteration 7706, loss = 0.04860681\n",
      "Iteration 7707, loss = 0.04859404\n",
      "Iteration 7708, loss = 0.04858128\n",
      "Iteration 7709, loss = 0.04856853\n",
      "Iteration 7710, loss = 0.04855578\n",
      "Iteration 7711, loss = 0.04854304\n",
      "Iteration 7712, loss = 0.04853030\n",
      "Iteration 7713, loss = 0.04851757\n",
      "Iteration 7714, loss = 0.04850485\n",
      "Iteration 7715, loss = 0.04849213\n",
      "Iteration 7716, loss = 0.04847942\n",
      "Iteration 7717, loss = 0.04846672\n",
      "Iteration 7718, loss = 0.04845402\n",
      "Iteration 7719, loss = 0.04844132\n",
      "Iteration 7720, loss = 0.04842864\n",
      "Iteration 7721, loss = 0.04841595\n",
      "Iteration 7722, loss = 0.04840328\n",
      "Iteration 7723, loss = 0.04839061\n",
      "Iteration 7724, loss = 0.04837795\n",
      "Iteration 7725, loss = 0.04836529\n",
      "Iteration 7726, loss = 0.04835264\n",
      "Iteration 7727, loss = 0.04833999\n",
      "Iteration 7728, loss = 0.04832735\n",
      "Iteration 7729, loss = 0.04831472\n",
      "Iteration 7730, loss = 0.04830209\n",
      "Iteration 7731, loss = 0.04828947\n",
      "Iteration 7732, loss = 0.04827685\n",
      "Iteration 7733, loss = 0.04826424\n",
      "Iteration 7734, loss = 0.04825164\n",
      "Iteration 7735, loss = 0.04823904\n",
      "Iteration 7736, loss = 0.04822645\n",
      "Iteration 7737, loss = 0.04821386\n",
      "Iteration 7738, loss = 0.04820128\n",
      "Iteration 7739, loss = 0.04818871\n",
      "Iteration 7740, loss = 0.04817614\n",
      "Iteration 7741, loss = 0.04816358\n",
      "Iteration 7742, loss = 0.04815102\n",
      "Iteration 7743, loss = 0.04813847\n",
      "Iteration 7744, loss = 0.04812592\n",
      "Iteration 7745, loss = 0.04811338\n",
      "Iteration 7746, loss = 0.04810085\n",
      "Iteration 7747, loss = 0.04808832\n",
      "Iteration 7748, loss = 0.04807580\n",
      "Iteration 7749, loss = 0.04806329\n",
      "Iteration 7750, loss = 0.04805078\n",
      "Iteration 7751, loss = 0.04803827\n",
      "Iteration 7752, loss = 0.04802578\n",
      "Iteration 7753, loss = 0.04801328\n",
      "Iteration 7754, loss = 0.04800080\n",
      "Iteration 7755, loss = 0.04798832\n",
      "Iteration 7756, loss = 0.04797584\n",
      "Iteration 7757, loss = 0.04796338\n",
      "Iteration 7758, loss = 0.04795091\n",
      "Iteration 7759, loss = 0.04793846\n",
      "Iteration 7760, loss = 0.04792601\n",
      "Iteration 7761, loss = 0.04791356\n",
      "Iteration 7762, loss = 0.04790112\n",
      "Iteration 7763, loss = 0.04788869\n",
      "Iteration 7764, loss = 0.04787626\n",
      "Iteration 7765, loss = 0.04786384\n",
      "Iteration 7766, loss = 0.04785142\n",
      "Iteration 7767, loss = 0.04783901\n",
      "Iteration 7768, loss = 0.04782661\n",
      "Iteration 7769, loss = 0.04781421\n",
      "Iteration 7770, loss = 0.04780182\n",
      "Iteration 7771, loss = 0.04778943\n",
      "Iteration 7772, loss = 0.04777705\n",
      "Iteration 7773, loss = 0.04776467\n",
      "Iteration 7774, loss = 0.04775230\n",
      "Iteration 7775, loss = 0.04773994\n",
      "Iteration 7776, loss = 0.04772758\n",
      "Iteration 7777, loss = 0.04771523\n",
      "Iteration 7778, loss = 0.04770288\n",
      "Iteration 7779, loss = 0.04769054\n",
      "Iteration 7780, loss = 0.04767821\n",
      "Iteration 7781, loss = 0.04766588\n",
      "Iteration 7782, loss = 0.04765355\n",
      "Iteration 7783, loss = 0.04764124\n",
      "Iteration 7784, loss = 0.04762892\n",
      "Iteration 7785, loss = 0.04761662\n",
      "Iteration 7786, loss = 0.04760432\n",
      "Iteration 7787, loss = 0.04759202\n",
      "Iteration 7788, loss = 0.04757973\n",
      "Iteration 7789, loss = 0.04756745\n",
      "Iteration 7790, loss = 0.04755517\n",
      "Iteration 7791, loss = 0.04754290\n",
      "Iteration 7792, loss = 0.04753063\n",
      "Iteration 7793, loss = 0.04751837\n",
      "Iteration 7794, loss = 0.04750612\n",
      "Iteration 7795, loss = 0.04749387\n",
      "Iteration 7796, loss = 0.04748163\n",
      "Iteration 7797, loss = 0.04746939\n",
      "Iteration 7798, loss = 0.04745716\n",
      "Iteration 7799, loss = 0.04744493\n",
      "Iteration 7800, loss = 0.04743271\n",
      "Iteration 7801, loss = 0.04742050\n",
      "Iteration 7802, loss = 0.04740829\n",
      "Iteration 7803, loss = 0.04739608\n",
      "Iteration 7804, loss = 0.04738389\n",
      "Iteration 7805, loss = 0.04737169\n",
      "Iteration 7806, loss = 0.04735951\n",
      "Iteration 7807, loss = 0.04734733\n",
      "Iteration 7808, loss = 0.04733515\n",
      "Iteration 7809, loss = 0.04732298\n",
      "Iteration 7810, loss = 0.04731082\n",
      "Iteration 7811, loss = 0.04729866\n",
      "Iteration 7812, loss = 0.04728651\n",
      "Iteration 7813, loss = 0.04727436\n",
      "Iteration 7814, loss = 0.04726222\n",
      "Iteration 7815, loss = 0.04725008\n",
      "Iteration 7816, loss = 0.04723795\n",
      "Iteration 7817, loss = 0.04722583\n",
      "Iteration 7818, loss = 0.04721371\n",
      "Iteration 7819, loss = 0.04720160\n",
      "Iteration 7820, loss = 0.04718949\n",
      "Iteration 7821, loss = 0.04717739\n",
      "Iteration 7822, loss = 0.04716529\n",
      "Iteration 7823, loss = 0.04715320\n",
      "Iteration 7824, loss = 0.04714112\n",
      "Iteration 7825, loss = 0.04712904\n",
      "Iteration 7826, loss = 0.04711696\n",
      "Iteration 7827, loss = 0.04710490\n",
      "Iteration 7828, loss = 0.04709283\n",
      "Iteration 7829, loss = 0.04708078\n",
      "Iteration 7830, loss = 0.04706872\n",
      "Iteration 7831, loss = 0.04705668\n",
      "Iteration 7832, loss = 0.04704464\n",
      "Iteration 7833, loss = 0.04703260\n",
      "Iteration 7834, loss = 0.04702057\n",
      "Iteration 7835, loss = 0.04700855\n",
      "Iteration 7836, loss = 0.04699653\n",
      "Iteration 7837, loss = 0.04698452\n",
      "Iteration 7838, loss = 0.04697251\n",
      "Iteration 7839, loss = 0.04696051\n",
      "Iteration 7840, loss = 0.04694852\n",
      "Iteration 7841, loss = 0.04693653\n",
      "Iteration 7842, loss = 0.04692454\n",
      "Iteration 7843, loss = 0.04691256\n",
      "Iteration 7844, loss = 0.04690059\n",
      "Iteration 7845, loss = 0.04688862\n",
      "Iteration 7846, loss = 0.04687666\n",
      "Iteration 7847, loss = 0.04686470\n",
      "Iteration 7848, loss = 0.04685275\n",
      "Iteration 7849, loss = 0.04684080\n",
      "Iteration 7850, loss = 0.04682886\n",
      "Iteration 7851, loss = 0.04681693\n",
      "Iteration 7852, loss = 0.04680500\n",
      "Iteration 7853, loss = 0.04679307\n",
      "Iteration 7854, loss = 0.04678116\n",
      "Iteration 7855, loss = 0.04676924\n",
      "Iteration 7856, loss = 0.04675734\n",
      "Iteration 7857, loss = 0.04674543\n",
      "Iteration 7858, loss = 0.04673354\n",
      "Iteration 7859, loss = 0.04672165\n",
      "Iteration 7860, loss = 0.04670976\n",
      "Iteration 7861, loss = 0.04669788\n",
      "Iteration 7862, loss = 0.04668601\n",
      "Iteration 7863, loss = 0.04667414\n",
      "Iteration 7864, loss = 0.04666227\n",
      "Iteration 7865, loss = 0.04665041\n",
      "Iteration 7866, loss = 0.04663856\n",
      "Iteration 7867, loss = 0.04662671\n",
      "Iteration 7868, loss = 0.04661487\n",
      "Iteration 7869, loss = 0.04660304\n",
      "Iteration 7870, loss = 0.04659120\n",
      "Iteration 7871, loss = 0.04657938\n",
      "Iteration 7872, loss = 0.04656756\n",
      "Iteration 7873, loss = 0.04655574\n",
      "Iteration 7874, loss = 0.04654393\n",
      "Iteration 7875, loss = 0.04653213\n",
      "Iteration 7876, loss = 0.04652033\n",
      "Iteration 7877, loss = 0.04650854\n",
      "Iteration 7878, loss = 0.04649675\n",
      "Iteration 7879, loss = 0.04648497\n",
      "Iteration 7880, loss = 0.04647319\n",
      "Iteration 7881, loss = 0.04646142\n",
      "Iteration 7882, loss = 0.04644966\n",
      "Iteration 7883, loss = 0.04643789\n",
      "Iteration 7884, loss = 0.04642614\n",
      "Iteration 7885, loss = 0.04641439\n",
      "Iteration 7886, loss = 0.04640265\n",
      "Iteration 7887, loss = 0.04639091\n",
      "Iteration 7888, loss = 0.04637917\n",
      "Iteration 7889, loss = 0.04636744\n",
      "Iteration 7890, loss = 0.04635572\n",
      "Iteration 7891, loss = 0.04634400\n",
      "Iteration 7892, loss = 0.04633229\n",
      "Iteration 7893, loss = 0.04632059\n",
      "Iteration 7894, loss = 0.04630888\n",
      "Iteration 7895, loss = 0.04629719\n",
      "Iteration 7896, loss = 0.04628550\n",
      "Iteration 7897, loss = 0.04627381\n",
      "Iteration 7898, loss = 0.04626213\n",
      "Iteration 7899, loss = 0.04625046\n",
      "Iteration 7900, loss = 0.04623879\n",
      "Iteration 7901, loss = 0.04622712\n",
      "Iteration 7902, loss = 0.04621546\n",
      "Iteration 7903, loss = 0.04620381\n",
      "Iteration 7904, loss = 0.04619216\n",
      "Iteration 7905, loss = 0.04618052\n",
      "Iteration 7906, loss = 0.04616888\n",
      "Iteration 7907, loss = 0.04615725\n",
      "Iteration 7908, loss = 0.04614562\n",
      "Iteration 7909, loss = 0.04613400\n",
      "Iteration 7910, loss = 0.04612239\n",
      "Iteration 7911, loss = 0.04611078\n",
      "Iteration 7912, loss = 0.04609917\n",
      "Iteration 7913, loss = 0.04608757\n",
      "Iteration 7914, loss = 0.04607598\n",
      "Iteration 7915, loss = 0.04606439\n",
      "Iteration 7916, loss = 0.04605280\n",
      "Iteration 7917, loss = 0.04604122\n",
      "Iteration 7918, loss = 0.04602965\n",
      "Iteration 7919, loss = 0.04601808\n",
      "Iteration 7920, loss = 0.04600652\n",
      "Iteration 7921, loss = 0.04599496\n",
      "Iteration 7922, loss = 0.04598341\n",
      "Iteration 7923, loss = 0.04597186\n",
      "Iteration 7924, loss = 0.04596032\n",
      "Iteration 7925, loss = 0.04594878\n",
      "Iteration 7926, loss = 0.04593725\n",
      "Iteration 7927, loss = 0.04592572\n",
      "Iteration 7928, loss = 0.04591420\n",
      "Iteration 7929, loss = 0.04590269\n",
      "Iteration 7930, loss = 0.04589117\n",
      "Iteration 7931, loss = 0.04587967\n",
      "Iteration 7932, loss = 0.04586817\n",
      "Iteration 7933, loss = 0.04585667\n",
      "Iteration 7934, loss = 0.04584518\n",
      "Iteration 7935, loss = 0.04583370\n",
      "Iteration 7936, loss = 0.04582222\n",
      "Iteration 7937, loss = 0.04581075\n",
      "Iteration 7938, loss = 0.04579928\n",
      "Iteration 7939, loss = 0.04578781\n",
      "Iteration 7940, loss = 0.04577636\n",
      "Iteration 7941, loss = 0.04576490\n",
      "Iteration 7942, loss = 0.04575345\n",
      "Iteration 7943, loss = 0.04574201\n",
      "Iteration 7944, loss = 0.04573057\n",
      "Iteration 7945, loss = 0.04571914\n",
      "Iteration 7946, loss = 0.04570771\n",
      "Iteration 7947, loss = 0.04569629\n",
      "Iteration 7948, loss = 0.04568488\n",
      "Iteration 7949, loss = 0.04567346\n",
      "Iteration 7950, loss = 0.04566206\n",
      "Iteration 7951, loss = 0.04565066\n",
      "Iteration 7952, loss = 0.04563926\n",
      "Iteration 7953, loss = 0.04562787\n",
      "Iteration 7954, loss = 0.04561648\n",
      "Iteration 7955, loss = 0.04560510\n",
      "Iteration 7956, loss = 0.04559373\n",
      "Iteration 7957, loss = 0.04558236\n",
      "Iteration 7958, loss = 0.04557099\n",
      "Iteration 7959, loss = 0.04555963\n",
      "Iteration 7960, loss = 0.04554828\n",
      "Iteration 7961, loss = 0.04553693\n",
      "Iteration 7962, loss = 0.04552558\n",
      "Iteration 7963, loss = 0.04551424\n",
      "Iteration 7964, loss = 0.04550291\n",
      "Iteration 7965, loss = 0.04549158\n",
      "Iteration 7966, loss = 0.04548026\n",
      "Iteration 7967, loss = 0.04546894\n",
      "Iteration 7968, loss = 0.04545762\n",
      "Iteration 7969, loss = 0.04544631\n",
      "Iteration 7970, loss = 0.04543501\n",
      "Iteration 7971, loss = 0.04542371\n",
      "Iteration 7972, loss = 0.04541242\n",
      "Iteration 7973, loss = 0.04540113\n",
      "Iteration 7974, loss = 0.04538985\n",
      "Iteration 7975, loss = 0.04537857\n",
      "Iteration 7976, loss = 0.04536730\n",
      "Iteration 7977, loss = 0.04535603\n",
      "Iteration 7978, loss = 0.04534476\n",
      "Iteration 7979, loss = 0.04533351\n",
      "Iteration 7980, loss = 0.04532225\n",
      "Iteration 7981, loss = 0.04531101\n",
      "Iteration 7982, loss = 0.04529976\n",
      "Iteration 7983, loss = 0.04528853\n",
      "Iteration 7984, loss = 0.04527729\n",
      "Iteration 7985, loss = 0.04526607\n",
      "Iteration 7986, loss = 0.04525484\n",
      "Iteration 7987, loss = 0.04524363\n",
      "Iteration 7988, loss = 0.04523241\n",
      "Iteration 7989, loss = 0.04522121\n",
      "Iteration 7990, loss = 0.04521001\n",
      "Iteration 7991, loss = 0.04519881\n",
      "Iteration 7992, loss = 0.04518762\n",
      "Iteration 7993, loss = 0.04517643\n",
      "Iteration 7994, loss = 0.04516525\n",
      "Iteration 7995, loss = 0.04515407\n",
      "Iteration 7996, loss = 0.04514290\n",
      "Iteration 7997, loss = 0.04513173\n",
      "Iteration 7998, loss = 0.04512057\n",
      "Iteration 7999, loss = 0.04510941\n",
      "Iteration 8000, loss = 0.04509826\n",
      "Iteration 8001, loss = 0.04508712\n",
      "Iteration 8002, loss = 0.04507597\n",
      "Iteration 8003, loss = 0.04506484\n",
      "Iteration 8004, loss = 0.04505371\n",
      "Iteration 8005, loss = 0.04504258\n",
      "Iteration 8006, loss = 0.04503146\n",
      "Iteration 8007, loss = 0.04502034\n",
      "Iteration 8008, loss = 0.04500923\n",
      "Iteration 8009, loss = 0.04499812\n",
      "Iteration 8010, loss = 0.04498702\n",
      "Iteration 8011, loss = 0.04497592\n",
      "Iteration 8012, loss = 0.04496483\n",
      "Iteration 8013, loss = 0.04495374\n",
      "Iteration 8014, loss = 0.04494266\n",
      "Iteration 8015, loss = 0.04493159\n",
      "Iteration 8016, loss = 0.04492051\n",
      "Iteration 8017, loss = 0.04490945\n",
      "Iteration 8018, loss = 0.04489839\n",
      "Iteration 8019, loss = 0.04488733\n",
      "Iteration 8020, loss = 0.04487628\n",
      "Iteration 8021, loss = 0.04486523\n",
      "Iteration 8022, loss = 0.04485419\n",
      "Iteration 8023, loss = 0.04484315\n",
      "Iteration 8024, loss = 0.04483212\n",
      "Iteration 8025, loss = 0.04482109\n",
      "Iteration 8026, loss = 0.04481007\n",
      "Iteration 8027, loss = 0.04479905\n",
      "Iteration 8028, loss = 0.04478804\n",
      "Iteration 8029, loss = 0.04477703\n",
      "Iteration 8030, loss = 0.04476603\n",
      "Iteration 8031, loss = 0.04475503\n",
      "Iteration 8032, loss = 0.04474404\n",
      "Iteration 8033, loss = 0.04473305\n",
      "Iteration 8034, loss = 0.04472207\n",
      "Iteration 8035, loss = 0.04471109\n",
      "Iteration 8036, loss = 0.04470012\n",
      "Iteration 8037, loss = 0.04468915\n",
      "Iteration 8038, loss = 0.04467818\n",
      "Iteration 8039, loss = 0.04466723\n",
      "Iteration 8040, loss = 0.04465627\n",
      "Iteration 8041, loss = 0.04464532\n",
      "Iteration 8042, loss = 0.04463438\n",
      "Iteration 8043, loss = 0.04462344\n",
      "Iteration 8044, loss = 0.04461251\n",
      "Iteration 8045, loss = 0.04460158\n",
      "Iteration 8046, loss = 0.04459065\n",
      "Iteration 8047, loss = 0.04457973\n",
      "Iteration 8048, loss = 0.04456882\n",
      "Iteration 8049, loss = 0.04455791\n",
      "Iteration 8050, loss = 0.04454701\n",
      "Iteration 8051, loss = 0.04453611\n",
      "Iteration 8052, loss = 0.04452521\n",
      "Iteration 8053, loss = 0.04451432\n",
      "Iteration 8054, loss = 0.04450344\n",
      "Iteration 8055, loss = 0.04449256\n",
      "Iteration 8056, loss = 0.04448168\n",
      "Iteration 8057, loss = 0.04447081\n",
      "Iteration 8058, loss = 0.04445994\n",
      "Iteration 8059, loss = 0.04444908\n",
      "Iteration 8060, loss = 0.04443823\n",
      "Iteration 8061, loss = 0.04442737\n",
      "Iteration 8062, loss = 0.04441653\n",
      "Iteration 8063, loss = 0.04440569\n",
      "Iteration 8064, loss = 0.04439485\n",
      "Iteration 8065, loss = 0.04438402\n",
      "Iteration 8066, loss = 0.04437319\n",
      "Iteration 8067, loss = 0.04436237\n",
      "Iteration 8068, loss = 0.04435155\n",
      "Iteration 8069, loss = 0.04434074\n",
      "Iteration 8070, loss = 0.04432993\n",
      "Iteration 8071, loss = 0.04431913\n",
      "Iteration 8072, loss = 0.04430833\n",
      "Iteration 8073, loss = 0.04429753\n",
      "Iteration 8074, loss = 0.04428674\n",
      "Iteration 8075, loss = 0.04427596\n",
      "Iteration 8076, loss = 0.04426518\n",
      "Iteration 8077, loss = 0.04425441\n",
      "Iteration 8078, loss = 0.04424364\n",
      "Iteration 8079, loss = 0.04423287\n",
      "Iteration 8080, loss = 0.04422211\n",
      "Iteration 8081, loss = 0.04421136\n",
      "Iteration 8082, loss = 0.04420061\n",
      "Iteration 8083, loss = 0.04418986\n",
      "Iteration 8084, loss = 0.04417912\n",
      "Iteration 8085, loss = 0.04416838\n",
      "Iteration 8086, loss = 0.04415765\n",
      "Iteration 8087, loss = 0.04414693\n",
      "Iteration 8088, loss = 0.04413620\n",
      "Iteration 8089, loss = 0.04412549\n",
      "Iteration 8090, loss = 0.04411477\n",
      "Iteration 8091, loss = 0.04410407\n",
      "Iteration 8092, loss = 0.04409336\n",
      "Iteration 8093, loss = 0.04408266\n",
      "Iteration 8094, loss = 0.04407197\n",
      "Iteration 8095, loss = 0.04406128\n",
      "Iteration 8096, loss = 0.04405060\n",
      "Iteration 8097, loss = 0.04403992\n",
      "Iteration 8098, loss = 0.04402924\n",
      "Iteration 8099, loss = 0.04401857\n",
      "Iteration 8100, loss = 0.04400791\n",
      "Iteration 8101, loss = 0.04399725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8102, loss = 0.04398659\n",
      "Iteration 8103, loss = 0.04397594\n",
      "Iteration 8104, loss = 0.04396530\n",
      "Iteration 8105, loss = 0.04395465\n",
      "Iteration 8106, loss = 0.04394402\n",
      "Iteration 8107, loss = 0.04393338\n",
      "Iteration 8108, loss = 0.04392276\n",
      "Iteration 8109, loss = 0.04391213\n",
      "Iteration 8110, loss = 0.04390152\n",
      "Iteration 8111, loss = 0.04389090\n",
      "Iteration 8112, loss = 0.04388029\n",
      "Iteration 8113, loss = 0.04386969\n",
      "Iteration 8114, loss = 0.04385909\n",
      "Iteration 8115, loss = 0.04384850\n",
      "Iteration 8116, loss = 0.04383791\n",
      "Iteration 8117, loss = 0.04382732\n",
      "Iteration 8118, loss = 0.04381674\n",
      "Iteration 8119, loss = 0.04380616\n",
      "Iteration 8120, loss = 0.04379559\n",
      "Iteration 8121, loss = 0.04378503\n",
      "Iteration 8122, loss = 0.04377446\n",
      "Iteration 8123, loss = 0.04376391\n",
      "Iteration 8124, loss = 0.04375335\n",
      "Iteration 8125, loss = 0.04374280\n",
      "Iteration 8126, loss = 0.04373226\n",
      "Iteration 8127, loss = 0.04372172\n",
      "Iteration 8128, loss = 0.04371119\n",
      "Iteration 8129, loss = 0.04370066\n",
      "Iteration 8130, loss = 0.04369013\n",
      "Iteration 8131, loss = 0.04367961\n",
      "Iteration 8132, loss = 0.04366910\n",
      "Iteration 8133, loss = 0.04365859\n",
      "Iteration 8134, loss = 0.04364808\n",
      "Iteration 8135, loss = 0.04363758\n",
      "Iteration 8136, loss = 0.04362708\n",
      "Iteration 8137, loss = 0.04361659\n",
      "Iteration 8138, loss = 0.04360610\n",
      "Iteration 8139, loss = 0.04359562\n",
      "Iteration 8140, loss = 0.04358514\n",
      "Iteration 8141, loss = 0.04357467\n",
      "Iteration 8142, loss = 0.04356420\n",
      "Iteration 8143, loss = 0.04355373\n",
      "Iteration 8144, loss = 0.04354327\n",
      "Iteration 8145, loss = 0.04353281\n",
      "Iteration 8146, loss = 0.04352236\n",
      "Iteration 8147, loss = 0.04351192\n",
      "Iteration 8148, loss = 0.04350147\n",
      "Iteration 8149, loss = 0.04349104\n",
      "Iteration 8150, loss = 0.04348060\n",
      "Iteration 8151, loss = 0.04347018\n",
      "Iteration 8152, loss = 0.04345975\n",
      "Iteration 8153, loss = 0.04344933\n",
      "Iteration 8154, loss = 0.04343892\n",
      "Iteration 8155, loss = 0.04342851\n",
      "Iteration 8156, loss = 0.04341810\n",
      "Iteration 8157, loss = 0.04340770\n",
      "Iteration 8158, loss = 0.04339731\n",
      "Iteration 8159, loss = 0.04338692\n",
      "Iteration 8160, loss = 0.04337653\n",
      "Iteration 8161, loss = 0.04336615\n",
      "Iteration 8162, loss = 0.04335577\n",
      "Iteration 8163, loss = 0.04334539\n",
      "Iteration 8164, loss = 0.04333502\n",
      "Iteration 8165, loss = 0.04332466\n",
      "Iteration 8166, loss = 0.04331430\n",
      "Iteration 8167, loss = 0.04330394\n",
      "Iteration 8168, loss = 0.04329359\n",
      "Iteration 8169, loss = 0.04328325\n",
      "Iteration 8170, loss = 0.04327291\n",
      "Iteration 8171, loss = 0.04326257\n",
      "Iteration 8172, loss = 0.04325224\n",
      "Iteration 8173, loss = 0.04324191\n",
      "Iteration 8174, loss = 0.04323158\n",
      "Iteration 8175, loss = 0.04322126\n",
      "Iteration 8176, loss = 0.04321095\n",
      "Iteration 8177, loss = 0.04320064\n",
      "Iteration 8178, loss = 0.04319033\n",
      "Iteration 8179, loss = 0.04318003\n",
      "Iteration 8180, loss = 0.04316974\n",
      "Iteration 8181, loss = 0.04315944\n",
      "Iteration 8182, loss = 0.04314916\n",
      "Iteration 8183, loss = 0.04313887\n",
      "Iteration 8184, loss = 0.04312859\n",
      "Iteration 8185, loss = 0.04311832\n",
      "Iteration 8186, loss = 0.04310805\n",
      "Iteration 8187, loss = 0.04309778\n",
      "Iteration 8188, loss = 0.04308752\n",
      "Iteration 8189, loss = 0.04307727\n",
      "Iteration 8190, loss = 0.04306702\n",
      "Iteration 8191, loss = 0.04305677\n",
      "Iteration 8192, loss = 0.04304653\n",
      "Iteration 8193, loss = 0.04303629\n",
      "Iteration 8194, loss = 0.04302605\n",
      "Iteration 8195, loss = 0.04301582\n",
      "Iteration 8196, loss = 0.04300560\n",
      "Iteration 8197, loss = 0.04299538\n",
      "Iteration 8198, loss = 0.04298516\n",
      "Iteration 8199, loss = 0.04297495\n",
      "Iteration 8200, loss = 0.04296474\n",
      "Iteration 8201, loss = 0.04295454\n",
      "Iteration 8202, loss = 0.04294434\n",
      "Iteration 8203, loss = 0.04293415\n",
      "Iteration 8204, loss = 0.04292396\n",
      "Iteration 8205, loss = 0.04291377\n",
      "Iteration 8206, loss = 0.04290359\n",
      "Iteration 8207, loss = 0.04289341\n",
      "Iteration 8208, loss = 0.04288324\n",
      "Iteration 8209, loss = 0.04287308\n",
      "Iteration 8210, loss = 0.04286291\n",
      "Iteration 8211, loss = 0.04285275\n",
      "Iteration 8212, loss = 0.04284260\n",
      "Iteration 8213, loss = 0.04283245\n",
      "Iteration 8214, loss = 0.04282230\n",
      "Iteration 8215, loss = 0.04281216\n",
      "Iteration 8216, loss = 0.04280203\n",
      "Iteration 8217, loss = 0.04279189\n",
      "Iteration 8218, loss = 0.04278177\n",
      "Iteration 8219, loss = 0.04277164\n",
      "Iteration 8220, loss = 0.04276152\n",
      "Iteration 8221, loss = 0.04275141\n",
      "Iteration 8222, loss = 0.04274130\n",
      "Iteration 8223, loss = 0.04273119\n",
      "Iteration 8224, loss = 0.04272109\n",
      "Iteration 8225, loss = 0.04271099\n",
      "Iteration 8226, loss = 0.04270090\n",
      "Iteration 8227, loss = 0.04269081\n",
      "Iteration 8228, loss = 0.04268073\n",
      "Iteration 8229, loss = 0.04267065\n",
      "Iteration 8230, loss = 0.04266057\n",
      "Iteration 8231, loss = 0.04265050\n",
      "Iteration 8232, loss = 0.04264044\n",
      "Iteration 8233, loss = 0.04263037\n",
      "Iteration 8234, loss = 0.04262032\n",
      "Iteration 8235, loss = 0.04261026\n",
      "Iteration 8236, loss = 0.04260021\n",
      "Iteration 8237, loss = 0.04259017\n",
      "Iteration 8238, loss = 0.04258013\n",
      "Iteration 8239, loss = 0.04257009\n",
      "Iteration 8240, loss = 0.04256006\n",
      "Iteration 8241, loss = 0.04255003\n",
      "Iteration 8242, loss = 0.04254001\n",
      "Iteration 8243, loss = 0.04252999\n",
      "Iteration 8244, loss = 0.04251998\n",
      "Iteration 8245, loss = 0.04250997\n",
      "Iteration 8246, loss = 0.04249996\n",
      "Iteration 8247, loss = 0.04248996\n",
      "Iteration 8248, loss = 0.04247996\n",
      "Iteration 8249, loss = 0.04246997\n",
      "Iteration 8250, loss = 0.04245998\n",
      "Iteration 8251, loss = 0.04245000\n",
      "Iteration 8252, loss = 0.04244002\n",
      "Iteration 8253, loss = 0.04243004\n",
      "Iteration 8254, loss = 0.04242007\n",
      "Iteration 8255, loss = 0.04241010\n",
      "Iteration 8256, loss = 0.04240014\n",
      "Iteration 8257, loss = 0.04239018\n",
      "Iteration 8258, loss = 0.04238023\n",
      "Iteration 8259, loss = 0.04237028\n",
      "Iteration 8260, loss = 0.04236033\n",
      "Iteration 8261, loss = 0.04235039\n",
      "Iteration 8262, loss = 0.04234045\n",
      "Iteration 8263, loss = 0.04233052\n",
      "Iteration 8264, loss = 0.04232059\n",
      "Iteration 8265, loss = 0.04231067\n",
      "Iteration 8266, loss = 0.04230075\n",
      "Iteration 8267, loss = 0.04229083\n",
      "Iteration 8268, loss = 0.04228092\n",
      "Iteration 8269, loss = 0.04227102\n",
      "Iteration 8270, loss = 0.04226111\n",
      "Iteration 8271, loss = 0.04225121\n",
      "Iteration 8272, loss = 0.04224132\n",
      "Iteration 8273, loss = 0.04223143\n",
      "Iteration 8274, loss = 0.04222154\n",
      "Iteration 8275, loss = 0.04221166\n",
      "Iteration 8276, loss = 0.04220178\n",
      "Iteration 8277, loss = 0.04219191\n",
      "Iteration 8278, loss = 0.04218204\n",
      "Iteration 8279, loss = 0.04217218\n",
      "Iteration 8280, loss = 0.04216232\n",
      "Iteration 8281, loss = 0.04215246\n",
      "Iteration 8282, loss = 0.04214261\n",
      "Iteration 8283, loss = 0.04213276\n",
      "Iteration 8284, loss = 0.04212292\n",
      "Iteration 8285, loss = 0.04211308\n",
      "Iteration 8286, loss = 0.04210324\n",
      "Iteration 8287, loss = 0.04209341\n",
      "Iteration 8288, loss = 0.04208359\n",
      "Iteration 8289, loss = 0.04207377\n",
      "Iteration 8290, loss = 0.04206395\n",
      "Iteration 8291, loss = 0.04205413\n",
      "Iteration 8292, loss = 0.04204432\n",
      "Iteration 8293, loss = 0.04203452\n",
      "Iteration 8294, loss = 0.04202472\n",
      "Iteration 8295, loss = 0.04201492\n",
      "Iteration 8296, loss = 0.04200513\n",
      "Iteration 8297, loss = 0.04199534\n",
      "Iteration 8298, loss = 0.04198555\n",
      "Iteration 8299, loss = 0.04197577\n",
      "Iteration 8300, loss = 0.04196600\n",
      "Iteration 8301, loss = 0.04195623\n",
      "Iteration 8302, loss = 0.04194646\n",
      "Iteration 8303, loss = 0.04193669\n",
      "Iteration 8304, loss = 0.04192693\n",
      "Iteration 8305, loss = 0.04191718\n",
      "Iteration 8306, loss = 0.04190743\n",
      "Iteration 8307, loss = 0.04189768\n",
      "Iteration 8308, loss = 0.04188794\n",
      "Iteration 8309, loss = 0.04187820\n",
      "Iteration 8310, loss = 0.04186847\n",
      "Iteration 8311, loss = 0.04185874\n",
      "Iteration 8312, loss = 0.04184901\n",
      "Iteration 8313, loss = 0.04183929\n",
      "Iteration 8314, loss = 0.04182957\n",
      "Iteration 8315, loss = 0.04181986\n",
      "Iteration 8316, loss = 0.04181015\n",
      "Iteration 8317, loss = 0.04180044\n",
      "Iteration 8318, loss = 0.04179074\n",
      "Iteration 8319, loss = 0.04178104\n",
      "Iteration 8320, loss = 0.04177135\n",
      "Iteration 8321, loss = 0.04176166\n",
      "Iteration 8322, loss = 0.04175198\n",
      "Iteration 8323, loss = 0.04174230\n",
      "Iteration 8324, loss = 0.04173262\n",
      "Iteration 8325, loss = 0.04172295\n",
      "Iteration 8326, loss = 0.04171328\n",
      "Iteration 8327, loss = 0.04170362\n",
      "Iteration 8328, loss = 0.04169396\n",
      "Iteration 8329, loss = 0.04168430\n",
      "Iteration 8330, loss = 0.04167465\n",
      "Iteration 8331, loss = 0.04166500\n",
      "Iteration 8332, loss = 0.04165536\n",
      "Iteration 8333, loss = 0.04164572\n",
      "Iteration 8334, loss = 0.04163608\n",
      "Iteration 8335, loss = 0.04162645\n",
      "Iteration 8336, loss = 0.04161682\n",
      "Iteration 8337, loss = 0.04160720\n",
      "Iteration 8338, loss = 0.04159758\n",
      "Iteration 8339, loss = 0.04158797\n",
      "Iteration 8340, loss = 0.04157836\n",
      "Iteration 8341, loss = 0.04156875\n",
      "Iteration 8342, loss = 0.04155915\n",
      "Iteration 8343, loss = 0.04154955\n",
      "Iteration 8344, loss = 0.04153995\n",
      "Iteration 8345, loss = 0.04153036\n",
      "Iteration 8346, loss = 0.04152078\n",
      "Iteration 8347, loss = 0.04151120\n",
      "Iteration 8348, loss = 0.04150162\n",
      "Iteration 8349, loss = 0.04149204\n",
      "Iteration 8350, loss = 0.04148247\n",
      "Iteration 8351, loss = 0.04147291\n",
      "Iteration 8352, loss = 0.04146335\n",
      "Iteration 8353, loss = 0.04145379\n",
      "Iteration 8354, loss = 0.04144424\n",
      "Iteration 8355, loss = 0.04143469\n",
      "Iteration 8356, loss = 0.04142514\n",
      "Iteration 8357, loss = 0.04141560\n",
      "Iteration 8358, loss = 0.04140606\n",
      "Iteration 8359, loss = 0.04139653\n",
      "Iteration 8360, loss = 0.04138700\n",
      "Iteration 8361, loss = 0.04137747\n",
      "Iteration 8362, loss = 0.04136795\n",
      "Iteration 8363, loss = 0.04135843\n",
      "Iteration 8364, loss = 0.04134892\n",
      "Iteration 8365, loss = 0.04133941\n",
      "Iteration 8366, loss = 0.04132991\n",
      "Iteration 8367, loss = 0.04132041\n",
      "Iteration 8368, loss = 0.04131091\n",
      "Iteration 8369, loss = 0.04130142\n",
      "Iteration 8370, loss = 0.04129193\n",
      "Iteration 8371, loss = 0.04128244\n",
      "Iteration 8372, loss = 0.04127296\n",
      "Iteration 8373, loss = 0.04126348\n",
      "Iteration 8374, loss = 0.04125401\n",
      "Iteration 8375, loss = 0.04124454\n",
      "Iteration 8376, loss = 0.04123508\n",
      "Iteration 8377, loss = 0.04122562\n",
      "Iteration 8378, loss = 0.04121616\n",
      "Iteration 8379, loss = 0.04120671\n",
      "Iteration 8380, loss = 0.04119726\n",
      "Iteration 8381, loss = 0.04118781\n",
      "Iteration 8382, loss = 0.04117837\n",
      "Iteration 8383, loss = 0.04116893\n",
      "Iteration 8384, loss = 0.04115950\n",
      "Iteration 8385, loss = 0.04115007\n",
      "Iteration 8386, loss = 0.04114065\n",
      "Iteration 8387, loss = 0.04113122\n",
      "Iteration 8388, loss = 0.04112181\n",
      "Iteration 8389, loss = 0.04111239\n",
      "Iteration 8390, loss = 0.04110299\n",
      "Iteration 8391, loss = 0.04109358\n",
      "Iteration 8392, loss = 0.04108418\n",
      "Iteration 8393, loss = 0.04107478\n",
      "Iteration 8394, loss = 0.04106539\n",
      "Iteration 8395, loss = 0.04105600\n",
      "Iteration 8396, loss = 0.04104661\n",
      "Iteration 8397, loss = 0.04103723\n",
      "Iteration 8398, loss = 0.04102785\n",
      "Iteration 8399, loss = 0.04101848\n",
      "Iteration 8400, loss = 0.04100911\n",
      "Iteration 8401, loss = 0.04099975\n",
      "Iteration 8402, loss = 0.04099038\n",
      "Iteration 8403, loss = 0.04098103\n",
      "Iteration 8404, loss = 0.04097167\n",
      "Iteration 8405, loss = 0.04096232\n",
      "Iteration 8406, loss = 0.04095298\n",
      "Iteration 8407, loss = 0.04094363\n",
      "Iteration 8408, loss = 0.04093430\n",
      "Iteration 8409, loss = 0.04092496\n",
      "Iteration 8410, loss = 0.04091563\n",
      "Iteration 8411, loss = 0.04090631\n",
      "Iteration 8412, loss = 0.04089698\n",
      "Iteration 8413, loss = 0.04088766\n",
      "Iteration 8414, loss = 0.04087835\n",
      "Iteration 8415, loss = 0.04086904\n",
      "Iteration 8416, loss = 0.04085973\n",
      "Iteration 8417, loss = 0.04085043\n",
      "Iteration 8418, loss = 0.04084113\n",
      "Iteration 8419, loss = 0.04083184\n",
      "Iteration 8420, loss = 0.04082254\n",
      "Iteration 8421, loss = 0.04081326\n",
      "Iteration 8422, loss = 0.04080397\n",
      "Iteration 8423, loss = 0.04079469\n",
      "Iteration 8424, loss = 0.04078542\n",
      "Iteration 8425, loss = 0.04077615\n",
      "Iteration 8426, loss = 0.04076688\n",
      "Iteration 8427, loss = 0.04075762\n",
      "Iteration 8428, loss = 0.04074836\n",
      "Iteration 8429, loss = 0.04073910\n",
      "Iteration 8430, loss = 0.04072985\n",
      "Iteration 8431, loss = 0.04072060\n",
      "Iteration 8432, loss = 0.04071135\n",
      "Iteration 8433, loss = 0.04070211\n",
      "Iteration 8434, loss = 0.04069288\n",
      "Iteration 8435, loss = 0.04068364\n",
      "Iteration 8436, loss = 0.04067442\n",
      "Iteration 8437, loss = 0.04066519\n",
      "Iteration 8438, loss = 0.04065597\n",
      "Iteration 8439, loss = 0.04064675\n",
      "Iteration 8440, loss = 0.04063754\n",
      "Iteration 8441, loss = 0.04062833\n",
      "Iteration 8442, loss = 0.04061912\n",
      "Iteration 8443, loss = 0.04060992\n",
      "Iteration 8444, loss = 0.04060072\n",
      "Iteration 8445, loss = 0.04059153\n",
      "Iteration 8446, loss = 0.04058234\n",
      "Iteration 8447, loss = 0.04057315\n",
      "Iteration 8448, loss = 0.04056397\n",
      "Iteration 8449, loss = 0.04055479\n",
      "Iteration 8450, loss = 0.04054561\n",
      "Iteration 8451, loss = 0.04053644\n",
      "Iteration 8452, loss = 0.04052728\n",
      "Iteration 8453, loss = 0.04051811\n",
      "Iteration 8454, loss = 0.04050895\n",
      "Iteration 8455, loss = 0.04049980\n",
      "Iteration 8456, loss = 0.04049064\n",
      "Iteration 8457, loss = 0.04048150\n",
      "Iteration 8458, loss = 0.04047235\n",
      "Iteration 8459, loss = 0.04046321\n",
      "Iteration 8460, loss = 0.04045407\n",
      "Iteration 8461, loss = 0.04044494\n",
      "Iteration 8462, loss = 0.04043581\n",
      "Iteration 8463, loss = 0.04042669\n",
      "Iteration 8464, loss = 0.04041757\n",
      "Iteration 8465, loss = 0.04040845\n",
      "Iteration 8466, loss = 0.04039933\n",
      "Iteration 8467, loss = 0.04039022\n",
      "Iteration 8468, loss = 0.04038112\n",
      "Iteration 8469, loss = 0.04037201\n",
      "Iteration 8470, loss = 0.04036292\n",
      "Iteration 8471, loss = 0.04035382\n",
      "Iteration 8472, loss = 0.04034473\n",
      "Iteration 8473, loss = 0.04033564\n",
      "Iteration 8474, loss = 0.04032656\n",
      "Iteration 8475, loss = 0.04031748\n",
      "Iteration 8476, loss = 0.04030840\n",
      "Iteration 8477, loss = 0.04029933\n",
      "Iteration 8478, loss = 0.04029026\n",
      "Iteration 8479, loss = 0.04028120\n",
      "Iteration 8480, loss = 0.04027214\n",
      "Iteration 8481, loss = 0.04026308\n",
      "Iteration 8482, loss = 0.04025402\n",
      "Iteration 8483, loss = 0.04024497\n",
      "Iteration 8484, loss = 0.04023593\n",
      "Iteration 8485, loss = 0.04022689\n",
      "Iteration 8486, loss = 0.04021785\n",
      "Iteration 8487, loss = 0.04020881\n",
      "Iteration 8488, loss = 0.04019978\n",
      "Iteration 8489, loss = 0.04019076\n",
      "Iteration 8490, loss = 0.04018173\n",
      "Iteration 8491, loss = 0.04017271\n",
      "Iteration 8492, loss = 0.04016370\n",
      "Iteration 8493, loss = 0.04015468\n",
      "Iteration 8494, loss = 0.04014568\n",
      "Iteration 8495, loss = 0.04013667\n",
      "Iteration 8496, loss = 0.04012767\n",
      "Iteration 8497, loss = 0.04011867\n",
      "Iteration 8498, loss = 0.04010968\n",
      "Iteration 8499, loss = 0.04010069\n",
      "Iteration 8500, loss = 0.04009170\n",
      "Iteration 8501, loss = 0.04008272\n",
      "Iteration 8502, loss = 0.04007374\n",
      "Iteration 8503, loss = 0.04006477\n",
      "Iteration 8504, loss = 0.04005580\n",
      "Iteration 8505, loss = 0.04004683\n",
      "Iteration 8506, loss = 0.04003787\n",
      "Iteration 8507, loss = 0.04002891\n",
      "Iteration 8508, loss = 0.04001995\n",
      "Iteration 8509, loss = 0.04001100\n",
      "Iteration 8510, loss = 0.04000205\n",
      "Iteration 8511, loss = 0.03999310\n",
      "Iteration 8512, loss = 0.03998416\n",
      "Iteration 8513, loss = 0.03997522\n",
      "Iteration 8514, loss = 0.03996629\n",
      "Iteration 8515, loss = 0.03995736\n",
      "Iteration 8516, loss = 0.03994843\n",
      "Iteration 8517, loss = 0.03993951\n",
      "Iteration 8518, loss = 0.03993059\n",
      "Iteration 8519, loss = 0.03992167\n",
      "Iteration 8520, loss = 0.03991276\n",
      "Iteration 8521, loss = 0.03990385\n",
      "Iteration 8522, loss = 0.03989495\n",
      "Iteration 8523, loss = 0.03988605\n",
      "Iteration 8524, loss = 0.03987715\n",
      "Iteration 8525, loss = 0.03986826\n",
      "Iteration 8526, loss = 0.03985937\n",
      "Iteration 8527, loss = 0.03985048\n",
      "Iteration 8528, loss = 0.03984160\n",
      "Iteration 8529, loss = 0.03983272\n",
      "Iteration 8530, loss = 0.03982385\n",
      "Iteration 8531, loss = 0.03981497\n",
      "Iteration 8532, loss = 0.03980611\n",
      "Iteration 8533, loss = 0.03979724\n",
      "Iteration 8534, loss = 0.03978838\n",
      "Iteration 8535, loss = 0.03977952\n",
      "Iteration 8536, loss = 0.03977067\n",
      "Iteration 8537, loss = 0.03976182\n",
      "Iteration 8538, loss = 0.03975298\n",
      "Iteration 8539, loss = 0.03974413\n",
      "Iteration 8540, loss = 0.03973529\n",
      "Iteration 8541, loss = 0.03972646\n",
      "Iteration 8542, loss = 0.03971763\n",
      "Iteration 8543, loss = 0.03970880\n",
      "Iteration 8544, loss = 0.03969998\n",
      "Iteration 8545, loss = 0.03969116\n",
      "Iteration 8546, loss = 0.03968234\n",
      "Iteration 8547, loss = 0.03967353\n",
      "Iteration 8548, loss = 0.03966472\n",
      "Iteration 8549, loss = 0.03965591\n",
      "Iteration 8550, loss = 0.03964711\n",
      "Iteration 8551, loss = 0.03963831\n",
      "Iteration 8552, loss = 0.03962951\n",
      "Iteration 8553, loss = 0.03962072\n",
      "Iteration 8554, loss = 0.03961193\n",
      "Iteration 8555, loss = 0.03960315\n",
      "Iteration 8556, loss = 0.03959437\n",
      "Iteration 8557, loss = 0.03958559\n",
      "Iteration 8558, loss = 0.03957682\n",
      "Iteration 8559, loss = 0.03956805\n",
      "Iteration 8560, loss = 0.03955928\n",
      "Iteration 8561, loss = 0.03955052\n",
      "Iteration 8562, loss = 0.03954176\n",
      "Iteration 8563, loss = 0.03953301\n",
      "Iteration 8564, loss = 0.03952425\n",
      "Iteration 8565, loss = 0.03951551\n",
      "Iteration 8566, loss = 0.03950676\n",
      "Iteration 8567, loss = 0.03949802\n",
      "Iteration 8568, loss = 0.03948928\n",
      "Iteration 8569, loss = 0.03948055\n",
      "Iteration 8570, loss = 0.03947182\n",
      "Iteration 8571, loss = 0.03946309\n",
      "Iteration 8572, loss = 0.03945437\n",
      "Iteration 8573, loss = 0.03944565\n",
      "Iteration 8574, loss = 0.03943693\n",
      "Iteration 8575, loss = 0.03942822\n",
      "Iteration 8576, loss = 0.03941951\n",
      "Iteration 8577, loss = 0.03941081\n",
      "Iteration 8578, loss = 0.03940210\n",
      "Iteration 8579, loss = 0.03939341\n",
      "Iteration 8580, loss = 0.03938471\n",
      "Iteration 8581, loss = 0.03937602\n",
      "Iteration 8582, loss = 0.03936733\n",
      "Iteration 8583, loss = 0.03935865\n",
      "Iteration 8584, loss = 0.03934997\n",
      "Iteration 8585, loss = 0.03934129\n",
      "Iteration 8586, loss = 0.03933262\n",
      "Iteration 8587, loss = 0.03932395\n",
      "Iteration 8588, loss = 0.03931528\n",
      "Iteration 8589, loss = 0.03930662\n",
      "Iteration 8590, loss = 0.03929796\n",
      "Iteration 8591, loss = 0.03928931\n",
      "Iteration 8592, loss = 0.03928065\n",
      "Iteration 8593, loss = 0.03927201\n",
      "Iteration 8594, loss = 0.03926336\n",
      "Iteration 8595, loss = 0.03925472\n",
      "Iteration 8596, loss = 0.03924608\n",
      "Iteration 8597, loss = 0.03923745\n",
      "Iteration 8598, loss = 0.03922882\n",
      "Iteration 8599, loss = 0.03922019\n",
      "Iteration 8600, loss = 0.03921156\n",
      "Iteration 8601, loss = 0.03920294\n",
      "Iteration 8602, loss = 0.03919433\n",
      "Iteration 8603, loss = 0.03918571\n",
      "Iteration 8604, loss = 0.03917710\n",
      "Iteration 8605, loss = 0.03916850\n",
      "Iteration 8606, loss = 0.03915990\n",
      "Iteration 8607, loss = 0.03915130\n",
      "Iteration 8608, loss = 0.03914270\n",
      "Iteration 8609, loss = 0.03913411\n",
      "Iteration 8610, loss = 0.03912552\n",
      "Iteration 8611, loss = 0.03911693\n",
      "Iteration 8612, loss = 0.03910835\n",
      "Iteration 8613, loss = 0.03909977\n",
      "Iteration 8614, loss = 0.03909120\n",
      "Iteration 8615, loss = 0.03908263\n",
      "Iteration 8616, loss = 0.03907406\n",
      "Iteration 8617, loss = 0.03906550\n",
      "Iteration 8618, loss = 0.03905694\n",
      "Iteration 8619, loss = 0.03904838\n",
      "Iteration 8620, loss = 0.03903983\n",
      "Iteration 8621, loss = 0.03903128\n",
      "Iteration 8622, loss = 0.03902273\n",
      "Iteration 8623, loss = 0.03901418\n",
      "Iteration 8624, loss = 0.03900565\n",
      "Iteration 8625, loss = 0.03899711\n",
      "Iteration 8626, loss = 0.03898858\n",
      "Iteration 8627, loss = 0.03898005\n",
      "Iteration 8628, loss = 0.03897152\n",
      "Iteration 8629, loss = 0.03896300\n",
      "Iteration 8630, loss = 0.03895448\n",
      "Iteration 8631, loss = 0.03894596\n",
      "Iteration 8632, loss = 0.03893745\n",
      "Iteration 8633, loss = 0.03892894\n",
      "Iteration 8634, loss = 0.03892044\n",
      "Iteration 8635, loss = 0.03891194\n",
      "Iteration 8636, loss = 0.03890344\n",
      "Iteration 8637, loss = 0.03889494\n",
      "Iteration 8638, loss = 0.03888645\n",
      "Iteration 8639, loss = 0.03887796\n",
      "Iteration 8640, loss = 0.03886948\n",
      "Iteration 8641, loss = 0.03886100\n",
      "Iteration 8642, loss = 0.03885252\n",
      "Iteration 8643, loss = 0.03884405\n",
      "Iteration 8644, loss = 0.03883557\n",
      "Iteration 8645, loss = 0.03882711\n",
      "Iteration 8646, loss = 0.03881864\n",
      "Iteration 8647, loss = 0.03881018\n",
      "Iteration 8648, loss = 0.03880173\n",
      "Iteration 8649, loss = 0.03879327\n",
      "Iteration 8650, loss = 0.03878482\n",
      "Iteration 8651, loss = 0.03877638\n",
      "Iteration 8652, loss = 0.03876793\n",
      "Iteration 8653, loss = 0.03875949\n",
      "Iteration 8654, loss = 0.03875106\n",
      "Iteration 8655, loss = 0.03874262\n",
      "Iteration 8656, loss = 0.03873419\n",
      "Iteration 8657, loss = 0.03872577\n",
      "Iteration 8658, loss = 0.03871734\n",
      "Iteration 8659, loss = 0.03870892\n",
      "Iteration 8660, loss = 0.03870051\n",
      "Iteration 8661, loss = 0.03869210\n",
      "Iteration 8662, loss = 0.03868369\n",
      "Iteration 8663, loss = 0.03867528\n",
      "Iteration 8664, loss = 0.03866688\n",
      "Iteration 8665, loss = 0.03865848\n",
      "Iteration 8666, loss = 0.03865008\n",
      "Iteration 8667, loss = 0.03864169\n",
      "Iteration 8668, loss = 0.03863330\n",
      "Iteration 8669, loss = 0.03862492\n",
      "Iteration 8670, loss = 0.03861653\n",
      "Iteration 8671, loss = 0.03860816\n",
      "Iteration 8672, loss = 0.03859978\n",
      "Iteration 8673, loss = 0.03859141\n",
      "Iteration 8674, loss = 0.03858304\n",
      "Iteration 8675, loss = 0.03857467\n",
      "Iteration 8676, loss = 0.03856631\n",
      "Iteration 8677, loss = 0.03855795\n",
      "Iteration 8678, loss = 0.03854960\n",
      "Iteration 8679, loss = 0.03854125\n",
      "Iteration 8680, loss = 0.03853290\n",
      "Iteration 8681, loss = 0.03852455\n",
      "Iteration 8682, loss = 0.03851621\n",
      "Iteration 8683, loss = 0.03850787\n",
      "Iteration 8684, loss = 0.03849954\n",
      "Iteration 8685, loss = 0.03849121\n",
      "Iteration 8686, loss = 0.03848288\n",
      "Iteration 8687, loss = 0.03847455\n",
      "Iteration 8688, loss = 0.03846623\n",
      "Iteration 8689, loss = 0.03845792\n",
      "Iteration 8690, loss = 0.03844960\n",
      "Iteration 8691, loss = 0.03844129\n",
      "Iteration 8692, loss = 0.03843298\n",
      "Iteration 8693, loss = 0.03842468\n",
      "Iteration 8694, loss = 0.03841637\n",
      "Iteration 8695, loss = 0.03840808\n",
      "Iteration 8696, loss = 0.03839978\n",
      "Iteration 8697, loss = 0.03839149\n",
      "Iteration 8698, loss = 0.03838320\n",
      "Iteration 8699, loss = 0.03837492\n",
      "Iteration 8700, loss = 0.03836664\n",
      "Iteration 8701, loss = 0.03835836\n",
      "Iteration 8702, loss = 0.03835008\n",
      "Iteration 8703, loss = 0.03834181\n",
      "Iteration 8704, loss = 0.03833354\n",
      "Iteration 8705, loss = 0.03832528\n",
      "Iteration 8706, loss = 0.03831702\n",
      "Iteration 8707, loss = 0.03830876\n",
      "Iteration 8708, loss = 0.03830050\n",
      "Iteration 8709, loss = 0.03829225\n",
      "Iteration 8710, loss = 0.03828401\n",
      "Iteration 8711, loss = 0.03827576\n",
      "Iteration 8712, loss = 0.03826752\n",
      "Iteration 8713, loss = 0.03825928\n",
      "Iteration 8714, loss = 0.03825105\n",
      "Iteration 8715, loss = 0.03824281\n",
      "Iteration 8716, loss = 0.03823459\n",
      "Iteration 8717, loss = 0.03822636\n",
      "Iteration 8718, loss = 0.03821814\n",
      "Iteration 8719, loss = 0.03820992\n",
      "Iteration 8720, loss = 0.03820171\n",
      "Iteration 8721, loss = 0.03819349\n",
      "Iteration 8722, loss = 0.03818528\n",
      "Iteration 8723, loss = 0.03817708\n",
      "Iteration 8724, loss = 0.03816888\n",
      "Iteration 8725, loss = 0.03816068\n",
      "Iteration 8726, loss = 0.03815248\n",
      "Iteration 8727, loss = 0.03814429\n",
      "Iteration 8728, loss = 0.03813610\n",
      "Iteration 8729, loss = 0.03812792\n",
      "Iteration 8730, loss = 0.03811973\n",
      "Iteration 8731, loss = 0.03811156\n",
      "Iteration 8732, loss = 0.03810338\n",
      "Iteration 8733, loss = 0.03809521\n",
      "Iteration 8734, loss = 0.03808704\n",
      "Iteration 8735, loss = 0.03807887\n",
      "Iteration 8736, loss = 0.03807071\n",
      "Iteration 8737, loss = 0.03806255\n",
      "Iteration 8738, loss = 0.03805439\n",
      "Iteration 8739, loss = 0.03804624\n",
      "Iteration 8740, loss = 0.03803809\n",
      "Iteration 8741, loss = 0.03802994\n",
      "Iteration 8742, loss = 0.03802180\n",
      "Iteration 8743, loss = 0.03801366\n",
      "Iteration 8744, loss = 0.03800552\n",
      "Iteration 8745, loss = 0.03799739\n",
      "Iteration 8746, loss = 0.03798926\n",
      "Iteration 8747, loss = 0.03798113\n",
      "Iteration 8748, loss = 0.03797301\n",
      "Iteration 8749, loss = 0.03796489\n",
      "Iteration 8750, loss = 0.03795677\n",
      "Iteration 8751, loss = 0.03794866\n",
      "Iteration 8752, loss = 0.03794055\n",
      "Iteration 8753, loss = 0.03793244\n",
      "Iteration 8754, loss = 0.03792434\n",
      "Iteration 8755, loss = 0.03791624\n",
      "Iteration 8756, loss = 0.03790814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8757, loss = 0.03790004\n",
      "Iteration 8758, loss = 0.03789195\n",
      "Iteration 8759, loss = 0.03788386\n",
      "Iteration 8760, loss = 0.03787578\n",
      "Iteration 8761, loss = 0.03786770\n",
      "Iteration 8762, loss = 0.03785962\n",
      "Iteration 8763, loss = 0.03785154\n",
      "Iteration 8764, loss = 0.03784347\n",
      "Iteration 8765, loss = 0.03783540\n",
      "Iteration 8766, loss = 0.03782734\n",
      "Iteration 8767, loss = 0.03781927\n",
      "Iteration 8768, loss = 0.03781121\n",
      "Iteration 8769, loss = 0.03780316\n",
      "Iteration 8770, loss = 0.03779511\n",
      "Iteration 8771, loss = 0.03778706\n",
      "Iteration 8772, loss = 0.03777901\n",
      "Iteration 8773, loss = 0.03777097\n",
      "Iteration 8774, loss = 0.03776293\n",
      "Iteration 8775, loss = 0.03775489\n",
      "Iteration 8776, loss = 0.03774686\n",
      "Iteration 8777, loss = 0.03773883\n",
      "Iteration 8778, loss = 0.03773080\n",
      "Iteration 8779, loss = 0.03772277\n",
      "Iteration 8780, loss = 0.03771475\n",
      "Iteration 8781, loss = 0.03770674\n",
      "Iteration 8782, loss = 0.03769872\n",
      "Iteration 8783, loss = 0.03769071\n",
      "Iteration 8784, loss = 0.03768270\n",
      "Iteration 8785, loss = 0.03767470\n",
      "Iteration 8786, loss = 0.03766670\n",
      "Iteration 8787, loss = 0.03765870\n",
      "Iteration 8788, loss = 0.03765070\n",
      "Iteration 8789, loss = 0.03764271\n",
      "Iteration 8790, loss = 0.03763472\n",
      "Iteration 8791, loss = 0.03762673\n",
      "Iteration 8792, loss = 0.03761875\n",
      "Iteration 8793, loss = 0.03761077\n",
      "Iteration 8794, loss = 0.03760280\n",
      "Iteration 8795, loss = 0.03759482\n",
      "Iteration 8796, loss = 0.03758685\n",
      "Iteration 8797, loss = 0.03757889\n",
      "Iteration 8798, loss = 0.03757092\n",
      "Iteration 8799, loss = 0.03756296\n",
      "Iteration 8800, loss = 0.03755500\n",
      "Iteration 8801, loss = 0.03754705\n",
      "Iteration 8802, loss = 0.03753910\n",
      "Iteration 8803, loss = 0.03753115\n",
      "Iteration 8804, loss = 0.03752321\n",
      "Iteration 8805, loss = 0.03751526\n",
      "Iteration 8806, loss = 0.03750732\n",
      "Iteration 8807, loss = 0.03749939\n",
      "Iteration 8808, loss = 0.03749146\n",
      "Iteration 8809, loss = 0.03748353\n",
      "Iteration 8810, loss = 0.03747560\n",
      "Iteration 8811, loss = 0.03746768\n",
      "Iteration 8812, loss = 0.03745976\n",
      "Iteration 8813, loss = 0.03745184\n",
      "Iteration 8814, loss = 0.03744393\n",
      "Iteration 8815, loss = 0.03743602\n",
      "Iteration 8816, loss = 0.03742811\n",
      "Iteration 8817, loss = 0.03742021\n",
      "Iteration 8818, loss = 0.03741231\n",
      "Iteration 8819, loss = 0.03740441\n",
      "Iteration 8820, loss = 0.03739651\n",
      "Iteration 8821, loss = 0.03738862\n",
      "Iteration 8822, loss = 0.03738073\n",
      "Iteration 8823, loss = 0.03737285\n",
      "Iteration 8824, loss = 0.03736497\n",
      "Iteration 8825, loss = 0.03735709\n",
      "Iteration 8826, loss = 0.03734921\n",
      "Iteration 8827, loss = 0.03734134\n",
      "Iteration 8828, loss = 0.03733347\n",
      "Iteration 8829, loss = 0.03732560\n",
      "Iteration 8830, loss = 0.03731774\n",
      "Iteration 8831, loss = 0.03730988\n",
      "Iteration 8832, loss = 0.03730202\n",
      "Iteration 8833, loss = 0.03729416\n",
      "Iteration 8834, loss = 0.03728631\n",
      "Iteration 8835, loss = 0.03727846\n",
      "Iteration 8836, loss = 0.03727062\n",
      "Iteration 8837, loss = 0.03726278\n",
      "Iteration 8838, loss = 0.03725494\n",
      "Iteration 8839, loss = 0.03724710\n",
      "Iteration 8840, loss = 0.03723927\n",
      "Iteration 8841, loss = 0.03723144\n",
      "Iteration 8842, loss = 0.03722361\n",
      "Iteration 8843, loss = 0.03721579\n",
      "Iteration 8844, loss = 0.03720797\n",
      "Iteration 8845, loss = 0.03720015\n",
      "Iteration 8846, loss = 0.03719234\n",
      "Iteration 8847, loss = 0.03718453\n",
      "Iteration 8848, loss = 0.03717672\n",
      "Iteration 8849, loss = 0.03716891\n",
      "Iteration 8850, loss = 0.03716111\n",
      "Iteration 8851, loss = 0.03715331\n",
      "Iteration 8852, loss = 0.03714552\n",
      "Iteration 8853, loss = 0.03713772\n",
      "Iteration 8854, loss = 0.03712993\n",
      "Iteration 8855, loss = 0.03712215\n",
      "Iteration 8856, loss = 0.03711436\n",
      "Iteration 8857, loss = 0.03710658\n",
      "Iteration 8858, loss = 0.03709880\n",
      "Iteration 8859, loss = 0.03709103\n",
      "Iteration 8860, loss = 0.03708326\n",
      "Iteration 8861, loss = 0.03707549\n",
      "Iteration 8862, loss = 0.03706772\n",
      "Iteration 8863, loss = 0.03705996\n",
      "Iteration 8864, loss = 0.03705220\n",
      "Iteration 8865, loss = 0.03704445\n",
      "Iteration 8866, loss = 0.03703669\n",
      "Iteration 8867, loss = 0.03702894\n",
      "Iteration 8868, loss = 0.03702120\n",
      "Iteration 8869, loss = 0.03701345\n",
      "Iteration 8870, loss = 0.03700571\n",
      "Iteration 8871, loss = 0.03699797\n",
      "Iteration 8872, loss = 0.03699024\n",
      "Iteration 8873, loss = 0.03698251\n",
      "Iteration 8874, loss = 0.03697478\n",
      "Iteration 8875, loss = 0.03696705\n",
      "Iteration 8876, loss = 0.03695933\n",
      "Iteration 8877, loss = 0.03695161\n",
      "Iteration 8878, loss = 0.03694389\n",
      "Iteration 8879, loss = 0.03693618\n",
      "Iteration 8880, loss = 0.03692847\n",
      "Iteration 8881, loss = 0.03692076\n",
      "Iteration 8882, loss = 0.03691305\n",
      "Iteration 8883, loss = 0.03690535\n",
      "Iteration 8884, loss = 0.03689765\n",
      "Iteration 8885, loss = 0.03688996\n",
      "Iteration 8886, loss = 0.03688226\n",
      "Iteration 8887, loss = 0.03687457\n",
      "Iteration 8888, loss = 0.03686689\n",
      "Iteration 8889, loss = 0.03685920\n",
      "Iteration 8890, loss = 0.03685152\n",
      "Iteration 8891, loss = 0.03684385\n",
      "Iteration 8892, loss = 0.03683617\n",
      "Iteration 8893, loss = 0.03682850\n",
      "Iteration 8894, loss = 0.03682083\n",
      "Iteration 8895, loss = 0.03681316\n",
      "Iteration 8896, loss = 0.03680550\n",
      "Iteration 8897, loss = 0.03679784\n",
      "Iteration 8898, loss = 0.03679019\n",
      "Iteration 8899, loss = 0.03678253\n",
      "Iteration 8900, loss = 0.03677488\n",
      "Iteration 8901, loss = 0.03676723\n",
      "Iteration 8902, loss = 0.03675959\n",
      "Iteration 8903, loss = 0.03675195\n",
      "Iteration 8904, loss = 0.03674431\n",
      "Iteration 8905, loss = 0.03673667\n",
      "Iteration 8906, loss = 0.03672904\n",
      "Iteration 8907, loss = 0.03672141\n",
      "Iteration 8908, loss = 0.03671378\n",
      "Iteration 8909, loss = 0.03670616\n",
      "Iteration 8910, loss = 0.03669854\n",
      "Iteration 8911, loss = 0.03669092\n",
      "Iteration 8912, loss = 0.03668330\n",
      "Iteration 8913, loss = 0.03667569\n",
      "Iteration 8914, loss = 0.03666808\n",
      "Iteration 8915, loss = 0.03666048\n",
      "Iteration 8916, loss = 0.03665287\n",
      "Iteration 8917, loss = 0.03664527\n",
      "Iteration 8918, loss = 0.03663767\n",
      "Iteration 8919, loss = 0.03663008\n",
      "Iteration 8920, loss = 0.03662249\n",
      "Iteration 8921, loss = 0.03661490\n",
      "Iteration 8922, loss = 0.03660731\n",
      "Iteration 8923, loss = 0.03659973\n",
      "Iteration 8924, loss = 0.03659215\n",
      "Iteration 8925, loss = 0.03658458\n",
      "Iteration 8926, loss = 0.03657700\n",
      "Iteration 8927, loss = 0.03656943\n",
      "Iteration 8928, loss = 0.03656186\n",
      "Iteration 8929, loss = 0.03655430\n",
      "Iteration 8930, loss = 0.03654674\n",
      "Iteration 8931, loss = 0.03653918\n",
      "Iteration 8932, loss = 0.03653162\n",
      "Iteration 8933, loss = 0.03652407\n",
      "Iteration 8934, loss = 0.03651652\n",
      "Iteration 8935, loss = 0.03650897\n",
      "Iteration 8936, loss = 0.03650143\n",
      "Iteration 8937, loss = 0.03649388\n",
      "Iteration 8938, loss = 0.03648635\n",
      "Iteration 8939, loss = 0.03647881\n",
      "Iteration 8940, loss = 0.03647128\n",
      "Iteration 8941, loss = 0.03646375\n",
      "Iteration 8942, loss = 0.03645622\n",
      "Iteration 8943, loss = 0.03644870\n",
      "Iteration 8944, loss = 0.03644118\n",
      "Iteration 8945, loss = 0.03643366\n",
      "Iteration 8946, loss = 0.03642614\n",
      "Iteration 8947, loss = 0.03641863\n",
      "Iteration 8948, loss = 0.03641112\n",
      "Iteration 8949, loss = 0.03640361\n",
      "Iteration 8950, loss = 0.03639611\n",
      "Iteration 8951, loss = 0.03638861\n",
      "Iteration 8952, loss = 0.03638111\n",
      "Iteration 8953, loss = 0.03637362\n",
      "Iteration 8954, loss = 0.03636613\n",
      "Iteration 8955, loss = 0.03635864\n",
      "Iteration 8956, loss = 0.03635115\n",
      "Iteration 8957, loss = 0.03634367\n",
      "Iteration 8958, loss = 0.03633619\n",
      "Iteration 8959, loss = 0.03632871\n",
      "Iteration 8960, loss = 0.03632123\n",
      "Iteration 8961, loss = 0.03631376\n",
      "Iteration 8962, loss = 0.03630629\n",
      "Iteration 8963, loss = 0.03629883\n",
      "Iteration 8964, loss = 0.03629136\n",
      "Iteration 8965, loss = 0.03628390\n",
      "Iteration 8966, loss = 0.03627645\n",
      "Iteration 8967, loss = 0.03626899\n",
      "Iteration 8968, loss = 0.03626154\n",
      "Iteration 8969, loss = 0.03625409\n",
      "Iteration 8970, loss = 0.03624665\n",
      "Iteration 8971, loss = 0.03623920\n",
      "Iteration 8972, loss = 0.03623176\n",
      "Iteration 8973, loss = 0.03622433\n",
      "Iteration 8974, loss = 0.03621689\n",
      "Iteration 8975, loss = 0.03620946\n",
      "Iteration 8976, loss = 0.03620203\n",
      "Iteration 8977, loss = 0.03619461\n",
      "Iteration 8978, loss = 0.03618718\n",
      "Iteration 8979, loss = 0.03617976\n",
      "Iteration 8980, loss = 0.03617235\n",
      "Iteration 8981, loss = 0.03616493\n",
      "Iteration 8982, loss = 0.03615752\n",
      "Iteration 8983, loss = 0.03615011\n",
      "Iteration 8984, loss = 0.03614271\n",
      "Iteration 8985, loss = 0.03613530\n",
      "Iteration 8986, loss = 0.03612790\n",
      "Iteration 8987, loss = 0.03612051\n",
      "Iteration 8988, loss = 0.03611311\n",
      "Iteration 8989, loss = 0.03610572\n",
      "Iteration 8990, loss = 0.03609833\n",
      "Iteration 8991, loss = 0.03609095\n",
      "Iteration 8992, loss = 0.03608356\n",
      "Iteration 8993, loss = 0.03607618\n",
      "Iteration 8994, loss = 0.03606881\n",
      "Iteration 8995, loss = 0.03606143\n",
      "Iteration 8996, loss = 0.03605406\n",
      "Iteration 8997, loss = 0.03604669\n",
      "Iteration 8998, loss = 0.03603932\n",
      "Iteration 8999, loss = 0.03603196\n",
      "Iteration 9000, loss = 0.03602460\n",
      "Iteration 9001, loss = 0.03601724\n",
      "Iteration 9002, loss = 0.03600989\n",
      "Iteration 9003, loss = 0.03600254\n",
      "Iteration 9004, loss = 0.03599519\n",
      "Iteration 9005, loss = 0.03598784\n",
      "Iteration 9006, loss = 0.03598050\n",
      "Iteration 9007, loss = 0.03597316\n",
      "Iteration 9008, loss = 0.03596582\n",
      "Iteration 9009, loss = 0.03595849\n",
      "Iteration 9010, loss = 0.03595115\n",
      "Iteration 9011, loss = 0.03594383\n",
      "Iteration 9012, loss = 0.03593650\n",
      "Iteration 9013, loss = 0.03592918\n",
      "Iteration 9014, loss = 0.03592185\n",
      "Iteration 9015, loss = 0.03591454\n",
      "Iteration 9016, loss = 0.03590722\n",
      "Iteration 9017, loss = 0.03589991\n",
      "Iteration 9018, loss = 0.03589260\n",
      "Iteration 9019, loss = 0.03588529\n",
      "Iteration 9020, loss = 0.03587799\n",
      "Iteration 9021, loss = 0.03587069\n",
      "Iteration 9022, loss = 0.03586339\n",
      "Iteration 9023, loss = 0.03585609\n",
      "Iteration 9024, loss = 0.03584880\n",
      "Iteration 9025, loss = 0.03584151\n",
      "Iteration 9026, loss = 0.03583422\n",
      "Iteration 9027, loss = 0.03582694\n",
      "Iteration 9028, loss = 0.03581966\n",
      "Iteration 9029, loss = 0.03581238\n",
      "Iteration 9030, loss = 0.03580510\n",
      "Iteration 9031, loss = 0.03579783\n",
      "Iteration 9032, loss = 0.03579056\n",
      "Iteration 9033, loss = 0.03578329\n",
      "Iteration 9034, loss = 0.03577603\n",
      "Iteration 9035, loss = 0.03576876\n",
      "Iteration 9036, loss = 0.03576150\n",
      "Iteration 9037, loss = 0.03575425\n",
      "Iteration 9038, loss = 0.03574699\n",
      "Iteration 9039, loss = 0.03573974\n",
      "Iteration 9040, loss = 0.03573249\n",
      "Iteration 9041, loss = 0.03572525\n",
      "Iteration 9042, loss = 0.03571801\n",
      "Iteration 9043, loss = 0.03571077\n",
      "Iteration 9044, loss = 0.03570353\n",
      "Iteration 9045, loss = 0.03569629\n",
      "Iteration 9046, loss = 0.03568906\n",
      "Iteration 9047, loss = 0.03568183\n",
      "Iteration 9048, loss = 0.03567461\n",
      "Iteration 9049, loss = 0.03566738\n",
      "Iteration 9050, loss = 0.03566016\n",
      "Iteration 9051, loss = 0.03565295\n",
      "Iteration 9052, loss = 0.03564573\n",
      "Iteration 9053, loss = 0.03563852\n",
      "Iteration 9054, loss = 0.03563131\n",
      "Iteration 9055, loss = 0.03562410\n",
      "Iteration 9056, loss = 0.03561690\n",
      "Iteration 9057, loss = 0.03560970\n",
      "Iteration 9058, loss = 0.03560250\n",
      "Iteration 9059, loss = 0.03559530\n",
      "Iteration 9060, loss = 0.03558811\n",
      "Iteration 9061, loss = 0.03558092\n",
      "Iteration 9062, loss = 0.03557373\n",
      "Iteration 9063, loss = 0.03556654\n",
      "Iteration 9064, loss = 0.03555936\n",
      "Iteration 9065, loss = 0.03555218\n",
      "Iteration 9066, loss = 0.03554501\n",
      "Iteration 9067, loss = 0.03553783\n",
      "Iteration 9068, loss = 0.03553066\n",
      "Iteration 9069, loss = 0.03552349\n",
      "Iteration 9070, loss = 0.03551633\n",
      "Iteration 9071, loss = 0.03550916\n",
      "Iteration 9072, loss = 0.03550200\n",
      "Iteration 9073, loss = 0.03549484\n",
      "Iteration 9074, loss = 0.03548769\n",
      "Iteration 9075, loss = 0.03548054\n",
      "Iteration 9076, loss = 0.03547339\n",
      "Iteration 9077, loss = 0.03546624\n",
      "Iteration 9078, loss = 0.03545910\n",
      "Iteration 9079, loss = 0.03545196\n",
      "Iteration 9080, loss = 0.03544482\n",
      "Iteration 9081, loss = 0.03543768\n",
      "Iteration 9082, loss = 0.03543055\n",
      "Iteration 9083, loss = 0.03542342\n",
      "Iteration 9084, loss = 0.03541629\n",
      "Iteration 9085, loss = 0.03540916\n",
      "Iteration 9086, loss = 0.03540204\n",
      "Iteration 9087, loss = 0.03539492\n",
      "Iteration 9088, loss = 0.03538780\n",
      "Iteration 9089, loss = 0.03538069\n",
      "Iteration 9090, loss = 0.03537358\n",
      "Iteration 9091, loss = 0.03536647\n",
      "Iteration 9092, loss = 0.03535936\n",
      "Iteration 9093, loss = 0.03535226\n",
      "Iteration 9094, loss = 0.03534516\n",
      "Iteration 9095, loss = 0.03533806\n",
      "Iteration 9096, loss = 0.03533097\n",
      "Iteration 9097, loss = 0.03532387\n",
      "Iteration 9098, loss = 0.03531678\n",
      "Iteration 9099, loss = 0.03530970\n",
      "Iteration 9100, loss = 0.03530261\n",
      "Iteration 9101, loss = 0.03529553\n",
      "Iteration 9102, loss = 0.03528845\n",
      "Iteration 9103, loss = 0.03528137\n",
      "Iteration 9104, loss = 0.03527430\n",
      "Iteration 9105, loss = 0.03526723\n",
      "Iteration 9106, loss = 0.03526016\n",
      "Iteration 9107, loss = 0.03525309\n",
      "Iteration 9108, loss = 0.03524603\n",
      "Iteration 9109, loss = 0.03523897\n",
      "Iteration 9110, loss = 0.03523191\n",
      "Iteration 9111, loss = 0.03522485\n",
      "Iteration 9112, loss = 0.03521780\n",
      "Iteration 9113, loss = 0.03521075\n",
      "Iteration 9114, loss = 0.03520370\n",
      "Iteration 9115, loss = 0.03519666\n",
      "Iteration 9116, loss = 0.03518962\n",
      "Iteration 9117, loss = 0.03518258\n",
      "Iteration 9118, loss = 0.03517554\n",
      "Iteration 9119, loss = 0.03516851\n",
      "Iteration 9120, loss = 0.03516148\n",
      "Iteration 9121, loss = 0.03515445\n",
      "Iteration 9122, loss = 0.03514742\n",
      "Iteration 9123, loss = 0.03514040\n",
      "Iteration 9124, loss = 0.03513338\n",
      "Iteration 9125, loss = 0.03512636\n",
      "Iteration 9126, loss = 0.03511934\n",
      "Iteration 9127, loss = 0.03511233\n",
      "Iteration 9128, loss = 0.03510532\n",
      "Iteration 9129, loss = 0.03509831\n",
      "Iteration 9130, loss = 0.03509131\n",
      "Iteration 9131, loss = 0.03508431\n",
      "Iteration 9132, loss = 0.03507731\n",
      "Iteration 9133, loss = 0.03507031\n",
      "Iteration 9134, loss = 0.03506332\n",
      "Iteration 9135, loss = 0.03505632\n",
      "Iteration 9136, loss = 0.03504934\n",
      "Iteration 9137, loss = 0.03504235\n",
      "Iteration 9138, loss = 0.03503536\n",
      "Iteration 9139, loss = 0.03502838\n",
      "Iteration 9140, loss = 0.03502141\n",
      "Iteration 9141, loss = 0.03501443\n",
      "Iteration 9142, loss = 0.03500746\n",
      "Iteration 9143, loss = 0.03500049\n",
      "Iteration 9144, loss = 0.03499352\n",
      "Iteration 9145, loss = 0.03498655\n",
      "Iteration 9146, loss = 0.03497959\n",
      "Iteration 9147, loss = 0.03497263\n",
      "Iteration 9148, loss = 0.03496567\n",
      "Iteration 9149, loss = 0.03495872\n",
      "Iteration 9150, loss = 0.03495176\n",
      "Iteration 9151, loss = 0.03494481\n",
      "Iteration 9152, loss = 0.03493787\n",
      "Iteration 9153, loss = 0.03493092\n",
      "Iteration 9154, loss = 0.03492398\n",
      "Iteration 9155, loss = 0.03491704\n",
      "Iteration 9156, loss = 0.03491011\n",
      "Iteration 9157, loss = 0.03490317\n",
      "Iteration 9158, loss = 0.03489624\n",
      "Iteration 9159, loss = 0.03488931\n",
      "Iteration 9160, loss = 0.03488238\n",
      "Iteration 9161, loss = 0.03487546\n",
      "Iteration 9162, loss = 0.03486854\n",
      "Iteration 9163, loss = 0.03486162\n",
      "Iteration 9164, loss = 0.03485471\n",
      "Iteration 9165, loss = 0.03484779\n",
      "Iteration 9166, loss = 0.03484088\n",
      "Iteration 9167, loss = 0.03483397\n",
      "Iteration 9168, loss = 0.03482707\n",
      "Iteration 9169, loss = 0.03482017\n",
      "Iteration 9170, loss = 0.03481326\n",
      "Iteration 9171, loss = 0.03480637\n",
      "Iteration 9172, loss = 0.03479947\n",
      "Iteration 9173, loss = 0.03479258\n",
      "Iteration 9174, loss = 0.03478569\n",
      "Iteration 9175, loss = 0.03477880\n",
      "Iteration 9176, loss = 0.03477192\n",
      "Iteration 9177, loss = 0.03476503\n",
      "Iteration 9178, loss = 0.03475816\n",
      "Iteration 9179, loss = 0.03475128\n",
      "Iteration 9180, loss = 0.03474440\n",
      "Iteration 9181, loss = 0.03473753\n",
      "Iteration 9182, loss = 0.03473066\n",
      "Iteration 9183, loss = 0.03472380\n",
      "Iteration 9184, loss = 0.03471693\n",
      "Iteration 9185, loss = 0.03471007\n",
      "Iteration 9186, loss = 0.03470321\n",
      "Iteration 9187, loss = 0.03469635\n",
      "Iteration 9188, loss = 0.03468950\n",
      "Iteration 9189, loss = 0.03468265\n",
      "Iteration 9190, loss = 0.03467580\n",
      "Iteration 9191, loss = 0.03466895\n",
      "Iteration 9192, loss = 0.03466211\n",
      "Iteration 9193, loss = 0.03465527\n",
      "Iteration 9194, loss = 0.03464843\n",
      "Iteration 9195, loss = 0.03464159\n",
      "Iteration 9196, loss = 0.03463476\n",
      "Iteration 9197, loss = 0.03462793\n",
      "Iteration 9198, loss = 0.03462110\n",
      "Iteration 9199, loss = 0.03461428\n",
      "Iteration 9200, loss = 0.03460745\n",
      "Iteration 9201, loss = 0.03460063\n",
      "Iteration 9202, loss = 0.03459381\n",
      "Iteration 9203, loss = 0.03458700\n",
      "Iteration 9204, loss = 0.03458018\n",
      "Iteration 9205, loss = 0.03457337\n",
      "Iteration 9206, loss = 0.03456657\n",
      "Iteration 9207, loss = 0.03455976\n",
      "Iteration 9208, loss = 0.03455296\n",
      "Iteration 9209, loss = 0.03454616\n",
      "Iteration 9210, loss = 0.03453936\n",
      "Iteration 9211, loss = 0.03453256\n",
      "Iteration 9212, loss = 0.03452577\n",
      "Iteration 9213, loss = 0.03451898\n",
      "Iteration 9214, loss = 0.03451219\n",
      "Iteration 9215, loss = 0.03450541\n",
      "Iteration 9216, loss = 0.03449862\n",
      "Iteration 9217, loss = 0.03449184\n",
      "Iteration 9218, loss = 0.03448507\n",
      "Iteration 9219, loss = 0.03447829\n",
      "Iteration 9220, loss = 0.03447152\n",
      "Iteration 9221, loss = 0.03446475\n",
      "Iteration 9222, loss = 0.03445798\n",
      "Iteration 9223, loss = 0.03445122\n",
      "Iteration 9224, loss = 0.03444445\n",
      "Iteration 9225, loss = 0.03443769\n",
      "Iteration 9226, loss = 0.03443094\n",
      "Iteration 9227, loss = 0.03442418\n",
      "Iteration 9228, loss = 0.03441743\n",
      "Iteration 9229, loss = 0.03441068\n",
      "Iteration 9230, loss = 0.03440393\n",
      "Iteration 9231, loss = 0.03439719\n",
      "Iteration 9232, loss = 0.03439044\n",
      "Iteration 9233, loss = 0.03438370\n",
      "Iteration 9234, loss = 0.03437697\n",
      "Iteration 9235, loss = 0.03437023\n",
      "Iteration 9236, loss = 0.03436350\n",
      "Iteration 9237, loss = 0.03435677\n",
      "Iteration 9238, loss = 0.03435004\n",
      "Iteration 9239, loss = 0.03434332\n",
      "Iteration 9240, loss = 0.03433659\n",
      "Iteration 9241, loss = 0.03432987\n",
      "Iteration 9242, loss = 0.03432316\n",
      "Iteration 9243, loss = 0.03431644\n",
      "Iteration 9244, loss = 0.03430973\n",
      "Iteration 9245, loss = 0.03430302\n",
      "Iteration 9246, loss = 0.03429631\n",
      "Iteration 9247, loss = 0.03428961\n",
      "Iteration 9248, loss = 0.03428290\n",
      "Iteration 9249, loss = 0.03427620\n",
      "Iteration 9250, loss = 0.03426951\n",
      "Iteration 9251, loss = 0.03426281\n",
      "Iteration 9252, loss = 0.03425612\n",
      "Iteration 9253, loss = 0.03424943\n",
      "Iteration 9254, loss = 0.03424274\n",
      "Iteration 9255, loss = 0.03423606\n",
      "Iteration 9256, loss = 0.03422937\n",
      "Iteration 9257, loss = 0.03422269\n",
      "Iteration 9258, loss = 0.03421601\n",
      "Iteration 9259, loss = 0.03420934\n",
      "Iteration 9260, loss = 0.03420267\n",
      "Iteration 9261, loss = 0.03419600\n",
      "Iteration 9262, loss = 0.03418933\n",
      "Iteration 9263, loss = 0.03418266\n",
      "Iteration 9264, loss = 0.03417600\n",
      "Iteration 9265, loss = 0.03416934\n",
      "Iteration 9266, loss = 0.03416268\n",
      "Iteration 9267, loss = 0.03415603\n",
      "Iteration 9268, loss = 0.03414937\n",
      "Iteration 9269, loss = 0.03414272\n",
      "Iteration 9270, loss = 0.03413607\n",
      "Iteration 9271, loss = 0.03412943\n",
      "Iteration 9272, loss = 0.03412279\n",
      "Iteration 9273, loss = 0.03411614\n",
      "Iteration 9274, loss = 0.03410951\n",
      "Iteration 9275, loss = 0.03410287\n",
      "Iteration 9276, loss = 0.03409624\n",
      "Iteration 9277, loss = 0.03408961\n",
      "Iteration 9278, loss = 0.03408298\n",
      "Iteration 9279, loss = 0.03407635\n",
      "Iteration 9280, loss = 0.03406973\n",
      "Iteration 9281, loss = 0.03406311\n",
      "Iteration 9282, loss = 0.03405649\n",
      "Iteration 9283, loss = 0.03404987\n",
      "Iteration 9284, loss = 0.03404326\n",
      "Iteration 9285, loss = 0.03403665\n",
      "Iteration 9286, loss = 0.03403004\n",
      "Iteration 9287, loss = 0.03402343\n",
      "Iteration 9288, loss = 0.03401683\n",
      "Iteration 9289, loss = 0.03401022\n",
      "Iteration 9290, loss = 0.03400362\n",
      "Iteration 9291, loss = 0.03399703\n",
      "Iteration 9292, loss = 0.03399043\n",
      "Iteration 9293, loss = 0.03398384\n",
      "Iteration 9294, loss = 0.03397725\n",
      "Iteration 9295, loss = 0.03397066\n",
      "Iteration 9296, loss = 0.03396408\n",
      "Iteration 9297, loss = 0.03395750\n",
      "Iteration 9298, loss = 0.03395092\n",
      "Iteration 9299, loss = 0.03394434\n",
      "Iteration 9300, loss = 0.03393777\n",
      "Iteration 9301, loss = 0.03393119\n",
      "Iteration 9302, loss = 0.03392462\n",
      "Iteration 9303, loss = 0.03391805\n",
      "Iteration 9304, loss = 0.03391149\n",
      "Iteration 9305, loss = 0.03390493\n",
      "Iteration 9306, loss = 0.03389837\n",
      "Iteration 9307, loss = 0.03389181\n",
      "Iteration 9308, loss = 0.03388525\n",
      "Iteration 9309, loss = 0.03387870\n",
      "Iteration 9310, loss = 0.03387215\n",
      "Iteration 9311, loss = 0.03386560\n",
      "Iteration 9312, loss = 0.03385905\n",
      "Iteration 9313, loss = 0.03385251\n",
      "Iteration 9314, loss = 0.03384597\n",
      "Iteration 9315, loss = 0.03383943\n",
      "Iteration 9316, loss = 0.03383289\n",
      "Iteration 9317, loss = 0.03382636\n",
      "Iteration 9318, loss = 0.03381983\n",
      "Iteration 9319, loss = 0.03381330\n",
      "Iteration 9320, loss = 0.03380677\n",
      "Iteration 9321, loss = 0.03380025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9322, loss = 0.03379372\n",
      "Iteration 9323, loss = 0.03378720\n",
      "Iteration 9324, loss = 0.03378069\n",
      "Iteration 9325, loss = 0.03377417\n",
      "Iteration 9326, loss = 0.03376766\n",
      "Iteration 9327, loss = 0.03376115\n",
      "Iteration 9328, loss = 0.03375464\n",
      "Iteration 9329, loss = 0.03374814\n",
      "Iteration 9330, loss = 0.03374163\n",
      "Iteration 9331, loss = 0.03373513\n",
      "Iteration 9332, loss = 0.03372863\n",
      "Iteration 9333, loss = 0.03372214\n",
      "Iteration 9334, loss = 0.03371565\n",
      "Iteration 9335, loss = 0.03370915\n",
      "Iteration 9336, loss = 0.03370267\n",
      "Iteration 9337, loss = 0.03369618\n",
      "Iteration 9338, loss = 0.03368970\n",
      "Iteration 9339, loss = 0.03368321\n",
      "Iteration 9340, loss = 0.03367673\n",
      "Iteration 9341, loss = 0.03367026\n",
      "Iteration 9342, loss = 0.03366378\n",
      "Iteration 9343, loss = 0.03365731\n",
      "Iteration 9344, loss = 0.03365084\n",
      "Iteration 9345, loss = 0.03364437\n",
      "Iteration 9346, loss = 0.03363791\n",
      "Iteration 9347, loss = 0.03363145\n",
      "Iteration 9348, loss = 0.03362499\n",
      "Iteration 9349, loss = 0.03361853\n",
      "Iteration 9350, loss = 0.03361207\n",
      "Iteration 9351, loss = 0.03360562\n",
      "Iteration 9352, loss = 0.03359917\n",
      "Iteration 9353, loss = 0.03359272\n",
      "Iteration 9354, loss = 0.03358627\n",
      "Iteration 9355, loss = 0.03357983\n",
      "Iteration 9356, loss = 0.03357339\n",
      "Iteration 9357, loss = 0.03356695\n",
      "Iteration 9358, loss = 0.03356051\n",
      "Iteration 9359, loss = 0.03355408\n",
      "Iteration 9360, loss = 0.03354765\n",
      "Iteration 9361, loss = 0.03354122\n",
      "Iteration 9362, loss = 0.03353479\n",
      "Iteration 9363, loss = 0.03352836\n",
      "Iteration 9364, loss = 0.03352194\n",
      "Iteration 9365, loss = 0.03351552\n",
      "Iteration 9366, loss = 0.03350910\n",
      "Iteration 9367, loss = 0.03350269\n",
      "Iteration 9368, loss = 0.03349627\n",
      "Iteration 9369, loss = 0.03348986\n",
      "Iteration 9370, loss = 0.03348345\n",
      "Iteration 9371, loss = 0.03347705\n",
      "Iteration 9372, loss = 0.03347064\n",
      "Iteration 9373, loss = 0.03346424\n",
      "Iteration 9374, loss = 0.03345784\n",
      "Iteration 9375, loss = 0.03345145\n",
      "Iteration 9376, loss = 0.03344505\n",
      "Iteration 9377, loss = 0.03343866\n",
      "Iteration 9378, loss = 0.03343227\n",
      "Iteration 9379, loss = 0.03342588\n",
      "Iteration 9380, loss = 0.03341950\n",
      "Iteration 9381, loss = 0.03341311\n",
      "Iteration 9382, loss = 0.03340673\n",
      "Iteration 9383, loss = 0.03340035\n",
      "Iteration 9384, loss = 0.03339398\n",
      "Iteration 9385, loss = 0.03338760\n",
      "Iteration 9386, loss = 0.03338123\n",
      "Iteration 9387, loss = 0.03337486\n",
      "Iteration 9388, loss = 0.03336850\n",
      "Iteration 9389, loss = 0.03336213\n",
      "Iteration 9390, loss = 0.03335577\n",
      "Iteration 9391, loss = 0.03334941\n",
      "Iteration 9392, loss = 0.03334305\n",
      "Iteration 9393, loss = 0.03333670\n",
      "Iteration 9394, loss = 0.03333035\n",
      "Iteration 9395, loss = 0.03332399\n",
      "Iteration 9396, loss = 0.03331765\n",
      "Iteration 9397, loss = 0.03331130\n",
      "Iteration 9398, loss = 0.03330496\n",
      "Iteration 9399, loss = 0.03329862\n",
      "Iteration 9400, loss = 0.03329228\n",
      "Iteration 9401, loss = 0.03328594\n",
      "Iteration 9402, loss = 0.03327960\n",
      "Iteration 9403, loss = 0.03327327\n",
      "Iteration 9404, loss = 0.03326694\n",
      "Iteration 9405, loss = 0.03326062\n",
      "Iteration 9406, loss = 0.03325429\n",
      "Iteration 9407, loss = 0.03324797\n",
      "Iteration 9408, loss = 0.03324165\n",
      "Iteration 9409, loss = 0.03323533\n",
      "Iteration 9410, loss = 0.03322901\n",
      "Iteration 9411, loss = 0.03322270\n",
      "Iteration 9412, loss = 0.03321639\n",
      "Iteration 9413, loss = 0.03321008\n",
      "Iteration 9414, loss = 0.03320377\n",
      "Iteration 9415, loss = 0.03319746\n",
      "Iteration 9416, loss = 0.03319116\n",
      "Iteration 9417, loss = 0.03318486\n",
      "Iteration 9418, loss = 0.03317856\n",
      "Iteration 9419, loss = 0.03317227\n",
      "Iteration 9420, loss = 0.03316598\n",
      "Iteration 9421, loss = 0.03315968\n",
      "Iteration 9422, loss = 0.03315340\n",
      "Iteration 9423, loss = 0.03314711\n",
      "Iteration 9424, loss = 0.03314082\n",
      "Iteration 9425, loss = 0.03313454\n",
      "Iteration 9426, loss = 0.03312826\n",
      "Iteration 9427, loss = 0.03312199\n",
      "Iteration 9428, loss = 0.03311571\n",
      "Iteration 9429, loss = 0.03310944\n",
      "Iteration 9430, loss = 0.03310317\n",
      "Iteration 9431, loss = 0.03309690\n",
      "Iteration 9432, loss = 0.03309063\n",
      "Iteration 9433, loss = 0.03308437\n",
      "Iteration 9434, loss = 0.03307811\n",
      "Iteration 9435, loss = 0.03307185\n",
      "Iteration 9436, loss = 0.03306559\n",
      "Iteration 9437, loss = 0.03305934\n",
      "Iteration 9438, loss = 0.03305308\n",
      "Iteration 9439, loss = 0.03304683\n",
      "Iteration 9440, loss = 0.03304059\n",
      "Iteration 9441, loss = 0.03303434\n",
      "Iteration 9442, loss = 0.03302810\n",
      "Iteration 9443, loss = 0.03302185\n",
      "Iteration 9444, loss = 0.03301562\n",
      "Iteration 9445, loss = 0.03300938\n",
      "Iteration 9446, loss = 0.03300314\n",
      "Iteration 9447, loss = 0.03299691\n",
      "Iteration 9448, loss = 0.03299068\n",
      "Iteration 9449, loss = 0.03298445\n",
      "Iteration 9450, loss = 0.03297823\n",
      "Iteration 9451, loss = 0.03297201\n",
      "Iteration 9452, loss = 0.03296578\n",
      "Iteration 9453, loss = 0.03295957\n",
      "Iteration 9454, loss = 0.03295335\n",
      "Iteration 9455, loss = 0.03294713\n",
      "Iteration 9456, loss = 0.03294092\n",
      "Iteration 9457, loss = 0.03293471\n",
      "Iteration 9458, loss = 0.03292851\n",
      "Iteration 9459, loss = 0.03292230\n",
      "Iteration 9460, loss = 0.03291610\n",
      "Iteration 9461, loss = 0.03290990\n",
      "Iteration 9462, loss = 0.03290370\n",
      "Iteration 9463, loss = 0.03289750\n",
      "Iteration 9464, loss = 0.03289131\n",
      "Iteration 9465, loss = 0.03288511\n",
      "Iteration 9466, loss = 0.03287892\n",
      "Iteration 9467, loss = 0.03287274\n",
      "Iteration 9468, loss = 0.03286655\n",
      "Iteration 9469, loss = 0.03286037\n",
      "Iteration 9470, loss = 0.03285419\n",
      "Iteration 9471, loss = 0.03284801\n",
      "Iteration 9472, loss = 0.03284183\n",
      "Iteration 9473, loss = 0.03283566\n",
      "Iteration 9474, loss = 0.03282949\n",
      "Iteration 9475, loss = 0.03282332\n",
      "Iteration 9476, loss = 0.03281715\n",
      "Iteration 9477, loss = 0.03281098\n",
      "Iteration 9478, loss = 0.03280482\n",
      "Iteration 9479, loss = 0.03279866\n",
      "Iteration 9480, loss = 0.03279250\n",
      "Iteration 9481, loss = 0.03278634\n",
      "Iteration 9482, loss = 0.03278019\n",
      "Iteration 9483, loss = 0.03277404\n",
      "Iteration 9484, loss = 0.03276789\n",
      "Iteration 9485, loss = 0.03276174\n",
      "Iteration 9486, loss = 0.03275559\n",
      "Iteration 9487, loss = 0.03274945\n",
      "Iteration 9488, loss = 0.03274331\n",
      "Iteration 9489, loss = 0.03273717\n",
      "Iteration 9490, loss = 0.03273103\n",
      "Iteration 9491, loss = 0.03272490\n",
      "Iteration 9492, loss = 0.03271877\n",
      "Iteration 9493, loss = 0.03271264\n",
      "Iteration 9494, loss = 0.03270651\n",
      "Iteration 9495, loss = 0.03270038\n",
      "Iteration 9496, loss = 0.03269426\n",
      "Iteration 9497, loss = 0.03268814\n",
      "Iteration 9498, loss = 0.03268202\n",
      "Iteration 9499, loss = 0.03267590\n",
      "Iteration 9500, loss = 0.03266979\n",
      "Iteration 9501, loss = 0.03266368\n",
      "Iteration 9502, loss = 0.03265757\n",
      "Iteration 9503, loss = 0.03265146\n",
      "Iteration 9504, loss = 0.03264535\n",
      "Iteration 9505, loss = 0.03263925\n",
      "Iteration 9506, loss = 0.03263315\n",
      "Iteration 9507, loss = 0.03262705\n",
      "Iteration 9508, loss = 0.03262095\n",
      "Iteration 9509, loss = 0.03261485\n",
      "Iteration 9510, loss = 0.03260876\n",
      "Iteration 9511, loss = 0.03260267\n",
      "Iteration 9512, loss = 0.03259658\n",
      "Iteration 9513, loss = 0.03259050\n",
      "Iteration 9514, loss = 0.03258441\n",
      "Iteration 9515, loss = 0.03257833\n",
      "Iteration 9516, loss = 0.03257225\n",
      "Iteration 9517, loss = 0.03256617\n",
      "Iteration 9518, loss = 0.03256010\n",
      "Iteration 9519, loss = 0.03255402\n",
      "Iteration 9520, loss = 0.03254795\n",
      "Iteration 9521, loss = 0.03254188\n",
      "Iteration 9522, loss = 0.03253582\n",
      "Iteration 9523, loss = 0.03252975\n",
      "Iteration 9524, loss = 0.03252369\n",
      "Iteration 9525, loss = 0.03251763\n",
      "Iteration 9526, loss = 0.03251157\n",
      "Iteration 9527, loss = 0.03250551\n",
      "Iteration 9528, loss = 0.03249946\n",
      "Iteration 9529, loss = 0.03249341\n",
      "Iteration 9530, loss = 0.03248736\n",
      "Iteration 9531, loss = 0.03248131\n",
      "Iteration 9532, loss = 0.03247527\n",
      "Iteration 9533, loss = 0.03246922\n",
      "Iteration 9534, loss = 0.03246318\n",
      "Iteration 9535, loss = 0.03245714\n",
      "Iteration 9536, loss = 0.03245111\n",
      "Iteration 9537, loss = 0.03244507\n",
      "Iteration 9538, loss = 0.03243904\n",
      "Iteration 9539, loss = 0.03243301\n",
      "Iteration 9540, loss = 0.03242698\n",
      "Iteration 9541, loss = 0.03242096\n",
      "Iteration 9542, loss = 0.03241493\n",
      "Iteration 9543, loss = 0.03240891\n",
      "Iteration 9544, loss = 0.03240289\n",
      "Iteration 9545, loss = 0.03239687\n",
      "Iteration 9546, loss = 0.03239086\n",
      "Iteration 9547, loss = 0.03238484\n",
      "Iteration 9548, loss = 0.03237883\n",
      "Iteration 9549, loss = 0.03237282\n",
      "Iteration 9550, loss = 0.03236682\n",
      "Iteration 9551, loss = 0.03236081\n",
      "Iteration 9552, loss = 0.03235481\n",
      "Iteration 9553, loss = 0.03234881\n",
      "Iteration 9554, loss = 0.03234281\n",
      "Iteration 9555, loss = 0.03233682\n",
      "Iteration 9556, loss = 0.03233082\n",
      "Iteration 9557, loss = 0.03232483\n",
      "Iteration 9558, loss = 0.03231884\n",
      "Iteration 9559, loss = 0.03231285\n",
      "Iteration 9560, loss = 0.03230687\n",
      "Iteration 9561, loss = 0.03230088\n",
      "Iteration 9562, loss = 0.03229490\n",
      "Iteration 9563, loss = 0.03228892\n",
      "Iteration 9564, loss = 0.03228295\n",
      "Iteration 9565, loss = 0.03227697\n",
      "Iteration 9566, loss = 0.03227100\n",
      "Iteration 9567, loss = 0.03226503\n",
      "Iteration 9568, loss = 0.03225906\n",
      "Iteration 9569, loss = 0.03225310\n",
      "Iteration 9570, loss = 0.03224713\n",
      "Iteration 9571, loss = 0.03224117\n",
      "Iteration 9572, loss = 0.03223521\n",
      "Iteration 9573, loss = 0.03222925\n",
      "Iteration 9574, loss = 0.03222330\n",
      "Iteration 9575, loss = 0.03221734\n",
      "Iteration 9576, loss = 0.03221139\n",
      "Iteration 9577, loss = 0.03220544\n",
      "Iteration 9578, loss = 0.03219949\n",
      "Iteration 9579, loss = 0.03219355\n",
      "Iteration 9580, loss = 0.03218760\n",
      "Iteration 9581, loss = 0.03218166\n",
      "Iteration 9582, loss = 0.03217572\n",
      "Iteration 9583, loss = 0.03216979\n",
      "Iteration 9584, loss = 0.03216385\n",
      "Iteration 9585, loss = 0.03215792\n",
      "Iteration 9586, loss = 0.03215199\n",
      "Iteration 9587, loss = 0.03214606\n",
      "Iteration 9588, loss = 0.03214014\n",
      "Iteration 9589, loss = 0.03213421\n",
      "Iteration 9590, loss = 0.03212829\n",
      "Iteration 9591, loss = 0.03212237\n",
      "Iteration 9592, loss = 0.03211645\n",
      "Iteration 9593, loss = 0.03211054\n",
      "Iteration 9594, loss = 0.03210462\n",
      "Iteration 9595, loss = 0.03209871\n",
      "Iteration 9596, loss = 0.03209280\n",
      "Iteration 9597, loss = 0.03208689\n",
      "Iteration 9598, loss = 0.03208099\n",
      "Iteration 9599, loss = 0.03207508\n",
      "Iteration 9600, loss = 0.03206918\n",
      "Iteration 9601, loss = 0.03206328\n",
      "Iteration 9602, loss = 0.03205739\n",
      "Iteration 9603, loss = 0.03205149\n",
      "Iteration 9604, loss = 0.03204560\n",
      "Iteration 9605, loss = 0.03203971\n",
      "Iteration 9606, loss = 0.03203382\n",
      "Iteration 9607, loss = 0.03202793\n",
      "Iteration 9608, loss = 0.03202205\n",
      "Iteration 9609, loss = 0.03201616\n",
      "Iteration 9610, loss = 0.03201028\n",
      "Iteration 9611, loss = 0.03200441\n",
      "Iteration 9612, loss = 0.03199853\n",
      "Iteration 9613, loss = 0.03199266\n",
      "Iteration 9614, loss = 0.03198678\n",
      "Iteration 9615, loss = 0.03198091\n",
      "Iteration 9616, loss = 0.03197504\n",
      "Iteration 9617, loss = 0.03196918\n",
      "Iteration 9618, loss = 0.03196331\n",
      "Iteration 9619, loss = 0.03195745\n",
      "Iteration 9620, loss = 0.03195159\n",
      "Iteration 9621, loss = 0.03194574\n",
      "Iteration 9622, loss = 0.03193988\n",
      "Iteration 9623, loss = 0.03193403\n",
      "Iteration 9624, loss = 0.03192817\n",
      "Iteration 9625, loss = 0.03192233\n",
      "Iteration 9626, loss = 0.03191648\n",
      "Iteration 9627, loss = 0.03191063\n",
      "Iteration 9628, loss = 0.03190479\n",
      "Iteration 9629, loss = 0.03189895\n",
      "Iteration 9630, loss = 0.03189311\n",
      "Iteration 9631, loss = 0.03188727\n",
      "Iteration 9632, loss = 0.03188144\n",
      "Iteration 9633, loss = 0.03187560\n",
      "Iteration 9634, loss = 0.03186977\n",
      "Iteration 9635, loss = 0.03186394\n",
      "Iteration 9636, loss = 0.03185812\n",
      "Iteration 9637, loss = 0.03185229\n",
      "Iteration 9638, loss = 0.03184647\n",
      "Iteration 9639, loss = 0.03184065\n",
      "Iteration 9640, loss = 0.03183483\n",
      "Iteration 9641, loss = 0.03182901\n",
      "Iteration 9642, loss = 0.03182320\n",
      "Iteration 9643, loss = 0.03181738\n",
      "Iteration 9644, loss = 0.03181157\n",
      "Iteration 9645, loss = 0.03180577\n",
      "Iteration 9646, loss = 0.03179996\n",
      "Iteration 9647, loss = 0.03179415\n",
      "Iteration 9648, loss = 0.03178835\n",
      "Iteration 9649, loss = 0.03178255\n",
      "Iteration 9650, loss = 0.03177675\n",
      "Iteration 9651, loss = 0.03177096\n",
      "Iteration 9652, loss = 0.03176516\n",
      "Iteration 9653, loss = 0.03175937\n",
      "Iteration 9654, loss = 0.03175358\n",
      "Iteration 9655, loss = 0.03174779\n",
      "Iteration 9656, loss = 0.03174201\n",
      "Iteration 9657, loss = 0.03173622\n",
      "Iteration 9658, loss = 0.03173044\n",
      "Iteration 9659, loss = 0.03172466\n",
      "Iteration 9660, loss = 0.03171888\n",
      "Iteration 9661, loss = 0.03171311\n",
      "Iteration 9662, loss = 0.03170733\n",
      "Iteration 9663, loss = 0.03170156\n",
      "Iteration 9664, loss = 0.03169579\n",
      "Iteration 9665, loss = 0.03169002\n",
      "Iteration 9666, loss = 0.03168426\n",
      "Iteration 9667, loss = 0.03167849\n",
      "Iteration 9668, loss = 0.03167273\n",
      "Iteration 9669, loss = 0.03166697\n",
      "Iteration 9670, loss = 0.03166121\n",
      "Iteration 9671, loss = 0.03165546\n",
      "Iteration 9672, loss = 0.03164970\n",
      "Iteration 9673, loss = 0.03164395\n",
      "Iteration 9674, loss = 0.03163820\n",
      "Iteration 9675, loss = 0.03163245\n",
      "Iteration 9676, loss = 0.03162671\n",
      "Iteration 9677, loss = 0.03162096\n",
      "Iteration 9678, loss = 0.03161522\n",
      "Iteration 9679, loss = 0.03160948\n",
      "Iteration 9680, loss = 0.03160374\n",
      "Iteration 9681, loss = 0.03159801\n",
      "Iteration 9682, loss = 0.03159227\n",
      "Iteration 9683, loss = 0.03158654\n",
      "Iteration 9684, loss = 0.03158081\n",
      "Iteration 9685, loss = 0.03157508\n",
      "Iteration 9686, loss = 0.03156936\n",
      "Iteration 9687, loss = 0.03156363\n",
      "Iteration 9688, loss = 0.03155791\n",
      "Iteration 9689, loss = 0.03155219\n",
      "Iteration 9690, loss = 0.03154647\n",
      "Iteration 9691, loss = 0.03154076\n",
      "Iteration 9692, loss = 0.03153504\n",
      "Iteration 9693, loss = 0.03152933\n",
      "Iteration 9694, loss = 0.03152362\n",
      "Iteration 9695, loss = 0.03151791\n",
      "Iteration 9696, loss = 0.03151221\n",
      "Iteration 9697, loss = 0.03150650\n",
      "Iteration 9698, loss = 0.03150080\n",
      "Iteration 9699, loss = 0.03149510\n",
      "Iteration 9700, loss = 0.03148940\n",
      "Iteration 9701, loss = 0.03148371\n",
      "Iteration 9702, loss = 0.03147801\n",
      "Iteration 9703, loss = 0.03147232\n",
      "Iteration 9704, loss = 0.03146663\n",
      "Iteration 9705, loss = 0.03146094\n",
      "Iteration 9706, loss = 0.03145526\n",
      "Iteration 9707, loss = 0.03144957\n",
      "Iteration 9708, loss = 0.03144389\n",
      "Iteration 9709, loss = 0.03143821\n",
      "Iteration 9710, loss = 0.03143253\n",
      "Iteration 9711, loss = 0.03142686\n",
      "Iteration 9712, loss = 0.03142118\n",
      "Iteration 9713, loss = 0.03141551\n",
      "Iteration 9714, loss = 0.03140984\n",
      "Iteration 9715, loss = 0.03140417\n",
      "Iteration 9716, loss = 0.03139851\n",
      "Iteration 9717, loss = 0.03139284\n",
      "Iteration 9718, loss = 0.03138718\n",
      "Iteration 9719, loss = 0.03138152\n",
      "Iteration 9720, loss = 0.03137586\n",
      "Iteration 9721, loss = 0.03137020\n",
      "Iteration 9722, loss = 0.03136455\n",
      "Iteration 9723, loss = 0.03135890\n",
      "Iteration 9724, loss = 0.03135325\n",
      "Iteration 9725, loss = 0.03134760\n",
      "Iteration 9726, loss = 0.03134195\n",
      "Iteration 9727, loss = 0.03133631\n",
      "Iteration 9728, loss = 0.03133066\n",
      "Iteration 9729, loss = 0.03132502\n",
      "Iteration 9730, loss = 0.03131938\n",
      "Iteration 9731, loss = 0.03131375\n",
      "Iteration 9732, loss = 0.03130811\n",
      "Iteration 9733, loss = 0.03130248\n",
      "Iteration 9734, loss = 0.03129685\n",
      "Iteration 9735, loss = 0.03129122\n",
      "Iteration 9736, loss = 0.03128559\n",
      "Iteration 9737, loss = 0.03127997\n",
      "Iteration 9738, loss = 0.03127434\n",
      "Iteration 9739, loss = 0.03126872\n",
      "Iteration 9740, loss = 0.03126310\n",
      "Iteration 9741, loss = 0.03125748\n",
      "Iteration 9742, loss = 0.03125187\n",
      "Iteration 9743, loss = 0.03124626\n",
      "Iteration 9744, loss = 0.03124064\n",
      "Iteration 9745, loss = 0.03123503\n",
      "Iteration 9746, loss = 0.03122943\n",
      "Iteration 9747, loss = 0.03122382\n",
      "Iteration 9748, loss = 0.03121822\n",
      "Iteration 9749, loss = 0.03121262\n",
      "Iteration 9750, loss = 0.03120702\n",
      "Iteration 9751, loss = 0.03120142\n",
      "Iteration 9752, loss = 0.03119582\n",
      "Iteration 9753, loss = 0.03119023\n",
      "Iteration 9754, loss = 0.03118463\n",
      "Iteration 9755, loss = 0.03117904\n",
      "Iteration 9756, loss = 0.03117346\n",
      "Iteration 9757, loss = 0.03116787\n",
      "Iteration 9758, loss = 0.03116229\n",
      "Iteration 9759, loss = 0.03115670\n",
      "Iteration 9760, loss = 0.03115112\n",
      "Iteration 9761, loss = 0.03114554\n",
      "Iteration 9762, loss = 0.03113997\n",
      "Iteration 9763, loss = 0.03113439\n",
      "Iteration 9764, loss = 0.03112882\n",
      "Iteration 9765, loss = 0.03112325\n",
      "Iteration 9766, loss = 0.03111768\n",
      "Iteration 9767, loss = 0.03111211\n",
      "Iteration 9768, loss = 0.03110655\n",
      "Iteration 9769, loss = 0.03110098\n",
      "Iteration 9770, loss = 0.03109542\n",
      "Iteration 9771, loss = 0.03108986\n",
      "Iteration 9772, loss = 0.03108430\n",
      "Iteration 9773, loss = 0.03107875\n",
      "Iteration 9774, loss = 0.03107319\n",
      "Iteration 9775, loss = 0.03106764\n",
      "Iteration 9776, loss = 0.03106209\n",
      "Iteration 9777, loss = 0.03105655\n",
      "Iteration 9778, loss = 0.03105100\n",
      "Iteration 9779, loss = 0.03104545\n",
      "Iteration 9780, loss = 0.03103991\n",
      "Iteration 9781, loss = 0.03103437\n",
      "Iteration 9782, loss = 0.03102883\n",
      "Iteration 9783, loss = 0.03102330\n",
      "Iteration 9784, loss = 0.03101776\n",
      "Iteration 9785, loss = 0.03101223\n",
      "Iteration 9786, loss = 0.03100670\n",
      "Iteration 9787, loss = 0.03100117\n",
      "Iteration 9788, loss = 0.03099564\n",
      "Iteration 9789, loss = 0.03099012\n",
      "Iteration 9790, loss = 0.03098459\n",
      "Iteration 9791, loss = 0.03097907\n",
      "Iteration 9792, loss = 0.03097355\n",
      "Iteration 9793, loss = 0.03096804\n",
      "Iteration 9794, loss = 0.03096252\n",
      "Iteration 9795, loss = 0.03095701\n",
      "Iteration 9796, loss = 0.03095150\n",
      "Iteration 9797, loss = 0.03094599\n",
      "Iteration 9798, loss = 0.03094048\n",
      "Iteration 9799, loss = 0.03093497\n",
      "Iteration 9800, loss = 0.03092947\n",
      "Iteration 9801, loss = 0.03092396\n",
      "Iteration 9802, loss = 0.03091846\n",
      "Iteration 9803, loss = 0.03091297\n",
      "Iteration 9804, loss = 0.03090747\n",
      "Iteration 9805, loss = 0.03090197\n",
      "Iteration 9806, loss = 0.03089648\n",
      "Iteration 9807, loss = 0.03089099\n",
      "Iteration 9808, loss = 0.03088550\n",
      "Iteration 9809, loss = 0.03088001\n",
      "Iteration 9810, loss = 0.03087453\n",
      "Iteration 9811, loss = 0.03086904\n",
      "Iteration 9812, loss = 0.03086356\n",
      "Iteration 9813, loss = 0.03085808\n",
      "Iteration 9814, loss = 0.03085260\n",
      "Iteration 9815, loss = 0.03084713\n",
      "Iteration 9816, loss = 0.03084165\n",
      "Iteration 9817, loss = 0.03083618\n",
      "Iteration 9818, loss = 0.03083071\n",
      "Iteration 9819, loss = 0.03082524\n",
      "Iteration 9820, loss = 0.03081978\n",
      "Iteration 9821, loss = 0.03081431\n",
      "Iteration 9822, loss = 0.03080885\n",
      "Iteration 9823, loss = 0.03080339\n",
      "Iteration 9824, loss = 0.03079793\n",
      "Iteration 9825, loss = 0.03079247\n",
      "Iteration 9826, loss = 0.03078702\n",
      "Iteration 9827, loss = 0.03078156\n",
      "Iteration 9828, loss = 0.03077611\n",
      "Iteration 9829, loss = 0.03077066\n",
      "Iteration 9830, loss = 0.03076521\n",
      "Iteration 9831, loss = 0.03075977\n",
      "Iteration 9832, loss = 0.03075432\n",
      "Iteration 9833, loss = 0.03074888\n",
      "Iteration 9834, loss = 0.03074344\n",
      "Iteration 9835, loss = 0.03073800\n",
      "Iteration 9836, loss = 0.03073256\n",
      "Iteration 9837, loss = 0.03072713\n",
      "Iteration 9838, loss = 0.03072170\n",
      "Iteration 9839, loss = 0.03071627\n",
      "Iteration 9840, loss = 0.03071084\n",
      "Iteration 9841, loss = 0.03070541\n",
      "Iteration 9842, loss = 0.03069998\n",
      "Iteration 9843, loss = 0.03069456\n",
      "Iteration 9844, loss = 0.03068914\n",
      "Iteration 9845, loss = 0.03068372\n",
      "Iteration 9846, loss = 0.03067830\n",
      "Iteration 9847, loss = 0.03067288\n",
      "Iteration 9848, loss = 0.03066747\n",
      "Iteration 9849, loss = 0.03066206\n",
      "Iteration 9850, loss = 0.03065664\n",
      "Iteration 9851, loss = 0.03065124\n",
      "Iteration 9852, loss = 0.03064583\n",
      "Iteration 9853, loss = 0.03064042\n",
      "Iteration 9854, loss = 0.03063502\n",
      "Iteration 9855, loss = 0.03062962\n",
      "Iteration 9856, loss = 0.03062422\n",
      "Iteration 9857, loss = 0.03061882\n",
      "Iteration 9858, loss = 0.03061342\n",
      "Iteration 9859, loss = 0.03060803\n",
      "Iteration 9860, loss = 0.03060264\n",
      "Iteration 9861, loss = 0.03059725\n",
      "Iteration 9862, loss = 0.03059186\n",
      "Iteration 9863, loss = 0.03058647\n",
      "Iteration 9864, loss = 0.03058109\n",
      "Iteration 9865, loss = 0.03057570\n",
      "Iteration 9866, loss = 0.03057032\n",
      "Iteration 9867, loss = 0.03056494\n",
      "Iteration 9868, loss = 0.03055956\n",
      "Iteration 9869, loss = 0.03055419\n",
      "Iteration 9870, loss = 0.03054881\n",
      "Iteration 9871, loss = 0.03054344\n",
      "Iteration 9872, loss = 0.03053807\n",
      "Iteration 9873, loss = 0.03053270\n",
      "Iteration 9874, loss = 0.03052734\n",
      "Iteration 9875, loss = 0.03052197\n",
      "Iteration 9876, loss = 0.03051661\n",
      "Iteration 9877, loss = 0.03051125\n",
      "Iteration 9878, loss = 0.03050589\n",
      "Iteration 9879, loss = 0.03050053\n",
      "Iteration 9880, loss = 0.03049517\n",
      "Iteration 9881, loss = 0.03048982\n",
      "Iteration 9882, loss = 0.03048447\n",
      "Iteration 9883, loss = 0.03047912\n",
      "Iteration 9884, loss = 0.03047377\n",
      "Iteration 9885, loss = 0.03046842\n",
      "Iteration 9886, loss = 0.03046308\n",
      "Iteration 9887, loss = 0.03045773\n",
      "Iteration 9888, loss = 0.03045239\n",
      "Iteration 9889, loss = 0.03044705\n",
      "Iteration 9890, loss = 0.03044172\n",
      "Iteration 9891, loss = 0.03043638\n",
      "Iteration 9892, loss = 0.03043105\n",
      "Iteration 9893, loss = 0.03042571\n",
      "Iteration 9894, loss = 0.03042038\n",
      "Iteration 9895, loss = 0.03041505\n",
      "Iteration 9896, loss = 0.03040973\n",
      "Iteration 9897, loss = 0.03040440\n",
      "Iteration 9898, loss = 0.03039908\n",
      "Iteration 9899, loss = 0.03039376\n",
      "Iteration 9900, loss = 0.03038844\n",
      "Iteration 9901, loss = 0.03038312\n",
      "Iteration 9902, loss = 0.03037780\n",
      "Iteration 9903, loss = 0.03037249\n",
      "Iteration 9904, loss = 0.03036718\n",
      "Iteration 9905, loss = 0.03036187\n",
      "Iteration 9906, loss = 0.03035656\n",
      "Iteration 9907, loss = 0.03035125\n",
      "Iteration 9908, loss = 0.03034595\n",
      "Iteration 9909, loss = 0.03034064\n",
      "Iteration 9910, loss = 0.03033534\n",
      "Iteration 9911, loss = 0.03033004\n",
      "Iteration 9912, loss = 0.03032474\n",
      "Iteration 9913, loss = 0.03031945\n",
      "Iteration 9914, loss = 0.03031415\n",
      "Iteration 9915, loss = 0.03030886\n",
      "Iteration 9916, loss = 0.03030357\n",
      "Iteration 9917, loss = 0.03029828\n",
      "Iteration 9918, loss = 0.03029299\n",
      "Iteration 9919, loss = 0.03028771\n",
      "Iteration 9920, loss = 0.03028242\n",
      "Iteration 9921, loss = 0.03027714\n",
      "Iteration 9922, loss = 0.03027186\n",
      "Iteration 9923, loss = 0.03026658\n",
      "Iteration 9924, loss = 0.03026131\n",
      "Iteration 9925, loss = 0.03025603\n",
      "Iteration 9926, loss = 0.03025076\n",
      "Iteration 9927, loss = 0.03024549\n",
      "Iteration 9928, loss = 0.03024022\n",
      "Iteration 9929, loss = 0.03023495\n",
      "Iteration 9930, loss = 0.03022968\n",
      "Iteration 9931, loss = 0.03022442\n",
      "Iteration 9932, loss = 0.03021916\n",
      "Iteration 9933, loss = 0.03021390\n",
      "Iteration 9934, loss = 0.03020864\n",
      "Iteration 9935, loss = 0.03020338\n",
      "Iteration 9936, loss = 0.03019813\n",
      "Iteration 9937, loss = 0.03019287\n",
      "Iteration 9938, loss = 0.03018762\n",
      "Iteration 9939, loss = 0.03018237\n",
      "Iteration 9940, loss = 0.03017712\n",
      "Iteration 9941, loss = 0.03017188\n",
      "Iteration 9942, loss = 0.03016663\n",
      "Iteration 9943, loss = 0.03016139\n",
      "Iteration 9944, loss = 0.03015615\n",
      "Iteration 9945, loss = 0.03015091\n",
      "Iteration 9946, loss = 0.03014567\n",
      "Iteration 9947, loss = 0.03014044\n",
      "Iteration 9948, loss = 0.03013520\n",
      "Iteration 9949, loss = 0.03012997\n",
      "Iteration 9950, loss = 0.03012474\n",
      "Iteration 9951, loss = 0.03011951\n",
      "Iteration 9952, loss = 0.03011428\n",
      "Iteration 9953, loss = 0.03010906\n",
      "Iteration 9954, loss = 0.03010383\n",
      "Iteration 9955, loss = 0.03009861\n",
      "Iteration 9956, loss = 0.03009339\n",
      "Iteration 9957, loss = 0.03008817\n",
      "Iteration 9958, loss = 0.03008296\n",
      "Iteration 9959, loss = 0.03007774\n",
      "Iteration 9960, loss = 0.03007253\n",
      "Iteration 9961, loss = 0.03006732\n",
      "Iteration 9962, loss = 0.03006211\n",
      "Iteration 9963, loss = 0.03005690\n",
      "Iteration 9964, loss = 0.03005170\n",
      "Iteration 9965, loss = 0.03004649\n",
      "Iteration 9966, loss = 0.03004129\n",
      "Iteration 9967, loss = 0.03003609\n",
      "Iteration 9968, loss = 0.03003089\n",
      "Iteration 9969, loss = 0.03002569\n",
      "Iteration 9970, loss = 0.03002050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9971, loss = 0.03001530\n",
      "Iteration 9972, loss = 0.03001011\n",
      "Iteration 9973, loss = 0.03000492\n",
      "Iteration 9974, loss = 0.02999973\n",
      "Iteration 9975, loss = 0.02999455\n",
      "Iteration 9976, loss = 0.02998936\n",
      "Iteration 9977, loss = 0.02998418\n",
      "Iteration 9978, loss = 0.02997900\n",
      "Iteration 9979, loss = 0.02997382\n",
      "Iteration 9980, loss = 0.02996864\n",
      "Iteration 9981, loss = 0.02996346\n",
      "Iteration 9982, loss = 0.02995829\n",
      "Iteration 9983, loss = 0.02995312\n",
      "Iteration 9984, loss = 0.02994794\n",
      "Iteration 9985, loss = 0.02994278\n",
      "Iteration 9986, loss = 0.02993761\n",
      "Iteration 9987, loss = 0.02993244\n",
      "Iteration 9988, loss = 0.02992728\n",
      "Iteration 9989, loss = 0.02992212\n",
      "Iteration 9990, loss = 0.02991695\n",
      "Iteration 9991, loss = 0.02991180\n",
      "Iteration 9992, loss = 0.02990664\n",
      "Iteration 9993, loss = 0.02990148\n",
      "Iteration 9994, loss = 0.02989633\n",
      "Iteration 9995, loss = 0.02989118\n",
      "Iteration 9996, loss = 0.02988603\n",
      "Iteration 9997, loss = 0.02988088\n",
      "Iteration 9998, loss = 0.02987573\n",
      "Iteration 9999, loss = 0.02987059\n",
      "Iteration 10000, loss = 0.02986544\n",
      "Iteration 10001, loss = 0.02986030\n",
      "Iteration 10002, loss = 0.02985516\n",
      "Iteration 10003, loss = 0.02985002\n",
      "Iteration 10004, loss = 0.02984489\n",
      "Iteration 10005, loss = 0.02983975\n",
      "Iteration 10006, loss = 0.02983462\n",
      "Iteration 10007, loss = 0.02982949\n",
      "Iteration 10008, loss = 0.02982436\n",
      "Iteration 10009, loss = 0.02981923\n",
      "Iteration 10010, loss = 0.02981410\n",
      "Iteration 10011, loss = 0.02980898\n",
      "Iteration 10012, loss = 0.02980386\n",
      "Iteration 10013, loss = 0.02979873\n",
      "Iteration 10014, loss = 0.02979362\n",
      "Iteration 10015, loss = 0.02978850\n",
      "Iteration 10016, loss = 0.02978338\n",
      "Iteration 10017, loss = 0.02977827\n",
      "Iteration 10018, loss = 0.02977315\n",
      "Iteration 10019, loss = 0.02976804\n",
      "Iteration 10020, loss = 0.02976293\n",
      "Iteration 10021, loss = 0.02975783\n",
      "Iteration 10022, loss = 0.02975272\n",
      "Iteration 10023, loss = 0.02974762\n",
      "Iteration 10024, loss = 0.02974251\n",
      "Iteration 10025, loss = 0.02973741\n",
      "Iteration 10026, loss = 0.02973232\n",
      "Iteration 10027, loss = 0.02972722\n",
      "Iteration 10028, loss = 0.02972212\n",
      "Iteration 10029, loss = 0.02971703\n",
      "Iteration 10030, loss = 0.02971194\n",
      "Iteration 10031, loss = 0.02970685\n",
      "Iteration 10032, loss = 0.02970176\n",
      "Iteration 10033, loss = 0.02969667\n",
      "Iteration 10034, loss = 0.02969158\n",
      "Iteration 10035, loss = 0.02968650\n",
      "Iteration 10036, loss = 0.02968142\n",
      "Iteration 10037, loss = 0.02967634\n",
      "Iteration 10038, loss = 0.02967126\n",
      "Iteration 10039, loss = 0.02966618\n",
      "Iteration 10040, loss = 0.02966111\n",
      "Iteration 10041, loss = 0.02965603\n",
      "Iteration 10042, loss = 0.02965096\n",
      "Iteration 10043, loss = 0.02964589\n",
      "Iteration 10044, loss = 0.02964082\n",
      "Iteration 10045, loss = 0.02963576\n",
      "Iteration 10046, loss = 0.02963069\n",
      "Iteration 10047, loss = 0.02962563\n",
      "Iteration 10048, loss = 0.02962057\n",
      "Iteration 10049, loss = 0.02961551\n",
      "Iteration 10050, loss = 0.02961045\n",
      "Iteration 10051, loss = 0.02960539\n",
      "Iteration 10052, loss = 0.02960034\n",
      "Iteration 10053, loss = 0.02959528\n",
      "Iteration 10054, loss = 0.02959023\n",
      "Iteration 10055, loss = 0.02958518\n",
      "Iteration 10056, loss = 0.02958013\n",
      "Iteration 10057, loss = 0.02957509\n",
      "Iteration 10058, loss = 0.02957004\n",
      "Iteration 10059, loss = 0.02956500\n",
      "Iteration 10060, loss = 0.02955996\n",
      "Iteration 10061, loss = 0.02955492\n",
      "Iteration 10062, loss = 0.02954988\n",
      "Iteration 10063, loss = 0.02954484\n",
      "Iteration 10064, loss = 0.02953981\n",
      "Iteration 10065, loss = 0.02953477\n",
      "Iteration 10066, loss = 0.02952974\n",
      "Iteration 10067, loss = 0.02952471\n",
      "Iteration 10068, loss = 0.02951968\n",
      "Iteration 10069, loss = 0.02951466\n",
      "Iteration 10070, loss = 0.02950963\n",
      "Iteration 10071, loss = 0.02950461\n",
      "Iteration 10072, loss = 0.02949959\n",
      "Iteration 10073, loss = 0.02949457\n",
      "Iteration 10074, loss = 0.02948955\n",
      "Iteration 10075, loss = 0.02948453\n",
      "Iteration 10076, loss = 0.02947952\n",
      "Iteration 10077, loss = 0.02947451\n",
      "Iteration 10078, loss = 0.02946949\n",
      "Iteration 10079, loss = 0.02946448\n",
      "Iteration 10080, loss = 0.02945948\n",
      "Iteration 10081, loss = 0.02945447\n",
      "Iteration 10082, loss = 0.02944946\n",
      "Iteration 10083, loss = 0.02944446\n",
      "Iteration 10084, loss = 0.02943946\n",
      "Iteration 10085, loss = 0.02943446\n",
      "Iteration 10086, loss = 0.02942946\n",
      "Iteration 10087, loss = 0.02942446\n",
      "Iteration 10088, loss = 0.02941947\n",
      "Iteration 10089, loss = 0.02941448\n",
      "Iteration 10090, loss = 0.02940948\n",
      "Iteration 10091, loss = 0.02940449\n",
      "Iteration 10092, loss = 0.02939951\n",
      "Iteration 10093, loss = 0.02939452\n",
      "Iteration 10094, loss = 0.02938953\n",
      "Iteration 10095, loss = 0.02938455\n",
      "Iteration 10096, loss = 0.02937957\n",
      "Iteration 10097, loss = 0.02937459\n",
      "Iteration 10098, loss = 0.02936961\n",
      "Iteration 10099, loss = 0.02936463\n",
      "Iteration 10100, loss = 0.02935966\n",
      "Iteration 10101, loss = 0.02935468\n",
      "Iteration 10102, loss = 0.02934971\n",
      "Iteration 10103, loss = 0.02934474\n",
      "Iteration 10104, loss = 0.02933977\n",
      "Iteration 10105, loss = 0.02933481\n",
      "Iteration 10106, loss = 0.02932984\n",
      "Iteration 10107, loss = 0.02932488\n",
      "Iteration 10108, loss = 0.02931991\n",
      "Iteration 10109, loss = 0.02931495\n",
      "Iteration 10110, loss = 0.02930999\n",
      "Iteration 10111, loss = 0.02930504\n",
      "Iteration 10112, loss = 0.02930008\n",
      "Iteration 10113, loss = 0.02929513\n",
      "Iteration 10114, loss = 0.02929018\n",
      "Iteration 10115, loss = 0.02928522\n",
      "Iteration 10116, loss = 0.02928028\n",
      "Iteration 10117, loss = 0.02927533\n",
      "Iteration 10118, loss = 0.02927038\n",
      "Iteration 10119, loss = 0.02926544\n",
      "Iteration 10120, loss = 0.02926050\n",
      "Iteration 10121, loss = 0.02925556\n",
      "Iteration 10122, loss = 0.02925062\n",
      "Iteration 10123, loss = 0.02924568\n",
      "Iteration 10124, loss = 0.02924074\n",
      "Iteration 10125, loss = 0.02923581\n",
      "Iteration 10126, loss = 0.02923087\n",
      "Iteration 10127, loss = 0.02922594\n",
      "Iteration 10128, loss = 0.02922101\n",
      "Iteration 10129, loss = 0.02921609\n",
      "Iteration 10130, loss = 0.02921116\n",
      "Iteration 10131, loss = 0.02920624\n",
      "Iteration 10132, loss = 0.02920131\n",
      "Iteration 10133, loss = 0.02919639\n",
      "Iteration 10134, loss = 0.02919147\n",
      "Iteration 10135, loss = 0.02918655\n",
      "Iteration 10136, loss = 0.02918164\n",
      "Iteration 10137, loss = 0.02917672\n",
      "Iteration 10138, loss = 0.02917181\n",
      "Iteration 10139, loss = 0.02916690\n",
      "Iteration 10140, loss = 0.02916199\n",
      "Iteration 10141, loss = 0.02915708\n",
      "Iteration 10142, loss = 0.02915217\n",
      "Iteration 10143, loss = 0.02914727\n",
      "Iteration 10144, loss = 0.02914236\n",
      "Iteration 10145, loss = 0.02913746\n",
      "Iteration 10146, loss = 0.02913256\n",
      "Iteration 10147, loss = 0.02912766\n",
      "Iteration 10148, loss = 0.02912276\n",
      "Iteration 10149, loss = 0.02911787\n",
      "Iteration 10150, loss = 0.02911297\n",
      "Iteration 10151, loss = 0.02910808\n",
      "Iteration 10152, loss = 0.02910319\n",
      "Iteration 10153, loss = 0.02909830\n",
      "Iteration 10154, loss = 0.02909341\n",
      "Iteration 10155, loss = 0.02908853\n",
      "Iteration 10156, loss = 0.02908364\n",
      "Iteration 10157, loss = 0.02907876\n",
      "Iteration 10158, loss = 0.02907388\n",
      "Iteration 10159, loss = 0.02906900\n",
      "Iteration 10160, loss = 0.02906412\n",
      "Iteration 10161, loss = 0.02905924\n",
      "Iteration 10162, loss = 0.02905437\n",
      "Iteration 10163, loss = 0.02904950\n",
      "Iteration 10164, loss = 0.02904463\n",
      "Iteration 10165, loss = 0.02903976\n",
      "Iteration 10166, loss = 0.02903489\n",
      "Iteration 10167, loss = 0.02903002\n",
      "Iteration 10168, loss = 0.02902515\n",
      "Iteration 10169, loss = 0.02902029\n",
      "Iteration 10170, loss = 0.02901543\n",
      "Iteration 10171, loss = 0.02901057\n",
      "Iteration 10172, loss = 0.02900571\n",
      "Iteration 10173, loss = 0.02900085\n",
      "Iteration 10174, loss = 0.02899600\n",
      "Iteration 10175, loss = 0.02899114\n",
      "Iteration 10176, loss = 0.02898629\n",
      "Iteration 10177, loss = 0.02898144\n",
      "Iteration 10178, loss = 0.02897659\n",
      "Iteration 10179, loss = 0.02897174\n",
      "Iteration 10180, loss = 0.02896690\n",
      "Iteration 10181, loss = 0.02896205\n",
      "Iteration 10182, loss = 0.02895721\n",
      "Iteration 10183, loss = 0.02895237\n",
      "Iteration 10184, loss = 0.02894753\n",
      "Iteration 10185, loss = 0.02894269\n",
      "Iteration 10186, loss = 0.02893785\n",
      "Iteration 10187, loss = 0.02893302\n",
      "Iteration 10188, loss = 0.02892818\n",
      "Iteration 10189, loss = 0.02892335\n",
      "Iteration 10190, loss = 0.02891852\n",
      "Iteration 10191, loss = 0.02891369\n",
      "Iteration 10192, loss = 0.02890886\n",
      "Iteration 10193, loss = 0.02890404\n",
      "Iteration 10194, loss = 0.02889922\n",
      "Iteration 10195, loss = 0.02889439\n",
      "Iteration 10196, loss = 0.02888957\n",
      "Iteration 10197, loss = 0.02888475\n",
      "Iteration 10198, loss = 0.02887993\n",
      "Iteration 10199, loss = 0.02887512\n",
      "Iteration 10200, loss = 0.02887030\n",
      "Iteration 10201, loss = 0.02886549\n",
      "Iteration 10202, loss = 0.02886068\n",
      "Iteration 10203, loss = 0.02885587\n",
      "Iteration 10204, loss = 0.02885106\n",
      "Iteration 10205, loss = 0.02884625\n",
      "Iteration 10206, loss = 0.02884145\n",
      "Iteration 10207, loss = 0.02883665\n",
      "Iteration 10208, loss = 0.02883184\n",
      "Iteration 10209, loss = 0.02882704\n",
      "Iteration 10210, loss = 0.02882224\n",
      "Iteration 10211, loss = 0.02881745\n",
      "Iteration 10212, loss = 0.02881265\n",
      "Iteration 10213, loss = 0.02880786\n",
      "Iteration 10214, loss = 0.02880306\n",
      "Iteration 10215, loss = 0.02879827\n",
      "Iteration 10216, loss = 0.02879348\n",
      "Iteration 10217, loss = 0.02878869\n",
      "Iteration 10218, loss = 0.02878391\n",
      "Iteration 10219, loss = 0.02877912\n",
      "Iteration 10220, loss = 0.02877434\n",
      "Iteration 10221, loss = 0.02876956\n",
      "Iteration 10222, loss = 0.02876478\n",
      "Iteration 10223, loss = 0.02876000\n",
      "Iteration 10224, loss = 0.02875522\n",
      "Iteration 10225, loss = 0.02875045\n",
      "Iteration 10226, loss = 0.02874567\n",
      "Iteration 10227, loss = 0.02874090\n",
      "Iteration 10228, loss = 0.02873613\n",
      "Iteration 10229, loss = 0.02873136\n",
      "Iteration 10230, loss = 0.02872659\n",
      "Iteration 10231, loss = 0.02872183\n",
      "Iteration 10232, loss = 0.02871706\n",
      "Iteration 10233, loss = 0.02871230\n",
      "Iteration 10234, loss = 0.02870754\n",
      "Iteration 10235, loss = 0.02870278\n",
      "Iteration 10236, loss = 0.02869802\n",
      "Iteration 10237, loss = 0.02869326\n",
      "Iteration 10238, loss = 0.02868850\n",
      "Iteration 10239, loss = 0.02868375\n",
      "Iteration 10240, loss = 0.02867900\n",
      "Iteration 10241, loss = 0.02867425\n",
      "Iteration 10242, loss = 0.02866950\n",
      "Iteration 10243, loss = 0.02866475\n",
      "Iteration 10244, loss = 0.02866000\n",
      "Iteration 10245, loss = 0.02865526\n",
      "Iteration 10246, loss = 0.02865052\n",
      "Iteration 10247, loss = 0.02864577\n",
      "Iteration 10248, loss = 0.02864103\n",
      "Iteration 10249, loss = 0.02863630\n",
      "Iteration 10250, loss = 0.02863156\n",
      "Iteration 10251, loss = 0.02862682\n",
      "Iteration 10252, loss = 0.02862209\n",
      "Iteration 10253, loss = 0.02861736\n",
      "Iteration 10254, loss = 0.02861263\n",
      "Iteration 10255, loss = 0.02860790\n",
      "Iteration 10256, loss = 0.02860317\n",
      "Iteration 10257, loss = 0.02859844\n",
      "Iteration 10258, loss = 0.02859372\n",
      "Iteration 10259, loss = 0.02858899\n",
      "Iteration 10260, loss = 0.02858427\n",
      "Iteration 10261, loss = 0.02857955\n",
      "Iteration 10262, loss = 0.02857483\n",
      "Iteration 10263, loss = 0.02857012\n",
      "Iteration 10264, loss = 0.02856540\n",
      "Iteration 10265, loss = 0.02856069\n",
      "Iteration 10266, loss = 0.02855597\n",
      "Iteration 10267, loss = 0.02855126\n",
      "Iteration 10268, loss = 0.02854655\n",
      "Iteration 10269, loss = 0.02854185\n",
      "Iteration 10270, loss = 0.02853714\n",
      "Iteration 10271, loss = 0.02853243\n",
      "Iteration 10272, loss = 0.02852773\n",
      "Iteration 10273, loss = 0.02852303\n",
      "Iteration 10274, loss = 0.02851833\n",
      "Iteration 10275, loss = 0.02851363\n",
      "Iteration 10276, loss = 0.02850893\n",
      "Iteration 10277, loss = 0.02850424\n",
      "Iteration 10278, loss = 0.02849954\n",
      "Iteration 10279, loss = 0.02849485\n",
      "Iteration 10280, loss = 0.02849016\n",
      "Iteration 10281, loss = 0.02848547\n",
      "Iteration 10282, loss = 0.02848078\n",
      "Iteration 10283, loss = 0.02847609\n",
      "Iteration 10284, loss = 0.02847141\n",
      "Iteration 10285, loss = 0.02846672\n",
      "Iteration 10286, loss = 0.02846204\n",
      "Iteration 10287, loss = 0.02845736\n",
      "Iteration 10288, loss = 0.02845268\n",
      "Iteration 10289, loss = 0.02844800\n",
      "Iteration 10290, loss = 0.02844333\n",
      "Iteration 10291, loss = 0.02843865\n",
      "Iteration 10292, loss = 0.02843398\n",
      "Iteration 10293, loss = 0.02842931\n",
      "Iteration 10294, loss = 0.02842464\n",
      "Iteration 10295, loss = 0.02841997\n",
      "Iteration 10296, loss = 0.02841530\n",
      "Iteration 10297, loss = 0.02841064\n",
      "Iteration 10298, loss = 0.02840597\n",
      "Iteration 10299, loss = 0.02840131\n",
      "Iteration 10300, loss = 0.02839665\n",
      "Iteration 10301, loss = 0.02839199\n",
      "Iteration 10302, loss = 0.02838733\n",
      "Iteration 10303, loss = 0.02838268\n",
      "Iteration 10304, loss = 0.02837802\n",
      "Iteration 10305, loss = 0.02837337\n",
      "Iteration 10306, loss = 0.02836871\n",
      "Iteration 10307, loss = 0.02836406\n",
      "Iteration 10308, loss = 0.02835941\n",
      "Iteration 10309, loss = 0.02835477\n",
      "Iteration 10310, loss = 0.02835012\n",
      "Iteration 10311, loss = 0.02834548\n",
      "Iteration 10312, loss = 0.02834083\n",
      "Iteration 10313, loss = 0.02833619\n",
      "Iteration 10314, loss = 0.02833155\n",
      "Iteration 10315, loss = 0.02832691\n",
      "Iteration 10316, loss = 0.02832228\n",
      "Iteration 10317, loss = 0.02831764\n",
      "Iteration 10318, loss = 0.02831301\n",
      "Iteration 10319, loss = 0.02830837\n",
      "Iteration 10320, loss = 0.02830374\n",
      "Iteration 10321, loss = 0.02829911\n",
      "Iteration 10322, loss = 0.02829448\n",
      "Iteration 10323, loss = 0.02828986\n",
      "Iteration 10324, loss = 0.02828523\n",
      "Iteration 10325, loss = 0.02828061\n",
      "Iteration 10326, loss = 0.02827598\n",
      "Iteration 10327, loss = 0.02827136\n",
      "Iteration 10328, loss = 0.02826674\n",
      "Iteration 10329, loss = 0.02826213\n",
      "Iteration 10330, loss = 0.02825751\n",
      "Iteration 10331, loss = 0.02825290\n",
      "Iteration 10332, loss = 0.02824828\n",
      "Iteration 10333, loss = 0.02824367\n",
      "Iteration 10334, loss = 0.02823906\n",
      "Iteration 10335, loss = 0.02823445\n",
      "Iteration 10336, loss = 0.02822984\n",
      "Iteration 10337, loss = 0.02822524\n",
      "Iteration 10338, loss = 0.02822063\n",
      "Iteration 10339, loss = 0.02821603\n",
      "Iteration 10340, loss = 0.02821143\n",
      "Iteration 10341, loss = 0.02820683\n",
      "Iteration 10342, loss = 0.02820223\n",
      "Iteration 10343, loss = 0.02819763\n",
      "Iteration 10344, loss = 0.02819304\n",
      "Iteration 10345, loss = 0.02818844\n",
      "Iteration 10346, loss = 0.02818385\n",
      "Iteration 10347, loss = 0.02817926\n",
      "Iteration 10348, loss = 0.02817467\n",
      "Iteration 10349, loss = 0.02817008\n",
      "Iteration 10350, loss = 0.02816549\n",
      "Iteration 10351, loss = 0.02816091\n",
      "Iteration 10352, loss = 0.02815632\n",
      "Iteration 10353, loss = 0.02815174\n",
      "Iteration 10354, loss = 0.02814716\n",
      "Iteration 10355, loss = 0.02814258\n",
      "Iteration 10356, loss = 0.02813800\n",
      "Iteration 10357, loss = 0.02813342\n",
      "Iteration 10358, loss = 0.02812885\n",
      "Iteration 10359, loss = 0.02812428\n",
      "Iteration 10360, loss = 0.02811970\n",
      "Iteration 10361, loss = 0.02811513\n",
      "Iteration 10362, loss = 0.02811056\n",
      "Iteration 10363, loss = 0.02810600\n",
      "Iteration 10364, loss = 0.02810143\n",
      "Iteration 10365, loss = 0.02809686\n",
      "Iteration 10366, loss = 0.02809230\n",
      "Iteration 10367, loss = 0.02808774\n",
      "Iteration 10368, loss = 0.02808318\n",
      "Iteration 10369, loss = 0.02807862\n",
      "Iteration 10370, loss = 0.02807406\n",
      "Iteration 10371, loss = 0.02806950\n",
      "Iteration 10372, loss = 0.02806495\n",
      "Iteration 10373, loss = 0.02806040\n",
      "Iteration 10374, loss = 0.02805584\n",
      "Iteration 10375, loss = 0.02805129\n",
      "Iteration 10376, loss = 0.02804674\n",
      "Iteration 10377, loss = 0.02804220\n",
      "Iteration 10378, loss = 0.02803765\n",
      "Iteration 10379, loss = 0.02803311\n",
      "Iteration 10380, loss = 0.02802856\n",
      "Iteration 10381, loss = 0.02802402\n",
      "Iteration 10382, loss = 0.02801948\n",
      "Iteration 10383, loss = 0.02801494\n",
      "Iteration 10384, loss = 0.02801041\n",
      "Iteration 10385, loss = 0.02800587\n",
      "Iteration 10386, loss = 0.02800133\n",
      "Iteration 10387, loss = 0.02799680\n",
      "Iteration 10388, loss = 0.02799227\n",
      "Iteration 10389, loss = 0.02798774\n",
      "Iteration 10390, loss = 0.02798321\n",
      "Iteration 10391, loss = 0.02797868\n",
      "Iteration 10392, loss = 0.02797416\n",
      "Iteration 10393, loss = 0.02796963\n",
      "Iteration 10394, loss = 0.02796511\n",
      "Iteration 10395, loss = 0.02796059\n",
      "Iteration 10396, loss = 0.02795607\n",
      "Iteration 10397, loss = 0.02795155\n",
      "Iteration 10398, loss = 0.02794703\n",
      "Iteration 10399, loss = 0.02794252\n",
      "Iteration 10400, loss = 0.02793800\n",
      "Iteration 10401, loss = 0.02793349\n",
      "Iteration 10402, loss = 0.02792898\n",
      "Iteration 10403, loss = 0.02792447\n",
      "Iteration 10404, loss = 0.02791996\n",
      "Iteration 10405, loss = 0.02791545\n",
      "Iteration 10406, loss = 0.02791095\n",
      "Iteration 10407, loss = 0.02790644\n",
      "Iteration 10408, loss = 0.02790194\n",
      "Iteration 10409, loss = 0.02789744\n",
      "Iteration 10410, loss = 0.02789294\n",
      "Iteration 10411, loss = 0.02788844\n",
      "Iteration 10412, loss = 0.02788394\n",
      "Iteration 10413, loss = 0.02787945\n",
      "Iteration 10414, loss = 0.02787495\n",
      "Iteration 10415, loss = 0.02787046\n",
      "Iteration 10416, loss = 0.02786597\n",
      "Iteration 10417, loss = 0.02786148\n",
      "Iteration 10418, loss = 0.02785699\n",
      "Iteration 10419, loss = 0.02785250\n",
      "Iteration 10420, loss = 0.02784802\n",
      "Iteration 10421, loss = 0.02784353\n",
      "Iteration 10422, loss = 0.02783905\n",
      "Iteration 10423, loss = 0.02783457\n",
      "Iteration 10424, loss = 0.02783009\n",
      "Iteration 10425, loss = 0.02782561\n",
      "Iteration 10426, loss = 0.02782113\n",
      "Iteration 10427, loss = 0.02781666\n",
      "Iteration 10428, loss = 0.02781218\n",
      "Iteration 10429, loss = 0.02780771\n",
      "Iteration 10430, loss = 0.02780324\n",
      "Iteration 10431, loss = 0.02779877\n",
      "Iteration 10432, loss = 0.02779430\n",
      "Iteration 10433, loss = 0.02778983\n",
      "Iteration 10434, loss = 0.02778536\n",
      "Iteration 10435, loss = 0.02778090\n",
      "Iteration 10436, loss = 0.02777644\n",
      "Iteration 10437, loss = 0.02777197\n",
      "Iteration 10438, loss = 0.02776751\n",
      "Iteration 10439, loss = 0.02776306\n",
      "Iteration 10440, loss = 0.02775860\n",
      "Iteration 10441, loss = 0.02775414\n",
      "Iteration 10442, loss = 0.02774969\n",
      "Iteration 10443, loss = 0.02774523\n",
      "Iteration 10444, loss = 0.02774078\n",
      "Iteration 10445, loss = 0.02773633\n",
      "Iteration 10446, loss = 0.02773188\n",
      "Iteration 10447, loss = 0.02772744\n",
      "Iteration 10448, loss = 0.02772299\n",
      "Iteration 10449, loss = 0.02771854\n",
      "Iteration 10450, loss = 0.02771410\n",
      "Iteration 10451, loss = 0.02770966\n",
      "Iteration 10452, loss = 0.02770522\n",
      "Iteration 10453, loss = 0.02770078\n",
      "Iteration 10454, loss = 0.02769634\n",
      "Iteration 10455, loss = 0.02769191\n",
      "Iteration 10456, loss = 0.02768747\n",
      "Iteration 10457, loss = 0.02768304\n",
      "Iteration 10458, loss = 0.02767860\n",
      "Iteration 10459, loss = 0.02767417\n",
      "Iteration 10460, loss = 0.02766974\n",
      "Iteration 10461, loss = 0.02766532\n",
      "Iteration 10462, loss = 0.02766089\n",
      "Iteration 10463, loss = 0.02765646\n",
      "Iteration 10464, loss = 0.02765204\n",
      "Iteration 10465, loss = 0.02764762\n",
      "Iteration 10466, loss = 0.02764320\n",
      "Iteration 10467, loss = 0.02763878\n",
      "Iteration 10468, loss = 0.02763436\n",
      "Iteration 10469, loss = 0.02762994\n",
      "Iteration 10470, loss = 0.02762553\n",
      "Iteration 10471, loss = 0.02762111\n",
      "Iteration 10472, loss = 0.02761670\n",
      "Iteration 10473, loss = 0.02761229\n",
      "Iteration 10474, loss = 0.02760788\n",
      "Iteration 10475, loss = 0.02760347\n",
      "Iteration 10476, loss = 0.02759906\n",
      "Iteration 10477, loss = 0.02759466\n",
      "Iteration 10478, loss = 0.02759025\n",
      "Iteration 10479, loss = 0.02758585\n",
      "Iteration 10480, loss = 0.02758145\n",
      "Iteration 10481, loss = 0.02757705\n",
      "Iteration 10482, loss = 0.02757265\n",
      "Iteration 10483, loss = 0.02756825\n",
      "Iteration 10484, loss = 0.02756386\n",
      "Iteration 10485, loss = 0.02755946\n",
      "Iteration 10486, loss = 0.02755507\n",
      "Iteration 10487, loss = 0.02755068\n",
      "Iteration 10488, loss = 0.02754629\n",
      "Iteration 10489, loss = 0.02754190\n",
      "Iteration 10490, loss = 0.02753751\n",
      "Iteration 10491, loss = 0.02753312\n",
      "Iteration 10492, loss = 0.02752874\n",
      "Iteration 10493, loss = 0.02752436\n",
      "Iteration 10494, loss = 0.02751997\n",
      "Iteration 10495, loss = 0.02751559\n",
      "Iteration 10496, loss = 0.02751121\n",
      "Iteration 10497, loss = 0.02750684\n",
      "Iteration 10498, loss = 0.02750246\n",
      "Iteration 10499, loss = 0.02749808\n",
      "Iteration 10500, loss = 0.02749371\n",
      "Iteration 10501, loss = 0.02748934\n",
      "Iteration 10502, loss = 0.02748497\n",
      "Iteration 10503, loss = 0.02748060\n",
      "Iteration 10504, loss = 0.02747623\n",
      "Iteration 10505, loss = 0.02747186\n",
      "Iteration 10506, loss = 0.02746749\n",
      "Iteration 10507, loss = 0.02746313\n",
      "Iteration 10508, loss = 0.02745877\n",
      "Iteration 10509, loss = 0.02745441\n",
      "Iteration 10510, loss = 0.02745005\n",
      "Iteration 10511, loss = 0.02744569\n",
      "Iteration 10512, loss = 0.02744133\n",
      "Iteration 10513, loss = 0.02743697\n",
      "Iteration 10514, loss = 0.02743262\n",
      "Iteration 10515, loss = 0.02742827\n",
      "Iteration 10516, loss = 0.02742391\n",
      "Iteration 10517, loss = 0.02741956\n",
      "Iteration 10518, loss = 0.02741521\n",
      "Iteration 10519, loss = 0.02741087\n",
      "Iteration 10520, loss = 0.02740652\n",
      "Iteration 10521, loss = 0.02740217\n",
      "Iteration 10522, loss = 0.02739783\n",
      "Iteration 10523, loss = 0.02739349\n",
      "Iteration 10524, loss = 0.02738915\n",
      "Iteration 10525, loss = 0.02738481\n",
      "Iteration 10526, loss = 0.02738047\n",
      "Iteration 10527, loss = 0.02737613\n",
      "Iteration 10528, loss = 0.02737179\n",
      "Iteration 10529, loss = 0.02736746\n",
      "Iteration 10530, loss = 0.02736313\n",
      "Iteration 10531, loss = 0.02735880\n",
      "Iteration 10532, loss = 0.02735447\n",
      "Iteration 10533, loss = 0.02735014\n",
      "Iteration 10534, loss = 0.02734581\n",
      "Iteration 10535, loss = 0.02734148\n",
      "Iteration 10536, loss = 0.02733716\n",
      "Iteration 10537, loss = 0.02733283\n",
      "Iteration 10538, loss = 0.02732851\n",
      "Iteration 10539, loss = 0.02732419\n",
      "Iteration 10540, loss = 0.02731987\n",
      "Iteration 10541, loss = 0.02731555\n",
      "Iteration 10542, loss = 0.02731124\n",
      "Iteration 10543, loss = 0.02730692\n",
      "Iteration 10544, loss = 0.02730261\n",
      "Iteration 10545, loss = 0.02729829\n",
      "Iteration 10546, loss = 0.02729398\n",
      "Iteration 10547, loss = 0.02728967\n",
      "Iteration 10548, loss = 0.02728536\n",
      "Iteration 10549, loss = 0.02728106\n",
      "Iteration 10550, loss = 0.02727675\n",
      "Iteration 10551, loss = 0.02727245\n",
      "Iteration 10552, loss = 0.02726814\n",
      "Iteration 10553, loss = 0.02726384\n",
      "Iteration 10554, loss = 0.02725954\n",
      "Iteration 10555, loss = 0.02725524\n",
      "Iteration 10556, loss = 0.02725094\n",
      "Iteration 10557, loss = 0.02724665\n",
      "Iteration 10558, loss = 0.02724235\n",
      "Iteration 10559, loss = 0.02723806\n",
      "Iteration 10560, loss = 0.02723377\n",
      "Iteration 10561, loss = 0.02722947\n",
      "Iteration 10562, loss = 0.02722518\n",
      "Iteration 10563, loss = 0.02722090\n",
      "Iteration 10564, loss = 0.02721661\n",
      "Iteration 10565, loss = 0.02721232\n",
      "Iteration 10566, loss = 0.02720804\n",
      "Iteration 10567, loss = 0.02720375\n",
      "Iteration 10568, loss = 0.02719947\n",
      "Iteration 10569, loss = 0.02719519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10570, loss = 0.02719091\n",
      "Iteration 10571, loss = 0.02718663\n",
      "Iteration 10572, loss = 0.02718236\n",
      "Iteration 10573, loss = 0.02717808\n",
      "Iteration 10574, loss = 0.02717381\n",
      "Iteration 10575, loss = 0.02716954\n",
      "Iteration 10576, loss = 0.02716527\n",
      "Iteration 10577, loss = 0.02716100\n",
      "Iteration 10578, loss = 0.02715673\n",
      "Iteration 10579, loss = 0.02715246\n",
      "Iteration 10580, loss = 0.02714819\n",
      "Iteration 10581, loss = 0.02714393\n",
      "Iteration 10582, loss = 0.02713967\n",
      "Iteration 10583, loss = 0.02713540\n",
      "Iteration 10584, loss = 0.02713114\n",
      "Iteration 10585, loss = 0.02712688\n",
      "Iteration 10586, loss = 0.02712263\n",
      "Iteration 10587, loss = 0.02711837\n",
      "Iteration 10588, loss = 0.02711411\n",
      "Iteration 10589, loss = 0.02710986\n",
      "Iteration 10590, loss = 0.02710561\n",
      "Iteration 10591, loss = 0.02710136\n",
      "Iteration 10592, loss = 0.02709711\n",
      "Iteration 10593, loss = 0.02709286\n",
      "Iteration 10594, loss = 0.02708861\n",
      "Iteration 10595, loss = 0.02708436\n",
      "Iteration 10596, loss = 0.02708012\n",
      "Iteration 10597, loss = 0.02707588\n",
      "Iteration 10598, loss = 0.02707163\n",
      "Iteration 10599, loss = 0.02706739\n",
      "Iteration 10600, loss = 0.02706315\n",
      "Iteration 10601, loss = 0.02705891\n",
      "Iteration 10602, loss = 0.02705468\n",
      "Iteration 10603, loss = 0.02705044\n",
      "Iteration 10604, loss = 0.02704621\n",
      "Iteration 10605, loss = 0.02704197\n",
      "Iteration 10606, loss = 0.02703774\n",
      "Iteration 10607, loss = 0.02703351\n",
      "Iteration 10608, loss = 0.02702928\n",
      "Iteration 10609, loss = 0.02702506\n",
      "Iteration 10610, loss = 0.02702083\n",
      "Iteration 10611, loss = 0.02701660\n",
      "Iteration 10612, loss = 0.02701238\n",
      "Iteration 10613, loss = 0.02700816\n",
      "Iteration 10614, loss = 0.02700394\n",
      "Iteration 10615, loss = 0.02699972\n",
      "Iteration 10616, loss = 0.02699550\n",
      "Iteration 10617, loss = 0.02699128\n",
      "Iteration 10618, loss = 0.02698707\n",
      "Iteration 10619, loss = 0.02698285\n",
      "Iteration 10620, loss = 0.02697864\n",
      "Iteration 10621, loss = 0.02697443\n",
      "Iteration 10622, loss = 0.02697021\n",
      "Iteration 10623, loss = 0.02696601\n",
      "Iteration 10624, loss = 0.02696180\n",
      "Iteration 10625, loss = 0.02695759\n",
      "Iteration 10626, loss = 0.02695338\n",
      "Iteration 10627, loss = 0.02694918\n",
      "Iteration 10628, loss = 0.02694498\n",
      "Iteration 10629, loss = 0.02694078\n",
      "Iteration 10630, loss = 0.02693658\n",
      "Iteration 10631, loss = 0.02693238\n",
      "Iteration 10632, loss = 0.02692818\n",
      "Iteration 10633, loss = 0.02692398\n",
      "Iteration 10634, loss = 0.02691979\n",
      "Iteration 10635, loss = 0.02691559\n",
      "Iteration 10636, loss = 0.02691140\n",
      "Iteration 10637, loss = 0.02690721\n",
      "Iteration 10638, loss = 0.02690302\n",
      "Iteration 10639, loss = 0.02689883\n",
      "Iteration 10640, loss = 0.02689464\n",
      "Iteration 10641, loss = 0.02689046\n",
      "Iteration 10642, loss = 0.02688627\n",
      "Iteration 10643, loss = 0.02688209\n",
      "Iteration 10644, loss = 0.02687791\n",
      "Iteration 10645, loss = 0.02687373\n",
      "Iteration 10646, loss = 0.02686955\n",
      "Iteration 10647, loss = 0.02686537\n",
      "Iteration 10648, loss = 0.02686119\n",
      "Iteration 10649, loss = 0.02685702\n",
      "Iteration 10650, loss = 0.02685284\n",
      "Iteration 10651, loss = 0.02684867\n",
      "Iteration 10652, loss = 0.02684450\n",
      "Iteration 10653, loss = 0.02684033\n",
      "Iteration 10654, loss = 0.02683616\n",
      "Iteration 10655, loss = 0.02683199\n",
      "Iteration 10656, loss = 0.02682782\n",
      "Iteration 10657, loss = 0.02682366\n",
      "Iteration 10658, loss = 0.02681949\n",
      "Iteration 10659, loss = 0.02681533\n",
      "Iteration 10660, loss = 0.02681117\n",
      "Iteration 10661, loss = 0.02680701\n",
      "Iteration 10662, loss = 0.02680285\n",
      "Iteration 10663, loss = 0.02679869\n",
      "Iteration 10664, loss = 0.02679453\n",
      "Iteration 10665, loss = 0.02679038\n",
      "Iteration 10666, loss = 0.02678622\n",
      "Iteration 10667, loss = 0.02678207\n",
      "Iteration 10668, loss = 0.02677792\n",
      "Iteration 10669, loss = 0.02677377\n",
      "Iteration 10670, loss = 0.02676962\n",
      "Iteration 10671, loss = 0.02676547\n",
      "Iteration 10672, loss = 0.02676133\n",
      "Iteration 10673, loss = 0.02675718\n",
      "Iteration 10674, loss = 0.02675304\n",
      "Iteration 10675, loss = 0.02674890\n",
      "Iteration 10676, loss = 0.02674476\n",
      "Iteration 10677, loss = 0.02674062\n",
      "Iteration 10678, loss = 0.02673648\n",
      "Iteration 10679, loss = 0.02673234\n",
      "Iteration 10680, loss = 0.02672820\n",
      "Iteration 10681, loss = 0.02672407\n",
      "Iteration 10682, loss = 0.02671993\n",
      "Iteration 10683, loss = 0.02671580\n",
      "Iteration 10684, loss = 0.02671167\n",
      "Iteration 10685, loss = 0.02670754\n",
      "Iteration 10686, loss = 0.02670341\n",
      "Iteration 10687, loss = 0.02669929\n",
      "Iteration 10688, loss = 0.02669516\n",
      "Iteration 10689, loss = 0.02669104\n",
      "Iteration 10690, loss = 0.02668691\n",
      "Iteration 10691, loss = 0.02668279\n",
      "Iteration 10692, loss = 0.02667867\n",
      "Iteration 10693, loss = 0.02667455\n",
      "Iteration 10694, loss = 0.02667043\n",
      "Iteration 10695, loss = 0.02666631\n",
      "Iteration 10696, loss = 0.02666220\n",
      "Iteration 10697, loss = 0.02665808\n",
      "Iteration 10698, loss = 0.02665397\n",
      "Iteration 10699, loss = 0.02664986\n",
      "Iteration 10700, loss = 0.02664575\n",
      "Iteration 10701, loss = 0.02664164\n",
      "Iteration 10702, loss = 0.02663753\n",
      "Iteration 10703, loss = 0.02663342\n",
      "Iteration 10704, loss = 0.02662932\n",
      "Iteration 10705, loss = 0.02662521\n",
      "Iteration 10706, loss = 0.02662111\n",
      "Iteration 10707, loss = 0.02661701\n",
      "Iteration 10708, loss = 0.02661291\n",
      "Iteration 10709, loss = 0.02660881\n",
      "Iteration 10710, loss = 0.02660471\n",
      "Iteration 10711, loss = 0.02660061\n",
      "Iteration 10712, loss = 0.02659652\n",
      "Iteration 10713, loss = 0.02659242\n",
      "Iteration 10714, loss = 0.02658833\n",
      "Iteration 10715, loss = 0.02658424\n",
      "Iteration 10716, loss = 0.02658015\n",
      "Iteration 10717, loss = 0.02657606\n",
      "Iteration 10718, loss = 0.02657197\n",
      "Iteration 10719, loss = 0.02656788\n",
      "Iteration 10720, loss = 0.02656380\n",
      "Iteration 10721, loss = 0.02655971\n",
      "Iteration 10722, loss = 0.02655563\n",
      "Iteration 10723, loss = 0.02655155\n",
      "Iteration 10724, loss = 0.02654746\n",
      "Iteration 10725, loss = 0.02654339\n",
      "Iteration 10726, loss = 0.02653931\n",
      "Iteration 10727, loss = 0.02653523\n",
      "Iteration 10728, loss = 0.02653115\n",
      "Iteration 10729, loss = 0.02652708\n",
      "Iteration 10730, loss = 0.02652301\n",
      "Iteration 10731, loss = 0.02651893\n",
      "Iteration 10732, loss = 0.02651486\n",
      "Iteration 10733, loss = 0.02651079\n",
      "Iteration 10734, loss = 0.02650673\n",
      "Iteration 10735, loss = 0.02650266\n",
      "Iteration 10736, loss = 0.02649859\n",
      "Iteration 10737, loss = 0.02649453\n",
      "Iteration 10738, loss = 0.02649047\n",
      "Iteration 10739, loss = 0.02648640\n",
      "Iteration 10740, loss = 0.02648234\n",
      "Iteration 10741, loss = 0.02647828\n",
      "Iteration 10742, loss = 0.02647422\n",
      "Iteration 10743, loss = 0.02647017\n",
      "Iteration 10744, loss = 0.02646611\n",
      "Iteration 10745, loss = 0.02646206\n",
      "Iteration 10746, loss = 0.02645800\n",
      "Iteration 10747, loss = 0.02645395\n",
      "Iteration 10748, loss = 0.02644990\n",
      "Iteration 10749, loss = 0.02644585\n",
      "Iteration 10750, loss = 0.02644180\n",
      "Iteration 10751, loss = 0.02643776\n",
      "Iteration 10752, loss = 0.02643371\n",
      "Iteration 10753, loss = 0.02642966\n",
      "Iteration 10754, loss = 0.02642562\n",
      "Iteration 10755, loss = 0.02642158\n",
      "Iteration 10756, loss = 0.02641754\n",
      "Iteration 10757, loss = 0.02641350\n",
      "Iteration 10758, loss = 0.02640946\n",
      "Iteration 10759, loss = 0.02640542\n",
      "Iteration 10760, loss = 0.02640139\n",
      "Iteration 10761, loss = 0.02639735\n",
      "Iteration 10762, loss = 0.02639332\n",
      "Iteration 10763, loss = 0.02638928\n",
      "Iteration 10764, loss = 0.02638525\n",
      "Iteration 10765, loss = 0.02638122\n",
      "Iteration 10766, loss = 0.02637719\n",
      "Iteration 10767, loss = 0.02637317\n",
      "Iteration 10768, loss = 0.02636914\n",
      "Iteration 10769, loss = 0.02636512\n",
      "Iteration 10770, loss = 0.02636109\n",
      "Iteration 10771, loss = 0.02635707\n",
      "Iteration 10772, loss = 0.02635305\n",
      "Iteration 10773, loss = 0.02634903\n",
      "Iteration 10774, loss = 0.02634501\n",
      "Iteration 10775, loss = 0.02634099\n",
      "Iteration 10776, loss = 0.02633697\n",
      "Iteration 10777, loss = 0.02633296\n",
      "Iteration 10778, loss = 0.02632894\n",
      "Iteration 10779, loss = 0.02632493\n",
      "Iteration 10780, loss = 0.02632092\n",
      "Iteration 10781, loss = 0.02631691\n",
      "Iteration 10782, loss = 0.02631290\n",
      "Iteration 10783, loss = 0.02630889\n",
      "Iteration 10784, loss = 0.02630489\n",
      "Iteration 10785, loss = 0.02630088\n",
      "Iteration 10786, loss = 0.02629688\n",
      "Iteration 10787, loss = 0.02629287\n",
      "Iteration 10788, loss = 0.02628887\n",
      "Iteration 10789, loss = 0.02628487\n",
      "Iteration 10790, loss = 0.02628087\n",
      "Iteration 10791, loss = 0.02627687\n",
      "Iteration 10792, loss = 0.02627287\n",
      "Iteration 10793, loss = 0.02626888\n",
      "Iteration 10794, loss = 0.02626488\n",
      "Iteration 10795, loss = 0.02626089\n",
      "Iteration 10796, loss = 0.02625690\n",
      "Iteration 10797, loss = 0.02625291\n",
      "Iteration 10798, loss = 0.02624892\n",
      "Iteration 10799, loss = 0.02624493\n",
      "Iteration 10800, loss = 0.02624094\n",
      "Iteration 10801, loss = 0.02623695\n",
      "Iteration 10802, loss = 0.02623297\n",
      "Iteration 10803, loss = 0.02622899\n",
      "Iteration 10804, loss = 0.02622500\n",
      "Iteration 10805, loss = 0.02622102\n",
      "Iteration 10806, loss = 0.02621704\n",
      "Iteration 10807, loss = 0.02621306\n",
      "Iteration 10808, loss = 0.02620908\n",
      "Iteration 10809, loss = 0.02620511\n",
      "Iteration 10810, loss = 0.02620113\n",
      "Iteration 10811, loss = 0.02619716\n",
      "Iteration 10812, loss = 0.02619319\n",
      "Iteration 10813, loss = 0.02618921\n",
      "Iteration 10814, loss = 0.02618524\n",
      "Iteration 10815, loss = 0.02618127\n",
      "Iteration 10816, loss = 0.02617731\n",
      "Iteration 10817, loss = 0.02617334\n",
      "Iteration 10818, loss = 0.02616937\n",
      "Iteration 10819, loss = 0.02616541\n",
      "Iteration 10820, loss = 0.02616144\n",
      "Iteration 10821, loss = 0.02615748\n",
      "Iteration 10822, loss = 0.02615352\n",
      "Iteration 10823, loss = 0.02614956\n",
      "Iteration 10824, loss = 0.02614560\n",
      "Iteration 10825, loss = 0.02614165\n",
      "Iteration 10826, loss = 0.02613769\n",
      "Iteration 10827, loss = 0.02613373\n",
      "Iteration 10828, loss = 0.02612978\n",
      "Iteration 10829, loss = 0.02612583\n",
      "Iteration 10830, loss = 0.02612188\n",
      "Iteration 10831, loss = 0.02611793\n",
      "Iteration 10832, loss = 0.02611398\n",
      "Iteration 10833, loss = 0.02611003\n",
      "Iteration 10834, loss = 0.02610608\n",
      "Iteration 10835, loss = 0.02610214\n",
      "Iteration 10836, loss = 0.02609819\n",
      "Iteration 10837, loss = 0.02609425\n",
      "Iteration 10838, loss = 0.02609031\n",
      "Iteration 10839, loss = 0.02608637\n",
      "Iteration 10840, loss = 0.02608243\n",
      "Iteration 10841, loss = 0.02607849\n",
      "Iteration 10842, loss = 0.02607455\n",
      "Iteration 10843, loss = 0.02607062\n",
      "Iteration 10844, loss = 0.02606668\n",
      "Iteration 10845, loss = 0.02606275\n",
      "Iteration 10846, loss = 0.02605882\n",
      "Iteration 10847, loss = 0.02605488\n",
      "Iteration 10848, loss = 0.02605095\n",
      "Iteration 10849, loss = 0.02604703\n",
      "Iteration 10850, loss = 0.02604310\n",
      "Iteration 10851, loss = 0.02603917\n",
      "Iteration 10852, loss = 0.02603525\n",
      "Iteration 10853, loss = 0.02603132\n",
      "Iteration 10854, loss = 0.02602740\n",
      "Iteration 10855, loss = 0.02602348\n",
      "Iteration 10856, loss = 0.02601956\n",
      "Iteration 10857, loss = 0.02601564\n",
      "Iteration 10858, loss = 0.02601172\n",
      "Iteration 10859, loss = 0.02600780\n",
      "Iteration 10860, loss = 0.02600389\n",
      "Iteration 10861, loss = 0.02599997\n",
      "Iteration 10862, loss = 0.02599606\n",
      "Iteration 10863, loss = 0.02599215\n",
      "Iteration 10864, loss = 0.02598823\n",
      "Iteration 10865, loss = 0.02598432\n",
      "Iteration 10866, loss = 0.02598041\n",
      "Iteration 10867, loss = 0.02597651\n",
      "Iteration 10868, loss = 0.02597260\n",
      "Iteration 10869, loss = 0.02596870\n",
      "Iteration 10870, loss = 0.02596479\n",
      "Iteration 10871, loss = 0.02596089\n",
      "Iteration 10872, loss = 0.02595699\n",
      "Iteration 10873, loss = 0.02595309\n",
      "Iteration 10874, loss = 0.02594919\n",
      "Iteration 10875, loss = 0.02594529\n",
      "Iteration 10876, loss = 0.02594139\n",
      "Iteration 10877, loss = 0.02593750\n",
      "Iteration 10878, loss = 0.02593360\n",
      "Iteration 10879, loss = 0.02592971\n",
      "Iteration 10880, loss = 0.02592581\n",
      "Iteration 10881, loss = 0.02592192\n",
      "Iteration 10882, loss = 0.02591803\n",
      "Iteration 10883, loss = 0.02591414\n",
      "Iteration 10884, loss = 0.02591026\n",
      "Iteration 10885, loss = 0.02590637\n",
      "Iteration 10886, loss = 0.02590248\n",
      "Iteration 10887, loss = 0.02589860\n",
      "Iteration 10888, loss = 0.02589472\n",
      "Iteration 10889, loss = 0.02589083\n",
      "Iteration 10890, loss = 0.02588695\n",
      "Iteration 10891, loss = 0.02588307\n",
      "Iteration 10892, loss = 0.02587920\n",
      "Iteration 10893, loss = 0.02587532\n",
      "Iteration 10894, loss = 0.02587144\n",
      "Iteration 10895, loss = 0.02586757\n",
      "Iteration 10896, loss = 0.02586369\n",
      "Iteration 10897, loss = 0.02585982\n",
      "Iteration 10898, loss = 0.02585595\n",
      "Iteration 10899, loss = 0.02585208\n",
      "Iteration 10900, loss = 0.02584821\n",
      "Iteration 10901, loss = 0.02584434\n",
      "Iteration 10902, loss = 0.02584048\n",
      "Iteration 10903, loss = 0.02583661\n",
      "Iteration 10904, loss = 0.02583274\n",
      "Iteration 10905, loss = 0.02582888\n",
      "Iteration 10906, loss = 0.02582502\n",
      "Iteration 10907, loss = 0.02582116\n",
      "Iteration 10908, loss = 0.02581730\n",
      "Iteration 10909, loss = 0.02581344\n",
      "Iteration 10910, loss = 0.02580958\n",
      "Iteration 10911, loss = 0.02580573\n",
      "Iteration 10912, loss = 0.02580187\n",
      "Iteration 10913, loss = 0.02579802\n",
      "Iteration 10914, loss = 0.02579416\n",
      "Iteration 10915, loss = 0.02579031\n",
      "Iteration 10916, loss = 0.02578646\n",
      "Iteration 10917, loss = 0.02578261\n",
      "Iteration 10918, loss = 0.02577876\n",
      "Iteration 10919, loss = 0.02577491\n",
      "Iteration 10920, loss = 0.02577107\n",
      "Iteration 10921, loss = 0.02576722\n",
      "Iteration 10922, loss = 0.02576338\n",
      "Iteration 10923, loss = 0.02575954\n",
      "Iteration 10924, loss = 0.02575570\n",
      "Iteration 10925, loss = 0.02575186\n",
      "Iteration 10926, loss = 0.02574802\n",
      "Iteration 10927, loss = 0.02574418\n",
      "Iteration 10928, loss = 0.02574034\n",
      "Iteration 10929, loss = 0.02573651\n",
      "Iteration 10930, loss = 0.02573267\n",
      "Iteration 10931, loss = 0.02572884\n",
      "Iteration 10932, loss = 0.02572500\n",
      "Iteration 10933, loss = 0.02572117\n",
      "Iteration 10934, loss = 0.02571734\n",
      "Iteration 10935, loss = 0.02571351\n",
      "Iteration 10936, loss = 0.02570969\n",
      "Iteration 10937, loss = 0.02570586\n",
      "Iteration 10938, loss = 0.02570203\n",
      "Iteration 10939, loss = 0.02569821\n",
      "Iteration 10940, loss = 0.02569439\n",
      "Iteration 10941, loss = 0.02569056\n",
      "Iteration 10942, loss = 0.02568674\n",
      "Iteration 10943, loss = 0.02568292\n",
      "Iteration 10944, loss = 0.02567910\n",
      "Iteration 10945, loss = 0.02567529\n",
      "Iteration 10946, loss = 0.02567147\n",
      "Iteration 10947, loss = 0.02566766\n",
      "Iteration 10948, loss = 0.02566384\n",
      "Iteration 10949, loss = 0.02566003\n",
      "Iteration 10950, loss = 0.02565622\n",
      "Iteration 10951, loss = 0.02565241\n",
      "Iteration 10952, loss = 0.02564860\n",
      "Iteration 10953, loss = 0.02564479\n",
      "Iteration 10954, loss = 0.02564098\n",
      "Iteration 10955, loss = 0.02563717\n",
      "Iteration 10956, loss = 0.02563337\n",
      "Iteration 10957, loss = 0.02562957\n",
      "Iteration 10958, loss = 0.02562576\n",
      "Iteration 10959, loss = 0.02562196\n",
      "Iteration 10960, loss = 0.02561816\n",
      "Iteration 10961, loss = 0.02561436\n",
      "Iteration 10962, loss = 0.02561056\n",
      "Iteration 10963, loss = 0.02560676\n",
      "Iteration 10964, loss = 0.02560297\n",
      "Iteration 10965, loss = 0.02559917\n",
      "Iteration 10966, loss = 0.02559538\n",
      "Iteration 10967, loss = 0.02559159\n",
      "Iteration 10968, loss = 0.02558780\n",
      "Iteration 10969, loss = 0.02558401\n",
      "Iteration 10970, loss = 0.02558022\n",
      "Iteration 10971, loss = 0.02557643\n",
      "Iteration 10972, loss = 0.02557264\n",
      "Iteration 10973, loss = 0.02556886\n",
      "Iteration 10974, loss = 0.02556507\n",
      "Iteration 10975, loss = 0.02556129\n",
      "Iteration 10976, loss = 0.02555750\n",
      "Iteration 10977, loss = 0.02555372\n",
      "Iteration 10978, loss = 0.02554994\n",
      "Iteration 10979, loss = 0.02554616\n",
      "Iteration 10980, loss = 0.02554238\n",
      "Iteration 10981, loss = 0.02553861\n",
      "Iteration 10982, loss = 0.02553483\n",
      "Iteration 10983, loss = 0.02553106\n",
      "Iteration 10984, loss = 0.02552728\n",
      "Iteration 10985, loss = 0.02552351\n",
      "Iteration 10986, loss = 0.02551974\n",
      "Iteration 10987, loss = 0.02551597\n",
      "Iteration 10988, loss = 0.02551220\n",
      "Iteration 10989, loss = 0.02550843\n",
      "Iteration 10990, loss = 0.02550467\n",
      "Iteration 10991, loss = 0.02550090\n",
      "Iteration 10992, loss = 0.02549714\n",
      "Iteration 10993, loss = 0.02549337\n",
      "Iteration 10994, loss = 0.02548961\n",
      "Iteration 10995, loss = 0.02548585\n",
      "Iteration 10996, loss = 0.02548209\n",
      "Iteration 10997, loss = 0.02547833\n",
      "Iteration 10998, loss = 0.02547457\n",
      "Iteration 10999, loss = 0.02547081\n",
      "Iteration 11000, loss = 0.02546706\n",
      "Iteration 11001, loss = 0.02546330\n",
      "Iteration 11002, loss = 0.02545955\n",
      "Iteration 11003, loss = 0.02545580\n",
      "Iteration 11004, loss = 0.02545205\n",
      "Iteration 11005, loss = 0.02544830\n",
      "Iteration 11006, loss = 0.02544455\n",
      "Iteration 11007, loss = 0.02544080\n",
      "Iteration 11008, loss = 0.02543705\n",
      "Iteration 11009, loss = 0.02543331\n",
      "Iteration 11010, loss = 0.02542956\n",
      "Iteration 11011, loss = 0.02542582\n",
      "Iteration 11012, loss = 0.02542208\n",
      "Iteration 11013, loss = 0.02541833\n",
      "Iteration 11014, loss = 0.02541459\n",
      "Iteration 11015, loss = 0.02541085\n",
      "Iteration 11016, loss = 0.02540712\n",
      "Iteration 11017, loss = 0.02540338\n",
      "Iteration 11018, loss = 0.02539964\n",
      "Iteration 11019, loss = 0.02539591\n",
      "Iteration 11020, loss = 0.02539218\n",
      "Iteration 11021, loss = 0.02538844\n",
      "Iteration 11022, loss = 0.02538471\n",
      "Iteration 11023, loss = 0.02538098\n",
      "Iteration 11024, loss = 0.02537725\n",
      "Iteration 11025, loss = 0.02537352\n",
      "Iteration 11026, loss = 0.02536980\n",
      "Iteration 11027, loss = 0.02536607\n",
      "Iteration 11028, loss = 0.02536235\n",
      "Iteration 11029, loss = 0.02535862\n",
      "Iteration 11030, loss = 0.02535490\n",
      "Iteration 11031, loss = 0.02535118\n",
      "Iteration 11032, loss = 0.02534746\n",
      "Iteration 11033, loss = 0.02534374\n",
      "Iteration 11034, loss = 0.02534002\n",
      "Iteration 11035, loss = 0.02533630\n",
      "Iteration 11036, loss = 0.02533259\n",
      "Iteration 11037, loss = 0.02532887\n",
      "Iteration 11038, loss = 0.02532516\n",
      "Iteration 11039, loss = 0.02532144\n",
      "Iteration 11040, loss = 0.02531773\n",
      "Iteration 11041, loss = 0.02531402\n",
      "Iteration 11042, loss = 0.02531031\n",
      "Iteration 11043, loss = 0.02530660\n",
      "Iteration 11044, loss = 0.02530290\n",
      "Iteration 11045, loss = 0.02529919\n",
      "Iteration 11046, loss = 0.02529549\n",
      "Iteration 11047, loss = 0.02529178\n",
      "Iteration 11048, loss = 0.02528808\n",
      "Iteration 11049, loss = 0.02528438\n",
      "Iteration 11050, loss = 0.02528068\n",
      "Iteration 11051, loss = 0.02527698\n",
      "Iteration 11052, loss = 0.02527328\n",
      "Iteration 11053, loss = 0.02526958\n",
      "Iteration 11054, loss = 0.02526588\n",
      "Iteration 11055, loss = 0.02526219\n",
      "Iteration 11056, loss = 0.02525849\n",
      "Iteration 11057, loss = 0.02525480\n",
      "Iteration 11058, loss = 0.02525111\n",
      "Iteration 11059, loss = 0.02524742\n",
      "Iteration 11060, loss = 0.02524373\n",
      "Iteration 11061, loss = 0.02524004\n",
      "Iteration 11062, loss = 0.02523635\n",
      "Iteration 11063, loss = 0.02523266\n",
      "Iteration 11064, loss = 0.02522898\n",
      "Iteration 11065, loss = 0.02522529\n",
      "Iteration 11066, loss = 0.02522161\n",
      "Iteration 11067, loss = 0.02521793\n",
      "Iteration 11068, loss = 0.02521425\n",
      "Iteration 11069, loss = 0.02521057\n",
      "Iteration 11070, loss = 0.02520689\n",
      "Iteration 11071, loss = 0.02520321\n",
      "Iteration 11072, loss = 0.02519953\n",
      "Iteration 11073, loss = 0.02519586\n",
      "Iteration 11074, loss = 0.02519218\n",
      "Iteration 11075, loss = 0.02518851\n",
      "Iteration 11076, loss = 0.02518484\n",
      "Iteration 11077, loss = 0.02518116\n",
      "Iteration 11078, loss = 0.02517749\n",
      "Iteration 11079, loss = 0.02517382\n",
      "Iteration 11080, loss = 0.02517016\n",
      "Iteration 11081, loss = 0.02516649\n",
      "Iteration 11082, loss = 0.02516282\n",
      "Iteration 11083, loss = 0.02515916\n",
      "Iteration 11084, loss = 0.02515549\n",
      "Iteration 11085, loss = 0.02515183\n",
      "Iteration 11086, loss = 0.02514817\n",
      "Iteration 11087, loss = 0.02514451\n",
      "Iteration 11088, loss = 0.02514085\n",
      "Iteration 11089, loss = 0.02513719\n",
      "Iteration 11090, loss = 0.02513353\n",
      "Iteration 11091, loss = 0.02512988\n",
      "Iteration 11092, loss = 0.02512622\n",
      "Iteration 11093, loss = 0.02512257\n",
      "Iteration 11094, loss = 0.02511891\n",
      "Iteration 11095, loss = 0.02511526\n",
      "Iteration 11096, loss = 0.02511161\n",
      "Iteration 11097, loss = 0.02510796\n",
      "Iteration 11098, loss = 0.02510431\n",
      "Iteration 11099, loss = 0.02510066\n",
      "Iteration 11100, loss = 0.02509701\n",
      "Iteration 11101, loss = 0.02509337\n",
      "Iteration 11102, loss = 0.02508972\n",
      "Iteration 11103, loss = 0.02508608\n",
      "Iteration 11104, loss = 0.02508244\n",
      "Iteration 11105, loss = 0.02507880\n",
      "Iteration 11106, loss = 0.02507516\n",
      "Iteration 11107, loss = 0.02507152\n",
      "Iteration 11108, loss = 0.02506788\n",
      "Iteration 11109, loss = 0.02506424\n",
      "Iteration 11110, loss = 0.02506060\n",
      "Iteration 11111, loss = 0.02505697\n",
      "Iteration 11112, loss = 0.02505333\n",
      "Iteration 11113, loss = 0.02504970\n",
      "Iteration 11114, loss = 0.02504607\n",
      "Iteration 11115, loss = 0.02504244\n",
      "Iteration 11116, loss = 0.02503881\n",
      "Iteration 11117, loss = 0.02503518\n",
      "Iteration 11118, loss = 0.02503155\n",
      "Iteration 11119, loss = 0.02502793\n",
      "Iteration 11120, loss = 0.02502430\n",
      "Iteration 11121, loss = 0.02502067\n",
      "Iteration 11122, loss = 0.02501705\n",
      "Iteration 11123, loss = 0.02501343\n",
      "Iteration 11124, loss = 0.02500981\n",
      "Iteration 11125, loss = 0.02500619\n",
      "Iteration 11126, loss = 0.02500257\n",
      "Iteration 11127, loss = 0.02499895\n",
      "Iteration 11128, loss = 0.02499533\n",
      "Iteration 11129, loss = 0.02499172\n",
      "Iteration 11130, loss = 0.02498810\n",
      "Iteration 11131, loss = 0.02498449\n",
      "Iteration 11132, loss = 0.02498087\n",
      "Iteration 11133, loss = 0.02497726\n",
      "Iteration 11134, loss = 0.02497365\n",
      "Iteration 11135, loss = 0.02497004\n",
      "Iteration 11136, loss = 0.02496643\n",
      "Iteration 11137, loss = 0.02496282\n",
      "Iteration 11138, loss = 0.02495922\n",
      "Iteration 11139, loss = 0.02495561\n",
      "Iteration 11140, loss = 0.02495201\n",
      "Iteration 11141, loss = 0.02494840\n",
      "Iteration 11142, loss = 0.02494480\n",
      "Iteration 11143, loss = 0.02494120\n",
      "Iteration 11144, loss = 0.02493760\n",
      "Iteration 11145, loss = 0.02493400\n",
      "Iteration 11146, loss = 0.02493040\n",
      "Iteration 11147, loss = 0.02492680\n",
      "Iteration 11148, loss = 0.02492321\n",
      "Iteration 11149, loss = 0.02491961\n",
      "Iteration 11150, loss = 0.02491602\n",
      "Iteration 11151, loss = 0.02491243\n",
      "Iteration 11152, loss = 0.02490883\n",
      "Iteration 11153, loss = 0.02490524\n",
      "Iteration 11154, loss = 0.02490165\n",
      "Iteration 11155, loss = 0.02489806\n",
      "Iteration 11156, loss = 0.02489448\n",
      "Iteration 11157, loss = 0.02489089\n",
      "Iteration 11158, loss = 0.02488730\n",
      "Iteration 11159, loss = 0.02488372\n",
      "Iteration 11160, loss = 0.02488013\n",
      "Iteration 11161, loss = 0.02487655\n",
      "Iteration 11162, loss = 0.02487297\n",
      "Iteration 11163, loss = 0.02486939\n",
      "Iteration 11164, loss = 0.02486581\n",
      "Iteration 11165, loss = 0.02486223\n",
      "Iteration 11166, loss = 0.02485865\n",
      "Iteration 11167, loss = 0.02485508\n",
      "Iteration 11168, loss = 0.02485150\n",
      "Iteration 11169, loss = 0.02484793\n",
      "Iteration 11170, loss = 0.02484435\n",
      "Iteration 11171, loss = 0.02484078\n",
      "Iteration 11172, loss = 0.02483721\n",
      "Iteration 11173, loss = 0.02483364\n",
      "Iteration 11174, loss = 0.02483007\n",
      "Iteration 11175, loss = 0.02482650\n",
      "Iteration 11176, loss = 0.02482294\n",
      "Iteration 11177, loss = 0.02481937\n",
      "Iteration 11178, loss = 0.02481580\n",
      "Iteration 11179, loss = 0.02481224\n",
      "Iteration 11180, loss = 0.02480868\n",
      "Iteration 11181, loss = 0.02480512\n",
      "Iteration 11182, loss = 0.02480155\n",
      "Iteration 11183, loss = 0.02479799\n",
      "Iteration 11184, loss = 0.02479444\n",
      "Iteration 11185, loss = 0.02479088\n",
      "Iteration 11186, loss = 0.02478732\n",
      "Iteration 11187, loss = 0.02478377\n",
      "Iteration 11188, loss = 0.02478021\n",
      "Iteration 11189, loss = 0.02477666\n",
      "Iteration 11190, loss = 0.02477310\n",
      "Iteration 11191, loss = 0.02476955\n",
      "Iteration 11192, loss = 0.02476600\n",
      "Iteration 11193, loss = 0.02476245\n",
      "Iteration 11194, loss = 0.02475890\n",
      "Iteration 11195, loss = 0.02475536\n",
      "Iteration 11196, loss = 0.02475181\n",
      "Iteration 11197, loss = 0.02474826\n",
      "Iteration 11198, loss = 0.02474472\n",
      "Iteration 11199, loss = 0.02474118\n",
      "Iteration 11200, loss = 0.02473763\n",
      "Iteration 11201, loss = 0.02473409\n",
      "Iteration 11202, loss = 0.02473055\n",
      "Iteration 11203, loss = 0.02472701\n",
      "Iteration 11204, loss = 0.02472347\n",
      "Iteration 11205, loss = 0.02471994\n",
      "Iteration 11206, loss = 0.02471640\n",
      "Iteration 11207, loss = 0.02471287\n",
      "Iteration 11208, loss = 0.02470933\n",
      "Iteration 11209, loss = 0.02470580\n",
      "Iteration 11210, loss = 0.02470227\n",
      "Iteration 11211, loss = 0.02469873\n",
      "Iteration 11212, loss = 0.02469520\n",
      "Iteration 11213, loss = 0.02469167\n",
      "Iteration 11214, loss = 0.02468815\n",
      "Iteration 11215, loss = 0.02468462\n",
      "Iteration 11216, loss = 0.02468109\n",
      "Iteration 11217, loss = 0.02467757\n",
      "Iteration 11218, loss = 0.02467404\n",
      "Iteration 11219, loss = 0.02467052\n",
      "Iteration 11220, loss = 0.02466700\n",
      "Iteration 11221, loss = 0.02466348\n",
      "Iteration 11222, loss = 0.02465996\n",
      "Iteration 11223, loss = 0.02465644\n",
      "Iteration 11224, loss = 0.02465292\n",
      "Iteration 11225, loss = 0.02464940\n",
      "Iteration 11226, loss = 0.02464589\n",
      "Iteration 11227, loss = 0.02464237\n",
      "Iteration 11228, loss = 0.02463886\n",
      "Iteration 11229, loss = 0.02463535\n",
      "Iteration 11230, loss = 0.02463183\n",
      "Iteration 11231, loss = 0.02462832\n",
      "Iteration 11232, loss = 0.02462481\n",
      "Iteration 11233, loss = 0.02462130\n",
      "Iteration 11234, loss = 0.02461780\n",
      "Iteration 11235, loss = 0.02461429\n",
      "Iteration 11236, loss = 0.02461078\n",
      "Iteration 11237, loss = 0.02460728\n",
      "Iteration 11238, loss = 0.02460378\n",
      "Iteration 11239, loss = 0.02460027\n",
      "Iteration 11240, loss = 0.02459677\n",
      "Iteration 11241, loss = 0.02459327\n",
      "Iteration 11242, loss = 0.02458977\n",
      "Iteration 11243, loss = 0.02458627\n",
      "Iteration 11244, loss = 0.02458277\n",
      "Iteration 11245, loss = 0.02457928\n",
      "Iteration 11246, loss = 0.02457578\n",
      "Iteration 11247, loss = 0.02457229\n",
      "Iteration 11248, loss = 0.02456879\n",
      "Iteration 11249, loss = 0.02456530\n",
      "Iteration 11250, loss = 0.02456181\n",
      "Iteration 11251, loss = 0.02455832\n",
      "Iteration 11252, loss = 0.02455483\n",
      "Iteration 11253, loss = 0.02455134\n",
      "Iteration 11254, loss = 0.02454785\n",
      "Iteration 11255, loss = 0.02454436\n",
      "Iteration 11256, loss = 0.02454088\n",
      "Iteration 11257, loss = 0.02453739\n",
      "Iteration 11258, loss = 0.02453391\n",
      "Iteration 11259, loss = 0.02453043\n",
      "Iteration 11260, loss = 0.02452695\n",
      "Iteration 11261, loss = 0.02452347\n",
      "Iteration 11262, loss = 0.02451999\n",
      "Iteration 11263, loss = 0.02451651\n",
      "Iteration 11264, loss = 0.02451303\n",
      "Iteration 11265, loss = 0.02450955\n",
      "Iteration 11266, loss = 0.02450608\n",
      "Iteration 11267, loss = 0.02450260\n",
      "Iteration 11268, loss = 0.02449913\n",
      "Iteration 11269, loss = 0.02449566\n",
      "Iteration 11270, loss = 0.02449218\n",
      "Iteration 11271, loss = 0.02448871\n",
      "Iteration 11272, loss = 0.02448524\n",
      "Iteration 11273, loss = 0.02448177\n",
      "Iteration 11274, loss = 0.02447831\n",
      "Iteration 11275, loss = 0.02447484\n",
      "Iteration 11276, loss = 0.02447137\n",
      "Iteration 11277, loss = 0.02446791\n",
      "Iteration 11278, loss = 0.02446444\n",
      "Iteration 11279, loss = 0.02446098\n",
      "Iteration 11280, loss = 0.02445752\n",
      "Iteration 11281, loss = 0.02445406\n",
      "Iteration 11282, loss = 0.02445060\n",
      "Iteration 11283, loss = 0.02444714\n",
      "Iteration 11284, loss = 0.02444368\n",
      "Iteration 11285, loss = 0.02444023\n",
      "Iteration 11286, loss = 0.02443677\n",
      "Iteration 11287, loss = 0.02443332\n",
      "Iteration 11288, loss = 0.02442986\n",
      "Iteration 11289, loss = 0.02442641\n",
      "Iteration 11290, loss = 0.02442296\n",
      "Iteration 11291, loss = 0.02441951\n",
      "Iteration 11292, loss = 0.02441606\n",
      "Iteration 11293, loss = 0.02441261\n",
      "Iteration 11294, loss = 0.02440916\n",
      "Iteration 11295, loss = 0.02440571\n",
      "Iteration 11296, loss = 0.02440227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11297, loss = 0.02439882\n",
      "Iteration 11298, loss = 0.02439538\n",
      "Iteration 11299, loss = 0.02439193\n",
      "Iteration 11300, loss = 0.02438849\n",
      "Iteration 11301, loss = 0.02438505\n",
      "Iteration 11302, loss = 0.02438161\n",
      "Iteration 11303, loss = 0.02437817\n",
      "Iteration 11304, loss = 0.02437473\n",
      "Iteration 11305, loss = 0.02437130\n",
      "Iteration 11306, loss = 0.02436786\n",
      "Iteration 11307, loss = 0.02436442\n",
      "Iteration 11308, loss = 0.02436099\n",
      "Iteration 11309, loss = 0.02435756\n",
      "Iteration 11310, loss = 0.02435412\n",
      "Iteration 11311, loss = 0.02435069\n",
      "Iteration 11312, loss = 0.02434726\n",
      "Iteration 11313, loss = 0.02434383\n",
      "Iteration 11314, loss = 0.02434041\n",
      "Iteration 11315, loss = 0.02433698\n",
      "Iteration 11316, loss = 0.02433355\n",
      "Iteration 11317, loss = 0.02433013\n",
      "Iteration 11318, loss = 0.02432670\n",
      "Iteration 11319, loss = 0.02432328\n",
      "Iteration 11320, loss = 0.02431986\n",
      "Iteration 11321, loss = 0.02431643\n",
      "Iteration 11322, loss = 0.02431301\n",
      "Iteration 11323, loss = 0.02430959\n",
      "Iteration 11324, loss = 0.02430618\n",
      "Iteration 11325, loss = 0.02430276\n",
      "Iteration 11326, loss = 0.02429934\n",
      "Iteration 11327, loss = 0.02429593\n",
      "Iteration 11328, loss = 0.02429251\n",
      "Iteration 11329, loss = 0.02428910\n",
      "Iteration 11330, loss = 0.02428568\n",
      "Iteration 11331, loss = 0.02428227\n",
      "Iteration 11332, loss = 0.02427886\n",
      "Iteration 11333, loss = 0.02427545\n",
      "Iteration 11334, loss = 0.02427204\n",
      "Iteration 11335, loss = 0.02426864\n",
      "Iteration 11336, loss = 0.02426523\n",
      "Iteration 11337, loss = 0.02426182\n",
      "Iteration 11338, loss = 0.02425842\n",
      "Iteration 11339, loss = 0.02425501\n",
      "Iteration 11340, loss = 0.02425161\n",
      "Iteration 11341, loss = 0.02424821\n",
      "Iteration 11342, loss = 0.02424481\n",
      "Iteration 11343, loss = 0.02424141\n",
      "Iteration 11344, loss = 0.02423801\n",
      "Iteration 11345, loss = 0.02423461\n",
      "Iteration 11346, loss = 0.02423121\n",
      "Iteration 11347, loss = 0.02422782\n",
      "Iteration 11348, loss = 0.02422442\n",
      "Iteration 11349, loss = 0.02422103\n",
      "Iteration 11350, loss = 0.02421763\n",
      "Iteration 11351, loss = 0.02421424\n",
      "Iteration 11352, loss = 0.02421085\n",
      "Iteration 11353, loss = 0.02420746\n",
      "Iteration 11354, loss = 0.02420407\n",
      "Iteration 11355, loss = 0.02420068\n",
      "Iteration 11356, loss = 0.02419729\n",
      "Iteration 11357, loss = 0.02419391\n",
      "Iteration 11358, loss = 0.02419052\n",
      "Iteration 11359, loss = 0.02418714\n",
      "Iteration 11360, loss = 0.02418375\n",
      "Iteration 11361, loss = 0.02418037\n",
      "Iteration 11362, loss = 0.02417699\n",
      "Iteration 11363, loss = 0.02417361\n",
      "Iteration 11364, loss = 0.02417023\n",
      "Iteration 11365, loss = 0.02416685\n",
      "Iteration 11366, loss = 0.02416347\n",
      "Iteration 11367, loss = 0.02416009\n",
      "Iteration 11368, loss = 0.02415672\n",
      "Iteration 11369, loss = 0.02415334\n",
      "Iteration 11370, loss = 0.02414997\n",
      "Iteration 11371, loss = 0.02414659\n",
      "Iteration 11372, loss = 0.02414322\n",
      "Iteration 11373, loss = 0.02413985\n",
      "Iteration 11374, loss = 0.02413648\n",
      "Iteration 11375, loss = 0.02413311\n",
      "Iteration 11376, loss = 0.02412974\n",
      "Iteration 11377, loss = 0.02412637\n",
      "Iteration 11378, loss = 0.02412301\n",
      "Iteration 11379, loss = 0.02411964\n",
      "Iteration 11380, loss = 0.02411628\n",
      "Iteration 11381, loss = 0.02411291\n",
      "Iteration 11382, loss = 0.02410955\n",
      "Iteration 11383, loss = 0.02410619\n",
      "Iteration 11384, loss = 0.02410283\n",
      "Iteration 11385, loss = 0.02409947\n",
      "Iteration 11386, loss = 0.02409611\n",
      "Iteration 11387, loss = 0.02409275\n",
      "Iteration 11388, loss = 0.02408940\n",
      "Iteration 11389, loss = 0.02408604\n",
      "Iteration 11390, loss = 0.02408268\n",
      "Iteration 11391, loss = 0.02407933\n",
      "Iteration 11392, loss = 0.02407598\n",
      "Iteration 11393, loss = 0.02407262\n",
      "Iteration 11394, loss = 0.02406927\n",
      "Iteration 11395, loss = 0.02406592\n",
      "Iteration 11396, loss = 0.02406257\n",
      "Iteration 11397, loss = 0.02405922\n",
      "Iteration 11398, loss = 0.02405588\n",
      "Iteration 11399, loss = 0.02405253\n",
      "Iteration 11400, loss = 0.02404919\n",
      "Iteration 11401, loss = 0.02404584\n",
      "Iteration 11402, loss = 0.02404250\n",
      "Iteration 11403, loss = 0.02403915\n",
      "Iteration 11404, loss = 0.02403581\n",
      "Iteration 11405, loss = 0.02403247\n",
      "Iteration 11406, loss = 0.02402913\n",
      "Iteration 11407, loss = 0.02402579\n",
      "Iteration 11408, loss = 0.02402245\n",
      "Iteration 11409, loss = 0.02401912\n",
      "Iteration 11410, loss = 0.02401578\n",
      "Iteration 11411, loss = 0.02401245\n",
      "Iteration 11412, loss = 0.02400911\n",
      "Iteration 11413, loss = 0.02400578\n",
      "Iteration 11414, loss = 0.02400245\n",
      "Iteration 11415, loss = 0.02399911\n",
      "Iteration 11416, loss = 0.02399578\n",
      "Iteration 11417, loss = 0.02399245\n",
      "Iteration 11418, loss = 0.02398912\n",
      "Iteration 11419, loss = 0.02398580\n",
      "Iteration 11420, loss = 0.02398247\n",
      "Iteration 11421, loss = 0.02397914\n",
      "Iteration 11422, loss = 0.02397582\n",
      "Iteration 11423, loss = 0.02397250\n",
      "Iteration 11424, loss = 0.02396917\n",
      "Iteration 11425, loss = 0.02396585\n",
      "Iteration 11426, loss = 0.02396253\n",
      "Iteration 11427, loss = 0.02395921\n",
      "Iteration 11428, loss = 0.02395589\n",
      "Iteration 11429, loss = 0.02395257\n",
      "Iteration 11430, loss = 0.02394925\n",
      "Iteration 11431, loss = 0.02394594\n",
      "Iteration 11432, loss = 0.02394262\n",
      "Iteration 11433, loss = 0.02393931\n",
      "Iteration 11434, loss = 0.02393599\n",
      "Iteration 11435, loss = 0.02393268\n",
      "Iteration 11436, loss = 0.02392937\n",
      "Iteration 11437, loss = 0.02392606\n",
      "Iteration 11438, loss = 0.02392275\n",
      "Iteration 11439, loss = 0.02391944\n",
      "Iteration 11440, loss = 0.02391613\n",
      "Iteration 11441, loss = 0.02391282\n",
      "Iteration 11442, loss = 0.02390952\n",
      "Iteration 11443, loss = 0.02390621\n",
      "Iteration 11444, loss = 0.02390291\n",
      "Iteration 11445, loss = 0.02389960\n",
      "Iteration 11446, loss = 0.02389630\n",
      "Iteration 11447, loss = 0.02389300\n",
      "Iteration 11448, loss = 0.02388970\n",
      "Iteration 11449, loss = 0.02388640\n",
      "Iteration 11450, loss = 0.02388310\n",
      "Iteration 11451, loss = 0.02387980\n",
      "Iteration 11452, loss = 0.02387651\n",
      "Iteration 11453, loss = 0.02387321\n",
      "Iteration 11454, loss = 0.02386991\n",
      "Iteration 11455, loss = 0.02386662\n",
      "Iteration 11456, loss = 0.02386333\n",
      "Iteration 11457, loss = 0.02386003\n",
      "Iteration 11458, loss = 0.02385674\n",
      "Iteration 11459, loss = 0.02385345\n",
      "Iteration 11460, loss = 0.02385016\n",
      "Iteration 11461, loss = 0.02384687\n",
      "Iteration 11462, loss = 0.02384359\n",
      "Iteration 11463, loss = 0.02384030\n",
      "Iteration 11464, loss = 0.02383701\n",
      "Iteration 11465, loss = 0.02383373\n",
      "Iteration 11466, loss = 0.02383044\n",
      "Iteration 11467, loss = 0.02382716\n",
      "Iteration 11468, loss = 0.02382388\n",
      "Iteration 11469, loss = 0.02382060\n",
      "Iteration 11470, loss = 0.02381732\n",
      "Iteration 11471, loss = 0.02381404\n",
      "Iteration 11472, loss = 0.02381076\n",
      "Iteration 11473, loss = 0.02380748\n",
      "Iteration 11474, loss = 0.02380421\n",
      "Iteration 11475, loss = 0.02380093\n",
      "Iteration 11476, loss = 0.02379765\n",
      "Iteration 11477, loss = 0.02379438\n",
      "Iteration 11478, loss = 0.02379111\n",
      "Iteration 11479, loss = 0.02378784\n",
      "Iteration 11480, loss = 0.02378456\n",
      "Iteration 11481, loss = 0.02378129\n",
      "Iteration 11482, loss = 0.02377802\n",
      "Iteration 11483, loss = 0.02377476\n",
      "Iteration 11484, loss = 0.02377149\n",
      "Iteration 11485, loss = 0.02376822\n",
      "Iteration 11486, loss = 0.02376496\n",
      "Iteration 11487, loss = 0.02376169\n",
      "Iteration 11488, loss = 0.02375843\n",
      "Iteration 11489, loss = 0.02375517\n",
      "Iteration 11490, loss = 0.02375190\n",
      "Iteration 11491, loss = 0.02374864\n",
      "Iteration 11492, loss = 0.02374538\n",
      "Iteration 11493, loss = 0.02374212\n",
      "Iteration 11494, loss = 0.02373886\n",
      "Iteration 11495, loss = 0.02373561\n",
      "Iteration 11496, loss = 0.02373235\n",
      "Iteration 11497, loss = 0.02372909\n",
      "Iteration 11498, loss = 0.02372584\n",
      "Iteration 11499, loss = 0.02372259\n",
      "Iteration 11500, loss = 0.02371933\n",
      "Iteration 11501, loss = 0.02371608\n",
      "Iteration 11502, loss = 0.02371283\n",
      "Iteration 11503, loss = 0.02370958\n",
      "Iteration 11504, loss = 0.02370633\n",
      "Iteration 11505, loss = 0.02370308\n",
      "Iteration 11506, loss = 0.02369983\n",
      "Iteration 11507, loss = 0.02369659\n",
      "Iteration 11508, loss = 0.02369334\n",
      "Iteration 11509, loss = 0.02369010\n",
      "Iteration 11510, loss = 0.02368685\n",
      "Iteration 11511, loss = 0.02368361\n",
      "Iteration 11512, loss = 0.02368037\n",
      "Iteration 11513, loss = 0.02367713\n",
      "Iteration 11514, loss = 0.02367389\n",
      "Iteration 11515, loss = 0.02367065\n",
      "Iteration 11516, loss = 0.02366741\n",
      "Iteration 11517, loss = 0.02366417\n",
      "Iteration 11518, loss = 0.02366093\n",
      "Iteration 11519, loss = 0.02365770\n",
      "Iteration 11520, loss = 0.02365446\n",
      "Iteration 11521, loss = 0.02365123\n",
      "Iteration 11522, loss = 0.02364800\n",
      "Iteration 11523, loss = 0.02364477\n",
      "Iteration 11524, loss = 0.02364153\n",
      "Iteration 11525, loss = 0.02363830\n",
      "Iteration 11526, loss = 0.02363507\n",
      "Iteration 11527, loss = 0.02363185\n",
      "Iteration 11528, loss = 0.02362862\n",
      "Iteration 11529, loss = 0.02362539\n",
      "Iteration 11530, loss = 0.02362217\n",
      "Iteration 11531, loss = 0.02361894\n",
      "Iteration 11532, loss = 0.02361572\n",
      "Iteration 11533, loss = 0.02361249\n",
      "Iteration 11534, loss = 0.02360927\n",
      "Iteration 11535, loss = 0.02360605\n",
      "Iteration 11536, loss = 0.02360283\n",
      "Iteration 11537, loss = 0.02359961\n",
      "Iteration 11538, loss = 0.02359639\n",
      "Iteration 11539, loss = 0.02359317\n",
      "Iteration 11540, loss = 0.02358996\n",
      "Iteration 11541, loss = 0.02358674\n",
      "Iteration 11542, loss = 0.02358353\n",
      "Iteration 11543, loss = 0.02358031\n",
      "Iteration 11544, loss = 0.02357710\n",
      "Iteration 11545, loss = 0.02357389\n",
      "Iteration 11546, loss = 0.02357067\n",
      "Iteration 11547, loss = 0.02356746\n",
      "Iteration 11548, loss = 0.02356425\n",
      "Iteration 11549, loss = 0.02356105\n",
      "Iteration 11550, loss = 0.02355784\n",
      "Iteration 11551, loss = 0.02355463\n",
      "Iteration 11552, loss = 0.02355142\n",
      "Iteration 11553, loss = 0.02354822\n",
      "Iteration 11554, loss = 0.02354501\n",
      "Iteration 11555, loss = 0.02354181\n",
      "Iteration 11556, loss = 0.02353861\n",
      "Iteration 11557, loss = 0.02353541\n",
      "Iteration 11558, loss = 0.02353221\n",
      "Iteration 11559, loss = 0.02352901\n",
      "Iteration 11560, loss = 0.02352581\n",
      "Iteration 11561, loss = 0.02352261\n",
      "Iteration 11562, loss = 0.02351941\n",
      "Iteration 11563, loss = 0.02351622\n",
      "Iteration 11564, loss = 0.02351302\n",
      "Iteration 11565, loss = 0.02350983\n",
      "Iteration 11566, loss = 0.02350663\n",
      "Iteration 11567, loss = 0.02350344\n",
      "Iteration 11568, loss = 0.02350025\n",
      "Iteration 11569, loss = 0.02349706\n",
      "Iteration 11570, loss = 0.02349387\n",
      "Iteration 11571, loss = 0.02349068\n",
      "Iteration 11572, loss = 0.02348749\n",
      "Iteration 11573, loss = 0.02348430\n",
      "Iteration 11574, loss = 0.02348111\n",
      "Iteration 11575, loss = 0.02347793\n",
      "Iteration 11576, loss = 0.02347474\n",
      "Iteration 11577, loss = 0.02347156\n",
      "Iteration 11578, loss = 0.02346838\n",
      "Iteration 11579, loss = 0.02346519\n",
      "Iteration 11580, loss = 0.02346201\n",
      "Iteration 11581, loss = 0.02345883\n",
      "Iteration 11582, loss = 0.02345565\n",
      "Iteration 11583, loss = 0.02345247\n",
      "Iteration 11584, loss = 0.02344930\n",
      "Iteration 11585, loss = 0.02344612\n",
      "Iteration 11586, loss = 0.02344294\n",
      "Iteration 11587, loss = 0.02343977\n",
      "Iteration 11588, loss = 0.02343659\n",
      "Iteration 11589, loss = 0.02343342\n",
      "Iteration 11590, loss = 0.02343025\n",
      "Iteration 11591, loss = 0.02342708\n",
      "Iteration 11592, loss = 0.02342391\n",
      "Iteration 11593, loss = 0.02342074\n",
      "Iteration 11594, loss = 0.02341757\n",
      "Iteration 11595, loss = 0.02341440\n",
      "Iteration 11596, loss = 0.02341123\n",
      "Iteration 11597, loss = 0.02340806\n",
      "Iteration 11598, loss = 0.02340490\n",
      "Iteration 11599, loss = 0.02340173\n",
      "Iteration 11600, loss = 0.02339857\n",
      "Iteration 11601, loss = 0.02339541\n",
      "Iteration 11602, loss = 0.02339225\n",
      "Iteration 11603, loss = 0.02338908\n",
      "Iteration 11604, loss = 0.02338592\n",
      "Iteration 11605, loss = 0.02338276\n",
      "Iteration 11606, loss = 0.02337961\n",
      "Iteration 11607, loss = 0.02337645\n",
      "Iteration 11608, loss = 0.02337329\n",
      "Iteration 11609, loss = 0.02337014\n",
      "Iteration 11610, loss = 0.02336698\n",
      "Iteration 11611, loss = 0.02336383\n",
      "Iteration 11612, loss = 0.02336067\n",
      "Iteration 11613, loss = 0.02335752\n",
      "Iteration 11614, loss = 0.02335437\n",
      "Iteration 11615, loss = 0.02335122\n",
      "Iteration 11616, loss = 0.02334807\n",
      "Iteration 11617, loss = 0.02334492\n",
      "Iteration 11618, loss = 0.02334177\n",
      "Iteration 11619, loss = 0.02333862\n",
      "Iteration 11620, loss = 0.02333548\n",
      "Iteration 11621, loss = 0.02333233\n",
      "Iteration 11622, loss = 0.02332919\n",
      "Iteration 11623, loss = 0.02332604\n",
      "Iteration 11624, loss = 0.02332290\n",
      "Iteration 11625, loss = 0.02331976\n",
      "Iteration 11626, loss = 0.02331662\n",
      "Iteration 11627, loss = 0.02331348\n",
      "Iteration 11628, loss = 0.02331034\n",
      "Iteration 11629, loss = 0.02330720\n",
      "Iteration 11630, loss = 0.02330406\n",
      "Iteration 11631, loss = 0.02330093\n",
      "Iteration 11632, loss = 0.02329779\n",
      "Iteration 11633, loss = 0.02329465\n",
      "Iteration 11634, loss = 0.02329152\n",
      "Iteration 11635, loss = 0.02328839\n",
      "Iteration 11636, loss = 0.02328525\n",
      "Iteration 11637, loss = 0.02328212\n",
      "Iteration 11638, loss = 0.02327899\n",
      "Iteration 11639, loss = 0.02327586\n",
      "Iteration 11640, loss = 0.02327273\n",
      "Iteration 11641, loss = 0.02326960\n",
      "Iteration 11642, loss = 0.02326648\n",
      "Iteration 11643, loss = 0.02326335\n",
      "Iteration 11644, loss = 0.02326022\n",
      "Iteration 11645, loss = 0.02325710\n",
      "Iteration 11646, loss = 0.02325398\n",
      "Iteration 11647, loss = 0.02325085\n",
      "Iteration 11648, loss = 0.02324773\n",
      "Iteration 11649, loss = 0.02324461\n",
      "Iteration 11650, loss = 0.02324149\n",
      "Iteration 11651, loss = 0.02323837\n",
      "Iteration 11652, loss = 0.02323525\n",
      "Iteration 11653, loss = 0.02323213\n",
      "Iteration 11654, loss = 0.02322902\n",
      "Iteration 11655, loss = 0.02322590\n",
      "Iteration 11656, loss = 0.02322278\n",
      "Iteration 11657, loss = 0.02321967\n",
      "Iteration 11658, loss = 0.02321656\n",
      "Iteration 11659, loss = 0.02321344\n",
      "Iteration 11660, loss = 0.02321033\n",
      "Iteration 11661, loss = 0.02320722\n",
      "Iteration 11662, loss = 0.02320411\n",
      "Iteration 11663, loss = 0.02320100\n",
      "Iteration 11664, loss = 0.02319789\n",
      "Iteration 11665, loss = 0.02319478\n",
      "Iteration 11666, loss = 0.02319168\n",
      "Iteration 11667, loss = 0.02318857\n",
      "Iteration 11668, loss = 0.02318547\n",
      "Iteration 11669, loss = 0.02318236\n",
      "Iteration 11670, loss = 0.02317926\n",
      "Iteration 11671, loss = 0.02317616\n",
      "Iteration 11672, loss = 0.02317306\n",
      "Iteration 11673, loss = 0.02316995\n",
      "Iteration 11674, loss = 0.02316685\n",
      "Iteration 11675, loss = 0.02316376\n",
      "Iteration 11676, loss = 0.02316066\n",
      "Iteration 11677, loss = 0.02315756\n",
      "Iteration 11678, loss = 0.02315446\n",
      "Iteration 11679, loss = 0.02315137\n",
      "Iteration 11680, loss = 0.02314827\n",
      "Iteration 11681, loss = 0.02314518\n",
      "Iteration 11682, loss = 0.02314208\n",
      "Iteration 11683, loss = 0.02313899\n",
      "Iteration 11684, loss = 0.02313590\n",
      "Iteration 11685, loss = 0.02313281\n",
      "Iteration 11686, loss = 0.02312972\n",
      "Iteration 11687, loss = 0.02312663\n",
      "Iteration 11688, loss = 0.02312354\n",
      "Iteration 11689, loss = 0.02312046\n",
      "Iteration 11690, loss = 0.02311737\n",
      "Iteration 11691, loss = 0.02311428\n",
      "Iteration 11692, loss = 0.02311120\n",
      "Iteration 11693, loss = 0.02310812\n",
      "Iteration 11694, loss = 0.02310503\n",
      "Iteration 11695, loss = 0.02310195\n",
      "Iteration 11696, loss = 0.02309887\n",
      "Iteration 11697, loss = 0.02309579\n",
      "Iteration 11698, loss = 0.02309271\n",
      "Iteration 11699, loss = 0.02308963\n",
      "Iteration 11700, loss = 0.02308655\n",
      "Iteration 11701, loss = 0.02308347\n",
      "Iteration 11702, loss = 0.02308040\n",
      "Iteration 11703, loss = 0.02307732\n",
      "Iteration 11704, loss = 0.02307425\n",
      "Iteration 11705, loss = 0.02307117\n",
      "Iteration 11706, loss = 0.02306810\n",
      "Iteration 11707, loss = 0.02306503\n",
      "Iteration 11708, loss = 0.02306196\n",
      "Iteration 11709, loss = 0.02305889\n",
      "Iteration 11710, loss = 0.02305582\n",
      "Iteration 11711, loss = 0.02305275\n",
      "Iteration 11712, loss = 0.02304968\n",
      "Iteration 11713, loss = 0.02304661\n",
      "Iteration 11714, loss = 0.02304355\n",
      "Iteration 11715, loss = 0.02304048\n",
      "Iteration 11716, loss = 0.02303742\n",
      "Iteration 11717, loss = 0.02303435\n",
      "Iteration 11718, loss = 0.02303129\n",
      "Iteration 11719, loss = 0.02302823\n",
      "Iteration 11720, loss = 0.02302517\n",
      "Iteration 11721, loss = 0.02302211\n",
      "Iteration 11722, loss = 0.02301905\n",
      "Iteration 11723, loss = 0.02301599\n",
      "Iteration 11724, loss = 0.02301293\n",
      "Iteration 11725, loss = 0.02300987\n",
      "Iteration 11726, loss = 0.02300682\n",
      "Iteration 11727, loss = 0.02300376\n",
      "Iteration 11728, loss = 0.02300071\n",
      "Iteration 11729, loss = 0.02299765\n",
      "Iteration 11730, loss = 0.02299460\n",
      "Iteration 11731, loss = 0.02299155\n",
      "Iteration 11732, loss = 0.02298850\n",
      "Iteration 11733, loss = 0.02298545\n",
      "Iteration 11734, loss = 0.02298240\n",
      "Iteration 11735, loss = 0.02297935\n",
      "Iteration 11736, loss = 0.02297630\n",
      "Iteration 11737, loss = 0.02297326\n",
      "Iteration 11738, loss = 0.02297021\n",
      "Iteration 11739, loss = 0.02296716\n",
      "Iteration 11740, loss = 0.02296412\n",
      "Iteration 11741, loss = 0.02296108\n",
      "Iteration 11742, loss = 0.02295803\n",
      "Iteration 11743, loss = 0.02295499\n",
      "Iteration 11744, loss = 0.02295195\n",
      "Iteration 11745, loss = 0.02294891\n",
      "Iteration 11746, loss = 0.02294587\n",
      "Iteration 11747, loss = 0.02294283\n",
      "Iteration 11748, loss = 0.02293979\n",
      "Iteration 11749, loss = 0.02293675\n",
      "Iteration 11750, loss = 0.02293372\n",
      "Iteration 11751, loss = 0.02293068\n",
      "Iteration 11752, loss = 0.02292765\n",
      "Iteration 11753, loss = 0.02292461\n",
      "Iteration 11754, loss = 0.02292158\n",
      "Iteration 11755, loss = 0.02291855\n",
      "Iteration 11756, loss = 0.02291552\n",
      "Iteration 11757, loss = 0.02291249\n",
      "Iteration 11758, loss = 0.02290946\n",
      "Iteration 11759, loss = 0.02290643\n",
      "Iteration 11760, loss = 0.02290340\n",
      "Iteration 11761, loss = 0.02290037\n",
      "Iteration 11762, loss = 0.02289735\n",
      "Iteration 11763, loss = 0.02289432\n",
      "Iteration 11764, loss = 0.02289130\n",
      "Iteration 11765, loss = 0.02288827\n",
      "Iteration 11766, loss = 0.02288525\n",
      "Iteration 11767, loss = 0.02288223\n",
      "Iteration 11768, loss = 0.02287921\n",
      "Iteration 11769, loss = 0.02287618\n",
      "Iteration 11770, loss = 0.02287317\n",
      "Iteration 11771, loss = 0.02287015\n",
      "Iteration 11772, loss = 0.02286713\n",
      "Iteration 11773, loss = 0.02286411\n",
      "Iteration 11774, loss = 0.02286109\n",
      "Iteration 11775, loss = 0.02285808\n",
      "Iteration 11776, loss = 0.02285506\n",
      "Iteration 11777, loss = 0.02285205\n",
      "Iteration 11778, loss = 0.02284904\n",
      "Iteration 11779, loss = 0.02284602\n",
      "Iteration 11780, loss = 0.02284301\n",
      "Iteration 11781, loss = 0.02284000\n",
      "Iteration 11782, loss = 0.02283699\n",
      "Iteration 11783, loss = 0.02283398\n",
      "Iteration 11784, loss = 0.02283097\n",
      "Iteration 11785, loss = 0.02282797\n",
      "Iteration 11786, loss = 0.02282496\n",
      "Iteration 11787, loss = 0.02282195\n",
      "Iteration 11788, loss = 0.02281895\n",
      "Iteration 11789, loss = 0.02281594\n",
      "Iteration 11790, loss = 0.02281294\n",
      "Iteration 11791, loss = 0.02280994\n",
      "Iteration 11792, loss = 0.02280693\n",
      "Iteration 11793, loss = 0.02280393\n",
      "Iteration 11794, loss = 0.02280093\n",
      "Iteration 11795, loss = 0.02279793\n",
      "Iteration 11796, loss = 0.02279494\n",
      "Iteration 11797, loss = 0.02279194\n",
      "Iteration 11798, loss = 0.02278894\n",
      "Iteration 11799, loss = 0.02278594\n",
      "Iteration 11800, loss = 0.02278295\n",
      "Iteration 11801, loss = 0.02277995\n",
      "Iteration 11802, loss = 0.02277696\n",
      "Iteration 11803, loss = 0.02277397\n",
      "Iteration 11804, loss = 0.02277097\n",
      "Iteration 11805, loss = 0.02276798\n",
      "Iteration 11806, loss = 0.02276499\n",
      "Iteration 11807, loss = 0.02276200\n",
      "Iteration 11808, loss = 0.02275901\n",
      "Iteration 11809, loss = 0.02275603\n",
      "Iteration 11810, loss = 0.02275304\n",
      "Iteration 11811, loss = 0.02275005\n",
      "Iteration 11812, loss = 0.02274707\n",
      "Iteration 11813, loss = 0.02274408\n",
      "Iteration 11814, loss = 0.02274110\n",
      "Iteration 11815, loss = 0.02273811\n",
      "Iteration 11816, loss = 0.02273513\n",
      "Iteration 11817, loss = 0.02273215\n",
      "Iteration 11818, loss = 0.02272917\n",
      "Iteration 11819, loss = 0.02272619\n",
      "Iteration 11820, loss = 0.02272321\n",
      "Iteration 11821, loss = 0.02272023\n",
      "Iteration 11822, loss = 0.02271725\n",
      "Iteration 11823, loss = 0.02271428\n",
      "Iteration 11824, loss = 0.02271130\n",
      "Iteration 11825, loss = 0.02270833\n",
      "Iteration 11826, loss = 0.02270535\n",
      "Iteration 11827, loss = 0.02270238\n",
      "Iteration 11828, loss = 0.02269940\n",
      "Iteration 11829, loss = 0.02269643\n",
      "Iteration 11830, loss = 0.02269346\n",
      "Iteration 11831, loss = 0.02269049\n",
      "Iteration 11832, loss = 0.02268752\n",
      "Iteration 11833, loss = 0.02268455\n",
      "Iteration 11834, loss = 0.02268158\n",
      "Iteration 11835, loss = 0.02267862\n",
      "Iteration 11836, loss = 0.02267565\n",
      "Iteration 11837, loss = 0.02267268\n",
      "Iteration 11838, loss = 0.02266972\n",
      "Iteration 11839, loss = 0.02266676\n",
      "Iteration 11840, loss = 0.02266379\n",
      "Iteration 11841, loss = 0.02266083\n",
      "Iteration 11842, loss = 0.02265787\n",
      "Iteration 11843, loss = 0.02265491\n",
      "Iteration 11844, loss = 0.02265195\n",
      "Iteration 11845, loss = 0.02264899\n",
      "Iteration 11846, loss = 0.02264603\n",
      "Iteration 11847, loss = 0.02264307\n",
      "Iteration 11848, loss = 0.02264012\n",
      "Iteration 11849, loss = 0.02263716\n",
      "Iteration 11850, loss = 0.02263420\n",
      "Iteration 11851, loss = 0.02263125\n",
      "Iteration 11852, loss = 0.02262830\n",
      "Iteration 11853, loss = 0.02262534\n",
      "Iteration 11854, loss = 0.02262239\n",
      "Iteration 11855, loss = 0.02261944\n",
      "Iteration 11856, loss = 0.02261649\n",
      "Iteration 11857, loss = 0.02261354\n",
      "Iteration 11858, loss = 0.02261059\n",
      "Iteration 11859, loss = 0.02260764\n",
      "Iteration 11860, loss = 0.02260469\n",
      "Iteration 11861, loss = 0.02260175\n",
      "Iteration 11862, loss = 0.02259880\n",
      "Iteration 11863, loss = 0.02259586\n",
      "Iteration 11864, loss = 0.02259291\n",
      "Iteration 11865, loss = 0.02258997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11866, loss = 0.02258703\n",
      "Iteration 11867, loss = 0.02258409\n",
      "Iteration 11868, loss = 0.02258114\n",
      "Iteration 11869, loss = 0.02257820\n",
      "Iteration 11870, loss = 0.02257526\n",
      "Iteration 11871, loss = 0.02257233\n",
      "Iteration 11872, loss = 0.02256939\n",
      "Iteration 11873, loss = 0.02256645\n",
      "Iteration 11874, loss = 0.02256351\n",
      "Iteration 11875, loss = 0.02256058\n",
      "Iteration 11876, loss = 0.02255764\n",
      "Iteration 11877, loss = 0.02255471\n",
      "Iteration 11878, loss = 0.02255178\n",
      "Iteration 11879, loss = 0.02254884\n",
      "Iteration 11880, loss = 0.02254591\n",
      "Iteration 11881, loss = 0.02254298\n",
      "Iteration 11882, loss = 0.02254005\n",
      "Iteration 11883, loss = 0.02253712\n",
      "Iteration 11884, loss = 0.02253419\n",
      "Iteration 11885, loss = 0.02253127\n",
      "Iteration 11886, loss = 0.02252834\n",
      "Iteration 11887, loss = 0.02252541\n",
      "Iteration 11888, loss = 0.02252249\n",
      "Iteration 11889, loss = 0.02251956\n",
      "Iteration 11890, loss = 0.02251664\n",
      "Iteration 11891, loss = 0.02251372\n",
      "Iteration 11892, loss = 0.02251080\n",
      "Iteration 11893, loss = 0.02250787\n",
      "Iteration 11894, loss = 0.02250495\n",
      "Iteration 11895, loss = 0.02250203\n",
      "Iteration 11896, loss = 0.02249911\n",
      "Iteration 11897, loss = 0.02249620\n",
      "Iteration 11898, loss = 0.02249328\n",
      "Iteration 11899, loss = 0.02249036\n",
      "Iteration 11900, loss = 0.02248745\n",
      "Iteration 11901, loss = 0.02248453\n",
      "Iteration 11902, loss = 0.02248162\n",
      "Iteration 11903, loss = 0.02247870\n",
      "Iteration 11904, loss = 0.02247579\n",
      "Iteration 11905, loss = 0.02247288\n",
      "Iteration 11906, loss = 0.02246997\n",
      "Iteration 11907, loss = 0.02246706\n",
      "Iteration 11908, loss = 0.02246415\n",
      "Iteration 11909, loss = 0.02246124\n",
      "Iteration 11910, loss = 0.02245833\n",
      "Iteration 11911, loss = 0.02245542\n",
      "Iteration 11912, loss = 0.02245252\n",
      "Iteration 11913, loss = 0.02244961\n",
      "Iteration 11914, loss = 0.02244671\n",
      "Iteration 11915, loss = 0.02244380\n",
      "Iteration 11916, loss = 0.02244090\n",
      "Iteration 11917, loss = 0.02243799\n",
      "Iteration 11918, loss = 0.02243509\n",
      "Iteration 11919, loss = 0.02243219\n",
      "Iteration 11920, loss = 0.02242929\n",
      "Iteration 11921, loss = 0.02242639\n",
      "Iteration 11922, loss = 0.02242349\n",
      "Iteration 11923, loss = 0.02242060\n",
      "Iteration 11924, loss = 0.02241770\n",
      "Iteration 11925, loss = 0.02241480\n",
      "Iteration 11926, loss = 0.02241191\n",
      "Iteration 11927, loss = 0.02240901\n",
      "Iteration 11928, loss = 0.02240612\n",
      "Iteration 11929, loss = 0.02240322\n",
      "Iteration 11930, loss = 0.02240033\n",
      "Iteration 11931, loss = 0.02239744\n",
      "Iteration 11932, loss = 0.02239455\n",
      "Iteration 11933, loss = 0.02239166\n",
      "Iteration 11934, loss = 0.02238877\n",
      "Iteration 11935, loss = 0.02238588\n",
      "Iteration 11936, loss = 0.02238299\n",
      "Iteration 11937, loss = 0.02238010\n",
      "Iteration 11938, loss = 0.02237722\n",
      "Iteration 11939, loss = 0.02237433\n",
      "Iteration 11940, loss = 0.02237145\n",
      "Iteration 11941, loss = 0.02236856\n",
      "Iteration 11942, loss = 0.02236568\n",
      "Iteration 11943, loss = 0.02236280\n",
      "Iteration 11944, loss = 0.02235991\n",
      "Iteration 11945, loss = 0.02235703\n",
      "Iteration 11946, loss = 0.02235415\n",
      "Iteration 11947, loss = 0.02235127\n",
      "Iteration 11948, loss = 0.02234839\n",
      "Iteration 11949, loss = 0.02234552\n",
      "Iteration 11950, loss = 0.02234264\n",
      "Iteration 11951, loss = 0.02233976\n",
      "Iteration 11952, loss = 0.02233689\n",
      "Iteration 11953, loss = 0.02233401\n",
      "Iteration 11954, loss = 0.02233114\n",
      "Iteration 11955, loss = 0.02232826\n",
      "Iteration 11956, loss = 0.02232539\n",
      "Iteration 11957, loss = 0.02232252\n",
      "Iteration 11958, loss = 0.02231965\n",
      "Iteration 11959, loss = 0.02231678\n",
      "Iteration 11960, loss = 0.02231391\n",
      "Iteration 11961, loss = 0.02231104\n",
      "Iteration 11962, loss = 0.02230817\n",
      "Iteration 11963, loss = 0.02230530\n",
      "Iteration 11964, loss = 0.02230244\n",
      "Iteration 11965, loss = 0.02229957\n",
      "Iteration 11966, loss = 0.02229671\n",
      "Iteration 11967, loss = 0.02229384\n",
      "Iteration 11968, loss = 0.02229098\n",
      "Iteration 11969, loss = 0.02228812\n",
      "Iteration 11970, loss = 0.02228525\n",
      "Iteration 11971, loss = 0.02228239\n",
      "Iteration 11972, loss = 0.02227953\n",
      "Iteration 11973, loss = 0.02227667\n",
      "Iteration 11974, loss = 0.02227381\n",
      "Iteration 11975, loss = 0.02227096\n",
      "Iteration 11976, loss = 0.02226810\n",
      "Iteration 11977, loss = 0.02226524\n",
      "Iteration 11978, loss = 0.02226239\n",
      "Iteration 11979, loss = 0.02225953\n",
      "Iteration 11980, loss = 0.02225668\n",
      "Iteration 11981, loss = 0.02225382\n",
      "Iteration 11982, loss = 0.02225097\n",
      "Iteration 11983, loss = 0.02224812\n",
      "Iteration 11984, loss = 0.02224527\n",
      "Iteration 11985, loss = 0.02224242\n",
      "Iteration 11986, loss = 0.02223957\n",
      "Iteration 11987, loss = 0.02223672\n",
      "Iteration 11988, loss = 0.02223387\n",
      "Iteration 11989, loss = 0.02223102\n",
      "Iteration 11990, loss = 0.02222817\n",
      "Iteration 11991, loss = 0.02222533\n",
      "Iteration 11992, loss = 0.02222248\n",
      "Iteration 11993, loss = 0.02221964\n",
      "Iteration 11994, loss = 0.02221679\n",
      "Iteration 11995, loss = 0.02221395\n",
      "Iteration 11996, loss = 0.02221111\n",
      "Iteration 11997, loss = 0.02220827\n",
      "Iteration 11998, loss = 0.02220543\n",
      "Iteration 11999, loss = 0.02220259\n",
      "Iteration 12000, loss = 0.02219975\n",
      "Iteration 12001, loss = 0.02219691\n",
      "Iteration 12002, loss = 0.02219407\n",
      "Iteration 12003, loss = 0.02219123\n",
      "Iteration 12004, loss = 0.02218840\n",
      "Iteration 12005, loss = 0.02218556\n",
      "Iteration 12006, loss = 0.02218273\n",
      "Iteration 12007, loss = 0.02217989\n",
      "Iteration 12008, loss = 0.02217706\n",
      "Iteration 12009, loss = 0.02217423\n",
      "Iteration 12010, loss = 0.02217139\n",
      "Iteration 12011, loss = 0.02216856\n",
      "Iteration 12012, loss = 0.02216573\n",
      "Iteration 12013, loss = 0.02216290\n",
      "Iteration 12014, loss = 0.02216007\n",
      "Iteration 12015, loss = 0.02215725\n",
      "Iteration 12016, loss = 0.02215442\n",
      "Iteration 12017, loss = 0.02215159\n",
      "Iteration 12018, loss = 0.02214877\n",
      "Iteration 12019, loss = 0.02214594\n",
      "Iteration 12020, loss = 0.02214312\n",
      "Iteration 12021, loss = 0.02214029\n",
      "Iteration 12022, loss = 0.02213747\n",
      "Iteration 12023, loss = 0.02213465\n",
      "Iteration 12024, loss = 0.02213183\n",
      "Iteration 12025, loss = 0.02212901\n",
      "Iteration 12026, loss = 0.02212619\n",
      "Iteration 12027, loss = 0.02212337\n",
      "Iteration 12028, loss = 0.02212055\n",
      "Iteration 12029, loss = 0.02211773\n",
      "Iteration 12030, loss = 0.02211492\n",
      "Iteration 12031, loss = 0.02211210\n",
      "Iteration 12032, loss = 0.02210928\n",
      "Iteration 12033, loss = 0.02210647\n",
      "Iteration 12034, loss = 0.02210365\n",
      "Iteration 12035, loss = 0.02210084\n",
      "Iteration 12036, loss = 0.02209803\n",
      "Iteration 12037, loss = 0.02209522\n",
      "Iteration 12038, loss = 0.02209241\n",
      "Iteration 12039, loss = 0.02208960\n",
      "Iteration 12040, loss = 0.02208679\n",
      "Iteration 12041, loss = 0.02208398\n",
      "Iteration 12042, loss = 0.02208117\n",
      "Iteration 12043, loss = 0.02207836\n",
      "Iteration 12044, loss = 0.02207556\n",
      "Iteration 12045, loss = 0.02207275\n",
      "Iteration 12046, loss = 0.02206995\n",
      "Iteration 12047, loss = 0.02206714\n",
      "Iteration 12048, loss = 0.02206434\n",
      "Iteration 12049, loss = 0.02206153\n",
      "Iteration 12050, loss = 0.02205873\n",
      "Iteration 12051, loss = 0.02205593\n",
      "Iteration 12052, loss = 0.02205313\n",
      "Iteration 12053, loss = 0.02205033\n",
      "Iteration 12054, loss = 0.02204753\n",
      "Iteration 12055, loss = 0.02204473\n",
      "Iteration 12056, loss = 0.02204194\n",
      "Iteration 12057, loss = 0.02203914\n",
      "Iteration 12058, loss = 0.02203634\n",
      "Iteration 12059, loss = 0.02203355\n",
      "Iteration 12060, loss = 0.02203075\n",
      "Iteration 12061, loss = 0.02202796\n",
      "Iteration 12062, loss = 0.02202517\n",
      "Iteration 12063, loss = 0.02202237\n",
      "Iteration 12064, loss = 0.02201958\n",
      "Iteration 12065, loss = 0.02201679\n",
      "Iteration 12066, loss = 0.02201400\n",
      "Iteration 12067, loss = 0.02201121\n",
      "Iteration 12068, loss = 0.02200842\n",
      "Iteration 12069, loss = 0.02200563\n",
      "Iteration 12070, loss = 0.02200285\n",
      "Iteration 12071, loss = 0.02200006\n",
      "Iteration 12072, loss = 0.02199727\n",
      "Iteration 12073, loss = 0.02199449\n",
      "Iteration 12074, loss = 0.02199170\n",
      "Iteration 12075, loss = 0.02198892\n",
      "Iteration 12076, loss = 0.02198614\n",
      "Iteration 12077, loss = 0.02198335\n",
      "Iteration 12078, loss = 0.02198057\n",
      "Iteration 12079, loss = 0.02197779\n",
      "Iteration 12080, loss = 0.02197501\n",
      "Iteration 12081, loss = 0.02197223\n",
      "Iteration 12082, loss = 0.02196945\n",
      "Iteration 12083, loss = 0.02196668\n",
      "Iteration 12084, loss = 0.02196390\n",
      "Iteration 12085, loss = 0.02196112\n",
      "Iteration 12086, loss = 0.02195835\n",
      "Iteration 12087, loss = 0.02195557\n",
      "Iteration 12088, loss = 0.02195280\n",
      "Iteration 12089, loss = 0.02195002\n",
      "Iteration 12090, loss = 0.02194725\n",
      "Iteration 12091, loss = 0.02194448\n",
      "Iteration 12092, loss = 0.02194171\n",
      "Iteration 12093, loss = 0.02193894\n",
      "Iteration 12094, loss = 0.02193617\n",
      "Iteration 12095, loss = 0.02193340\n",
      "Iteration 12096, loss = 0.02193063\n",
      "Iteration 12097, loss = 0.02192786\n",
      "Iteration 12098, loss = 0.02192510\n",
      "Iteration 12099, loss = 0.02192233\n",
      "Iteration 12100, loss = 0.02191956\n",
      "Iteration 12101, loss = 0.02191680\n",
      "Iteration 12102, loss = 0.02191404\n",
      "Iteration 12103, loss = 0.02191127\n",
      "Iteration 12104, loss = 0.02190851\n",
      "Iteration 12105, loss = 0.02190575\n",
      "Iteration 12106, loss = 0.02190299\n",
      "Iteration 12107, loss = 0.02190023\n",
      "Iteration 12108, loss = 0.02189747\n",
      "Iteration 12109, loss = 0.02189471\n",
      "Iteration 12110, loss = 0.02189195\n",
      "Iteration 12111, loss = 0.02188919\n",
      "Iteration 12112, loss = 0.02188643\n",
      "Iteration 12113, loss = 0.02188368\n",
      "Iteration 12114, loss = 0.02188092\n",
      "Iteration 12115, loss = 0.02187817\n",
      "Iteration 12116, loss = 0.02187541\n",
      "Iteration 12117, loss = 0.02187266\n",
      "Iteration 12118, loss = 0.02186991\n",
      "Iteration 12119, loss = 0.02186716\n",
      "Iteration 12120, loss = 0.02186441\n",
      "Iteration 12121, loss = 0.02186166\n",
      "Iteration 12122, loss = 0.02185891\n",
      "Iteration 12123, loss = 0.02185616\n",
      "Iteration 12124, loss = 0.02185341\n",
      "Iteration 12125, loss = 0.02185066\n",
      "Iteration 12126, loss = 0.02184791\n",
      "Iteration 12127, loss = 0.02184517\n",
      "Iteration 12128, loss = 0.02184242\n",
      "Iteration 12129, loss = 0.02183968\n",
      "Iteration 12130, loss = 0.02183693\n",
      "Iteration 12131, loss = 0.02183419\n",
      "Iteration 12132, loss = 0.02183145\n",
      "Iteration 12133, loss = 0.02182871\n",
      "Iteration 12134, loss = 0.02182597\n",
      "Iteration 12135, loss = 0.02182323\n",
      "Iteration 12136, loss = 0.02182049\n",
      "Iteration 12137, loss = 0.02181775\n",
      "Iteration 12138, loss = 0.02181501\n",
      "Iteration 12139, loss = 0.02181227\n",
      "Iteration 12140, loss = 0.02180953\n",
      "Iteration 12141, loss = 0.02180680\n",
      "Iteration 12142, loss = 0.02180406\n",
      "Iteration 12143, loss = 0.02180133\n",
      "Iteration 12144, loss = 0.02179859\n",
      "Iteration 12145, loss = 0.02179586\n",
      "Iteration 12146, loss = 0.02179313\n",
      "Iteration 12147, loss = 0.02179040\n",
      "Iteration 12148, loss = 0.02178767\n",
      "Iteration 12149, loss = 0.02178494\n",
      "Iteration 12150, loss = 0.02178221\n",
      "Iteration 12151, loss = 0.02177948\n",
      "Iteration 12152, loss = 0.02177675\n",
      "Iteration 12153, loss = 0.02177402\n",
      "Iteration 12154, loss = 0.02177130\n",
      "Iteration 12155, loss = 0.02176857\n",
      "Iteration 12156, loss = 0.02176584\n",
      "Iteration 12157, loss = 0.02176312\n",
      "Iteration 12158, loss = 0.02176040\n",
      "Iteration 12159, loss = 0.02175767\n",
      "Iteration 12160, loss = 0.02175495\n",
      "Iteration 12161, loss = 0.02175223\n",
      "Iteration 12162, loss = 0.02174951\n",
      "Iteration 12163, loss = 0.02174679\n",
      "Iteration 12164, loss = 0.02174407\n",
      "Iteration 12165, loss = 0.02174135\n",
      "Iteration 12166, loss = 0.02173863\n",
      "Iteration 12167, loss = 0.02173591\n",
      "Iteration 12168, loss = 0.02173319\n",
      "Iteration 12169, loss = 0.02173048\n",
      "Iteration 12170, loss = 0.02172776\n",
      "Iteration 12171, loss = 0.02172505\n",
      "Iteration 12172, loss = 0.02172233\n",
      "Iteration 12173, loss = 0.02171962\n",
      "Iteration 12174, loss = 0.02171691\n",
      "Iteration 12175, loss = 0.02171420\n",
      "Iteration 12176, loss = 0.02171149\n",
      "Iteration 12177, loss = 0.02170877\n",
      "Iteration 12178, loss = 0.02170607\n",
      "Iteration 12179, loss = 0.02170336\n",
      "Iteration 12180, loss = 0.02170065\n",
      "Iteration 12181, loss = 0.02169794\n",
      "Iteration 12182, loss = 0.02169523\n",
      "Iteration 12183, loss = 0.02169253\n",
      "Iteration 12184, loss = 0.02168982\n",
      "Iteration 12185, loss = 0.02168712\n",
      "Iteration 12186, loss = 0.02168441\n",
      "Iteration 12187, loss = 0.02168171\n",
      "Iteration 12188, loss = 0.02167901\n",
      "Iteration 12189, loss = 0.02167630\n",
      "Iteration 12190, loss = 0.02167360\n",
      "Iteration 12191, loss = 0.02167090\n",
      "Iteration 12192, loss = 0.02166820\n",
      "Iteration 12193, loss = 0.02166550\n",
      "Iteration 12194, loss = 0.02166280\n",
      "Iteration 12195, loss = 0.02166011\n",
      "Iteration 12196, loss = 0.02165741\n",
      "Iteration 12197, loss = 0.02165471\n",
      "Iteration 12198, loss = 0.02165202\n",
      "Iteration 12199, loss = 0.02164932\n",
      "Iteration 12200, loss = 0.02164663\n",
      "Iteration 12201, loss = 0.02164393\n",
      "Iteration 12202, loss = 0.02164124\n",
      "Iteration 12203, loss = 0.02163855\n",
      "Iteration 12204, loss = 0.02163586\n",
      "Iteration 12205, loss = 0.02163317\n",
      "Iteration 12206, loss = 0.02163048\n",
      "Iteration 12207, loss = 0.02162779\n",
      "Iteration 12208, loss = 0.02162510\n",
      "Iteration 12209, loss = 0.02162241\n",
      "Iteration 12210, loss = 0.02161972\n",
      "Iteration 12211, loss = 0.02161704\n",
      "Iteration 12212, loss = 0.02161435\n",
      "Iteration 12213, loss = 0.02161166\n",
      "Iteration 12214, loss = 0.02160898\n",
      "Iteration 12215, loss = 0.02160630\n",
      "Iteration 12216, loss = 0.02160361\n",
      "Iteration 12217, loss = 0.02160093\n",
      "Iteration 12218, loss = 0.02159825\n",
      "Iteration 12219, loss = 0.02159557\n",
      "Iteration 12220, loss = 0.02159289\n",
      "Iteration 12221, loss = 0.02159021\n",
      "Iteration 12222, loss = 0.02158753\n",
      "Iteration 12223, loss = 0.02158485\n",
      "Iteration 12224, loss = 0.02158217\n",
      "Iteration 12225, loss = 0.02157949\n",
      "Iteration 12226, loss = 0.02157682\n",
      "Iteration 12227, loss = 0.02157414\n",
      "Iteration 12228, loss = 0.02157147\n",
      "Iteration 12229, loss = 0.02156879\n",
      "Iteration 12230, loss = 0.02156612\n",
      "Iteration 12231, loss = 0.02156345\n",
      "Iteration 12232, loss = 0.02156078\n",
      "Iteration 12233, loss = 0.02155810\n",
      "Iteration 12234, loss = 0.02155543\n",
      "Iteration 12235, loss = 0.02155276\n",
      "Iteration 12236, loss = 0.02155009\n",
      "Iteration 12237, loss = 0.02154743\n",
      "Iteration 12238, loss = 0.02154476\n",
      "Iteration 12239, loss = 0.02154209\n",
      "Iteration 12240, loss = 0.02153942\n",
      "Iteration 12241, loss = 0.02153676\n",
      "Iteration 12242, loss = 0.02153409\n",
      "Iteration 12243, loss = 0.02153143\n",
      "Iteration 12244, loss = 0.02152876\n",
      "Iteration 12245, loss = 0.02152610\n",
      "Iteration 12246, loss = 0.02152344\n",
      "Iteration 12247, loss = 0.02152078\n",
      "Iteration 12248, loss = 0.02151811\n",
      "Iteration 12249, loss = 0.02151545\n",
      "Iteration 12250, loss = 0.02151279\n",
      "Iteration 12251, loss = 0.02151014\n",
      "Iteration 12252, loss = 0.02150748\n",
      "Iteration 12253, loss = 0.02150482\n",
      "Iteration 12254, loss = 0.02150216\n",
      "Iteration 12255, loss = 0.02149951\n",
      "Iteration 12256, loss = 0.02149685\n",
      "Iteration 12257, loss = 0.02149420\n",
      "Iteration 12258, loss = 0.02149154\n",
      "Iteration 12259, loss = 0.02148889\n",
      "Iteration 12260, loss = 0.02148624\n",
      "Iteration 12261, loss = 0.02148358\n",
      "Iteration 12262, loss = 0.02148093\n",
      "Iteration 12263, loss = 0.02147828\n",
      "Iteration 12264, loss = 0.02147563\n",
      "Iteration 12265, loss = 0.02147298\n",
      "Iteration 12266, loss = 0.02147033\n",
      "Iteration 12267, loss = 0.02146768\n",
      "Iteration 12268, loss = 0.02146504\n",
      "Iteration 12269, loss = 0.02146239\n",
      "Iteration 12270, loss = 0.02145974\n",
      "Iteration 12271, loss = 0.02145710\n",
      "Iteration 12272, loss = 0.02145445\n",
      "Iteration 12273, loss = 0.02145181\n",
      "Iteration 12274, loss = 0.02144917\n",
      "Iteration 12275, loss = 0.02144652\n",
      "Iteration 12276, loss = 0.02144388\n",
      "Iteration 12277, loss = 0.02144124\n",
      "Iteration 12278, loss = 0.02143860\n",
      "Iteration 12279, loss = 0.02143596\n",
      "Iteration 12280, loss = 0.02143332\n",
      "Iteration 12281, loss = 0.02143068\n",
      "Iteration 12282, loss = 0.02142804\n",
      "Iteration 12283, loss = 0.02142541\n",
      "Iteration 12284, loss = 0.02142277\n",
      "Iteration 12285, loss = 0.02142013\n",
      "Iteration 12286, loss = 0.02141750\n",
      "Iteration 12287, loss = 0.02141486\n",
      "Iteration 12288, loss = 0.02141223\n",
      "Iteration 12289, loss = 0.02140960\n",
      "Iteration 12290, loss = 0.02140697\n",
      "Iteration 12291, loss = 0.02140433\n",
      "Iteration 12292, loss = 0.02140170\n",
      "Iteration 12293, loss = 0.02139907\n",
      "Iteration 12294, loss = 0.02139644\n",
      "Iteration 12295, loss = 0.02139381\n",
      "Iteration 12296, loss = 0.02139118\n",
      "Iteration 12297, loss = 0.02138856\n",
      "Iteration 12298, loss = 0.02138593\n",
      "Iteration 12299, loss = 0.02138330\n",
      "Iteration 12300, loss = 0.02138068\n",
      "Iteration 12301, loss = 0.02137805\n",
      "Iteration 12302, loss = 0.02137543\n",
      "Iteration 12303, loss = 0.02137280\n",
      "Iteration 12304, loss = 0.02137018\n",
      "Iteration 12305, loss = 0.02136756\n",
      "Iteration 12306, loss = 0.02136494\n",
      "Iteration 12307, loss = 0.02136232\n",
      "Iteration 12308, loss = 0.02135970\n",
      "Iteration 12309, loss = 0.02135708\n",
      "Iteration 12310, loss = 0.02135446\n",
      "Iteration 12311, loss = 0.02135184\n",
      "Iteration 12312, loss = 0.02134922\n",
      "Iteration 12313, loss = 0.02134660\n",
      "Iteration 12314, loss = 0.02134399\n",
      "Iteration 12315, loss = 0.02134137\n",
      "Iteration 12316, loss = 0.02133876\n",
      "Iteration 12317, loss = 0.02133614\n",
      "Iteration 12318, loss = 0.02133353\n",
      "Iteration 12319, loss = 0.02133092\n",
      "Iteration 12320, loss = 0.02132830\n",
      "Iteration 12321, loss = 0.02132569\n",
      "Iteration 12322, loss = 0.02132308\n",
      "Iteration 12323, loss = 0.02132047\n",
      "Iteration 12324, loss = 0.02131786\n",
      "Iteration 12325, loss = 0.02131525\n",
      "Iteration 12326, loss = 0.02131264\n",
      "Iteration 12327, loss = 0.02131004\n",
      "Iteration 12328, loss = 0.02130743\n",
      "Iteration 12329, loss = 0.02130482\n",
      "Iteration 12330, loss = 0.02130222\n",
      "Iteration 12331, loss = 0.02129961\n",
      "Iteration 12332, loss = 0.02129701\n",
      "Iteration 12333, loss = 0.02129440\n",
      "Iteration 12334, loss = 0.02129180\n",
      "Iteration 12335, loss = 0.02128920\n",
      "Iteration 12336, loss = 0.02128660\n",
      "Iteration 12337, loss = 0.02128400\n",
      "Iteration 12338, loss = 0.02128140\n",
      "Iteration 12339, loss = 0.02127880\n",
      "Iteration 12340, loss = 0.02127620\n",
      "Iteration 12341, loss = 0.02127360\n",
      "Iteration 12342, loss = 0.02127100\n",
      "Iteration 12343, loss = 0.02126840\n",
      "Iteration 12344, loss = 0.02126581\n",
      "Iteration 12345, loss = 0.02126321\n",
      "Iteration 12346, loss = 0.02126062\n",
      "Iteration 12347, loss = 0.02125802\n",
      "Iteration 12348, loss = 0.02125543\n",
      "Iteration 12349, loss = 0.02125284\n",
      "Iteration 12350, loss = 0.02125024\n",
      "Iteration 12351, loss = 0.02124765\n",
      "Iteration 12352, loss = 0.02124506\n",
      "Iteration 12353, loss = 0.02124247\n",
      "Iteration 12354, loss = 0.02123988\n",
      "Iteration 12355, loss = 0.02123729\n",
      "Iteration 12356, loss = 0.02123470\n",
      "Iteration 12357, loss = 0.02123212\n",
      "Iteration 12358, loss = 0.02122953\n",
      "Iteration 12359, loss = 0.02122694\n",
      "Iteration 12360, loss = 0.02122436\n",
      "Iteration 12361, loss = 0.02122177\n",
      "Iteration 12362, loss = 0.02121919\n",
      "Iteration 12363, loss = 0.02121660\n",
      "Iteration 12364, loss = 0.02121402\n",
      "Iteration 12365, loss = 0.02121144\n",
      "Iteration 12366, loss = 0.02120886\n",
      "Iteration 12367, loss = 0.02120628\n",
      "Iteration 12368, loss = 0.02120370\n",
      "Iteration 12369, loss = 0.02120112\n",
      "Iteration 12370, loss = 0.02119854\n",
      "Iteration 12371, loss = 0.02119596\n",
      "Iteration 12372, loss = 0.02119338\n",
      "Iteration 12373, loss = 0.02119080\n",
      "Iteration 12374, loss = 0.02118823\n",
      "Iteration 12375, loss = 0.02118565\n",
      "Iteration 12376, loss = 0.02118308\n",
      "Iteration 12377, loss = 0.02118050\n",
      "Iteration 12378, loss = 0.02117793\n",
      "Iteration 12379, loss = 0.02117535\n",
      "Iteration 12380, loss = 0.02117278\n",
      "Iteration 12381, loss = 0.02117021\n",
      "Iteration 12382, loss = 0.02116764\n",
      "Iteration 12383, loss = 0.02116507\n",
      "Iteration 12384, loss = 0.02116250\n",
      "Iteration 12385, loss = 0.02115993\n",
      "Iteration 12386, loss = 0.02115736\n",
      "Iteration 12387, loss = 0.02115479\n",
      "Iteration 12388, loss = 0.02115222\n",
      "Iteration 12389, loss = 0.02114966\n",
      "Iteration 12390, loss = 0.02114709\n",
      "Iteration 12391, loss = 0.02114453\n",
      "Iteration 12392, loss = 0.02114196\n",
      "Iteration 12393, loss = 0.02113940\n",
      "Iteration 12394, loss = 0.02113683\n",
      "Iteration 12395, loss = 0.02113427\n",
      "Iteration 12396, loss = 0.02113171\n",
      "Iteration 12397, loss = 0.02112915\n",
      "Iteration 12398, loss = 0.02112659\n",
      "Iteration 12399, loss = 0.02112403\n",
      "Iteration 12400, loss = 0.02112147\n",
      "Iteration 12401, loss = 0.02111891\n",
      "Iteration 12402, loss = 0.02111635\n",
      "Iteration 12403, loss = 0.02111379\n",
      "Iteration 12404, loss = 0.02111123\n",
      "Iteration 12405, loss = 0.02110868\n",
      "Iteration 12406, loss = 0.02110612\n",
      "Iteration 12407, loss = 0.02110357\n",
      "Iteration 12408, loss = 0.02110101\n",
      "Iteration 12409, loss = 0.02109846\n",
      "Iteration 12410, loss = 0.02109591\n",
      "Iteration 12411, loss = 0.02109335\n",
      "Iteration 12412, loss = 0.02109080\n",
      "Iteration 12413, loss = 0.02108825\n",
      "Iteration 12414, loss = 0.02108570\n",
      "Iteration 12415, loss = 0.02108315\n",
      "Iteration 12416, loss = 0.02108060\n",
      "Iteration 12417, loss = 0.02107805\n",
      "Iteration 12418, loss = 0.02107551\n",
      "Iteration 12419, loss = 0.02107296\n",
      "Iteration 12420, loss = 0.02107041\n",
      "Iteration 12421, loss = 0.02106787\n",
      "Iteration 12422, loss = 0.02106532\n",
      "Iteration 12423, loss = 0.02106278\n",
      "Iteration 12424, loss = 0.02106023\n",
      "Iteration 12425, loss = 0.02105769\n",
      "Iteration 12426, loss = 0.02105515\n",
      "Iteration 12427, loss = 0.02105260\n",
      "Iteration 12428, loss = 0.02105006\n",
      "Iteration 12429, loss = 0.02104752\n",
      "Iteration 12430, loss = 0.02104498\n",
      "Iteration 12431, loss = 0.02104244\n",
      "Iteration 12432, loss = 0.02103990\n",
      "Iteration 12433, loss = 0.02103736\n",
      "Iteration 12434, loss = 0.02103483\n",
      "Iteration 12435, loss = 0.02103229\n",
      "Iteration 12436, loss = 0.02102975\n",
      "Iteration 12437, loss = 0.02102722\n",
      "Iteration 12438, loss = 0.02102468\n",
      "Iteration 12439, loss = 0.02102215\n",
      "Iteration 12440, loss = 0.02101962\n",
      "Iteration 12441, loss = 0.02101708\n",
      "Iteration 12442, loss = 0.02101455\n",
      "Iteration 12443, loss = 0.02101202\n",
      "Iteration 12444, loss = 0.02100949\n",
      "Iteration 12445, loss = 0.02100696\n",
      "Iteration 12446, loss = 0.02100443\n",
      "Iteration 12447, loss = 0.02100190\n",
      "Iteration 12448, loss = 0.02099937\n",
      "Iteration 12449, loss = 0.02099684\n",
      "Iteration 12450, loss = 0.02099431\n",
      "Iteration 12451, loss = 0.02099179\n",
      "Iteration 12452, loss = 0.02098926\n",
      "Iteration 12453, loss = 0.02098674\n",
      "Iteration 12454, loss = 0.02098421\n",
      "Iteration 12455, loss = 0.02098169\n",
      "Iteration 12456, loss = 0.02097916\n",
      "Iteration 12457, loss = 0.02097664\n",
      "Iteration 12458, loss = 0.02097412\n",
      "Iteration 12459, loss = 0.02097160\n",
      "Iteration 12460, loss = 0.02096908\n",
      "Iteration 12461, loss = 0.02096656\n",
      "Iteration 12462, loss = 0.02096404\n",
      "Iteration 12463, loss = 0.02096152\n",
      "Iteration 12464, loss = 0.02095900\n",
      "Iteration 12465, loss = 0.02095648\n",
      "Iteration 12466, loss = 0.02095396\n",
      "Iteration 12467, loss = 0.02095145\n",
      "Iteration 12468, loss = 0.02094893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12469, loss = 0.02094642\n",
      "Iteration 12470, loss = 0.02094390\n",
      "Iteration 12471, loss = 0.02094139\n",
      "Iteration 12472, loss = 0.02093887\n",
      "Iteration 12473, loss = 0.02093636\n",
      "Iteration 12474, loss = 0.02093385\n",
      "Iteration 12475, loss = 0.02093134\n",
      "Iteration 12476, loss = 0.02092883\n",
      "Iteration 12477, loss = 0.02092632\n",
      "Iteration 12478, loss = 0.02092381\n",
      "Iteration 12479, loss = 0.02092130\n",
      "Iteration 12480, loss = 0.02091879\n",
      "Iteration 12481, loss = 0.02091628\n",
      "Iteration 12482, loss = 0.02091378\n",
      "Iteration 12483, loss = 0.02091127\n",
      "Iteration 12484, loss = 0.02090876\n",
      "Iteration 12485, loss = 0.02090626\n",
      "Iteration 12486, loss = 0.02090375\n",
      "Iteration 12487, loss = 0.02090125\n",
      "Iteration 12488, loss = 0.02089875\n",
      "Iteration 12489, loss = 0.02089624\n",
      "Iteration 12490, loss = 0.02089374\n",
      "Iteration 12491, loss = 0.02089124\n",
      "Iteration 12492, loss = 0.02088874\n",
      "Iteration 12493, loss = 0.02088624\n",
      "Iteration 12494, loss = 0.02088374\n",
      "Iteration 12495, loss = 0.02088124\n",
      "Iteration 12496, loss = 0.02087874\n",
      "Iteration 12497, loss = 0.02087625\n",
      "Iteration 12498, loss = 0.02087375\n",
      "Iteration 12499, loss = 0.02087125\n",
      "Iteration 12500, loss = 0.02086876\n",
      "Iteration 12501, loss = 0.02086626\n",
      "Iteration 12502, loss = 0.02086377\n",
      "Iteration 12503, loss = 0.02086127\n",
      "Iteration 12504, loss = 0.02085878\n",
      "Iteration 12505, loss = 0.02085629\n",
      "Iteration 12506, loss = 0.02085380\n",
      "Iteration 12507, loss = 0.02085131\n",
      "Iteration 12508, loss = 0.02084882\n",
      "Iteration 12509, loss = 0.02084633\n",
      "Iteration 12510, loss = 0.02084384\n",
      "Iteration 12511, loss = 0.02084135\n",
      "Iteration 12512, loss = 0.02083886\n",
      "Iteration 12513, loss = 0.02083637\n",
      "Iteration 12514, loss = 0.02083388\n",
      "Iteration 12515, loss = 0.02083140\n",
      "Iteration 12516, loss = 0.02082891\n",
      "Iteration 12517, loss = 0.02082643\n",
      "Iteration 12518, loss = 0.02082394\n",
      "Iteration 12519, loss = 0.02082146\n",
      "Iteration 12520, loss = 0.02081898\n",
      "Iteration 12521, loss = 0.02081649\n",
      "Iteration 12522, loss = 0.02081401\n",
      "Iteration 12523, loss = 0.02081153\n",
      "Iteration 12524, loss = 0.02080905\n",
      "Iteration 12525, loss = 0.02080657\n",
      "Iteration 12526, loss = 0.02080409\n",
      "Iteration 12527, loss = 0.02080161\n",
      "Iteration 12528, loss = 0.02079913\n",
      "Iteration 12529, loss = 0.02079666\n",
      "Iteration 12530, loss = 0.02079418\n",
      "Iteration 12531, loss = 0.02079170\n",
      "Iteration 12532, loss = 0.02078923\n",
      "Iteration 12533, loss = 0.02078675\n",
      "Iteration 12534, loss = 0.02078428\n",
      "Iteration 12535, loss = 0.02078180\n",
      "Iteration 12536, loss = 0.02077933\n",
      "Iteration 12537, loss = 0.02077686\n",
      "Iteration 12538, loss = 0.02077439\n",
      "Iteration 12539, loss = 0.02077191\n",
      "Iteration 12540, loss = 0.02076944\n",
      "Iteration 12541, loss = 0.02076697\n",
      "Iteration 12542, loss = 0.02076450\n",
      "Iteration 12543, loss = 0.02076204\n",
      "Iteration 12544, loss = 0.02075957\n",
      "Iteration 12545, loss = 0.02075710\n",
      "Iteration 12546, loss = 0.02075463\n",
      "Iteration 12547, loss = 0.02075217\n",
      "Iteration 12548, loss = 0.02074970\n",
      "Iteration 12549, loss = 0.02074724\n",
      "Iteration 12550, loss = 0.02074477\n",
      "Iteration 12551, loss = 0.02074231\n",
      "Iteration 12552, loss = 0.02073984\n",
      "Iteration 12553, loss = 0.02073738\n",
      "Iteration 12554, loss = 0.02073492\n",
      "Iteration 12555, loss = 0.02073246\n",
      "Iteration 12556, loss = 0.02073000\n",
      "Iteration 12557, loss = 0.02072754\n",
      "Iteration 12558, loss = 0.02072508\n",
      "Iteration 12559, loss = 0.02072262\n",
      "Iteration 12560, loss = 0.02072016\n",
      "Iteration 12561, loss = 0.02071770\n",
      "Iteration 12562, loss = 0.02071524\n",
      "Iteration 12563, loss = 0.02071279\n",
      "Iteration 12564, loss = 0.02071033\n",
      "Iteration 12565, loss = 0.02070788\n",
      "Iteration 12566, loss = 0.02070542\n",
      "Iteration 12567, loss = 0.02070297\n",
      "Iteration 12568, loss = 0.02070051\n",
      "Iteration 12569, loss = 0.02069806\n",
      "Iteration 12570, loss = 0.02069561\n",
      "Iteration 12571, loss = 0.02069316\n",
      "Iteration 12572, loss = 0.02069071\n",
      "Iteration 12573, loss = 0.02068826\n",
      "Iteration 12574, loss = 0.02068581\n",
      "Iteration 12575, loss = 0.02068336\n",
      "Iteration 12576, loss = 0.02068091\n",
      "Iteration 12577, loss = 0.02067846\n",
      "Iteration 12578, loss = 0.02067601\n",
      "Iteration 12579, loss = 0.02067357\n",
      "Iteration 12580, loss = 0.02067112\n",
      "Iteration 12581, loss = 0.02066868\n",
      "Iteration 12582, loss = 0.02066623\n",
      "Iteration 12583, loss = 0.02066379\n",
      "Iteration 12584, loss = 0.02066134\n",
      "Iteration 12585, loss = 0.02065890\n",
      "Iteration 12586, loss = 0.02065646\n",
      "Iteration 12587, loss = 0.02065402\n",
      "Iteration 12588, loss = 0.02065157\n",
      "Iteration 12589, loss = 0.02064913\n",
      "Iteration 12590, loss = 0.02064669\n",
      "Iteration 12591, loss = 0.02064425\n",
      "Iteration 12592, loss = 0.02064181\n",
      "Iteration 12593, loss = 0.02063938\n",
      "Iteration 12594, loss = 0.02063694\n",
      "Iteration 12595, loss = 0.02063450\n",
      "Iteration 12596, loss = 0.02063207\n",
      "Iteration 12597, loss = 0.02062963\n",
      "Iteration 12598, loss = 0.02062719\n",
      "Iteration 12599, loss = 0.02062476\n",
      "Iteration 12600, loss = 0.02062233\n",
      "Iteration 12601, loss = 0.02061989\n",
      "Iteration 12602, loss = 0.02061746\n",
      "Iteration 12603, loss = 0.02061503\n",
      "Iteration 12604, loss = 0.02061260\n",
      "Iteration 12605, loss = 0.02061017\n",
      "Iteration 12606, loss = 0.02060774\n",
      "Iteration 12607, loss = 0.02060531\n",
      "Iteration 12608, loss = 0.02060288\n",
      "Iteration 12609, loss = 0.02060045\n",
      "Iteration 12610, loss = 0.02059802\n",
      "Iteration 12611, loss = 0.02059559\n",
      "Iteration 12612, loss = 0.02059317\n",
      "Iteration 12613, loss = 0.02059074\n",
      "Iteration 12614, loss = 0.02058831\n",
      "Iteration 12615, loss = 0.02058589\n",
      "Iteration 12616, loss = 0.02058347\n",
      "Iteration 12617, loss = 0.02058104\n",
      "Iteration 12618, loss = 0.02057862\n",
      "Iteration 12619, loss = 0.02057620\n",
      "Iteration 12620, loss = 0.02057377\n",
      "Iteration 12621, loss = 0.02057135\n",
      "Iteration 12622, loss = 0.02056893\n",
      "Iteration 12623, loss = 0.02056651\n",
      "Iteration 12624, loss = 0.02056409\n",
      "Iteration 12625, loss = 0.02056167\n",
      "Iteration 12626, loss = 0.02055926\n",
      "Iteration 12627, loss = 0.02055684\n",
      "Iteration 12628, loss = 0.02055442\n",
      "Iteration 12629, loss = 0.02055200\n",
      "Iteration 12630, loss = 0.02054959\n",
      "Iteration 12631, loss = 0.02054717\n",
      "Iteration 12632, loss = 0.02054476\n",
      "Iteration 12633, loss = 0.02054234\n",
      "Iteration 12634, loss = 0.02053993\n",
      "Iteration 12635, loss = 0.02053752\n",
      "Iteration 12636, loss = 0.02053511\n",
      "Iteration 12637, loss = 0.02053269\n",
      "Iteration 12638, loss = 0.02053028\n",
      "Iteration 12639, loss = 0.02052787\n",
      "Iteration 12640, loss = 0.02052546\n",
      "Iteration 12641, loss = 0.02052305\n",
      "Iteration 12642, loss = 0.02052064\n",
      "Iteration 12643, loss = 0.02051824\n",
      "Iteration 12644, loss = 0.02051583\n",
      "Iteration 12645, loss = 0.02051342\n",
      "Iteration 12646, loss = 0.02051102\n",
      "Iteration 12647, loss = 0.02050861\n",
      "Iteration 12648, loss = 0.02050620\n",
      "Iteration 12649, loss = 0.02050380\n",
      "Iteration 12650, loss = 0.02050140\n",
      "Iteration 12651, loss = 0.02049899\n",
      "Iteration 12652, loss = 0.02049659\n",
      "Iteration 12653, loss = 0.02049419\n",
      "Iteration 12654, loss = 0.02049179\n",
      "Iteration 12655, loss = 0.02048939\n",
      "Iteration 12656, loss = 0.02048699\n",
      "Iteration 12657, loss = 0.02048459\n",
      "Iteration 12658, loss = 0.02048219\n",
      "Iteration 12659, loss = 0.02047979\n",
      "Iteration 12660, loss = 0.02047739\n",
      "Iteration 12661, loss = 0.02047499\n",
      "Iteration 12662, loss = 0.02047260\n",
      "Iteration 12663, loss = 0.02047020\n",
      "Iteration 12664, loss = 0.02046780\n",
      "Iteration 12665, loss = 0.02046541\n",
      "Iteration 12666, loss = 0.02046301\n",
      "Iteration 12667, loss = 0.02046062\n",
      "Iteration 12668, loss = 0.02045823\n",
      "Iteration 12669, loss = 0.02045583\n",
      "Iteration 12670, loss = 0.02045344\n",
      "Iteration 12671, loss = 0.02045105\n",
      "Iteration 12672, loss = 0.02044866\n",
      "Iteration 12673, loss = 0.02044627\n",
      "Iteration 12674, loss = 0.02044388\n",
      "Iteration 12675, loss = 0.02044149\n",
      "Iteration 12676, loss = 0.02043910\n",
      "Iteration 12677, loss = 0.02043671\n",
      "Iteration 12678, loss = 0.02043433\n",
      "Iteration 12679, loss = 0.02043194\n",
      "Iteration 12680, loss = 0.02042955\n",
      "Iteration 12681, loss = 0.02042717\n",
      "Iteration 12682, loss = 0.02042478\n",
      "Iteration 12683, loss = 0.02042240\n",
      "Iteration 12684, loss = 0.02042002\n",
      "Iteration 12685, loss = 0.02041763\n",
      "Iteration 12686, loss = 0.02041525\n",
      "Iteration 12687, loss = 0.02041287\n",
      "Iteration 12688, loss = 0.02041049\n",
      "Iteration 12689, loss = 0.02040811\n",
      "Iteration 12690, loss = 0.02040573\n",
      "Iteration 12691, loss = 0.02040335\n",
      "Iteration 12692, loss = 0.02040097\n",
      "Iteration 12693, loss = 0.02039859\n",
      "Iteration 12694, loss = 0.02039621\n",
      "Iteration 12695, loss = 0.02039383\n",
      "Iteration 12696, loss = 0.02039146\n",
      "Iteration 12697, loss = 0.02038908\n",
      "Iteration 12698, loss = 0.02038670\n",
      "Iteration 12699, loss = 0.02038433\n",
      "Iteration 12700, loss = 0.02038196\n",
      "Iteration 12701, loss = 0.02037958\n",
      "Iteration 12702, loss = 0.02037721\n",
      "Iteration 12703, loss = 0.02037484\n",
      "Iteration 12704, loss = 0.02037246\n",
      "Iteration 12705, loss = 0.02037009\n",
      "Iteration 12706, loss = 0.02036772\n",
      "Iteration 12707, loss = 0.02036535\n",
      "Iteration 12708, loss = 0.02036298\n",
      "Iteration 12709, loss = 0.02036061\n",
      "Iteration 12710, loss = 0.02035824\n",
      "Iteration 12711, loss = 0.02035588\n",
      "Iteration 12712, loss = 0.02035351\n",
      "Iteration 12713, loss = 0.02035114\n",
      "Iteration 12714, loss = 0.02034878\n",
      "Iteration 12715, loss = 0.02034641\n",
      "Iteration 12716, loss = 0.02034404\n",
      "Iteration 12717, loss = 0.02034168\n",
      "Iteration 12718, loss = 0.02033932\n",
      "Iteration 12719, loss = 0.02033695\n",
      "Iteration 12720, loss = 0.02033459\n",
      "Iteration 12721, loss = 0.02033223\n",
      "Iteration 12722, loss = 0.02032987\n",
      "Iteration 12723, loss = 0.02032751\n",
      "Iteration 12724, loss = 0.02032515\n",
      "Iteration 12725, loss = 0.02032279\n",
      "Iteration 12726, loss = 0.02032043\n",
      "Iteration 12727, loss = 0.02031807\n",
      "Iteration 12728, loss = 0.02031571\n",
      "Iteration 12729, loss = 0.02031335\n",
      "Iteration 12730, loss = 0.02031099\n",
      "Iteration 12731, loss = 0.02030864\n",
      "Iteration 12732, loss = 0.02030628\n",
      "Iteration 12733, loss = 0.02030393\n",
      "Iteration 12734, loss = 0.02030157\n",
      "Iteration 12735, loss = 0.02029922\n",
      "Iteration 12736, loss = 0.02029687\n",
      "Iteration 12737, loss = 0.02029451\n",
      "Iteration 12738, loss = 0.02029216\n",
      "Iteration 12739, loss = 0.02028981\n",
      "Iteration 12740, loss = 0.02028746\n",
      "Iteration 12741, loss = 0.02028511\n",
      "Iteration 12742, loss = 0.02028276\n",
      "Iteration 12743, loss = 0.02028041\n",
      "Iteration 12744, loss = 0.02027806\n",
      "Iteration 12745, loss = 0.02027571\n",
      "Iteration 12746, loss = 0.02027336\n",
      "Iteration 12747, loss = 0.02027102\n",
      "Iteration 12748, loss = 0.02026867\n",
      "Iteration 12749, loss = 0.02026632\n",
      "Iteration 12750, loss = 0.02026398\n",
      "Iteration 12751, loss = 0.02026163\n",
      "Iteration 12752, loss = 0.02025929\n",
      "Iteration 12753, loss = 0.02025695\n",
      "Iteration 12754, loss = 0.02025460\n",
      "Iteration 12755, loss = 0.02025226\n",
      "Iteration 12756, loss = 0.02024992\n",
      "Iteration 12757, loss = 0.02024758\n",
      "Iteration 12758, loss = 0.02024524\n",
      "Iteration 12759, loss = 0.02024290\n",
      "Iteration 12760, loss = 0.02024056\n",
      "Iteration 12761, loss = 0.02023822\n",
      "Iteration 12762, loss = 0.02023588\n",
      "Iteration 12763, loss = 0.02023354\n",
      "Iteration 12764, loss = 0.02023120\n",
      "Iteration 12765, loss = 0.02022887\n",
      "Iteration 12766, loss = 0.02022653\n",
      "Iteration 12767, loss = 0.02022419\n",
      "Iteration 12768, loss = 0.02022186\n",
      "Iteration 12769, loss = 0.02021952\n",
      "Iteration 12770, loss = 0.02021719\n",
      "Iteration 12771, loss = 0.02021486\n",
      "Iteration 12772, loss = 0.02021252\n",
      "Iteration 12773, loss = 0.02021019\n",
      "Iteration 12774, loss = 0.02020786\n",
      "Iteration 12775, loss = 0.02020553\n",
      "Iteration 12776, loss = 0.02020320\n",
      "Iteration 12777, loss = 0.02020087\n",
      "Iteration 12778, loss = 0.02019854\n",
      "Iteration 12779, loss = 0.02019621\n",
      "Iteration 12780, loss = 0.02019388\n",
      "Iteration 12781, loss = 0.02019156\n",
      "Iteration 12782, loss = 0.02018923\n",
      "Iteration 12783, loss = 0.02018690\n",
      "Iteration 12784, loss = 0.02018458\n",
      "Iteration 12785, loss = 0.02018225\n",
      "Iteration 12786, loss = 0.02017993\n",
      "Iteration 12787, loss = 0.02017760\n",
      "Iteration 12788, loss = 0.02017528\n",
      "Iteration 12789, loss = 0.02017296\n",
      "Iteration 12790, loss = 0.02017063\n",
      "Iteration 12791, loss = 0.02016831\n",
      "Iteration 12792, loss = 0.02016599\n",
      "Iteration 12793, loss = 0.02016367\n",
      "Iteration 12794, loss = 0.02016135\n",
      "Iteration 12795, loss = 0.02015903\n",
      "Iteration 12796, loss = 0.02015671\n",
      "Iteration 12797, loss = 0.02015439\n",
      "Iteration 12798, loss = 0.02015207\n",
      "Iteration 12799, loss = 0.02014976\n",
      "Iteration 12800, loss = 0.02014744\n",
      "Iteration 12801, loss = 0.02014512\n",
      "Iteration 12802, loss = 0.02014281\n",
      "Iteration 12803, loss = 0.02014049\n",
      "Iteration 12804, loss = 0.02013818\n",
      "Iteration 12805, loss = 0.02013586\n",
      "Iteration 12806, loss = 0.02013355\n",
      "Iteration 12807, loss = 0.02013124\n",
      "Iteration 12808, loss = 0.02012892\n",
      "Iteration 12809, loss = 0.02012661\n",
      "Iteration 12810, loss = 0.02012430\n",
      "Iteration 12811, loss = 0.02012199\n",
      "Iteration 12812, loss = 0.02011968\n",
      "Iteration 12813, loss = 0.02011737\n",
      "Iteration 12814, loss = 0.02011506\n",
      "Iteration 12815, loss = 0.02011275\n",
      "Iteration 12816, loss = 0.02011045\n",
      "Iteration 12817, loss = 0.02010814\n",
      "Iteration 12818, loss = 0.02010583\n",
      "Iteration 12819, loss = 0.02010353\n",
      "Iteration 12820, loss = 0.02010122\n",
      "Iteration 12821, loss = 0.02009892\n",
      "Iteration 12822, loss = 0.02009661\n",
      "Iteration 12823, loss = 0.02009431\n",
      "Iteration 12824, loss = 0.02009200\n",
      "Iteration 12825, loss = 0.02008970\n",
      "Iteration 12826, loss = 0.02008740\n",
      "Iteration 12827, loss = 0.02008510\n",
      "Iteration 12828, loss = 0.02008280\n",
      "Iteration 12829, loss = 0.02008050\n",
      "Iteration 12830, loss = 0.02007820\n",
      "Iteration 12831, loss = 0.02007590\n",
      "Iteration 12832, loss = 0.02007360\n",
      "Iteration 12833, loss = 0.02007130\n",
      "Iteration 12834, loss = 0.02006900\n",
      "Iteration 12835, loss = 0.02006670\n",
      "Iteration 12836, loss = 0.02006441\n",
      "Iteration 12837, loss = 0.02006211\n",
      "Iteration 12838, loss = 0.02005982\n",
      "Iteration 12839, loss = 0.02005752\n",
      "Iteration 12840, loss = 0.02005523\n",
      "Iteration 12841, loss = 0.02005293\n",
      "Iteration 12842, loss = 0.02005064\n",
      "Iteration 12843, loss = 0.02004835\n",
      "Iteration 12844, loss = 0.02004605\n",
      "Iteration 12845, loss = 0.02004376\n",
      "Iteration 12846, loss = 0.02004147\n",
      "Iteration 12847, loss = 0.02003918\n",
      "Iteration 12848, loss = 0.02003689\n",
      "Iteration 12849, loss = 0.02003460\n",
      "Iteration 12850, loss = 0.02003231\n",
      "Iteration 12851, loss = 0.02003003\n",
      "Iteration 12852, loss = 0.02002774\n",
      "Iteration 12853, loss = 0.02002545\n",
      "Iteration 12854, loss = 0.02002316\n",
      "Iteration 12855, loss = 0.02002088\n",
      "Iteration 12856, loss = 0.02001859\n",
      "Iteration 12857, loss = 0.02001631\n",
      "Iteration 12858, loss = 0.02001402\n",
      "Iteration 12859, loss = 0.02001174\n",
      "Iteration 12860, loss = 0.02000946\n",
      "Iteration 12861, loss = 0.02000717\n",
      "Iteration 12862, loss = 0.02000489\n",
      "Iteration 12863, loss = 0.02000261\n",
      "Iteration 12864, loss = 0.02000033\n",
      "Iteration 12865, loss = 0.01999805\n",
      "Iteration 12866, loss = 0.01999577\n",
      "Iteration 12867, loss = 0.01999349\n",
      "Iteration 12868, loss = 0.01999121\n",
      "Iteration 12869, loss = 0.01998893\n",
      "Iteration 12870, loss = 0.01998665\n",
      "Iteration 12871, loss = 0.01998438\n",
      "Iteration 12872, loss = 0.01998210\n",
      "Iteration 12873, loss = 0.01997982\n",
      "Iteration 12874, loss = 0.01997755\n",
      "Iteration 12875, loss = 0.01997527\n",
      "Iteration 12876, loss = 0.01997300\n",
      "Iteration 12877, loss = 0.01997073\n",
      "Iteration 12878, loss = 0.01996845\n",
      "Iteration 12879, loss = 0.01996618\n",
      "Iteration 12880, loss = 0.01996391\n",
      "Iteration 12881, loss = 0.01996164\n",
      "Iteration 12882, loss = 0.01995936\n",
      "Iteration 12883, loss = 0.01995709\n",
      "Iteration 12884, loss = 0.01995482\n",
      "Iteration 12885, loss = 0.01995255\n",
      "Iteration 12886, loss = 0.01995029\n",
      "Iteration 12887, loss = 0.01994802\n",
      "Iteration 12888, loss = 0.01994575\n",
      "Iteration 12889, loss = 0.01994348\n",
      "Iteration 12890, loss = 0.01994122\n",
      "Iteration 12891, loss = 0.01993895\n",
      "Iteration 12892, loss = 0.01993668\n",
      "Iteration 12893, loss = 0.01993442\n",
      "Iteration 12894, loss = 0.01993215\n",
      "Iteration 12895, loss = 0.01992989\n",
      "Iteration 12896, loss = 0.01992763\n",
      "Iteration 12897, loss = 0.01992536\n",
      "Iteration 12898, loss = 0.01992310\n",
      "Iteration 12899, loss = 0.01992084\n",
      "Iteration 12900, loss = 0.01991858\n",
      "Iteration 12901, loss = 0.01991632\n",
      "Iteration 12902, loss = 0.01991406\n",
      "Iteration 12903, loss = 0.01991180\n",
      "Iteration 12904, loss = 0.01990954\n",
      "Iteration 12905, loss = 0.01990728\n",
      "Iteration 12906, loss = 0.01990502\n",
      "Iteration 12907, loss = 0.01990277\n",
      "Iteration 12908, loss = 0.01990051\n",
      "Iteration 12909, loss = 0.01989825\n",
      "Iteration 12910, loss = 0.01989600\n",
      "Iteration 12911, loss = 0.01989374\n",
      "Iteration 12912, loss = 0.01989149\n",
      "Iteration 12913, loss = 0.01988923\n",
      "Iteration 12914, loss = 0.01988698\n",
      "Iteration 12915, loss = 0.01988473\n",
      "Iteration 12916, loss = 0.01988247\n",
      "Iteration 12917, loss = 0.01988022\n",
      "Iteration 12918, loss = 0.01987797\n",
      "Iteration 12919, loss = 0.01987572\n",
      "Iteration 12920, loss = 0.01987347\n",
      "Iteration 12921, loss = 0.01987122\n",
      "Iteration 12922, loss = 0.01986897\n",
      "Iteration 12923, loss = 0.01986672\n",
      "Iteration 12924, loss = 0.01986447\n",
      "Iteration 12925, loss = 0.01986223\n",
      "Iteration 12926, loss = 0.01985998\n",
      "Iteration 12927, loss = 0.01985773\n",
      "Iteration 12928, loss = 0.01985549\n",
      "Iteration 12929, loss = 0.01985324\n",
      "Iteration 12930, loss = 0.01985100\n",
      "Iteration 12931, loss = 0.01984875\n",
      "Iteration 12932, loss = 0.01984651\n",
      "Iteration 12933, loss = 0.01984426\n",
      "Iteration 12934, loss = 0.01984202\n",
      "Iteration 12935, loss = 0.01983978\n",
      "Iteration 12936, loss = 0.01983754\n",
      "Iteration 12937, loss = 0.01983530\n",
      "Iteration 12938, loss = 0.01983306\n",
      "Iteration 12939, loss = 0.01983082\n",
      "Iteration 12940, loss = 0.01982858\n",
      "Iteration 12941, loss = 0.01982634\n",
      "Iteration 12942, loss = 0.01982410\n",
      "Iteration 12943, loss = 0.01982186\n",
      "Iteration 12944, loss = 0.01981962\n",
      "Iteration 12945, loss = 0.01981739\n",
      "Iteration 12946, loss = 0.01981515\n",
      "Iteration 12947, loss = 0.01981291\n",
      "Iteration 12948, loss = 0.01981068\n",
      "Iteration 12949, loss = 0.01980845\n",
      "Iteration 12950, loss = 0.01980621\n",
      "Iteration 12951, loss = 0.01980398\n",
      "Iteration 12952, loss = 0.01980174\n",
      "Iteration 12953, loss = 0.01979951\n",
      "Iteration 12954, loss = 0.01979728\n",
      "Iteration 12955, loss = 0.01979505\n",
      "Iteration 12956, loss = 0.01979282\n",
      "Iteration 12957, loss = 0.01979059\n",
      "Iteration 12958, loss = 0.01978836\n",
      "Iteration 12959, loss = 0.01978613\n",
      "Iteration 12960, loss = 0.01978390\n",
      "Iteration 12961, loss = 0.01978167\n",
      "Iteration 12962, loss = 0.01977944\n",
      "Iteration 12963, loss = 0.01977722\n",
      "Iteration 12964, loss = 0.01977499\n",
      "Iteration 12965, loss = 0.01977276\n",
      "Iteration 12966, loss = 0.01977054\n",
      "Iteration 12967, loss = 0.01976831\n",
      "Iteration 12968, loss = 0.01976609\n",
      "Iteration 12969, loss = 0.01976386\n",
      "Iteration 12970, loss = 0.01976164\n",
      "Iteration 12971, loss = 0.01975942\n",
      "Iteration 12972, loss = 0.01975720\n",
      "Iteration 12973, loss = 0.01975497\n",
      "Iteration 12974, loss = 0.01975275\n",
      "Iteration 12975, loss = 0.01975053\n",
      "Iteration 12976, loss = 0.01974831\n",
      "Iteration 12977, loss = 0.01974609\n",
      "Iteration 12978, loss = 0.01974387\n",
      "Iteration 12979, loss = 0.01974165\n",
      "Iteration 12980, loss = 0.01973944\n",
      "Iteration 12981, loss = 0.01973722\n",
      "Iteration 12982, loss = 0.01973500\n",
      "Iteration 12983, loss = 0.01973279\n",
      "Iteration 12984, loss = 0.01973057\n",
      "Iteration 12985, loss = 0.01972835\n",
      "Iteration 12986, loss = 0.01972614\n",
      "Iteration 12987, loss = 0.01972393\n",
      "Iteration 12988, loss = 0.01972171\n",
      "Iteration 12989, loss = 0.01971950\n",
      "Iteration 12990, loss = 0.01971729\n",
      "Iteration 12991, loss = 0.01971507\n",
      "Iteration 12992, loss = 0.01971286\n",
      "Iteration 12993, loss = 0.01971065\n",
      "Iteration 12994, loss = 0.01970844\n",
      "Iteration 12995, loss = 0.01970623\n",
      "Iteration 12996, loss = 0.01970402\n",
      "Iteration 12997, loss = 0.01970181\n",
      "Iteration 12998, loss = 0.01969960\n",
      "Iteration 12999, loss = 0.01969739\n",
      "Iteration 13000, loss = 0.01969519\n",
      "Iteration 13001, loss = 0.01969298\n",
      "Iteration 13002, loss = 0.01969077\n",
      "Iteration 13003, loss = 0.01968857\n",
      "Iteration 13004, loss = 0.01968636\n",
      "Iteration 13005, loss = 0.01968416\n",
      "Iteration 13006, loss = 0.01968195\n",
      "Iteration 13007, loss = 0.01967975\n",
      "Iteration 13008, loss = 0.01967755\n",
      "Iteration 13009, loss = 0.01967534\n",
      "Iteration 13010, loss = 0.01967314\n",
      "Iteration 13011, loss = 0.01967094\n",
      "Iteration 13012, loss = 0.01966874\n",
      "Iteration 13013, loss = 0.01966654\n",
      "Iteration 13014, loss = 0.01966434\n",
      "Iteration 13015, loss = 0.01966214\n",
      "Iteration 13016, loss = 0.01965994\n",
      "Iteration 13017, loss = 0.01965774\n",
      "Iteration 13018, loss = 0.01965554\n",
      "Iteration 13019, loss = 0.01965335\n",
      "Iteration 13020, loss = 0.01965115\n",
      "Iteration 13021, loss = 0.01964895\n",
      "Iteration 13022, loss = 0.01964676\n",
      "Iteration 13023, loss = 0.01964456\n",
      "Iteration 13024, loss = 0.01964237\n",
      "Iteration 13025, loss = 0.01964017\n",
      "Iteration 13026, loss = 0.01963798\n",
      "Iteration 13027, loss = 0.01963578\n",
      "Iteration 13028, loss = 0.01963359\n",
      "Iteration 13029, loss = 0.01963140\n",
      "Iteration 13030, loss = 0.01962921\n",
      "Iteration 13031, loss = 0.01962702\n",
      "Iteration 13032, loss = 0.01962483\n",
      "Iteration 13033, loss = 0.01962264\n",
      "Iteration 13034, loss = 0.01962045\n",
      "Iteration 13035, loss = 0.01961826\n",
      "Iteration 13036, loss = 0.01961607\n",
      "Iteration 13037, loss = 0.01961388\n",
      "Iteration 13038, loss = 0.01961169\n",
      "Iteration 13039, loss = 0.01960951\n",
      "Iteration 13040, loss = 0.01960732\n",
      "Iteration 13041, loss = 0.01960513\n",
      "Iteration 13042, loss = 0.01960295\n",
      "Iteration 13043, loss = 0.01960076\n",
      "Iteration 13044, loss = 0.01959858\n",
      "Iteration 13045, loss = 0.01959640\n",
      "Iteration 13046, loss = 0.01959421\n",
      "Iteration 13047, loss = 0.01959203\n",
      "Iteration 13048, loss = 0.01958985\n",
      "Iteration 13049, loss = 0.01958767\n",
      "Iteration 13050, loss = 0.01958548\n",
      "Iteration 13051, loss = 0.01958330\n",
      "Iteration 13052, loss = 0.01958112\n",
      "Iteration 13053, loss = 0.01957894\n",
      "Iteration 13054, loss = 0.01957676\n",
      "Iteration 13055, loss = 0.01957459\n",
      "Iteration 13056, loss = 0.01957241\n",
      "Iteration 13057, loss = 0.01957023\n",
      "Iteration 13058, loss = 0.01956805\n",
      "Iteration 13059, loss = 0.01956588\n",
      "Iteration 13060, loss = 0.01956370\n",
      "Iteration 13061, loss = 0.01956152\n",
      "Iteration 13062, loss = 0.01955935\n",
      "Iteration 13063, loss = 0.01955718\n",
      "Iteration 13064, loss = 0.01955500\n",
      "Iteration 13065, loss = 0.01955283\n",
      "Iteration 13066, loss = 0.01955065\n",
      "Iteration 13067, loss = 0.01954848\n",
      "Iteration 13068, loss = 0.01954631\n",
      "Iteration 13069, loss = 0.01954414\n",
      "Iteration 13070, loss = 0.01954197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13071, loss = 0.01953980\n",
      "Iteration 13072, loss = 0.01953763\n",
      "Iteration 13073, loss = 0.01953546\n",
      "Iteration 13074, loss = 0.01953329\n",
      "Iteration 13075, loss = 0.01953112\n",
      "Iteration 13076, loss = 0.01952895\n",
      "Iteration 13077, loss = 0.01952679\n",
      "Iteration 13078, loss = 0.01952462\n",
      "Iteration 13079, loss = 0.01952245\n",
      "Iteration 13080, loss = 0.01952029\n",
      "Iteration 13081, loss = 0.01951812\n",
      "Iteration 13082, loss = 0.01951596\n",
      "Iteration 13083, loss = 0.01951379\n",
      "Iteration 13084, loss = 0.01951163\n",
      "Iteration 13085, loss = 0.01950947\n",
      "Iteration 13086, loss = 0.01950730\n",
      "Iteration 13087, loss = 0.01950514\n",
      "Iteration 13088, loss = 0.01950298\n",
      "Iteration 13089, loss = 0.01950082\n",
      "Iteration 13090, loss = 0.01949866\n",
      "Iteration 13091, loss = 0.01949650\n",
      "Iteration 13092, loss = 0.01949434\n",
      "Iteration 13093, loss = 0.01949218\n",
      "Iteration 13094, loss = 0.01949002\n",
      "Iteration 13095, loss = 0.01948786\n",
      "Iteration 13096, loss = 0.01948571\n",
      "Iteration 13097, loss = 0.01948355\n",
      "Iteration 13098, loss = 0.01948139\n",
      "Iteration 13099, loss = 0.01947924\n",
      "Iteration 13100, loss = 0.01947708\n",
      "Iteration 13101, loss = 0.01947493\n",
      "Iteration 13102, loss = 0.01947277\n",
      "Iteration 13103, loss = 0.01947062\n",
      "Iteration 13104, loss = 0.01946846\n",
      "Iteration 13105, loss = 0.01946631\n",
      "Iteration 13106, loss = 0.01946416\n",
      "Iteration 13107, loss = 0.01946201\n",
      "Iteration 13108, loss = 0.01945986\n",
      "Iteration 13109, loss = 0.01945771\n",
      "Iteration 13110, loss = 0.01945555\n",
      "Iteration 13111, loss = 0.01945340\n",
      "Iteration 13112, loss = 0.01945126\n",
      "Iteration 13113, loss = 0.01944911\n",
      "Iteration 13114, loss = 0.01944696\n",
      "Iteration 13115, loss = 0.01944481\n",
      "Iteration 13116, loss = 0.01944266\n",
      "Iteration 13117, loss = 0.01944052\n",
      "Iteration 13118, loss = 0.01943837\n",
      "Iteration 13119, loss = 0.01943622\n",
      "Iteration 13120, loss = 0.01943408\n",
      "Iteration 13121, loss = 0.01943193\n",
      "Iteration 13122, loss = 0.01942979\n",
      "Iteration 13123, loss = 0.01942765\n",
      "Iteration 13124, loss = 0.01942550\n",
      "Iteration 13125, loss = 0.01942336\n",
      "Iteration 13126, loss = 0.01942122\n",
      "Iteration 13127, loss = 0.01941908\n",
      "Iteration 13128, loss = 0.01941694\n",
      "Iteration 13129, loss = 0.01941479\n",
      "Iteration 13130, loss = 0.01941265\n",
      "Iteration 13131, loss = 0.01941051\n",
      "Iteration 13132, loss = 0.01940838\n",
      "Iteration 13133, loss = 0.01940624\n",
      "Iteration 13134, loss = 0.01940410\n",
      "Iteration 13135, loss = 0.01940196\n",
      "Iteration 13136, loss = 0.01939982\n",
      "Iteration 13137, loss = 0.01939769\n",
      "Iteration 13138, loss = 0.01939555\n",
      "Iteration 13139, loss = 0.01939342\n",
      "Iteration 13140, loss = 0.01939128\n",
      "Iteration 13141, loss = 0.01938915\n",
      "Iteration 13142, loss = 0.01938701\n",
      "Iteration 13143, loss = 0.01938488\n",
      "Iteration 13144, loss = 0.01938274\n",
      "Iteration 13145, loss = 0.01938061\n",
      "Iteration 13146, loss = 0.01937848\n",
      "Iteration 13147, loss = 0.01937635\n",
      "Iteration 13148, loss = 0.01937422\n",
      "Iteration 13149, loss = 0.01937209\n",
      "Iteration 13150, loss = 0.01936996\n",
      "Iteration 13151, loss = 0.01936783\n",
      "Iteration 13152, loss = 0.01936570\n",
      "Iteration 13153, loss = 0.01936357\n",
      "Iteration 13154, loss = 0.01936144\n",
      "Iteration 13155, loss = 0.01935931\n",
      "Iteration 13156, loss = 0.01935719\n",
      "Iteration 13157, loss = 0.01935506\n",
      "Iteration 13158, loss = 0.01935293\n",
      "Iteration 13159, loss = 0.01935081\n",
      "Iteration 13160, loss = 0.01934868\n",
      "Iteration 13161, loss = 0.01934656\n",
      "Iteration 13162, loss = 0.01934443\n",
      "Iteration 13163, loss = 0.01934231\n",
      "Iteration 13164, loss = 0.01934019\n",
      "Iteration 13165, loss = 0.01933807\n",
      "Iteration 13166, loss = 0.01933594\n",
      "Iteration 13167, loss = 0.01933382\n",
      "Iteration 13168, loss = 0.01933170\n",
      "Iteration 13169, loss = 0.01932958\n",
      "Iteration 13170, loss = 0.01932746\n",
      "Iteration 13171, loss = 0.01932534\n",
      "Iteration 13172, loss = 0.01932322\n",
      "Iteration 13173, loss = 0.01932110\n",
      "Iteration 13174, loss = 0.01931898\n",
      "Iteration 13175, loss = 0.01931687\n",
      "Iteration 13176, loss = 0.01931475\n",
      "Iteration 13177, loss = 0.01931263\n",
      "Iteration 13178, loss = 0.01931052\n",
      "Iteration 13179, loss = 0.01930840\n",
      "Iteration 13180, loss = 0.01930629\n",
      "Iteration 13181, loss = 0.01930417\n",
      "Iteration 13182, loss = 0.01930206\n",
      "Iteration 13183, loss = 0.01929994\n",
      "Iteration 13184, loss = 0.01929783\n",
      "Iteration 13185, loss = 0.01929572\n",
      "Iteration 13186, loss = 0.01929361\n",
      "Iteration 13187, loss = 0.01929149\n",
      "Iteration 13188, loss = 0.01928938\n",
      "Iteration 13189, loss = 0.01928727\n",
      "Iteration 13190, loss = 0.01928516\n",
      "Iteration 13191, loss = 0.01928305\n",
      "Iteration 13192, loss = 0.01928094\n",
      "Iteration 13193, loss = 0.01927884\n",
      "Iteration 13194, loss = 0.01927673\n",
      "Iteration 13195, loss = 0.01927462\n",
      "Iteration 13196, loss = 0.01927251\n",
      "Iteration 13197, loss = 0.01927041\n",
      "Iteration 13198, loss = 0.01926830\n",
      "Iteration 13199, loss = 0.01926619\n",
      "Iteration 13200, loss = 0.01926409\n",
      "Iteration 13201, loss = 0.01926198\n",
      "Iteration 13202, loss = 0.01925988\n",
      "Iteration 13203, loss = 0.01925778\n",
      "Iteration 13204, loss = 0.01925567\n",
      "Iteration 13205, loss = 0.01925357\n",
      "Iteration 13206, loss = 0.01925147\n",
      "Iteration 13207, loss = 0.01924937\n",
      "Iteration 13208, loss = 0.01924727\n",
      "Iteration 13209, loss = 0.01924516\n",
      "Iteration 13210, loss = 0.01924306\n",
      "Iteration 13211, loss = 0.01924096\n",
      "Iteration 13212, loss = 0.01923887\n",
      "Iteration 13213, loss = 0.01923677\n",
      "Iteration 13214, loss = 0.01923467\n",
      "Iteration 13215, loss = 0.01923257\n",
      "Iteration 13216, loss = 0.01923047\n",
      "Iteration 13217, loss = 0.01922838\n",
      "Iteration 13218, loss = 0.01922628\n",
      "Iteration 13219, loss = 0.01922418\n",
      "Iteration 13220, loss = 0.01922209\n",
      "Iteration 13221, loss = 0.01921999\n",
      "Iteration 13222, loss = 0.01921790\n",
      "Iteration 13223, loss = 0.01921581\n",
      "Iteration 13224, loss = 0.01921371\n",
      "Iteration 13225, loss = 0.01921162\n",
      "Iteration 13226, loss = 0.01920953\n",
      "Iteration 13227, loss = 0.01920744\n",
      "Iteration 13228, loss = 0.01920534\n",
      "Iteration 13229, loss = 0.01920325\n",
      "Iteration 13230, loss = 0.01920116\n",
      "Iteration 13231, loss = 0.01919907\n",
      "Iteration 13232, loss = 0.01919698\n",
      "Iteration 13233, loss = 0.01919489\n",
      "Iteration 13234, loss = 0.01919281\n",
      "Iteration 13235, loss = 0.01919072\n",
      "Iteration 13236, loss = 0.01918863\n",
      "Iteration 13237, loss = 0.01918654\n",
      "Iteration 13238, loss = 0.01918446\n",
      "Iteration 13239, loss = 0.01918237\n",
      "Iteration 13240, loss = 0.01918029\n",
      "Iteration 13241, loss = 0.01917820\n",
      "Iteration 13242, loss = 0.01917612\n",
      "Iteration 13243, loss = 0.01917403\n",
      "Iteration 13244, loss = 0.01917195\n",
      "Iteration 13245, loss = 0.01916987\n",
      "Iteration 13246, loss = 0.01916778\n",
      "Iteration 13247, loss = 0.01916570\n",
      "Iteration 13248, loss = 0.01916362\n",
      "Iteration 13249, loss = 0.01916154\n",
      "Iteration 13250, loss = 0.01915946\n",
      "Iteration 13251, loss = 0.01915738\n",
      "Iteration 13252, loss = 0.01915530\n",
      "Iteration 13253, loss = 0.01915322\n",
      "Iteration 13254, loss = 0.01915114\n",
      "Iteration 13255, loss = 0.01914906\n",
      "Iteration 13256, loss = 0.01914698\n",
      "Iteration 13257, loss = 0.01914491\n",
      "Iteration 13258, loss = 0.01914283\n",
      "Iteration 13259, loss = 0.01914075\n",
      "Iteration 13260, loss = 0.01913868\n",
      "Iteration 13261, loss = 0.01913660\n",
      "Iteration 13262, loss = 0.01913453\n",
      "Iteration 13263, loss = 0.01913245\n",
      "Iteration 13264, loss = 0.01913038\n",
      "Iteration 13265, loss = 0.01912831\n",
      "Iteration 13266, loss = 0.01912623\n",
      "Iteration 13267, loss = 0.01912416\n",
      "Iteration 13268, loss = 0.01912209\n",
      "Iteration 13269, loss = 0.01912002\n",
      "Iteration 13270, loss = 0.01911795\n",
      "Iteration 13271, loss = 0.01911588\n",
      "Iteration 13272, loss = 0.01911381\n",
      "Iteration 13273, loss = 0.01911174\n",
      "Iteration 13274, loss = 0.01910967\n",
      "Iteration 13275, loss = 0.01910760\n",
      "Iteration 13276, loss = 0.01910553\n",
      "Iteration 13277, loss = 0.01910346\n",
      "Iteration 13278, loss = 0.01910140\n",
      "Iteration 13279, loss = 0.01909933\n",
      "Iteration 13280, loss = 0.01909726\n",
      "Iteration 13281, loss = 0.01909520\n",
      "Iteration 13282, loss = 0.01909313\n",
      "Iteration 13283, loss = 0.01909107\n",
      "Iteration 13284, loss = 0.01908900\n",
      "Iteration 13285, loss = 0.01908694\n",
      "Iteration 13286, loss = 0.01908488\n",
      "Iteration 13287, loss = 0.01908282\n",
      "Iteration 13288, loss = 0.01908075\n",
      "Iteration 13289, loss = 0.01907869\n",
      "Iteration 13290, loss = 0.01907663\n",
      "Iteration 13291, loss = 0.01907457\n",
      "Iteration 13292, loss = 0.01907251\n",
      "Iteration 13293, loss = 0.01907045\n",
      "Iteration 13294, loss = 0.01906839\n",
      "Iteration 13295, loss = 0.01906633\n",
      "Iteration 13296, loss = 0.01906427\n",
      "Iteration 13297, loss = 0.01906221\n",
      "Iteration 13298, loss = 0.01906016\n",
      "Iteration 13299, loss = 0.01905810\n",
      "Iteration 13300, loss = 0.01905604\n",
      "Iteration 13301, loss = 0.01905399\n",
      "Iteration 13302, loss = 0.01905193\n",
      "Iteration 13303, loss = 0.01904988\n",
      "Iteration 13304, loss = 0.01904782\n",
      "Iteration 13305, loss = 0.01904577\n",
      "Iteration 13306, loss = 0.01904371\n",
      "Iteration 13307, loss = 0.01904166\n",
      "Iteration 13308, loss = 0.01903961\n",
      "Iteration 13309, loss = 0.01903756\n",
      "Iteration 13310, loss = 0.01903550\n",
      "Iteration 13311, loss = 0.01903345\n",
      "Iteration 13312, loss = 0.01903140\n",
      "Iteration 13313, loss = 0.01902935\n",
      "Iteration 13314, loss = 0.01902730\n",
      "Iteration 13315, loss = 0.01902525\n",
      "Iteration 13316, loss = 0.01902320\n",
      "Iteration 13317, loss = 0.01902116\n",
      "Iteration 13318, loss = 0.01901911\n",
      "Iteration 13319, loss = 0.01901706\n",
      "Iteration 13320, loss = 0.01901501\n",
      "Iteration 13321, loss = 0.01901297\n",
      "Iteration 13322, loss = 0.01901092\n",
      "Iteration 13323, loss = 0.01900888\n",
      "Iteration 13324, loss = 0.01900683\n",
      "Iteration 13325, loss = 0.01900479\n",
      "Iteration 13326, loss = 0.01900274\n",
      "Iteration 13327, loss = 0.01900070\n",
      "Iteration 13328, loss = 0.01899866\n",
      "Iteration 13329, loss = 0.01899661\n",
      "Iteration 13330, loss = 0.01899457\n",
      "Iteration 13331, loss = 0.01899253\n",
      "Iteration 13332, loss = 0.01899049\n",
      "Iteration 13333, loss = 0.01898845\n",
      "Iteration 13334, loss = 0.01898641\n",
      "Iteration 13335, loss = 0.01898437\n",
      "Iteration 13336, loss = 0.01898233\n",
      "Iteration 13337, loss = 0.01898029\n",
      "Iteration 13338, loss = 0.01897825\n",
      "Iteration 13339, loss = 0.01897621\n",
      "Iteration 13340, loss = 0.01897417\n",
      "Iteration 13341, loss = 0.01897214\n",
      "Iteration 13342, loss = 0.01897010\n",
      "Iteration 13343, loss = 0.01896806\n",
      "Iteration 13344, loss = 0.01896603\n",
      "Iteration 13345, loss = 0.01896399\n",
      "Iteration 13346, loss = 0.01896196\n",
      "Iteration 13347, loss = 0.01895993\n",
      "Iteration 13348, loss = 0.01895789\n",
      "Iteration 13349, loss = 0.01895586\n",
      "Iteration 13350, loss = 0.01895383\n",
      "Iteration 13351, loss = 0.01895179\n",
      "Iteration 13352, loss = 0.01894976\n",
      "Iteration 13353, loss = 0.01894773\n",
      "Iteration 13354, loss = 0.01894570\n",
      "Iteration 13355, loss = 0.01894367\n",
      "Iteration 13356, loss = 0.01894164\n",
      "Iteration 13357, loss = 0.01893961\n",
      "Iteration 13358, loss = 0.01893758\n",
      "Iteration 13359, loss = 0.01893555\n",
      "Iteration 13360, loss = 0.01893352\n",
      "Iteration 13361, loss = 0.01893150\n",
      "Iteration 13362, loss = 0.01892947\n",
      "Iteration 13363, loss = 0.01892744\n",
      "Iteration 13364, loss = 0.01892542\n",
      "Iteration 13365, loss = 0.01892339\n",
      "Iteration 13366, loss = 0.01892137\n",
      "Iteration 13367, loss = 0.01891934\n",
      "Iteration 13368, loss = 0.01891732\n",
      "Iteration 13369, loss = 0.01891529\n",
      "Iteration 13370, loss = 0.01891327\n",
      "Iteration 13371, loss = 0.01891125\n",
      "Iteration 13372, loss = 0.01890922\n",
      "Iteration 13373, loss = 0.01890720\n",
      "Iteration 13374, loss = 0.01890518\n",
      "Iteration 13375, loss = 0.01890316\n",
      "Iteration 13376, loss = 0.01890114\n",
      "Iteration 13377, loss = 0.01889912\n",
      "Iteration 13378, loss = 0.01889710\n",
      "Iteration 13379, loss = 0.01889508\n",
      "Iteration 13380, loss = 0.01889306\n",
      "Iteration 13381, loss = 0.01889104\n",
      "Iteration 13382, loss = 0.01888903\n",
      "Iteration 13383, loss = 0.01888701\n",
      "Iteration 13384, loss = 0.01888499\n",
      "Iteration 13385, loss = 0.01888297\n",
      "Iteration 13386, loss = 0.01888096\n",
      "Iteration 13387, loss = 0.01887894\n",
      "Iteration 13388, loss = 0.01887693\n",
      "Iteration 13389, loss = 0.01887491\n",
      "Iteration 13390, loss = 0.01887290\n",
      "Iteration 13391, loss = 0.01887089\n",
      "Iteration 13392, loss = 0.01886887\n",
      "Iteration 13393, loss = 0.01886686\n",
      "Iteration 13394, loss = 0.01886485\n",
      "Iteration 13395, loss = 0.01886284\n",
      "Iteration 13396, loss = 0.01886083\n",
      "Iteration 13397, loss = 0.01885881\n",
      "Iteration 13398, loss = 0.01885680\n",
      "Iteration 13399, loss = 0.01885479\n",
      "Iteration 13400, loss = 0.01885279\n",
      "Iteration 13401, loss = 0.01885078\n",
      "Iteration 13402, loss = 0.01884877\n",
      "Iteration 13403, loss = 0.01884676\n",
      "Iteration 13404, loss = 0.01884475\n",
      "Iteration 13405, loss = 0.01884275\n",
      "Iteration 13406, loss = 0.01884074\n",
      "Iteration 13407, loss = 0.01883873\n",
      "Iteration 13408, loss = 0.01883673\n",
      "Iteration 13409, loss = 0.01883472\n",
      "Iteration 13410, loss = 0.01883272\n",
      "Iteration 13411, loss = 0.01883071\n",
      "Iteration 13412, loss = 0.01882871\n",
      "Iteration 13413, loss = 0.01882671\n",
      "Iteration 13414, loss = 0.01882470\n",
      "Iteration 13415, loss = 0.01882270\n",
      "Iteration 13416, loss = 0.01882070\n",
      "Iteration 13417, loss = 0.01881870\n",
      "Iteration 13418, loss = 0.01881670\n",
      "Iteration 13419, loss = 0.01881469\n",
      "Iteration 13420, loss = 0.01881269\n",
      "Iteration 13421, loss = 0.01881069\n",
      "Iteration 13422, loss = 0.01880870\n",
      "Iteration 13423, loss = 0.01880670\n",
      "Iteration 13424, loss = 0.01880470\n",
      "Iteration 13425, loss = 0.01880270\n",
      "Iteration 13426, loss = 0.01880070\n",
      "Iteration 13427, loss = 0.01879871\n",
      "Iteration 13428, loss = 0.01879671\n",
      "Iteration 13429, loss = 0.01879471\n",
      "Iteration 13430, loss = 0.01879272\n",
      "Iteration 13431, loss = 0.01879072\n",
      "Iteration 13432, loss = 0.01878873\n",
      "Iteration 13433, loss = 0.01878673\n",
      "Iteration 13434, loss = 0.01878474\n",
      "Iteration 13435, loss = 0.01878275\n",
      "Iteration 13436, loss = 0.01878075\n",
      "Iteration 13437, loss = 0.01877876\n",
      "Iteration 13438, loss = 0.01877677\n",
      "Iteration 13439, loss = 0.01877478\n",
      "Iteration 13440, loss = 0.01877279\n",
      "Iteration 13441, loss = 0.01877080\n",
      "Iteration 13442, loss = 0.01876881\n",
      "Iteration 13443, loss = 0.01876682\n",
      "Iteration 13444, loss = 0.01876483\n",
      "Iteration 13445, loss = 0.01876284\n",
      "Iteration 13446, loss = 0.01876085\n",
      "Iteration 13447, loss = 0.01875886\n",
      "Iteration 13448, loss = 0.01875688\n",
      "Iteration 13449, loss = 0.01875489\n",
      "Iteration 13450, loss = 0.01875290\n",
      "Iteration 13451, loss = 0.01875092\n",
      "Iteration 13452, loss = 0.01874893\n",
      "Iteration 13453, loss = 0.01874695\n",
      "Iteration 13454, loss = 0.01874496\n",
      "Iteration 13455, loss = 0.01874298\n",
      "Iteration 13456, loss = 0.01874099\n",
      "Iteration 13457, loss = 0.01873901\n",
      "Iteration 13458, loss = 0.01873703\n",
      "Iteration 13459, loss = 0.01873505\n",
      "Iteration 13460, loss = 0.01873306\n",
      "Iteration 13461, loss = 0.01873108\n",
      "Iteration 13462, loss = 0.01872910\n",
      "Iteration 13463, loss = 0.01872712\n",
      "Iteration 13464, loss = 0.01872514\n",
      "Iteration 13465, loss = 0.01872316\n",
      "Iteration 13466, loss = 0.01872118\n",
      "Iteration 13467, loss = 0.01871920\n",
      "Iteration 13468, loss = 0.01871723\n",
      "Iteration 13469, loss = 0.01871525\n",
      "Iteration 13470, loss = 0.01871327\n",
      "Iteration 13471, loss = 0.01871129\n",
      "Iteration 13472, loss = 0.01870932\n",
      "Iteration 13473, loss = 0.01870734\n",
      "Iteration 13474, loss = 0.01870537\n",
      "Iteration 13475, loss = 0.01870339\n",
      "Iteration 13476, loss = 0.01870142\n",
      "Iteration 13477, loss = 0.01869944\n",
      "Iteration 13478, loss = 0.01869747\n",
      "Iteration 13479, loss = 0.01869550\n",
      "Iteration 13480, loss = 0.01869352\n",
      "Iteration 13481, loss = 0.01869155\n",
      "Iteration 13482, loss = 0.01868958\n",
      "Iteration 13483, loss = 0.01868761\n",
      "Iteration 13484, loss = 0.01868564\n",
      "Iteration 13485, loss = 0.01868367\n",
      "Iteration 13486, loss = 0.01868170\n",
      "Iteration 13487, loss = 0.01867973\n",
      "Iteration 13488, loss = 0.01867776\n",
      "Iteration 13489, loss = 0.01867579\n",
      "Iteration 13490, loss = 0.01867382\n",
      "Iteration 13491, loss = 0.01867185\n",
      "Iteration 13492, loss = 0.01866988\n",
      "Iteration 13493, loss = 0.01866792\n",
      "Iteration 13494, loss = 0.01866595\n",
      "Iteration 13495, loss = 0.01866399\n",
      "Iteration 13496, loss = 0.01866202\n",
      "Iteration 13497, loss = 0.01866005\n",
      "Iteration 13498, loss = 0.01865809\n",
      "Iteration 13499, loss = 0.01865613\n",
      "Iteration 13500, loss = 0.01865416\n",
      "Iteration 13501, loss = 0.01865220\n",
      "Iteration 13502, loss = 0.01865024\n",
      "Iteration 13503, loss = 0.01864827\n",
      "Iteration 13504, loss = 0.01864631\n",
      "Iteration 13505, loss = 0.01864435\n",
      "Iteration 13506, loss = 0.01864239\n",
      "Iteration 13507, loss = 0.01864043\n",
      "Iteration 13508, loss = 0.01863847\n",
      "Iteration 13509, loss = 0.01863651\n",
      "Iteration 13510, loss = 0.01863455\n",
      "Iteration 13511, loss = 0.01863259\n",
      "Iteration 13512, loss = 0.01863063\n",
      "Iteration 13513, loss = 0.01862867\n",
      "Iteration 13514, loss = 0.01862672\n",
      "Iteration 13515, loss = 0.01862476\n",
      "Iteration 13516, loss = 0.01862280\n",
      "Iteration 13517, loss = 0.01862085\n",
      "Iteration 13518, loss = 0.01861889\n",
      "Iteration 13519, loss = 0.01861693\n",
      "Iteration 13520, loss = 0.01861498\n",
      "Iteration 13521, loss = 0.01861303\n",
      "Iteration 13522, loss = 0.01861107\n",
      "Iteration 13523, loss = 0.01860912\n",
      "Iteration 13524, loss = 0.01860717\n",
      "Iteration 13525, loss = 0.01860521\n",
      "Iteration 13526, loss = 0.01860326\n",
      "Iteration 13527, loss = 0.01860131\n",
      "Iteration 13528, loss = 0.01859936\n",
      "Iteration 13529, loss = 0.01859741\n",
      "Iteration 13530, loss = 0.01859546\n",
      "Iteration 13531, loss = 0.01859351\n",
      "Iteration 13532, loss = 0.01859156\n",
      "Iteration 13533, loss = 0.01858961\n",
      "Iteration 13534, loss = 0.01858766\n",
      "Iteration 13535, loss = 0.01858571\n",
      "Iteration 13536, loss = 0.01858376\n",
      "Iteration 13537, loss = 0.01858182\n",
      "Iteration 13538, loss = 0.01857987\n",
      "Iteration 13539, loss = 0.01857792\n",
      "Iteration 13540, loss = 0.01857598\n",
      "Iteration 13541, loss = 0.01857403\n",
      "Iteration 13542, loss = 0.01857209\n",
      "Iteration 13543, loss = 0.01857014\n",
      "Iteration 13544, loss = 0.01856820\n",
      "Iteration 13545, loss = 0.01856625\n",
      "Iteration 13546, loss = 0.01856431\n",
      "Iteration 13547, loss = 0.01856237\n",
      "Iteration 13548, loss = 0.01856043\n",
      "Iteration 13549, loss = 0.01855848\n",
      "Iteration 13550, loss = 0.01855654\n",
      "Iteration 13551, loss = 0.01855460\n",
      "Iteration 13552, loss = 0.01855266\n",
      "Iteration 13553, loss = 0.01855072\n",
      "Iteration 13554, loss = 0.01854878\n",
      "Iteration 13555, loss = 0.01854684\n",
      "Iteration 13556, loss = 0.01854490\n",
      "Iteration 13557, loss = 0.01854296\n",
      "Iteration 13558, loss = 0.01854103\n",
      "Iteration 13559, loss = 0.01853909\n",
      "Iteration 13560, loss = 0.01853715\n",
      "Iteration 13561, loss = 0.01853521\n",
      "Iteration 13562, loss = 0.01853328\n",
      "Iteration 13563, loss = 0.01853134\n",
      "Iteration 13564, loss = 0.01852941\n",
      "Iteration 13565, loss = 0.01852747\n",
      "Iteration 13566, loss = 0.01852554\n",
      "Iteration 13567, loss = 0.01852360\n",
      "Iteration 13568, loss = 0.01852167\n",
      "Iteration 13569, loss = 0.01851974\n",
      "Iteration 13570, loss = 0.01851780\n",
      "Iteration 13571, loss = 0.01851587\n",
      "Iteration 13572, loss = 0.01851394\n",
      "Iteration 13573, loss = 0.01851201\n",
      "Iteration 13574, loss = 0.01851008\n",
      "Iteration 13575, loss = 0.01850815\n",
      "Iteration 13576, loss = 0.01850622\n",
      "Iteration 13577, loss = 0.01850429\n",
      "Iteration 13578, loss = 0.01850236\n",
      "Iteration 13579, loss = 0.01850043\n",
      "Iteration 13580, loss = 0.01849850\n",
      "Iteration 13581, loss = 0.01849657\n",
      "Iteration 13582, loss = 0.01849465\n",
      "Iteration 13583, loss = 0.01849272\n",
      "Iteration 13584, loss = 0.01849079\n",
      "Iteration 13585, loss = 0.01848887\n",
      "Iteration 13586, loss = 0.01848694\n",
      "Iteration 13587, loss = 0.01848501\n",
      "Iteration 13588, loss = 0.01848309\n",
      "Iteration 13589, loss = 0.01848117\n",
      "Iteration 13590, loss = 0.01847924\n",
      "Iteration 13591, loss = 0.01847732\n",
      "Iteration 13592, loss = 0.01847540\n",
      "Iteration 13593, loss = 0.01847347\n",
      "Iteration 13594, loss = 0.01847155\n",
      "Iteration 13595, loss = 0.01846963\n",
      "Iteration 13596, loss = 0.01846771\n",
      "Iteration 13597, loss = 0.01846579\n",
      "Iteration 13598, loss = 0.01846387\n",
      "Iteration 13599, loss = 0.01846195\n",
      "Iteration 13600, loss = 0.01846003\n",
      "Iteration 13601, loss = 0.01845811\n",
      "Iteration 13602, loss = 0.01845619\n",
      "Iteration 13603, loss = 0.01845427\n",
      "Iteration 13604, loss = 0.01845235\n",
      "Iteration 13605, loss = 0.01845043\n",
      "Iteration 13606, loss = 0.01844852\n",
      "Iteration 13607, loss = 0.01844660\n",
      "Iteration 13608, loss = 0.01844468\n",
      "Iteration 13609, loss = 0.01844277\n",
      "Iteration 13610, loss = 0.01844085\n",
      "Iteration 13611, loss = 0.01843894\n",
      "Iteration 13612, loss = 0.01843702\n",
      "Iteration 13613, loss = 0.01843511\n",
      "Iteration 13614, loss = 0.01843320\n",
      "Iteration 13615, loss = 0.01843128\n",
      "Iteration 13616, loss = 0.01842937\n",
      "Iteration 13617, loss = 0.01842746\n",
      "Iteration 13618, loss = 0.01842555\n",
      "Iteration 13619, loss = 0.01842364\n",
      "Iteration 13620, loss = 0.01842173\n",
      "Iteration 13621, loss = 0.01841981\n",
      "Iteration 13622, loss = 0.01841790\n",
      "Iteration 13623, loss = 0.01841599\n",
      "Iteration 13624, loss = 0.01841409\n",
      "Iteration 13625, loss = 0.01841218\n",
      "Iteration 13626, loss = 0.01841027\n",
      "Iteration 13627, loss = 0.01840836\n",
      "Iteration 13628, loss = 0.01840645\n",
      "Iteration 13629, loss = 0.01840455\n",
      "Iteration 13630, loss = 0.01840264\n",
      "Iteration 13631, loss = 0.01840073\n",
      "Iteration 13632, loss = 0.01839883\n",
      "Iteration 13633, loss = 0.01839692\n",
      "Iteration 13634, loss = 0.01839502\n",
      "Iteration 13635, loss = 0.01839311\n",
      "Iteration 13636, loss = 0.01839121\n",
      "Iteration 13637, loss = 0.01838931\n",
      "Iteration 13638, loss = 0.01838740\n",
      "Iteration 13639, loss = 0.01838550\n",
      "Iteration 13640, loss = 0.01838360\n",
      "Iteration 13641, loss = 0.01838169\n",
      "Iteration 13642, loss = 0.01837979\n",
      "Iteration 13643, loss = 0.01837789\n",
      "Iteration 13644, loss = 0.01837599\n",
      "Iteration 13645, loss = 0.01837409\n",
      "Iteration 13646, loss = 0.01837219\n",
      "Iteration 13647, loss = 0.01837029\n",
      "Iteration 13648, loss = 0.01836839\n",
      "Iteration 13649, loss = 0.01836650\n",
      "Iteration 13650, loss = 0.01836460\n",
      "Iteration 13651, loss = 0.01836270\n",
      "Iteration 13652, loss = 0.01836080\n",
      "Iteration 13653, loss = 0.01835891\n",
      "Iteration 13654, loss = 0.01835701\n",
      "Iteration 13655, loss = 0.01835511\n",
      "Iteration 13656, loss = 0.01835322\n",
      "Iteration 13657, loss = 0.01835132\n",
      "Iteration 13658, loss = 0.01834943\n",
      "Iteration 13659, loss = 0.01834753\n",
      "Iteration 13660, loss = 0.01834564\n",
      "Iteration 13661, loss = 0.01834375\n",
      "Iteration 13662, loss = 0.01834185\n",
      "Iteration 13663, loss = 0.01833996\n",
      "Iteration 13664, loss = 0.01833807\n",
      "Iteration 13665, loss = 0.01833618\n",
      "Iteration 13666, loss = 0.01833429\n",
      "Iteration 13667, loss = 0.01833240\n",
      "Iteration 13668, loss = 0.01833051\n",
      "Iteration 13669, loss = 0.01832862\n",
      "Iteration 13670, loss = 0.01832673\n",
      "Iteration 13671, loss = 0.01832484\n",
      "Iteration 13672, loss = 0.01832295\n",
      "Iteration 13673, loss = 0.01832106\n",
      "Iteration 13674, loss = 0.01831917\n",
      "Iteration 13675, loss = 0.01831728\n",
      "Iteration 13676, loss = 0.01831540\n",
      "Iteration 13677, loss = 0.01831351\n",
      "Iteration 13678, loss = 0.01831163\n",
      "Iteration 13679, loss = 0.01830974\n",
      "Iteration 13680, loss = 0.01830785\n",
      "Iteration 13681, loss = 0.01830597\n",
      "Iteration 13682, loss = 0.01830409\n",
      "Iteration 13683, loss = 0.01830220\n",
      "Iteration 13684, loss = 0.01830032\n",
      "Iteration 13685, loss = 0.01829843\n",
      "Iteration 13686, loss = 0.01829655\n",
      "Iteration 13687, loss = 0.01829467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13688, loss = 0.01829279\n",
      "Iteration 13689, loss = 0.01829091\n",
      "Iteration 13690, loss = 0.01828903\n",
      "Iteration 13691, loss = 0.01828714\n",
      "Iteration 13692, loss = 0.01828526\n",
      "Iteration 13693, loss = 0.01828338\n",
      "Iteration 13694, loss = 0.01828151\n",
      "Iteration 13695, loss = 0.01827963\n",
      "Iteration 13696, loss = 0.01827775\n",
      "Iteration 13697, loss = 0.01827587\n",
      "Iteration 13698, loss = 0.01827399\n",
      "Iteration 13699, loss = 0.01827211\n",
      "Iteration 13700, loss = 0.01827024\n",
      "Iteration 13701, loss = 0.01826836\n",
      "Iteration 13702, loss = 0.01826649\n",
      "Iteration 13703, loss = 0.01826461\n",
      "Iteration 13704, loss = 0.01826273\n",
      "Iteration 13705, loss = 0.01826086\n",
      "Iteration 13706, loss = 0.01825899\n",
      "Iteration 13707, loss = 0.01825711\n",
      "Iteration 13708, loss = 0.01825524\n",
      "Iteration 13709, loss = 0.01825337\n",
      "Iteration 13710, loss = 0.01825149\n",
      "Iteration 13711, loss = 0.01824962\n",
      "Iteration 13712, loss = 0.01824775\n",
      "Iteration 13713, loss = 0.01824588\n",
      "Iteration 13714, loss = 0.01824401\n",
      "Iteration 13715, loss = 0.01824214\n",
      "Iteration 13716, loss = 0.01824027\n",
      "Iteration 13717, loss = 0.01823840\n",
      "Iteration 13718, loss = 0.01823653\n",
      "Iteration 13719, loss = 0.01823466\n",
      "Iteration 13720, loss = 0.01823279\n",
      "Iteration 13721, loss = 0.01823092\n",
      "Iteration 13722, loss = 0.01822905\n",
      "Iteration 13723, loss = 0.01822719\n",
      "Iteration 13724, loss = 0.01822532\n",
      "Iteration 13725, loss = 0.01822345\n",
      "Iteration 13726, loss = 0.01822159\n",
      "Iteration 13727, loss = 0.01821972\n",
      "Iteration 13728, loss = 0.01821786\n",
      "Iteration 13729, loss = 0.01821599\n",
      "Iteration 13730, loss = 0.01821413\n",
      "Iteration 13731, loss = 0.01821227\n",
      "Iteration 13732, loss = 0.01821040\n",
      "Iteration 13733, loss = 0.01820854\n",
      "Iteration 13734, loss = 0.01820668\n",
      "Iteration 13735, loss = 0.01820481\n",
      "Iteration 13736, loss = 0.01820295\n",
      "Iteration 13737, loss = 0.01820109\n",
      "Iteration 13738, loss = 0.01819923\n",
      "Iteration 13739, loss = 0.01819737\n",
      "Iteration 13740, loss = 0.01819551\n",
      "Iteration 13741, loss = 0.01819365\n",
      "Iteration 13742, loss = 0.01819179\n",
      "Iteration 13743, loss = 0.01818993\n",
      "Iteration 13744, loss = 0.01818807\n",
      "Iteration 13745, loss = 0.01818622\n",
      "Iteration 13746, loss = 0.01818436\n",
      "Iteration 13747, loss = 0.01818250\n",
      "Iteration 13748, loss = 0.01818064\n",
      "Iteration 13749, loss = 0.01817879\n",
      "Iteration 13750, loss = 0.01817693\n",
      "Iteration 13751, loss = 0.01817508\n",
      "Iteration 13752, loss = 0.01817322\n",
      "Iteration 13753, loss = 0.01817137\n",
      "Iteration 13754, loss = 0.01816951\n",
      "Iteration 13755, loss = 0.01816766\n",
      "Iteration 13756, loss = 0.01816580\n",
      "Iteration 13757, loss = 0.01816395\n",
      "Iteration 13758, loss = 0.01816210\n",
      "Iteration 13759, loss = 0.01816025\n",
      "Iteration 13760, loss = 0.01815840\n",
      "Iteration 13761, loss = 0.01815654\n",
      "Iteration 13762, loss = 0.01815469\n",
      "Iteration 13763, loss = 0.01815284\n",
      "Iteration 13764, loss = 0.01815099\n",
      "Iteration 13765, loss = 0.01814914\n",
      "Iteration 13766, loss = 0.01814729\n",
      "Iteration 13767, loss = 0.01814544\n",
      "Iteration 13768, loss = 0.01814360\n",
      "Iteration 13769, loss = 0.01814175\n",
      "Iteration 13770, loss = 0.01813990\n",
      "Iteration 13771, loss = 0.01813805\n",
      "Iteration 13772, loss = 0.01813621\n",
      "Iteration 13773, loss = 0.01813436\n",
      "Iteration 13774, loss = 0.01813251\n",
      "Iteration 13775, loss = 0.01813067\n",
      "Iteration 13776, loss = 0.01812882\n",
      "Iteration 13777, loss = 0.01812698\n",
      "Iteration 13778, loss = 0.01812513\n",
      "Iteration 13779, loss = 0.01812329\n",
      "Iteration 13780, loss = 0.01812145\n",
      "Iteration 13781, loss = 0.01811960\n",
      "Iteration 13782, loss = 0.01811776\n",
      "Iteration 13783, loss = 0.01811592\n",
      "Iteration 13784, loss = 0.01811408\n",
      "Iteration 13785, loss = 0.01811223\n",
      "Iteration 13786, loss = 0.01811039\n",
      "Iteration 13787, loss = 0.01810855\n",
      "Iteration 13788, loss = 0.01810671\n",
      "Iteration 13789, loss = 0.01810487\n",
      "Iteration 13790, loss = 0.01810303\n",
      "Iteration 13791, loss = 0.01810119\n",
      "Iteration 13792, loss = 0.01809936\n",
      "Iteration 13793, loss = 0.01809752\n",
      "Iteration 13794, loss = 0.01809568\n",
      "Iteration 13795, loss = 0.01809384\n",
      "Iteration 13796, loss = 0.01809201\n",
      "Iteration 13797, loss = 0.01809017\n",
      "Iteration 13798, loss = 0.01808833\n",
      "Iteration 13799, loss = 0.01808650\n",
      "Iteration 13800, loss = 0.01808466\n",
      "Iteration 13801, loss = 0.01808283\n",
      "Iteration 13802, loss = 0.01808099\n",
      "Iteration 13803, loss = 0.01807916\n",
      "Iteration 13804, loss = 0.01807732\n",
      "Iteration 13805, loss = 0.01807549\n",
      "Iteration 13806, loss = 0.01807366\n",
      "Iteration 13807, loss = 0.01807183\n",
      "Iteration 13808, loss = 0.01806999\n",
      "Iteration 13809, loss = 0.01806816\n",
      "Iteration 13810, loss = 0.01806633\n",
      "Iteration 13811, loss = 0.01806450\n",
      "Iteration 13812, loss = 0.01806267\n",
      "Iteration 13813, loss = 0.01806084\n",
      "Iteration 13814, loss = 0.01805901\n",
      "Iteration 13815, loss = 0.01805718\n",
      "Iteration 13816, loss = 0.01805535\n",
      "Iteration 13817, loss = 0.01805352\n",
      "Iteration 13818, loss = 0.01805170\n",
      "Iteration 13819, loss = 0.01804987\n",
      "Iteration 13820, loss = 0.01804804\n",
      "Iteration 13821, loss = 0.01804621\n",
      "Iteration 13822, loss = 0.01804439\n",
      "Iteration 13823, loss = 0.01804256\n",
      "Iteration 13824, loss = 0.01804074\n",
      "Iteration 13825, loss = 0.01803891\n",
      "Iteration 13826, loss = 0.01803709\n",
      "Iteration 13827, loss = 0.01803526\n",
      "Iteration 13828, loss = 0.01803344\n",
      "Iteration 13829, loss = 0.01803161\n",
      "Iteration 13830, loss = 0.01802979\n",
      "Iteration 13831, loss = 0.01802797\n",
      "Iteration 13832, loss = 0.01802615\n",
      "Iteration 13833, loss = 0.01802432\n",
      "Iteration 13834, loss = 0.01802250\n",
      "Iteration 13835, loss = 0.01802068\n",
      "Iteration 13836, loss = 0.01801886\n",
      "Iteration 13837, loss = 0.01801704\n",
      "Iteration 13838, loss = 0.01801522\n",
      "Iteration 13839, loss = 0.01801340\n",
      "Iteration 13840, loss = 0.01801158\n",
      "Iteration 13841, loss = 0.01800976\n",
      "Iteration 13842, loss = 0.01800794\n",
      "Iteration 13843, loss = 0.01800613\n",
      "Iteration 13844, loss = 0.01800431\n",
      "Iteration 13845, loss = 0.01800249\n",
      "Iteration 13846, loss = 0.01800068\n",
      "Iteration 13847, loss = 0.01799886\n",
      "Iteration 13848, loss = 0.01799704\n",
      "Iteration 13849, loss = 0.01799523\n",
      "Iteration 13850, loss = 0.01799341\n",
      "Iteration 13851, loss = 0.01799160\n",
      "Iteration 13852, loss = 0.01798978\n",
      "Iteration 13853, loss = 0.01798797\n",
      "Iteration 13854, loss = 0.01798616\n",
      "Iteration 13855, loss = 0.01798434\n",
      "Iteration 13856, loss = 0.01798253\n",
      "Iteration 13857, loss = 0.01798072\n",
      "Iteration 13858, loss = 0.01797891\n",
      "Iteration 13859, loss = 0.01797709\n",
      "Iteration 13860, loss = 0.01797528\n",
      "Iteration 13861, loss = 0.01797347\n",
      "Iteration 13862, loss = 0.01797166\n",
      "Iteration 13863, loss = 0.01796985\n",
      "Iteration 13864, loss = 0.01796804\n",
      "Iteration 13865, loss = 0.01796623\n",
      "Iteration 13866, loss = 0.01796443\n",
      "Iteration 13867, loss = 0.01796262\n",
      "Iteration 13868, loss = 0.01796081\n",
      "Iteration 13869, loss = 0.01795900\n",
      "Iteration 13870, loss = 0.01795720\n",
      "Iteration 13871, loss = 0.01795539\n",
      "Iteration 13872, loss = 0.01795358\n",
      "Iteration 13873, loss = 0.01795178\n",
      "Iteration 13874, loss = 0.01794997\n",
      "Iteration 13875, loss = 0.01794817\n",
      "Iteration 13876, loss = 0.01794636\n",
      "Iteration 13877, loss = 0.01794456\n",
      "Iteration 13878, loss = 0.01794275\n",
      "Iteration 13879, loss = 0.01794095\n",
      "Iteration 13880, loss = 0.01793915\n",
      "Iteration 13881, loss = 0.01793734\n",
      "Iteration 13882, loss = 0.01793554\n",
      "Iteration 13883, loss = 0.01793374\n",
      "Iteration 13884, loss = 0.01793194\n",
      "Iteration 13885, loss = 0.01793014\n",
      "Iteration 13886, loss = 0.01792834\n",
      "Iteration 13887, loss = 0.01792654\n",
      "Iteration 13888, loss = 0.01792474\n",
      "Iteration 13889, loss = 0.01792294\n",
      "Iteration 13890, loss = 0.01792114\n",
      "Iteration 13891, loss = 0.01791934\n",
      "Iteration 13892, loss = 0.01791754\n",
      "Iteration 13893, loss = 0.01791574\n",
      "Iteration 13894, loss = 0.01791394\n",
      "Iteration 13895, loss = 0.01791215\n",
      "Iteration 13896, loss = 0.01791035\n",
      "Iteration 13897, loss = 0.01790855\n",
      "Iteration 13898, loss = 0.01790676\n",
      "Iteration 13899, loss = 0.01790496\n",
      "Iteration 13900, loss = 0.01790317\n",
      "Iteration 13901, loss = 0.01790137\n",
      "Iteration 13902, loss = 0.01789958\n",
      "Iteration 13903, loss = 0.01789779\n",
      "Iteration 13904, loss = 0.01789599\n",
      "Iteration 13905, loss = 0.01789420\n",
      "Iteration 13906, loss = 0.01789241\n",
      "Iteration 13907, loss = 0.01789061\n",
      "Iteration 13908, loss = 0.01788882\n",
      "Iteration 13909, loss = 0.01788703\n",
      "Iteration 13910, loss = 0.01788524\n",
      "Iteration 13911, loss = 0.01788345\n",
      "Iteration 13912, loss = 0.01788166\n",
      "Iteration 13913, loss = 0.01787987\n",
      "Iteration 13914, loss = 0.01787808\n",
      "Iteration 13915, loss = 0.01787629\n",
      "Iteration 13916, loss = 0.01787450\n",
      "Iteration 13917, loss = 0.01787271\n",
      "Iteration 13918, loss = 0.01787092\n",
      "Iteration 13919, loss = 0.01786914\n",
      "Iteration 13920, loss = 0.01786735\n",
      "Iteration 13921, loss = 0.01786556\n",
      "Iteration 13922, loss = 0.01786377\n",
      "Iteration 13923, loss = 0.01786199\n",
      "Iteration 13924, loss = 0.01786020\n",
      "Iteration 13925, loss = 0.01785842\n",
      "Iteration 13926, loss = 0.01785663\n",
      "Iteration 13927, loss = 0.01785485\n",
      "Iteration 13928, loss = 0.01785306\n",
      "Iteration 13929, loss = 0.01785128\n",
      "Iteration 13930, loss = 0.01784950\n",
      "Iteration 13931, loss = 0.01784771\n",
      "Iteration 13932, loss = 0.01784593\n",
      "Iteration 13933, loss = 0.01784415\n",
      "Iteration 13934, loss = 0.01784237\n",
      "Iteration 13935, loss = 0.01784059\n",
      "Iteration 13936, loss = 0.01783881\n",
      "Iteration 13937, loss = 0.01783702\n",
      "Iteration 13938, loss = 0.01783524\n",
      "Iteration 13939, loss = 0.01783346\n",
      "Iteration 13940, loss = 0.01783169\n",
      "Iteration 13941, loss = 0.01782991\n",
      "Iteration 13942, loss = 0.01782813\n",
      "Iteration 13943, loss = 0.01782635\n",
      "Iteration 13944, loss = 0.01782457\n",
      "Iteration 13945, loss = 0.01782279\n",
      "Iteration 13946, loss = 0.01782102\n",
      "Iteration 13947, loss = 0.01781924\n",
      "Iteration 13948, loss = 0.01781746\n",
      "Iteration 13949, loss = 0.01781569\n",
      "Iteration 13950, loss = 0.01781391\n",
      "Iteration 13951, loss = 0.01781214\n",
      "Iteration 13952, loss = 0.01781036\n",
      "Iteration 13953, loss = 0.01780859\n",
      "Iteration 13954, loss = 0.01780681\n",
      "Iteration 13955, loss = 0.01780504\n",
      "Iteration 13956, loss = 0.01780327\n",
      "Iteration 13957, loss = 0.01780150\n",
      "Iteration 13958, loss = 0.01779972\n",
      "Iteration 13959, loss = 0.01779795\n",
      "Iteration 13960, loss = 0.01779618\n",
      "Iteration 13961, loss = 0.01779441\n",
      "Iteration 13962, loss = 0.01779264\n",
      "Iteration 13963, loss = 0.01779087\n",
      "Iteration 13964, loss = 0.01778910\n",
      "Iteration 13965, loss = 0.01778733\n",
      "Iteration 13966, loss = 0.01778556\n",
      "Iteration 13967, loss = 0.01778379\n",
      "Iteration 13968, loss = 0.01778202\n",
      "Iteration 13969, loss = 0.01778025\n",
      "Iteration 13970, loss = 0.01777848\n",
      "Iteration 13971, loss = 0.01777672\n",
      "Iteration 13972, loss = 0.01777495\n",
      "Iteration 13973, loss = 0.01777318\n",
      "Iteration 13974, loss = 0.01777142\n",
      "Iteration 13975, loss = 0.01776965\n",
      "Iteration 13976, loss = 0.01776789\n",
      "Iteration 13977, loss = 0.01776612\n",
      "Iteration 13978, loss = 0.01776436\n",
      "Iteration 13979, loss = 0.01776259\n",
      "Iteration 13980, loss = 0.01776083\n",
      "Iteration 13981, loss = 0.01775906\n",
      "Iteration 13982, loss = 0.01775730\n",
      "Iteration 13983, loss = 0.01775554\n",
      "Iteration 13984, loss = 0.01775378\n",
      "Iteration 13985, loss = 0.01775201\n",
      "Iteration 13986, loss = 0.01775025\n",
      "Iteration 13987, loss = 0.01774849\n",
      "Iteration 13988, loss = 0.01774673\n",
      "Iteration 13989, loss = 0.01774497\n",
      "Iteration 13990, loss = 0.01774321\n",
      "Iteration 13991, loss = 0.01774145\n",
      "Iteration 13992, loss = 0.01773969\n",
      "Iteration 13993, loss = 0.01773793\n",
      "Iteration 13994, loss = 0.01773617\n",
      "Iteration 13995, loss = 0.01773442\n",
      "Iteration 13996, loss = 0.01773266\n",
      "Iteration 13997, loss = 0.01773090\n",
      "Iteration 13998, loss = 0.01772914\n",
      "Iteration 13999, loss = 0.01772739\n",
      "Iteration 14000, loss = 0.01772563\n",
      "Iteration 14001, loss = 0.01772388\n",
      "Iteration 14002, loss = 0.01772212\n",
      "Iteration 14003, loss = 0.01772037\n",
      "Iteration 14004, loss = 0.01771861\n",
      "Iteration 14005, loss = 0.01771686\n",
      "Iteration 14006, loss = 0.01771510\n",
      "Iteration 14007, loss = 0.01771335\n",
      "Iteration 14008, loss = 0.01771160\n",
      "Iteration 14009, loss = 0.01770984\n",
      "Iteration 14010, loss = 0.01770809\n",
      "Iteration 14011, loss = 0.01770634\n",
      "Iteration 14012, loss = 0.01770459\n",
      "Iteration 14013, loss = 0.01770284\n",
      "Iteration 14014, loss = 0.01770109\n",
      "Iteration 14015, loss = 0.01769934\n",
      "Iteration 14016, loss = 0.01769759\n",
      "Iteration 14017, loss = 0.01769584\n",
      "Iteration 14018, loss = 0.01769409\n",
      "Iteration 14019, loss = 0.01769234\n",
      "Iteration 14020, loss = 0.01769059\n",
      "Iteration 14021, loss = 0.01768884\n",
      "Iteration 14022, loss = 0.01768709\n",
      "Iteration 14023, loss = 0.01768535\n",
      "Iteration 14024, loss = 0.01768360\n",
      "Iteration 14025, loss = 0.01768185\n",
      "Iteration 14026, loss = 0.01768011\n",
      "Iteration 14027, loss = 0.01767836\n",
      "Iteration 14028, loss = 0.01767661\n",
      "Iteration 14029, loss = 0.01767487\n",
      "Iteration 14030, loss = 0.01767312\n",
      "Iteration 14031, loss = 0.01767138\n",
      "Iteration 14032, loss = 0.01766964\n",
      "Iteration 14033, loss = 0.01766789\n",
      "Iteration 14034, loss = 0.01766615\n",
      "Iteration 14035, loss = 0.01766441\n",
      "Iteration 14036, loss = 0.01766266\n",
      "Iteration 14037, loss = 0.01766092\n",
      "Iteration 14038, loss = 0.01765918\n",
      "Iteration 14039, loss = 0.01765744\n",
      "Iteration 14040, loss = 0.01765570\n",
      "Iteration 14041, loss = 0.01765396\n",
      "Iteration 14042, loss = 0.01765222\n",
      "Iteration 14043, loss = 0.01765048\n",
      "Iteration 14044, loss = 0.01764874\n",
      "Iteration 14045, loss = 0.01764700\n",
      "Iteration 14046, loss = 0.01764526\n",
      "Iteration 14047, loss = 0.01764352\n",
      "Iteration 14048, loss = 0.01764179\n",
      "Iteration 14049, loss = 0.01764005\n",
      "Iteration 14050, loss = 0.01763831\n",
      "Iteration 14051, loss = 0.01763657\n",
      "Iteration 14052, loss = 0.01763484\n",
      "Iteration 14053, loss = 0.01763310\n",
      "Iteration 14054, loss = 0.01763137\n",
      "Iteration 14055, loss = 0.01762963\n",
      "Iteration 14056, loss = 0.01762790\n",
      "Iteration 14057, loss = 0.01762616\n",
      "Iteration 14058, loss = 0.01762443\n",
      "Iteration 14059, loss = 0.01762269\n",
      "Iteration 14060, loss = 0.01762096\n",
      "Iteration 14061, loss = 0.01761923\n",
      "Iteration 14062, loss = 0.01761749\n",
      "Iteration 14063, loss = 0.01761576\n",
      "Iteration 14064, loss = 0.01761403\n",
      "Iteration 14065, loss = 0.01761230\n",
      "Iteration 14066, loss = 0.01761057\n",
      "Iteration 14067, loss = 0.01760884\n",
      "Iteration 14068, loss = 0.01760711\n",
      "Iteration 14069, loss = 0.01760538\n",
      "Iteration 14070, loss = 0.01760365\n",
      "Iteration 14071, loss = 0.01760192\n",
      "Iteration 14072, loss = 0.01760019\n",
      "Iteration 14073, loss = 0.01759846\n",
      "Iteration 14074, loss = 0.01759673\n",
      "Iteration 14075, loss = 0.01759500\n",
      "Iteration 14076, loss = 0.01759328\n",
      "Iteration 14077, loss = 0.01759155\n",
      "Iteration 14078, loss = 0.01758982\n",
      "Iteration 14079, loss = 0.01758810\n",
      "Iteration 14080, loss = 0.01758637\n",
      "Iteration 14081, loss = 0.01758465\n",
      "Iteration 14082, loss = 0.01758292\n",
      "Iteration 14083, loss = 0.01758120\n",
      "Iteration 14084, loss = 0.01757947\n",
      "Iteration 14085, loss = 0.01757775\n",
      "Iteration 14086, loss = 0.01757602\n",
      "Iteration 14087, loss = 0.01757430\n",
      "Iteration 14088, loss = 0.01757258\n",
      "Iteration 14089, loss = 0.01757086\n",
      "Iteration 14090, loss = 0.01756913\n",
      "Iteration 14091, loss = 0.01756741\n",
      "Iteration 14092, loss = 0.01756569\n",
      "Iteration 14093, loss = 0.01756397\n",
      "Iteration 14094, loss = 0.01756225\n",
      "Iteration 14095, loss = 0.01756053\n",
      "Iteration 14096, loss = 0.01755881\n",
      "Iteration 14097, loss = 0.01755709\n",
      "Iteration 14098, loss = 0.01755537\n",
      "Iteration 14099, loss = 0.01755365\n",
      "Iteration 14100, loss = 0.01755193\n",
      "Iteration 14101, loss = 0.01755022\n",
      "Iteration 14102, loss = 0.01754850\n",
      "Iteration 14103, loss = 0.01754678\n",
      "Iteration 14104, loss = 0.01754506\n",
      "Iteration 14105, loss = 0.01754335\n",
      "Iteration 14106, loss = 0.01754163\n",
      "Iteration 14107, loss = 0.01753992\n",
      "Iteration 14108, loss = 0.01753820\n",
      "Iteration 14109, loss = 0.01753649\n",
      "Iteration 14110, loss = 0.01753477\n",
      "Iteration 14111, loss = 0.01753306\n",
      "Iteration 14112, loss = 0.01753134\n",
      "Iteration 14113, loss = 0.01752963\n",
      "Iteration 14114, loss = 0.01752792\n",
      "Iteration 14115, loss = 0.01752620\n",
      "Iteration 14116, loss = 0.01752449\n",
      "Iteration 14117, loss = 0.01752278\n",
      "Iteration 14118, loss = 0.01752107\n",
      "Iteration 14119, loss = 0.01751936\n",
      "Iteration 14120, loss = 0.01751764\n",
      "Iteration 14121, loss = 0.01751593\n",
      "Iteration 14122, loss = 0.01751422\n",
      "Iteration 14123, loss = 0.01751251\n",
      "Iteration 14124, loss = 0.01751080\n",
      "Iteration 14125, loss = 0.01750910\n",
      "Iteration 14126, loss = 0.01750739\n",
      "Iteration 14127, loss = 0.01750568\n",
      "Iteration 14128, loss = 0.01750397\n",
      "Iteration 14129, loss = 0.01750226\n",
      "Iteration 14130, loss = 0.01750056\n",
      "Iteration 14131, loss = 0.01749885\n",
      "Iteration 14132, loss = 0.01749714\n",
      "Iteration 14133, loss = 0.01749544\n",
      "Iteration 14134, loss = 0.01749373\n",
      "Iteration 14135, loss = 0.01749202\n",
      "Iteration 14136, loss = 0.01749032\n",
      "Iteration 14137, loss = 0.01748861\n",
      "Iteration 14138, loss = 0.01748691\n",
      "Iteration 14139, loss = 0.01748521\n",
      "Iteration 14140, loss = 0.01748350\n",
      "Iteration 14141, loss = 0.01748180\n",
      "Iteration 14142, loss = 0.01748010\n",
      "Iteration 14143, loss = 0.01747839\n",
      "Iteration 14144, loss = 0.01747669\n",
      "Iteration 14145, loss = 0.01747499\n",
      "Iteration 14146, loss = 0.01747329\n",
      "Iteration 14147, loss = 0.01747159\n",
      "Iteration 14148, loss = 0.01746989\n",
      "Iteration 14149, loss = 0.01746819\n",
      "Iteration 14150, loss = 0.01746649\n",
      "Iteration 14151, loss = 0.01746479\n",
      "Iteration 14152, loss = 0.01746309\n",
      "Iteration 14153, loss = 0.01746139\n",
      "Iteration 14154, loss = 0.01745969\n",
      "Iteration 14155, loss = 0.01745799\n",
      "Iteration 14156, loss = 0.01745630\n",
      "Iteration 14157, loss = 0.01745460\n",
      "Iteration 14158, loss = 0.01745290\n",
      "Iteration 14159, loss = 0.01745121\n",
      "Iteration 14160, loss = 0.01744951\n",
      "Iteration 14161, loss = 0.01744781\n",
      "Iteration 14162, loss = 0.01744612\n",
      "Iteration 14163, loss = 0.01744442\n",
      "Iteration 14164, loss = 0.01744273\n",
      "Iteration 14165, loss = 0.01744103\n",
      "Iteration 14166, loss = 0.01743934\n",
      "Iteration 14167, loss = 0.01743765\n",
      "Iteration 14168, loss = 0.01743595\n",
      "Iteration 14169, loss = 0.01743426\n",
      "Iteration 14170, loss = 0.01743257\n",
      "Iteration 14171, loss = 0.01743087\n",
      "Iteration 14172, loss = 0.01742918\n",
      "Iteration 14173, loss = 0.01742749\n",
      "Iteration 14174, loss = 0.01742580\n",
      "Iteration 14175, loss = 0.01742411\n",
      "Iteration 14176, loss = 0.01742242\n",
      "Iteration 14177, loss = 0.01742073\n",
      "Iteration 14178, loss = 0.01741904\n",
      "Iteration 14179, loss = 0.01741735\n",
      "Iteration 14180, loss = 0.01741566\n",
      "Iteration 14181, loss = 0.01741397\n",
      "Iteration 14182, loss = 0.01741228\n",
      "Iteration 14183, loss = 0.01741060\n",
      "Iteration 14184, loss = 0.01740891\n",
      "Iteration 14185, loss = 0.01740722\n",
      "Iteration 14186, loss = 0.01740553\n",
      "Iteration 14187, loss = 0.01740385\n",
      "Iteration 14188, loss = 0.01740216\n",
      "Iteration 14189, loss = 0.01740048\n",
      "Iteration 14190, loss = 0.01739879\n",
      "Iteration 14191, loss = 0.01739711\n",
      "Iteration 14192, loss = 0.01739542\n",
      "Iteration 14193, loss = 0.01739374\n",
      "Iteration 14194, loss = 0.01739205\n",
      "Iteration 14195, loss = 0.01739037\n",
      "Iteration 14196, loss = 0.01738869\n",
      "Iteration 14197, loss = 0.01738700\n",
      "Iteration 14198, loss = 0.01738532\n",
      "Iteration 14199, loss = 0.01738364\n",
      "Iteration 14200, loss = 0.01738196\n",
      "Iteration 14201, loss = 0.01738028\n",
      "Iteration 14202, loss = 0.01737860\n",
      "Iteration 14203, loss = 0.01737692\n",
      "Iteration 14204, loss = 0.01737523\n",
      "Iteration 14205, loss = 0.01737355\n",
      "Iteration 14206, loss = 0.01737188\n",
      "Iteration 14207, loss = 0.01737020\n",
      "Iteration 14208, loss = 0.01736852\n",
      "Iteration 14209, loss = 0.01736684\n",
      "Iteration 14210, loss = 0.01736516\n",
      "Iteration 14211, loss = 0.01736348\n",
      "Iteration 14212, loss = 0.01736181\n",
      "Iteration 14213, loss = 0.01736013\n",
      "Iteration 14214, loss = 0.01735845\n",
      "Iteration 14215, loss = 0.01735678\n",
      "Iteration 14216, loss = 0.01735510\n",
      "Iteration 14217, loss = 0.01735342\n",
      "Iteration 14218, loss = 0.01735175\n",
      "Iteration 14219, loss = 0.01735007\n",
      "Iteration 14220, loss = 0.01734840\n",
      "Iteration 14221, loss = 0.01734673\n",
      "Iteration 14222, loss = 0.01734505\n",
      "Iteration 14223, loss = 0.01734338\n",
      "Iteration 14224, loss = 0.01734171\n",
      "Iteration 14225, loss = 0.01734003\n",
      "Iteration 14226, loss = 0.01733836\n",
      "Iteration 14227, loss = 0.01733669\n",
      "Iteration 14228, loss = 0.01733502\n",
      "Iteration 14229, loss = 0.01733335\n",
      "Iteration 14230, loss = 0.01733167\n",
      "Iteration 14231, loss = 0.01733000\n",
      "Iteration 14232, loss = 0.01732833\n",
      "Iteration 14233, loss = 0.01732666\n",
      "Iteration 14234, loss = 0.01732499\n",
      "Iteration 14235, loss = 0.01732333\n",
      "Iteration 14236, loss = 0.01732166\n",
      "Iteration 14237, loss = 0.01731999\n",
      "Iteration 14238, loss = 0.01731832\n",
      "Iteration 14239, loss = 0.01731665\n",
      "Iteration 14240, loss = 0.01731499\n",
      "Iteration 14241, loss = 0.01731332\n",
      "Iteration 14242, loss = 0.01731165\n",
      "Iteration 14243, loss = 0.01730999\n",
      "Iteration 14244, loss = 0.01730832\n",
      "Iteration 14245, loss = 0.01730665\n",
      "Iteration 14246, loss = 0.01730499\n",
      "Iteration 14247, loss = 0.01730332\n",
      "Iteration 14248, loss = 0.01730166\n",
      "Iteration 14249, loss = 0.01730000\n",
      "Iteration 14250, loss = 0.01729833\n",
      "Iteration 14251, loss = 0.01729667\n",
      "Iteration 14252, loss = 0.01729500\n",
      "Iteration 14253, loss = 0.01729334\n",
      "Iteration 14254, loss = 0.01729168\n",
      "Iteration 14255, loss = 0.01729002\n",
      "Iteration 14256, loss = 0.01728836\n",
      "Iteration 14257, loss = 0.01728669\n",
      "Iteration 14258, loss = 0.01728503\n",
      "Iteration 14259, loss = 0.01728337\n",
      "Iteration 14260, loss = 0.01728171\n",
      "Iteration 14261, loss = 0.01728005\n",
      "Iteration 14262, loss = 0.01727839\n",
      "Iteration 14263, loss = 0.01727673\n",
      "Iteration 14264, loss = 0.01727508\n",
      "Iteration 14265, loss = 0.01727342\n",
      "Iteration 14266, loss = 0.01727176\n",
      "Iteration 14267, loss = 0.01727010\n",
      "Iteration 14268, loss = 0.01726844\n",
      "Iteration 14269, loss = 0.01726679\n",
      "Iteration 14270, loss = 0.01726513\n",
      "Iteration 14271, loss = 0.01726347\n",
      "Iteration 14272, loss = 0.01726182\n",
      "Iteration 14273, loss = 0.01726016\n",
      "Iteration 14274, loss = 0.01725851\n",
      "Iteration 14275, loss = 0.01725685\n",
      "Iteration 14276, loss = 0.01725520\n",
      "Iteration 14277, loss = 0.01725354\n",
      "Iteration 14278, loss = 0.01725189\n",
      "Iteration 14279, loss = 0.01725024\n",
      "Iteration 14280, loss = 0.01724858\n",
      "Iteration 14281, loss = 0.01724693\n",
      "Iteration 14282, loss = 0.01724528\n",
      "Iteration 14283, loss = 0.01724363\n",
      "Iteration 14284, loss = 0.01724197\n",
      "Iteration 14285, loss = 0.01724032\n",
      "Iteration 14286, loss = 0.01723867\n",
      "Iteration 14287, loss = 0.01723702\n",
      "Iteration 14288, loss = 0.01723537\n",
      "Iteration 14289, loss = 0.01723372\n",
      "Iteration 14290, loss = 0.01723207\n",
      "Iteration 14291, loss = 0.01723042\n",
      "Iteration 14292, loss = 0.01722877\n",
      "Iteration 14293, loss = 0.01722712\n",
      "Iteration 14294, loss = 0.01722548\n",
      "Iteration 14295, loss = 0.01722383\n",
      "Iteration 14296, loss = 0.01722218\n",
      "Iteration 14297, loss = 0.01722053\n",
      "Iteration 14298, loss = 0.01721889\n",
      "Iteration 14299, loss = 0.01721724\n",
      "Iteration 14300, loss = 0.01721559\n",
      "Iteration 14301, loss = 0.01721395\n",
      "Iteration 14302, loss = 0.01721230\n",
      "Iteration 14303, loss = 0.01721066\n",
      "Iteration 14304, loss = 0.01720901\n",
      "Iteration 14305, loss = 0.01720737\n",
      "Iteration 14306, loss = 0.01720572\n",
      "Iteration 14307, loss = 0.01720408\n",
      "Iteration 14308, loss = 0.01720244\n",
      "Iteration 14309, loss = 0.01720079\n",
      "Iteration 14310, loss = 0.01719915\n",
      "Iteration 14311, loss = 0.01719751\n",
      "Iteration 14312, loss = 0.01719587\n",
      "Iteration 14313, loss = 0.01719423\n",
      "Iteration 14314, loss = 0.01719259\n",
      "Iteration 14315, loss = 0.01719094\n",
      "Iteration 14316, loss = 0.01718930\n",
      "Iteration 14317, loss = 0.01718766\n",
      "Iteration 14318, loss = 0.01718602\n",
      "Iteration 14319, loss = 0.01718438\n",
      "Iteration 14320, loss = 0.01718275\n",
      "Iteration 14321, loss = 0.01718111\n",
      "Iteration 14322, loss = 0.01717947\n",
      "Iteration 14323, loss = 0.01717783\n",
      "Iteration 14324, loss = 0.01717619\n",
      "Iteration 14325, loss = 0.01717455\n",
      "Iteration 14326, loss = 0.01717292\n",
      "Iteration 14327, loss = 0.01717128\n",
      "Iteration 14328, loss = 0.01716964\n",
      "Iteration 14329, loss = 0.01716801\n",
      "Iteration 14330, loss = 0.01716637\n",
      "Iteration 14331, loss = 0.01716474\n",
      "Iteration 14332, loss = 0.01716310\n",
      "Iteration 14333, loss = 0.01716147\n",
      "Iteration 14334, loss = 0.01715983\n",
      "Iteration 14335, loss = 0.01715820\n",
      "Iteration 14336, loss = 0.01715657\n",
      "Iteration 14337, loss = 0.01715493\n",
      "Iteration 14338, loss = 0.01715330\n",
      "Iteration 14339, loss = 0.01715167\n",
      "Iteration 14340, loss = 0.01715004\n",
      "Iteration 14341, loss = 0.01714840\n",
      "Iteration 14342, loss = 0.01714677\n",
      "Iteration 14343, loss = 0.01714514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14344, loss = 0.01714351\n",
      "Iteration 14345, loss = 0.01714188\n",
      "Iteration 14346, loss = 0.01714025\n",
      "Iteration 14347, loss = 0.01713862\n",
      "Iteration 14348, loss = 0.01713699\n",
      "Iteration 14349, loss = 0.01713536\n",
      "Iteration 14350, loss = 0.01713373\n",
      "Iteration 14351, loss = 0.01713210\n",
      "Iteration 14352, loss = 0.01713048\n",
      "Iteration 14353, loss = 0.01712885\n",
      "Iteration 14354, loss = 0.01712722\n",
      "Iteration 14355, loss = 0.01712559\n",
      "Iteration 14356, loss = 0.01712397\n",
      "Iteration 14357, loss = 0.01712234\n",
      "Iteration 14358, loss = 0.01712072\n",
      "Iteration 14359, loss = 0.01711909\n",
      "Iteration 14360, loss = 0.01711746\n",
      "Iteration 14361, loss = 0.01711584\n",
      "Iteration 14362, loss = 0.01711422\n",
      "Iteration 14363, loss = 0.01711259\n",
      "Iteration 14364, loss = 0.01711097\n",
      "Iteration 14365, loss = 0.01710934\n",
      "Iteration 14366, loss = 0.01710772\n",
      "Iteration 14367, loss = 0.01710610\n",
      "Iteration 14368, loss = 0.01710448\n",
      "Iteration 14369, loss = 0.01710285\n",
      "Iteration 14370, loss = 0.01710123\n",
      "Iteration 14371, loss = 0.01709961\n",
      "Iteration 14372, loss = 0.01709799\n",
      "Iteration 14373, loss = 0.01709637\n",
      "Iteration 14374, loss = 0.01709475\n",
      "Iteration 14375, loss = 0.01709313\n",
      "Iteration 14376, loss = 0.01709151\n",
      "Iteration 14377, loss = 0.01708989\n",
      "Iteration 14378, loss = 0.01708827\n",
      "Iteration 14379, loss = 0.01708665\n",
      "Iteration 14380, loss = 0.01708503\n",
      "Iteration 14381, loss = 0.01708341\n",
      "Iteration 14382, loss = 0.01708180\n",
      "Iteration 14383, loss = 0.01708018\n",
      "Iteration 14384, loss = 0.01707856\n",
      "Iteration 14385, loss = 0.01707694\n",
      "Iteration 14386, loss = 0.01707533\n",
      "Iteration 14387, loss = 0.01707371\n",
      "Iteration 14388, loss = 0.01707210\n",
      "Iteration 14389, loss = 0.01707048\n",
      "Iteration 14390, loss = 0.01706887\n",
      "Iteration 14391, loss = 0.01706725\n",
      "Iteration 14392, loss = 0.01706564\n",
      "Iteration 14393, loss = 0.01706402\n",
      "Iteration 14394, loss = 0.01706241\n",
      "Iteration 14395, loss = 0.01706080\n",
      "Iteration 14396, loss = 0.01705918\n",
      "Iteration 14397, loss = 0.01705757\n",
      "Iteration 14398, loss = 0.01705596\n",
      "Iteration 14399, loss = 0.01705435\n",
      "Iteration 14400, loss = 0.01705274\n",
      "Iteration 14401, loss = 0.01705112\n",
      "Iteration 14402, loss = 0.01704951\n",
      "Iteration 14403, loss = 0.01704790\n",
      "Iteration 14404, loss = 0.01704629\n",
      "Iteration 14405, loss = 0.01704468\n",
      "Iteration 14406, loss = 0.01704307\n",
      "Iteration 14407, loss = 0.01704146\n",
      "Iteration 14408, loss = 0.01703985\n",
      "Iteration 14409, loss = 0.01703825\n",
      "Iteration 14410, loss = 0.01703664\n",
      "Iteration 14411, loss = 0.01703503\n",
      "Iteration 14412, loss = 0.01703342\n",
      "Iteration 14413, loss = 0.01703182\n",
      "Iteration 14414, loss = 0.01703021\n",
      "Iteration 14415, loss = 0.01702860\n",
      "Iteration 14416, loss = 0.01702700\n",
      "Iteration 14417, loss = 0.01702539\n",
      "Iteration 14418, loss = 0.01702378\n",
      "Iteration 14419, loss = 0.01702218\n",
      "Iteration 14420, loss = 0.01702057\n",
      "Iteration 14421, loss = 0.01701897\n",
      "Iteration 14422, loss = 0.01701737\n",
      "Iteration 14423, loss = 0.01701576\n",
      "Iteration 14424, loss = 0.01701416\n",
      "Iteration 14425, loss = 0.01701256\n",
      "Iteration 14426, loss = 0.01701095\n",
      "Iteration 14427, loss = 0.01700935\n",
      "Iteration 14428, loss = 0.01700775\n",
      "Iteration 14429, loss = 0.01700615\n",
      "Iteration 14430, loss = 0.01700455\n",
      "Iteration 14431, loss = 0.01700294\n",
      "Iteration 14432, loss = 0.01700134\n",
      "Iteration 14433, loss = 0.01699974\n",
      "Iteration 14434, loss = 0.01699814\n",
      "Iteration 14435, loss = 0.01699654\n",
      "Iteration 14436, loss = 0.01699494\n",
      "Iteration 14437, loss = 0.01699335\n",
      "Iteration 14438, loss = 0.01699175\n",
      "Iteration 14439, loss = 0.01699015\n",
      "Iteration 14440, loss = 0.01698855\n",
      "Iteration 14441, loss = 0.01698695\n",
      "Iteration 14442, loss = 0.01698535\n",
      "Iteration 14443, loss = 0.01698376\n",
      "Iteration 14444, loss = 0.01698216\n",
      "Iteration 14445, loss = 0.01698056\n",
      "Iteration 14446, loss = 0.01697897\n",
      "Iteration 14447, loss = 0.01697737\n",
      "Iteration 14448, loss = 0.01697578\n",
      "Iteration 14449, loss = 0.01697418\n",
      "Iteration 14450, loss = 0.01697259\n",
      "Iteration 14451, loss = 0.01697099\n",
      "Iteration 14452, loss = 0.01696940\n",
      "Iteration 14453, loss = 0.01696781\n",
      "Iteration 14454, loss = 0.01696621\n",
      "Iteration 14455, loss = 0.01696462\n",
      "Iteration 14456, loss = 0.01696303\n",
      "Iteration 14457, loss = 0.01696143\n",
      "Iteration 14458, loss = 0.01695984\n",
      "Iteration 14459, loss = 0.01695825\n",
      "Iteration 14460, loss = 0.01695666\n",
      "Iteration 14461, loss = 0.01695507\n",
      "Iteration 14462, loss = 0.01695348\n",
      "Iteration 14463, loss = 0.01695189\n",
      "Iteration 14464, loss = 0.01695030\n",
      "Iteration 14465, loss = 0.01694871\n",
      "Iteration 14466, loss = 0.01694712\n",
      "Iteration 14467, loss = 0.01694553\n",
      "Iteration 14468, loss = 0.01694394\n",
      "Iteration 14469, loss = 0.01694235\n",
      "Iteration 14470, loss = 0.01694076\n",
      "Iteration 14471, loss = 0.01693918\n",
      "Iteration 14472, loss = 0.01693759\n",
      "Iteration 14473, loss = 0.01693600\n",
      "Iteration 14474, loss = 0.01693442\n",
      "Iteration 14475, loss = 0.01693283\n",
      "Iteration 14476, loss = 0.01693124\n",
      "Iteration 14477, loss = 0.01692966\n",
      "Iteration 14478, loss = 0.01692807\n",
      "Iteration 14479, loss = 0.01692649\n",
      "Iteration 14480, loss = 0.01692490\n",
      "Iteration 14481, loss = 0.01692332\n",
      "Iteration 14482, loss = 0.01692173\n",
      "Iteration 14483, loss = 0.01692015\n",
      "Iteration 14484, loss = 0.01691857\n",
      "Iteration 14485, loss = 0.01691698\n",
      "Iteration 14486, loss = 0.01691540\n",
      "Iteration 14487, loss = 0.01691382\n",
      "Iteration 14488, loss = 0.01691224\n",
      "Iteration 14489, loss = 0.01691066\n",
      "Iteration 14490, loss = 0.01690907\n",
      "Iteration 14491, loss = 0.01690749\n",
      "Iteration 14492, loss = 0.01690591\n",
      "Iteration 14493, loss = 0.01690433\n",
      "Iteration 14494, loss = 0.01690275\n",
      "Iteration 14495, loss = 0.01690117\n",
      "Iteration 14496, loss = 0.01689959\n",
      "Iteration 14497, loss = 0.01689801\n",
      "Iteration 14498, loss = 0.01689644\n",
      "Iteration 14499, loss = 0.01689486\n",
      "Iteration 14500, loss = 0.01689328\n",
      "Iteration 14501, loss = 0.01689170\n",
      "Iteration 14502, loss = 0.01689012\n",
      "Iteration 14503, loss = 0.01688855\n",
      "Iteration 14504, loss = 0.01688697\n",
      "Iteration 14505, loss = 0.01688539\n",
      "Iteration 14506, loss = 0.01688382\n",
      "Iteration 14507, loss = 0.01688224\n",
      "Iteration 14508, loss = 0.01688067\n",
      "Iteration 14509, loss = 0.01687909\n",
      "Iteration 14510, loss = 0.01687752\n",
      "Iteration 14511, loss = 0.01687594\n",
      "Iteration 14512, loss = 0.01687437\n",
      "Iteration 14513, loss = 0.01687279\n",
      "Iteration 14514, loss = 0.01687122\n",
      "Iteration 14515, loss = 0.01686965\n",
      "Iteration 14516, loss = 0.01686808\n",
      "Iteration 14517, loss = 0.01686650\n",
      "Iteration 14518, loss = 0.01686493\n",
      "Iteration 14519, loss = 0.01686336\n",
      "Iteration 14520, loss = 0.01686179\n",
      "Iteration 14521, loss = 0.01686022\n",
      "Iteration 14522, loss = 0.01685865\n",
      "Iteration 14523, loss = 0.01685707\n",
      "Iteration 14524, loss = 0.01685550\n",
      "Iteration 14525, loss = 0.01685393\n",
      "Iteration 14526, loss = 0.01685237\n",
      "Iteration 14527, loss = 0.01685080\n",
      "Iteration 14528, loss = 0.01684923\n",
      "Iteration 14529, loss = 0.01684766\n",
      "Iteration 14530, loss = 0.01684609\n",
      "Iteration 14531, loss = 0.01684452\n",
      "Iteration 14532, loss = 0.01684296\n",
      "Iteration 14533, loss = 0.01684139\n",
      "Iteration 14534, loss = 0.01683982\n",
      "Iteration 14535, loss = 0.01683825\n",
      "Iteration 14536, loss = 0.01683669\n",
      "Iteration 14537, loss = 0.01683512\n",
      "Iteration 14538, loss = 0.01683356\n",
      "Iteration 14539, loss = 0.01683199\n",
      "Iteration 14540, loss = 0.01683043\n",
      "Iteration 14541, loss = 0.01682886\n",
      "Iteration 14542, loss = 0.01682730\n",
      "Iteration 14543, loss = 0.01682573\n",
      "Iteration 14544, loss = 0.01682417\n",
      "Iteration 14545, loss = 0.01682261\n",
      "Iteration 14546, loss = 0.01682104\n",
      "Iteration 14547, loss = 0.01681948\n",
      "Iteration 14548, loss = 0.01681792\n",
      "Iteration 14549, loss = 0.01681636\n",
      "Iteration 14550, loss = 0.01681480\n",
      "Iteration 14551, loss = 0.01681323\n",
      "Iteration 14552, loss = 0.01681167\n",
      "Iteration 14553, loss = 0.01681011\n",
      "Iteration 14554, loss = 0.01680855\n",
      "Iteration 14555, loss = 0.01680699\n",
      "Iteration 14556, loss = 0.01680543\n",
      "Iteration 14557, loss = 0.01680387\n",
      "Iteration 14558, loss = 0.01680231\n",
      "Iteration 14559, loss = 0.01680076\n",
      "Iteration 14560, loss = 0.01679920\n",
      "Iteration 14561, loss = 0.01679764\n",
      "Iteration 14562, loss = 0.01679608\n",
      "Iteration 14563, loss = 0.01679452\n",
      "Iteration 14564, loss = 0.01679297\n",
      "Iteration 14565, loss = 0.01679141\n",
      "Iteration 14566, loss = 0.01678985\n",
      "Iteration 14567, loss = 0.01678830\n",
      "Iteration 14568, loss = 0.01678674\n",
      "Iteration 14569, loss = 0.01678519\n",
      "Iteration 14570, loss = 0.01678363\n",
      "Iteration 14571, loss = 0.01678208\n",
      "Iteration 14572, loss = 0.01678052\n",
      "Iteration 14573, loss = 0.01677897\n",
      "Iteration 14574, loss = 0.01677741\n",
      "Iteration 14575, loss = 0.01677586\n",
      "Iteration 14576, loss = 0.01677431\n",
      "Iteration 14577, loss = 0.01677275\n",
      "Iteration 14578, loss = 0.01677120\n",
      "Iteration 14579, loss = 0.01676965\n",
      "Iteration 14580, loss = 0.01676810\n",
      "Iteration 14581, loss = 0.01676655\n",
      "Iteration 14582, loss = 0.01676499\n",
      "Iteration 14583, loss = 0.01676344\n",
      "Iteration 14584, loss = 0.01676189\n",
      "Iteration 14585, loss = 0.01676034\n",
      "Iteration 14586, loss = 0.01675879\n",
      "Iteration 14587, loss = 0.01675724\n",
      "Iteration 14588, loss = 0.01675569\n",
      "Iteration 14589, loss = 0.01675414\n",
      "Iteration 14590, loss = 0.01675260\n",
      "Iteration 14591, loss = 0.01675105\n",
      "Iteration 14592, loss = 0.01674950\n",
      "Iteration 14593, loss = 0.01674795\n",
      "Iteration 14594, loss = 0.01674640\n",
      "Iteration 14595, loss = 0.01674486\n",
      "Iteration 14596, loss = 0.01674331\n",
      "Iteration 14597, loss = 0.01674176\n",
      "Iteration 14598, loss = 0.01674022\n",
      "Iteration 14599, loss = 0.01673867\n",
      "Iteration 14600, loss = 0.01673713\n",
      "Iteration 14601, loss = 0.01673558\n",
      "Iteration 14602, loss = 0.01673404\n",
      "Iteration 14603, loss = 0.01673249\n",
      "Iteration 14604, loss = 0.01673095\n",
      "Iteration 14605, loss = 0.01672940\n",
      "Iteration 14606, loss = 0.01672786\n",
      "Iteration 14607, loss = 0.01672632\n",
      "Iteration 14608, loss = 0.01672477\n",
      "Iteration 14609, loss = 0.01672323\n",
      "Iteration 14610, loss = 0.01672169\n",
      "Iteration 14611, loss = 0.01672015\n",
      "Iteration 14612, loss = 0.01671861\n",
      "Iteration 14613, loss = 0.01671706\n",
      "Iteration 14614, loss = 0.01671552\n",
      "Iteration 14615, loss = 0.01671398\n",
      "Iteration 14616, loss = 0.01671244\n",
      "Iteration 14617, loss = 0.01671090\n",
      "Iteration 14618, loss = 0.01670936\n",
      "Iteration 14619, loss = 0.01670782\n",
      "Iteration 14620, loss = 0.01670628\n",
      "Iteration 14621, loss = 0.01670475\n",
      "Iteration 14622, loss = 0.01670321\n",
      "Iteration 14623, loss = 0.01670167\n",
      "Iteration 14624, loss = 0.01670013\n",
      "Iteration 14625, loss = 0.01669859\n",
      "Iteration 14626, loss = 0.01669706\n",
      "Iteration 14627, loss = 0.01669552\n",
      "Iteration 14628, loss = 0.01669398\n",
      "Iteration 14629, loss = 0.01669245\n",
      "Iteration 14630, loss = 0.01669091\n",
      "Iteration 14631, loss = 0.01668937\n",
      "Iteration 14632, loss = 0.01668784\n",
      "Iteration 14633, loss = 0.01668630\n",
      "Iteration 14634, loss = 0.01668477\n",
      "Iteration 14635, loss = 0.01668324\n",
      "Iteration 14636, loss = 0.01668170\n",
      "Iteration 14637, loss = 0.01668017\n",
      "Iteration 14638, loss = 0.01667863\n",
      "Iteration 14639, loss = 0.01667710\n",
      "Iteration 14640, loss = 0.01667557\n",
      "Iteration 14641, loss = 0.01667404\n",
      "Iteration 14642, loss = 0.01667250\n",
      "Iteration 14643, loss = 0.01667097\n",
      "Iteration 14644, loss = 0.01666944\n",
      "Iteration 14645, loss = 0.01666791\n",
      "Iteration 14646, loss = 0.01666638\n",
      "Iteration 14647, loss = 0.01666485\n",
      "Iteration 14648, loss = 0.01666332\n",
      "Iteration 14649, loss = 0.01666179\n",
      "Iteration 14650, loss = 0.01666026\n",
      "Iteration 14651, loss = 0.01665873\n",
      "Iteration 14652, loss = 0.01665720\n",
      "Iteration 14653, loss = 0.01665567\n",
      "Iteration 14654, loss = 0.01665414\n",
      "Iteration 14655, loss = 0.01665262\n",
      "Iteration 14656, loss = 0.01665109\n",
      "Iteration 14657, loss = 0.01664956\n",
      "Iteration 14658, loss = 0.01664803\n",
      "Iteration 14659, loss = 0.01664651\n",
      "Iteration 14660, loss = 0.01664498\n",
      "Iteration 14661, loss = 0.01664345\n",
      "Iteration 14662, loss = 0.01664193\n",
      "Iteration 14663, loss = 0.01664040\n",
      "Iteration 14664, loss = 0.01663888\n",
      "Iteration 14665, loss = 0.01663735\n",
      "Iteration 14666, loss = 0.01663583\n",
      "Iteration 14667, loss = 0.01663430\n",
      "Iteration 14668, loss = 0.01663278\n",
      "Iteration 14669, loss = 0.01663126\n",
      "Iteration 14670, loss = 0.01662973\n",
      "Iteration 14671, loss = 0.01662821\n",
      "Iteration 14672, loss = 0.01662669\n",
      "Iteration 14673, loss = 0.01662517\n",
      "Iteration 14674, loss = 0.01662364\n",
      "Iteration 14675, loss = 0.01662212\n",
      "Iteration 14676, loss = 0.01662060\n",
      "Iteration 14677, loss = 0.01661908\n",
      "Iteration 14678, loss = 0.01661756\n",
      "Iteration 14679, loss = 0.01661604\n",
      "Iteration 14680, loss = 0.01661452\n",
      "Iteration 14681, loss = 0.01661300\n",
      "Iteration 14682, loss = 0.01661148\n",
      "Iteration 14683, loss = 0.01660996\n",
      "Iteration 14684, loss = 0.01660844\n",
      "Iteration 14685, loss = 0.01660692\n",
      "Iteration 14686, loss = 0.01660540\n",
      "Iteration 14687, loss = 0.01660389\n",
      "Iteration 14688, loss = 0.01660237\n",
      "Iteration 14689, loss = 0.01660085\n",
      "Iteration 14690, loss = 0.01659933\n",
      "Iteration 14691, loss = 0.01659782\n",
      "Iteration 14692, loss = 0.01659630\n",
      "Iteration 14693, loss = 0.01659479\n",
      "Iteration 14694, loss = 0.01659327\n",
      "Iteration 14695, loss = 0.01659175\n",
      "Iteration 14696, loss = 0.01659024\n",
      "Iteration 14697, loss = 0.01658872\n",
      "Iteration 14698, loss = 0.01658721\n",
      "Iteration 14699, loss = 0.01658570\n",
      "Iteration 14700, loss = 0.01658418\n",
      "Iteration 14701, loss = 0.01658267\n",
      "Iteration 14702, loss = 0.01658116\n",
      "Iteration 14703, loss = 0.01657964\n",
      "Iteration 14704, loss = 0.01657813\n",
      "Iteration 14705, loss = 0.01657662\n",
      "Iteration 14706, loss = 0.01657511\n",
      "Iteration 14707, loss = 0.01657359\n",
      "Iteration 14708, loss = 0.01657208\n",
      "Iteration 14709, loss = 0.01657057\n",
      "Iteration 14710, loss = 0.01656906\n",
      "Iteration 14711, loss = 0.01656755\n",
      "Iteration 14712, loss = 0.01656604\n",
      "Iteration 14713, loss = 0.01656453\n",
      "Iteration 14714, loss = 0.01656302\n",
      "Iteration 14715, loss = 0.01656151\n",
      "Iteration 14716, loss = 0.01656000\n",
      "Iteration 14717, loss = 0.01655849\n",
      "Iteration 14718, loss = 0.01655699\n",
      "Iteration 14719, loss = 0.01655548\n",
      "Iteration 14720, loss = 0.01655397\n",
      "Iteration 14721, loss = 0.01655246\n",
      "Iteration 14722, loss = 0.01655096\n",
      "Iteration 14723, loss = 0.01654945\n",
      "Iteration 14724, loss = 0.01654794\n",
      "Iteration 14725, loss = 0.01654644\n",
      "Iteration 14726, loss = 0.01654493\n",
      "Iteration 14727, loss = 0.01654343\n",
      "Iteration 14728, loss = 0.01654192\n",
      "Iteration 14729, loss = 0.01654042\n",
      "Iteration 14730, loss = 0.01653891\n",
      "Iteration 14731, loss = 0.01653741\n",
      "Iteration 14732, loss = 0.01653590\n",
      "Iteration 14733, loss = 0.01653440\n",
      "Iteration 14734, loss = 0.01653290\n",
      "Iteration 14735, loss = 0.01653139\n",
      "Iteration 14736, loss = 0.01652989\n",
      "Iteration 14737, loss = 0.01652839\n",
      "Iteration 14738, loss = 0.01652689\n",
      "Iteration 14739, loss = 0.01652538\n",
      "Iteration 14740, loss = 0.01652388\n",
      "Iteration 14741, loss = 0.01652238\n",
      "Iteration 14742, loss = 0.01652088\n",
      "Iteration 14743, loss = 0.01651938\n",
      "Iteration 14744, loss = 0.01651788\n",
      "Iteration 14745, loss = 0.01651638\n",
      "Iteration 14746, loss = 0.01651488\n",
      "Iteration 14747, loss = 0.01651338\n",
      "Iteration 14748, loss = 0.01651188\n",
      "Iteration 14749, loss = 0.01651038\n",
      "Iteration 14750, loss = 0.01650888\n",
      "Iteration 14751, loss = 0.01650739\n",
      "Iteration 14752, loss = 0.01650589\n",
      "Iteration 14753, loss = 0.01650439\n",
      "Iteration 14754, loss = 0.01650289\n",
      "Iteration 14755, loss = 0.01650140\n",
      "Iteration 14756, loss = 0.01649990\n",
      "Iteration 14757, loss = 0.01649840\n",
      "Iteration 14758, loss = 0.01649691\n",
      "Iteration 14759, loss = 0.01649541\n",
      "Iteration 14760, loss = 0.01649392\n",
      "Iteration 14761, loss = 0.01649242\n",
      "Iteration 14762, loss = 0.01649093\n",
      "Iteration 14763, loss = 0.01648943\n",
      "Iteration 14764, loss = 0.01648794\n",
      "Iteration 14765, loss = 0.01648645\n",
      "Iteration 14766, loss = 0.01648495\n",
      "Iteration 14767, loss = 0.01648346\n",
      "Iteration 14768, loss = 0.01648197\n",
      "Iteration 14769, loss = 0.01648047\n",
      "Iteration 14770, loss = 0.01647898\n",
      "Iteration 14771, loss = 0.01647749\n",
      "Iteration 14772, loss = 0.01647600\n",
      "Iteration 14773, loss = 0.01647451\n",
      "Iteration 14774, loss = 0.01647302\n",
      "Iteration 14775, loss = 0.01647152\n",
      "Iteration 14776, loss = 0.01647003\n",
      "Iteration 14777, loss = 0.01646854\n",
      "Iteration 14778, loss = 0.01646705\n",
      "Iteration 14779, loss = 0.01646556\n",
      "Iteration 14780, loss = 0.01646408\n",
      "Iteration 14781, loss = 0.01646259\n",
      "Iteration 14782, loss = 0.01646110\n",
      "Iteration 14783, loss = 0.01645961\n",
      "Iteration 14784, loss = 0.01645812\n",
      "Iteration 14785, loss = 0.01645663\n",
      "Iteration 14786, loss = 0.01645515\n",
      "Iteration 14787, loss = 0.01645366\n",
      "Iteration 14788, loss = 0.01645217\n",
      "Iteration 14789, loss = 0.01645069\n",
      "Iteration 14790, loss = 0.01644920\n",
      "Iteration 14791, loss = 0.01644771\n",
      "Iteration 14792, loss = 0.01644623\n",
      "Iteration 14793, loss = 0.01644474\n",
      "Iteration 14794, loss = 0.01644326\n",
      "Iteration 14795, loss = 0.01644177\n",
      "Iteration 14796, loss = 0.01644029\n",
      "Iteration 14797, loss = 0.01643881\n",
      "Iteration 14798, loss = 0.01643732\n",
      "Iteration 14799, loss = 0.01643584\n",
      "Iteration 14800, loss = 0.01643436\n",
      "Iteration 14801, loss = 0.01643287\n",
      "Iteration 14802, loss = 0.01643139\n",
      "Iteration 14803, loss = 0.01642991\n",
      "Iteration 14804, loss = 0.01642843\n",
      "Iteration 14805, loss = 0.01642694\n",
      "Iteration 14806, loss = 0.01642546\n",
      "Iteration 14807, loss = 0.01642398\n",
      "Iteration 14808, loss = 0.01642250\n",
      "Iteration 14809, loss = 0.01642102\n",
      "Iteration 14810, loss = 0.01641954\n",
      "Iteration 14811, loss = 0.01641806\n",
      "Iteration 14812, loss = 0.01641658\n",
      "Iteration 14813, loss = 0.01641510\n",
      "Iteration 14814, loss = 0.01641362\n",
      "Iteration 14815, loss = 0.01641214\n",
      "Iteration 14816, loss = 0.01641067\n",
      "Iteration 14817, loss = 0.01640919\n",
      "Iteration 14818, loss = 0.01640771\n",
      "Iteration 14819, loss = 0.01640623\n",
      "Iteration 14820, loss = 0.01640476\n",
      "Iteration 14821, loss = 0.01640328\n",
      "Iteration 14822, loss = 0.01640180\n",
      "Iteration 14823, loss = 0.01640033\n",
      "Iteration 14824, loss = 0.01639885\n",
      "Iteration 14825, loss = 0.01639738\n",
      "Iteration 14826, loss = 0.01639590\n",
      "Iteration 14827, loss = 0.01639442\n",
      "Iteration 14828, loss = 0.01639295\n",
      "Iteration 14829, loss = 0.01639148\n",
      "Iteration 14830, loss = 0.01639000\n",
      "Iteration 14831, loss = 0.01638853\n",
      "Iteration 14832, loss = 0.01638705\n",
      "Iteration 14833, loss = 0.01638558\n",
      "Iteration 14834, loss = 0.01638411\n",
      "Iteration 14835, loss = 0.01638264\n",
      "Iteration 14836, loss = 0.01638116\n",
      "Iteration 14837, loss = 0.01637969\n",
      "Iteration 14838, loss = 0.01637822\n",
      "Iteration 14839, loss = 0.01637675\n",
      "Iteration 14840, loss = 0.01637528\n",
      "Iteration 14841, loss = 0.01637381\n",
      "Iteration 14842, loss = 0.01637234\n",
      "Iteration 14843, loss = 0.01637087\n",
      "Iteration 14844, loss = 0.01636940\n",
      "Iteration 14845, loss = 0.01636793\n",
      "Iteration 14846, loss = 0.01636646\n",
      "Iteration 14847, loss = 0.01636499\n",
      "Iteration 14848, loss = 0.01636352\n",
      "Iteration 14849, loss = 0.01636205\n",
      "Iteration 14850, loss = 0.01636058\n",
      "Iteration 14851, loss = 0.01635912\n",
      "Iteration 14852, loss = 0.01635765\n",
      "Iteration 14853, loss = 0.01635618\n",
      "Iteration 14854, loss = 0.01635471\n",
      "Iteration 14855, loss = 0.01635325\n",
      "Iteration 14856, loss = 0.01635178\n",
      "Iteration 14857, loss = 0.01635031\n",
      "Iteration 14858, loss = 0.01634885\n",
      "Iteration 14859, loss = 0.01634738\n",
      "Iteration 14860, loss = 0.01634592\n",
      "Iteration 14861, loss = 0.01634445\n",
      "Iteration 14862, loss = 0.01634299\n",
      "Iteration 14863, loss = 0.01634152\n",
      "Iteration 14864, loss = 0.01634006\n",
      "Iteration 14865, loss = 0.01633860\n",
      "Iteration 14866, loss = 0.01633713\n",
      "Iteration 14867, loss = 0.01633567\n",
      "Iteration 14868, loss = 0.01633421\n",
      "Iteration 14869, loss = 0.01633275\n",
      "Iteration 14870, loss = 0.01633128\n",
      "Iteration 14871, loss = 0.01632982\n",
      "Iteration 14872, loss = 0.01632836\n",
      "Iteration 14873, loss = 0.01632690\n",
      "Iteration 14874, loss = 0.01632544\n",
      "Iteration 14875, loss = 0.01632398\n",
      "Iteration 14876, loss = 0.01632252\n",
      "Iteration 14877, loss = 0.01632106\n",
      "Iteration 14878, loss = 0.01631960\n",
      "Iteration 14879, loss = 0.01631814\n",
      "Iteration 14880, loss = 0.01631668\n",
      "Iteration 14881, loss = 0.01631522\n",
      "Iteration 14882, loss = 0.01631376\n",
      "Iteration 14883, loss = 0.01631230\n",
      "Iteration 14884, loss = 0.01631084\n",
      "Iteration 14885, loss = 0.01630939\n",
      "Iteration 14886, loss = 0.01630793\n",
      "Iteration 14887, loss = 0.01630647\n",
      "Iteration 14888, loss = 0.01630502\n",
      "Iteration 14889, loss = 0.01630356\n",
      "Iteration 14890, loss = 0.01630210\n",
      "Iteration 14891, loss = 0.01630065\n",
      "Iteration 14892, loss = 0.01629919\n",
      "Iteration 14893, loss = 0.01629774\n",
      "Iteration 14894, loss = 0.01629628\n",
      "Iteration 14895, loss = 0.01629483\n",
      "Iteration 14896, loss = 0.01629337\n",
      "Iteration 14897, loss = 0.01629192\n",
      "Iteration 14898, loss = 0.01629046\n",
      "Iteration 14899, loss = 0.01628901\n",
      "Iteration 14900, loss = 0.01628756\n",
      "Iteration 14901, loss = 0.01628610\n",
      "Iteration 14902, loss = 0.01628465\n",
      "Iteration 14903, loss = 0.01628320\n",
      "Iteration 14904, loss = 0.01628175\n",
      "Iteration 14905, loss = 0.01628029\n",
      "Iteration 14906, loss = 0.01627884\n",
      "Iteration 14907, loss = 0.01627739\n",
      "Iteration 14908, loss = 0.01627594\n",
      "Iteration 14909, loss = 0.01627449\n",
      "Iteration 14910, loss = 0.01627304\n",
      "Iteration 14911, loss = 0.01627159\n",
      "Iteration 14912, loss = 0.01627014\n",
      "Iteration 14913, loss = 0.01626869\n",
      "Iteration 14914, loss = 0.01626724\n",
      "Iteration 14915, loss = 0.01626579\n",
      "Iteration 14916, loss = 0.01626434\n",
      "Iteration 14917, loss = 0.01626290\n",
      "Iteration 14918, loss = 0.01626145\n",
      "Iteration 14919, loss = 0.01626000\n",
      "Iteration 14920, loss = 0.01625855\n",
      "Iteration 14921, loss = 0.01625711\n",
      "Iteration 14922, loss = 0.01625566\n",
      "Iteration 14923, loss = 0.01625421\n",
      "Iteration 14924, loss = 0.01625277\n",
      "Iteration 14925, loss = 0.01625132\n",
      "Iteration 14926, loss = 0.01624987\n",
      "Iteration 14927, loss = 0.01624843\n",
      "Iteration 14928, loss = 0.01624698\n",
      "Iteration 14929, loss = 0.01624554\n",
      "Iteration 14930, loss = 0.01624410\n",
      "Iteration 14931, loss = 0.01624265\n",
      "Iteration 14932, loss = 0.01624121\n",
      "Iteration 14933, loss = 0.01623976\n",
      "Iteration 14934, loss = 0.01623832\n",
      "Iteration 14935, loss = 0.01623688\n",
      "Iteration 14936, loss = 0.01623543\n",
      "Iteration 14937, loss = 0.01623399\n",
      "Iteration 14938, loss = 0.01623255\n",
      "Iteration 14939, loss = 0.01623111\n",
      "Iteration 14940, loss = 0.01622967\n",
      "Iteration 14941, loss = 0.01622823\n",
      "Iteration 14942, loss = 0.01622678\n",
      "Iteration 14943, loss = 0.01622534\n",
      "Iteration 14944, loss = 0.01622390\n",
      "Iteration 14945, loss = 0.01622246\n",
      "Iteration 14946, loss = 0.01622102\n",
      "Iteration 14947, loss = 0.01621958\n",
      "Iteration 14948, loss = 0.01621814\n",
      "Iteration 14949, loss = 0.01621671\n",
      "Iteration 14950, loss = 0.01621527\n",
      "Iteration 14951, loss = 0.01621383\n",
      "Iteration 14952, loss = 0.01621239\n",
      "Iteration 14953, loss = 0.01621095\n",
      "Iteration 14954, loss = 0.01620951\n",
      "Iteration 14955, loss = 0.01620808\n",
      "Iteration 14956, loss = 0.01620664\n",
      "Iteration 14957, loss = 0.01620520\n",
      "Iteration 14958, loss = 0.01620377\n",
      "Iteration 14959, loss = 0.01620233\n",
      "Iteration 14960, loss = 0.01620090\n",
      "Iteration 14961, loss = 0.01619946\n",
      "Iteration 14962, loss = 0.01619803\n",
      "Iteration 14963, loss = 0.01619659\n",
      "Iteration 14964, loss = 0.01619516\n",
      "Iteration 14965, loss = 0.01619372\n",
      "Iteration 14966, loss = 0.01619229\n",
      "Iteration 14967, loss = 0.01619085\n",
      "Iteration 14968, loss = 0.01618942\n",
      "Iteration 14969, loss = 0.01618799\n",
      "Iteration 14970, loss = 0.01618655\n",
      "Iteration 14971, loss = 0.01618512\n",
      "Iteration 14972, loss = 0.01618369\n",
      "Iteration 14973, loss = 0.01618226\n",
      "Iteration 14974, loss = 0.01618083\n",
      "Iteration 14975, loss = 0.01617939\n",
      "Iteration 14976, loss = 0.01617796\n",
      "Iteration 14977, loss = 0.01617653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14978, loss = 0.01617510\n",
      "Iteration 14979, loss = 0.01617367\n",
      "Iteration 14980, loss = 0.01617224\n",
      "Iteration 14981, loss = 0.01617081\n",
      "Iteration 14982, loss = 0.01616938\n",
      "Iteration 14983, loss = 0.01616795\n",
      "Iteration 14984, loss = 0.01616652\n",
      "Iteration 14985, loss = 0.01616510\n",
      "Iteration 14986, loss = 0.01616367\n",
      "Iteration 14987, loss = 0.01616224\n",
      "Iteration 14988, loss = 0.01616081\n",
      "Iteration 14989, loss = 0.01615938\n",
      "Iteration 14990, loss = 0.01615796\n",
      "Iteration 14991, loss = 0.01615653\n",
      "Iteration 14992, loss = 0.01615510\n",
      "Iteration 14993, loss = 0.01615368\n",
      "Iteration 14994, loss = 0.01615225\n",
      "Iteration 14995, loss = 0.01615083\n",
      "Iteration 14996, loss = 0.01614940\n",
      "Iteration 14997, loss = 0.01614798\n",
      "Iteration 14998, loss = 0.01614655\n",
      "Iteration 14999, loss = 0.01614513\n",
      "Iteration 15000, loss = 0.01614370\n",
      "Iteration 15001, loss = 0.01614228\n",
      "Iteration 15002, loss = 0.01614085\n",
      "Iteration 15003, loss = 0.01613943\n",
      "Iteration 15004, loss = 0.01613801\n",
      "Iteration 15005, loss = 0.01613658\n",
      "Iteration 15006, loss = 0.01613516\n",
      "Iteration 15007, loss = 0.01613374\n",
      "Iteration 15008, loss = 0.01613232\n",
      "Iteration 15009, loss = 0.01613090\n",
      "Iteration 15010, loss = 0.01612948\n",
      "Iteration 15011, loss = 0.01612805\n",
      "Iteration 15012, loss = 0.01612663\n",
      "Iteration 15013, loss = 0.01612521\n",
      "Iteration 15014, loss = 0.01612379\n",
      "Iteration 15015, loss = 0.01612237\n",
      "Iteration 15016, loss = 0.01612095\n",
      "Iteration 15017, loss = 0.01611953\n",
      "Iteration 15018, loss = 0.01611811\n",
      "Iteration 15019, loss = 0.01611670\n",
      "Iteration 15020, loss = 0.01611528\n",
      "Iteration 15021, loss = 0.01611386\n",
      "Iteration 15022, loss = 0.01611244\n",
      "Iteration 15023, loss = 0.01611102\n",
      "Iteration 15024, loss = 0.01610961\n",
      "Iteration 15025, loss = 0.01610819\n",
      "Iteration 15026, loss = 0.01610677\n",
      "Iteration 15027, loss = 0.01610536\n",
      "Iteration 15028, loss = 0.01610394\n",
      "Iteration 15029, loss = 0.01610252\n",
      "Iteration 15030, loss = 0.01610111\n",
      "Iteration 15031, loss = 0.01609969\n",
      "Iteration 15032, loss = 0.01609828\n",
      "Iteration 15033, loss = 0.01609686\n",
      "Iteration 15034, loss = 0.01609545\n",
      "Iteration 15035, loss = 0.01609403\n",
      "Iteration 15036, loss = 0.01609262\n",
      "Iteration 15037, loss = 0.01609121\n",
      "Iteration 15038, loss = 0.01608979\n",
      "Iteration 15039, loss = 0.01608838\n",
      "Iteration 15040, loss = 0.01608697\n",
      "Iteration 15041, loss = 0.01608555\n",
      "Iteration 15042, loss = 0.01608414\n",
      "Iteration 15043, loss = 0.01608273\n",
      "Iteration 15044, loss = 0.01608132\n",
      "Iteration 15045, loss = 0.01607991\n",
      "Iteration 15046, loss = 0.01607849\n",
      "Iteration 15047, loss = 0.01607708\n",
      "Iteration 15048, loss = 0.01607567\n",
      "Iteration 15049, loss = 0.01607426\n",
      "Iteration 15050, loss = 0.01607285\n",
      "Iteration 15051, loss = 0.01607144\n",
      "Iteration 15052, loss = 0.01607003\n",
      "Iteration 15053, loss = 0.01606862\n",
      "Iteration 15054, loss = 0.01606722\n",
      "Iteration 15055, loss = 0.01606581\n",
      "Iteration 15056, loss = 0.01606440\n",
      "Iteration 15057, loss = 0.01606299\n",
      "Iteration 15058, loss = 0.01606158\n",
      "Iteration 15059, loss = 0.01606017\n",
      "Iteration 15060, loss = 0.01605877\n",
      "Iteration 15061, loss = 0.01605736\n",
      "Iteration 15062, loss = 0.01605595\n",
      "Iteration 15063, loss = 0.01605455\n",
      "Iteration 15064, loss = 0.01605314\n",
      "Iteration 15065, loss = 0.01605174\n",
      "Iteration 15066, loss = 0.01605033\n",
      "Iteration 15067, loss = 0.01604892\n",
      "Iteration 15068, loss = 0.01604752\n",
      "Iteration 15069, loss = 0.01604612\n",
      "Iteration 15070, loss = 0.01604471\n",
      "Iteration 15071, loss = 0.01604331\n",
      "Iteration 15072, loss = 0.01604190\n",
      "Iteration 15073, loss = 0.01604050\n",
      "Iteration 15074, loss = 0.01603910\n",
      "Iteration 15075, loss = 0.01603769\n",
      "Iteration 15076, loss = 0.01603629\n",
      "Iteration 15077, loss = 0.01603489\n",
      "Iteration 15078, loss = 0.01603349\n",
      "Iteration 15079, loss = 0.01603208\n",
      "Iteration 15080, loss = 0.01603068\n",
      "Iteration 15081, loss = 0.01602928\n",
      "Iteration 15082, loss = 0.01602788\n",
      "Iteration 15083, loss = 0.01602648\n",
      "Iteration 15084, loss = 0.01602508\n",
      "Iteration 15085, loss = 0.01602368\n",
      "Iteration 15086, loss = 0.01602228\n",
      "Iteration 15087, loss = 0.01602088\n",
      "Iteration 15088, loss = 0.01601948\n",
      "Iteration 15089, loss = 0.01601808\n",
      "Iteration 15090, loss = 0.01601668\n",
      "Iteration 15091, loss = 0.01601528\n",
      "Iteration 15092, loss = 0.01601388\n",
      "Iteration 15093, loss = 0.01601249\n",
      "Iteration 15094, loss = 0.01601109\n",
      "Iteration 15095, loss = 0.01600969\n",
      "Iteration 15096, loss = 0.01600829\n",
      "Iteration 15097, loss = 0.01600690\n",
      "Iteration 15098, loss = 0.01600550\n",
      "Iteration 15099, loss = 0.01600410\n",
      "Iteration 15100, loss = 0.01600271\n",
      "Iteration 15101, loss = 0.01600131\n",
      "Iteration 15102, loss = 0.01599992\n",
      "Iteration 15103, loss = 0.01599852\n",
      "Iteration 15104, loss = 0.01599713\n",
      "Iteration 15105, loss = 0.01599573\n",
      "Iteration 15106, loss = 0.01599434\n",
      "Iteration 15107, loss = 0.01599294\n",
      "Iteration 15108, loss = 0.01599155\n",
      "Iteration 15109, loss = 0.01599016\n",
      "Iteration 15110, loss = 0.01598876\n",
      "Iteration 15111, loss = 0.01598737\n",
      "Iteration 15112, loss = 0.01598598\n",
      "Iteration 15113, loss = 0.01598459\n",
      "Iteration 15114, loss = 0.01598319\n",
      "Iteration 15115, loss = 0.01598180\n",
      "Iteration 15116, loss = 0.01598041\n",
      "Iteration 15117, loss = 0.01597902\n",
      "Iteration 15118, loss = 0.01597763\n",
      "Iteration 15119, loss = 0.01597624\n",
      "Iteration 15120, loss = 0.01597485\n",
      "Iteration 15121, loss = 0.01597346\n",
      "Iteration 15122, loss = 0.01597207\n",
      "Iteration 15123, loss = 0.01597068\n",
      "Iteration 15124, loss = 0.01596929\n",
      "Iteration 15125, loss = 0.01596790\n",
      "Iteration 15126, loss = 0.01596651\n",
      "Iteration 15127, loss = 0.01596512\n",
      "Iteration 15128, loss = 0.01596373\n",
      "Iteration 15129, loss = 0.01596234\n",
      "Iteration 15130, loss = 0.01596096\n",
      "Iteration 15131, loss = 0.01595957\n",
      "Iteration 15132, loss = 0.01595818\n",
      "Iteration 15133, loss = 0.01595680\n",
      "Iteration 15134, loss = 0.01595541\n",
      "Iteration 15135, loss = 0.01595402\n",
      "Iteration 15136, loss = 0.01595264\n",
      "Iteration 15137, loss = 0.01595125\n",
      "Iteration 15138, loss = 0.01594986\n",
      "Iteration 15139, loss = 0.01594848\n",
      "Iteration 15140, loss = 0.01594709\n",
      "Iteration 15141, loss = 0.01594571\n",
      "Iteration 15142, loss = 0.01594433\n",
      "Iteration 15143, loss = 0.01594294\n",
      "Iteration 15144, loss = 0.01594156\n",
      "Iteration 15145, loss = 0.01594017\n",
      "Iteration 15146, loss = 0.01593879\n",
      "Iteration 15147, loss = 0.01593741\n",
      "Iteration 15148, loss = 0.01593602\n",
      "Iteration 15149, loss = 0.01593464\n",
      "Iteration 15150, loss = 0.01593326\n",
      "Iteration 15151, loss = 0.01593188\n",
      "Iteration 15152, loss = 0.01593050\n",
      "Iteration 15153, loss = 0.01592911\n",
      "Iteration 15154, loss = 0.01592773\n",
      "Iteration 15155, loss = 0.01592635\n",
      "Iteration 15156, loss = 0.01592497\n",
      "Iteration 15157, loss = 0.01592359\n",
      "Iteration 15158, loss = 0.01592221\n",
      "Iteration 15159, loss = 0.01592083\n",
      "Iteration 15160, loss = 0.01591945\n",
      "Iteration 15161, loss = 0.01591807\n",
      "Iteration 15162, loss = 0.01591669\n",
      "Iteration 15163, loss = 0.01591532\n",
      "Iteration 15164, loss = 0.01591394\n",
      "Iteration 15165, loss = 0.01591256\n",
      "Iteration 15166, loss = 0.01591118\n",
      "Iteration 15167, loss = 0.01590980\n",
      "Iteration 15168, loss = 0.01590843\n",
      "Iteration 15169, loss = 0.01590705\n",
      "Iteration 15170, loss = 0.01590567\n",
      "Iteration 15171, loss = 0.01590430\n",
      "Iteration 15172, loss = 0.01590292\n",
      "Iteration 15173, loss = 0.01590154\n",
      "Iteration 15174, loss = 0.01590017\n",
      "Iteration 15175, loss = 0.01589879\n",
      "Iteration 15176, loss = 0.01589742\n",
      "Iteration 15177, loss = 0.01589604\n",
      "Iteration 15178, loss = 0.01589467\n",
      "Iteration 15179, loss = 0.01589329\n",
      "Iteration 15180, loss = 0.01589192\n",
      "Iteration 15181, loss = 0.01589055\n",
      "Iteration 15182, loss = 0.01588917\n",
      "Iteration 15183, loss = 0.01588780\n",
      "Iteration 15184, loss = 0.01588643\n",
      "Iteration 15185, loss = 0.01588505\n",
      "Iteration 15186, loss = 0.01588368\n",
      "Iteration 15187, loss = 0.01588231\n",
      "Iteration 15188, loss = 0.01588094\n",
      "Iteration 15189, loss = 0.01587957\n",
      "Iteration 15190, loss = 0.01587820\n",
      "Iteration 15191, loss = 0.01587682\n",
      "Iteration 15192, loss = 0.01587545\n",
      "Iteration 15193, loss = 0.01587408\n",
      "Iteration 15194, loss = 0.01587271\n",
      "Iteration 15195, loss = 0.01587134\n",
      "Iteration 15196, loss = 0.01586997\n",
      "Iteration 15197, loss = 0.01586860\n",
      "Iteration 15198, loss = 0.01586723\n",
      "Iteration 15199, loss = 0.01586587\n",
      "Iteration 15200, loss = 0.01586450\n",
      "Iteration 15201, loss = 0.01586313\n",
      "Iteration 15202, loss = 0.01586176\n",
      "Iteration 15203, loss = 0.01586039\n",
      "Iteration 15204, loss = 0.01585903\n",
      "Iteration 15205, loss = 0.01585766\n",
      "Iteration 15206, loss = 0.01585629\n",
      "Iteration 15207, loss = 0.01585492\n",
      "Iteration 15208, loss = 0.01585356\n",
      "Iteration 15209, loss = 0.01585219\n",
      "Iteration 15210, loss = 0.01585083\n",
      "Iteration 15211, loss = 0.01584946\n",
      "Iteration 15212, loss = 0.01584810\n",
      "Iteration 15213, loss = 0.01584673\n",
      "Iteration 15214, loss = 0.01584537\n",
      "Iteration 15215, loss = 0.01584400\n",
      "Iteration 15216, loss = 0.01584264\n",
      "Iteration 15217, loss = 0.01584127\n",
      "Iteration 15218, loss = 0.01583991\n",
      "Iteration 15219, loss = 0.01583855\n",
      "Iteration 15220, loss = 0.01583718\n",
      "Iteration 15221, loss = 0.01583582\n",
      "Iteration 15222, loss = 0.01583446\n",
      "Iteration 15223, loss = 0.01583309\n",
      "Iteration 15224, loss = 0.01583173\n",
      "Iteration 15225, loss = 0.01583037\n",
      "Iteration 15226, loss = 0.01582901\n",
      "Iteration 15227, loss = 0.01582765\n",
      "Iteration 15228, loss = 0.01582629\n",
      "Iteration 15229, loss = 0.01582493\n",
      "Iteration 15230, loss = 0.01582357\n",
      "Iteration 15231, loss = 0.01582220\n",
      "Iteration 15232, loss = 0.01582084\n",
      "Iteration 15233, loss = 0.01581949\n",
      "Iteration 15234, loss = 0.01581813\n",
      "Iteration 15235, loss = 0.01581677\n",
      "Iteration 15236, loss = 0.01581541\n",
      "Iteration 15237, loss = 0.01581405\n",
      "Iteration 15238, loss = 0.01581269\n",
      "Iteration 15239, loss = 0.01581133\n",
      "Iteration 15240, loss = 0.01580997\n",
      "Iteration 15241, loss = 0.01580862\n",
      "Iteration 15242, loss = 0.01580726\n",
      "Iteration 15243, loss = 0.01580590\n",
      "Iteration 15244, loss = 0.01580455\n",
      "Iteration 15245, loss = 0.01580319\n",
      "Iteration 15246, loss = 0.01580183\n",
      "Iteration 15247, loss = 0.01580048\n",
      "Iteration 15248, loss = 0.01579912\n",
      "Iteration 15249, loss = 0.01579777\n",
      "Iteration 15250, loss = 0.01579641\n",
      "Iteration 15251, loss = 0.01579506\n",
      "Iteration 15252, loss = 0.01579370\n",
      "Iteration 15253, loss = 0.01579235\n",
      "Iteration 15254, loss = 0.01579099\n",
      "Iteration 15255, loss = 0.01578964\n",
      "Iteration 15256, loss = 0.01578829\n",
      "Iteration 15257, loss = 0.01578693\n",
      "Iteration 15258, loss = 0.01578558\n",
      "Iteration 15259, loss = 0.01578423\n",
      "Iteration 15260, loss = 0.01578287\n",
      "Iteration 15261, loss = 0.01578152\n",
      "Iteration 15262, loss = 0.01578017\n",
      "Iteration 15263, loss = 0.01577882\n",
      "Iteration 15264, loss = 0.01577747\n",
      "Iteration 15265, loss = 0.01577612\n",
      "Iteration 15266, loss = 0.01577476\n",
      "Iteration 15267, loss = 0.01577341\n",
      "Iteration 15268, loss = 0.01577206\n",
      "Iteration 15269, loss = 0.01577071\n",
      "Iteration 15270, loss = 0.01576936\n",
      "Iteration 15271, loss = 0.01576801\n",
      "Iteration 15272, loss = 0.01576667\n",
      "Iteration 15273, loss = 0.01576532\n",
      "Iteration 15274, loss = 0.01576397\n",
      "Iteration 15275, loss = 0.01576262\n",
      "Iteration 15276, loss = 0.01576127\n",
      "Iteration 15277, loss = 0.01575992\n",
      "Iteration 15278, loss = 0.01575857\n",
      "Iteration 15279, loss = 0.01575723\n",
      "Iteration 15280, loss = 0.01575588\n",
      "Iteration 15281, loss = 0.01575453\n",
      "Iteration 15282, loss = 0.01575319\n",
      "Iteration 15283, loss = 0.01575184\n",
      "Iteration 15284, loss = 0.01575049\n",
      "Iteration 15285, loss = 0.01574915\n",
      "Iteration 15286, loss = 0.01574780\n",
      "Iteration 15287, loss = 0.01574646\n",
      "Iteration 15288, loss = 0.01574511\n",
      "Iteration 15289, loss = 0.01574377\n",
      "Iteration 15290, loss = 0.01574242\n",
      "Iteration 15291, loss = 0.01574108\n",
      "Iteration 15292, loss = 0.01573973\n",
      "Iteration 15293, loss = 0.01573839\n",
      "Iteration 15294, loss = 0.01573705\n",
      "Iteration 15295, loss = 0.01573570\n",
      "Iteration 15296, loss = 0.01573436\n",
      "Iteration 15297, loss = 0.01573302\n",
      "Iteration 15298, loss = 0.01573168\n",
      "Iteration 15299, loss = 0.01573033\n",
      "Iteration 15300, loss = 0.01572899\n",
      "Iteration 15301, loss = 0.01572765\n",
      "Iteration 15302, loss = 0.01572631\n",
      "Iteration 15303, loss = 0.01572497\n",
      "Iteration 15304, loss = 0.01572363\n",
      "Iteration 15305, loss = 0.01572229\n",
      "Iteration 15306, loss = 0.01572095\n",
      "Iteration 15307, loss = 0.01571961\n",
      "Iteration 15308, loss = 0.01571827\n",
      "Iteration 15309, loss = 0.01571693\n",
      "Iteration 15310, loss = 0.01571559\n",
      "Iteration 15311, loss = 0.01571425\n",
      "Iteration 15312, loss = 0.01571291\n",
      "Iteration 15313, loss = 0.01571157\n",
      "Iteration 15314, loss = 0.01571023\n",
      "Iteration 15315, loss = 0.01570890\n",
      "Iteration 15316, loss = 0.01570756\n",
      "Iteration 15317, loss = 0.01570622\n",
      "Iteration 15318, loss = 0.01570488\n",
      "Iteration 15319, loss = 0.01570355\n",
      "Iteration 15320, loss = 0.01570221\n",
      "Iteration 15321, loss = 0.01570087\n",
      "Iteration 15322, loss = 0.01569954\n",
      "Iteration 15323, loss = 0.01569820\n",
      "Iteration 15324, loss = 0.01569687\n",
      "Iteration 15325, loss = 0.01569553\n",
      "Iteration 15326, loss = 0.01569419\n",
      "Iteration 15327, loss = 0.01569286\n",
      "Iteration 15328, loss = 0.01569153\n",
      "Iteration 15329, loss = 0.01569019\n",
      "Iteration 15330, loss = 0.01568886\n",
      "Iteration 15331, loss = 0.01568752\n",
      "Iteration 15332, loss = 0.01568619\n",
      "Iteration 15333, loss = 0.01568486\n",
      "Iteration 15334, loss = 0.01568352\n",
      "Iteration 15335, loss = 0.01568219\n",
      "Iteration 15336, loss = 0.01568086\n",
      "Iteration 15337, loss = 0.01567953\n",
      "Iteration 15338, loss = 0.01567819\n",
      "Iteration 15339, loss = 0.01567686\n",
      "Iteration 15340, loss = 0.01567553\n",
      "Iteration 15341, loss = 0.01567420\n",
      "Iteration 15342, loss = 0.01567287\n",
      "Iteration 15343, loss = 0.01567154\n",
      "Iteration 15344, loss = 0.01567021\n",
      "Iteration 15345, loss = 0.01566888\n",
      "Iteration 15346, loss = 0.01566755\n",
      "Iteration 15347, loss = 0.01566622\n",
      "Iteration 15348, loss = 0.01566489\n",
      "Iteration 15349, loss = 0.01566356\n",
      "Iteration 15350, loss = 0.01566223\n",
      "Iteration 15351, loss = 0.01566090\n",
      "Iteration 15352, loss = 0.01565957\n",
      "Iteration 15353, loss = 0.01565825\n",
      "Iteration 15354, loss = 0.01565692\n",
      "Iteration 15355, loss = 0.01565559\n",
      "Iteration 15356, loss = 0.01565426\n",
      "Iteration 15357, loss = 0.01565294\n",
      "Iteration 15358, loss = 0.01565161\n",
      "Iteration 15359, loss = 0.01565028\n",
      "Iteration 15360, loss = 0.01564896\n",
      "Iteration 15361, loss = 0.01564763\n",
      "Iteration 15362, loss = 0.01564631\n",
      "Iteration 15363, loss = 0.01564498\n",
      "Iteration 15364, loss = 0.01564365\n",
      "Iteration 15365, loss = 0.01564233\n",
      "Iteration 15366, loss = 0.01564101\n",
      "Iteration 15367, loss = 0.01563968\n",
      "Iteration 15368, loss = 0.01563836\n",
      "Iteration 15369, loss = 0.01563703\n",
      "Iteration 15370, loss = 0.01563571\n",
      "Iteration 15371, loss = 0.01563439\n",
      "Iteration 15372, loss = 0.01563306\n",
      "Iteration 15373, loss = 0.01563174\n",
      "Iteration 15374, loss = 0.01563042\n",
      "Iteration 15375, loss = 0.01562909\n",
      "Iteration 15376, loss = 0.01562777\n",
      "Iteration 15377, loss = 0.01562645\n",
      "Iteration 15378, loss = 0.01562513\n",
      "Iteration 15379, loss = 0.01562381\n",
      "Iteration 15380, loss = 0.01562249\n",
      "Iteration 15381, loss = 0.01562117\n",
      "Iteration 15382, loss = 0.01561985\n",
      "Iteration 15383, loss = 0.01561853\n",
      "Iteration 15384, loss = 0.01561720\n",
      "Iteration 15385, loss = 0.01561589\n",
      "Iteration 15386, loss = 0.01561457\n",
      "Iteration 15387, loss = 0.01561325\n",
      "Iteration 15388, loss = 0.01561193\n",
      "Iteration 15389, loss = 0.01561061\n",
      "Iteration 15390, loss = 0.01560929\n",
      "Iteration 15391, loss = 0.01560797\n",
      "Iteration 15392, loss = 0.01560665\n",
      "Iteration 15393, loss = 0.01560534\n",
      "Iteration 15394, loss = 0.01560402\n",
      "Iteration 15395, loss = 0.01560270\n",
      "Iteration 15396, loss = 0.01560138\n",
      "Iteration 15397, loss = 0.01560007\n",
      "Iteration 15398, loss = 0.01559875\n",
      "Iteration 15399, loss = 0.01559744\n",
      "Iteration 15400, loss = 0.01559612\n",
      "Iteration 15401, loss = 0.01559480\n",
      "Iteration 15402, loss = 0.01559349\n",
      "Iteration 15403, loss = 0.01559217\n",
      "Iteration 15404, loss = 0.01559086\n",
      "Iteration 15405, loss = 0.01558954\n",
      "Iteration 15406, loss = 0.01558823\n",
      "Iteration 15407, loss = 0.01558691\n",
      "Iteration 15408, loss = 0.01558560\n",
      "Iteration 15409, loss = 0.01558429\n",
      "Iteration 15410, loss = 0.01558297\n",
      "Iteration 15411, loss = 0.01558166\n",
      "Iteration 15412, loss = 0.01558035\n",
      "Iteration 15413, loss = 0.01557904\n",
      "Iteration 15414, loss = 0.01557772\n",
      "Iteration 15415, loss = 0.01557641\n",
      "Iteration 15416, loss = 0.01557510\n",
      "Iteration 15417, loss = 0.01557379\n",
      "Iteration 15418, loss = 0.01557248\n",
      "Iteration 15419, loss = 0.01557117\n",
      "Iteration 15420, loss = 0.01556985\n",
      "Iteration 15421, loss = 0.01556854\n",
      "Iteration 15422, loss = 0.01556723\n",
      "Iteration 15423, loss = 0.01556592\n",
      "Iteration 15424, loss = 0.01556461\n",
      "Iteration 15425, loss = 0.01556330\n",
      "Iteration 15426, loss = 0.01556199\n",
      "Iteration 15427, loss = 0.01556069\n",
      "Iteration 15428, loss = 0.01555938\n",
      "Iteration 15429, loss = 0.01555807\n",
      "Iteration 15430, loss = 0.01555676\n",
      "Iteration 15431, loss = 0.01555545\n",
      "Iteration 15432, loss = 0.01555414\n",
      "Iteration 15433, loss = 0.01555284\n",
      "Iteration 15434, loss = 0.01555153\n",
      "Iteration 15435, loss = 0.01555022\n",
      "Iteration 15436, loss = 0.01554892\n",
      "Iteration 15437, loss = 0.01554761\n",
      "Iteration 15438, loss = 0.01554630\n",
      "Iteration 15439, loss = 0.01554500\n",
      "Iteration 15440, loss = 0.01554369\n",
      "Iteration 15441, loss = 0.01554239\n",
      "Iteration 15442, loss = 0.01554108\n",
      "Iteration 15443, loss = 0.01553978\n",
      "Iteration 15444, loss = 0.01553847\n",
      "Iteration 15445, loss = 0.01553717\n",
      "Iteration 15446, loss = 0.01553586\n",
      "Iteration 15447, loss = 0.01553456\n",
      "Iteration 15448, loss = 0.01553325\n",
      "Iteration 15449, loss = 0.01553195\n",
      "Iteration 15450, loss = 0.01553065\n",
      "Iteration 15451, loss = 0.01552934\n",
      "Iteration 15452, loss = 0.01552804\n",
      "Iteration 15453, loss = 0.01552674\n",
      "Iteration 15454, loss = 0.01552544\n",
      "Iteration 15455, loss = 0.01552414\n",
      "Iteration 15456, loss = 0.01552283\n",
      "Iteration 15457, loss = 0.01552153\n",
      "Iteration 15458, loss = 0.01552023\n",
      "Iteration 15459, loss = 0.01551893\n",
      "Iteration 15460, loss = 0.01551763\n",
      "Iteration 15461, loss = 0.01551633\n",
      "Iteration 15462, loss = 0.01551503\n",
      "Iteration 15463, loss = 0.01551373\n",
      "Iteration 15464, loss = 0.01551243\n",
      "Iteration 15465, loss = 0.01551113\n",
      "Iteration 15466, loss = 0.01550983\n",
      "Iteration 15467, loss = 0.01550853\n",
      "Iteration 15468, loss = 0.01550723\n",
      "Iteration 15469, loss = 0.01550593\n",
      "Iteration 15470, loss = 0.01550464\n",
      "Iteration 15471, loss = 0.01550334\n",
      "Iteration 15472, loss = 0.01550204\n",
      "Iteration 15473, loss = 0.01550074\n",
      "Iteration 15474, loss = 0.01549945\n",
      "Iteration 15475, loss = 0.01549815\n",
      "Iteration 15476, loss = 0.01549685\n",
      "Iteration 15477, loss = 0.01549556\n",
      "Iteration 15478, loss = 0.01549426\n",
      "Iteration 15479, loss = 0.01549296\n",
      "Iteration 15480, loss = 0.01549167\n",
      "Iteration 15481, loss = 0.01549037\n",
      "Iteration 15482, loss = 0.01548908\n",
      "Iteration 15483, loss = 0.01548778\n",
      "Iteration 15484, loss = 0.01548649\n",
      "Iteration 15485, loss = 0.01548519\n",
      "Iteration 15486, loss = 0.01548390\n",
      "Iteration 15487, loss = 0.01548261\n",
      "Iteration 15488, loss = 0.01548131\n",
      "Iteration 15489, loss = 0.01548002\n",
      "Iteration 15490, loss = 0.01547873\n",
      "Iteration 15491, loss = 0.01547743\n",
      "Iteration 15492, loss = 0.01547614\n",
      "Iteration 15493, loss = 0.01547485\n",
      "Iteration 15494, loss = 0.01547356\n",
      "Iteration 15495, loss = 0.01547226\n",
      "Iteration 15496, loss = 0.01547097\n",
      "Iteration 15497, loss = 0.01546968\n",
      "Iteration 15498, loss = 0.01546839\n",
      "Iteration 15499, loss = 0.01546710\n",
      "Iteration 15500, loss = 0.01546581\n",
      "Iteration 15501, loss = 0.01546452\n",
      "Iteration 15502, loss = 0.01546323\n",
      "Iteration 15503, loss = 0.01546194\n",
      "Iteration 15504, loss = 0.01546065\n",
      "Iteration 15505, loss = 0.01545936\n",
      "Iteration 15506, loss = 0.01545807\n",
      "Iteration 15507, loss = 0.01545678\n",
      "Iteration 15508, loss = 0.01545549\n",
      "Iteration 15509, loss = 0.01545420\n",
      "Iteration 15510, loss = 0.01545291\n",
      "Iteration 15511, loss = 0.01545163\n",
      "Iteration 15512, loss = 0.01545034\n",
      "Iteration 15513, loss = 0.01544905\n",
      "Iteration 15514, loss = 0.01544776\n",
      "Iteration 15515, loss = 0.01544648\n",
      "Iteration 15516, loss = 0.01544519\n",
      "Iteration 15517, loss = 0.01544390\n",
      "Iteration 15518, loss = 0.01544262\n",
      "Iteration 15519, loss = 0.01544133\n",
      "Iteration 15520, loss = 0.01544005\n",
      "Iteration 15521, loss = 0.01543876\n",
      "Iteration 15522, loss = 0.01543748\n",
      "Iteration 15523, loss = 0.01543619\n",
      "Iteration 15524, loss = 0.01543491\n",
      "Iteration 15525, loss = 0.01543362\n",
      "Iteration 15526, loss = 0.01543234\n",
      "Iteration 15527, loss = 0.01543105\n",
      "Iteration 15528, loss = 0.01542977\n",
      "Iteration 15529, loss = 0.01542849\n",
      "Iteration 15530, loss = 0.01542720\n",
      "Iteration 15531, loss = 0.01542592\n",
      "Iteration 15532, loss = 0.01542464\n",
      "Iteration 15533, loss = 0.01542335\n",
      "Iteration 15534, loss = 0.01542207\n",
      "Iteration 15535, loss = 0.01542079\n",
      "Iteration 15536, loss = 0.01541951\n",
      "Iteration 15537, loss = 0.01541823\n",
      "Iteration 15538, loss = 0.01541695\n",
      "Iteration 15539, loss = 0.01541567\n",
      "Iteration 15540, loss = 0.01541438\n",
      "Iteration 15541, loss = 0.01541310\n",
      "Iteration 15542, loss = 0.01541182\n",
      "Iteration 15543, loss = 0.01541054\n",
      "Iteration 15544, loss = 0.01540926\n",
      "Iteration 15545, loss = 0.01540798\n",
      "Iteration 15546, loss = 0.01540671\n",
      "Iteration 15547, loss = 0.01540543\n",
      "Iteration 15548, loss = 0.01540415\n",
      "Iteration 15549, loss = 0.01540287\n",
      "Iteration 15550, loss = 0.01540159\n",
      "Iteration 15551, loss = 0.01540031\n",
      "Iteration 15552, loss = 0.01539903\n",
      "Iteration 15553, loss = 0.01539776\n",
      "Iteration 15554, loss = 0.01539648\n",
      "Iteration 15555, loss = 0.01539520\n",
      "Iteration 15556, loss = 0.01539393\n",
      "Iteration 15557, loss = 0.01539265\n",
      "Iteration 15558, loss = 0.01539137\n",
      "Iteration 15559, loss = 0.01539010\n",
      "Iteration 15560, loss = 0.01538882\n",
      "Iteration 15561, loss = 0.01538755\n",
      "Iteration 15562, loss = 0.01538627\n",
      "Iteration 15563, loss = 0.01538500\n",
      "Iteration 15564, loss = 0.01538372\n",
      "Iteration 15565, loss = 0.01538245\n",
      "Iteration 15566, loss = 0.01538117\n",
      "Iteration 15567, loss = 0.01537990\n",
      "Iteration 15568, loss = 0.01537862\n",
      "Iteration 15569, loss = 0.01537735\n",
      "Iteration 15570, loss = 0.01537608\n",
      "Iteration 15571, loss = 0.01537480\n",
      "Iteration 15572, loss = 0.01537353\n",
      "Iteration 15573, loss = 0.01537226\n",
      "Iteration 15574, loss = 0.01537099\n",
      "Iteration 15575, loss = 0.01536971\n",
      "Iteration 15576, loss = 0.01536844\n",
      "Iteration 15577, loss = 0.01536717\n",
      "Iteration 15578, loss = 0.01536590\n",
      "Iteration 15579, loss = 0.01536463\n",
      "Iteration 15580, loss = 0.01536336\n",
      "Iteration 15581, loss = 0.01536209\n",
      "Iteration 15582, loss = 0.01536081\n",
      "Iteration 15583, loss = 0.01535954\n",
      "Iteration 15584, loss = 0.01535827\n",
      "Iteration 15585, loss = 0.01535700\n",
      "Iteration 15586, loss = 0.01535574\n",
      "Iteration 15587, loss = 0.01535447\n",
      "Iteration 15588, loss = 0.01535320\n",
      "Iteration 15589, loss = 0.01535193\n",
      "Iteration 15590, loss = 0.01535066\n",
      "Iteration 15591, loss = 0.01534939\n",
      "Iteration 15592, loss = 0.01534812\n",
      "Iteration 15593, loss = 0.01534686\n",
      "Iteration 15594, loss = 0.01534559\n",
      "Iteration 15595, loss = 0.01534432\n",
      "Iteration 15596, loss = 0.01534305\n",
      "Iteration 15597, loss = 0.01534179\n",
      "Iteration 15598, loss = 0.01534052\n",
      "Iteration 15599, loss = 0.01533925\n",
      "Iteration 15600, loss = 0.01533799\n",
      "Iteration 15601, loss = 0.01533672\n",
      "Iteration 15602, loss = 0.01533546\n",
      "Iteration 15603, loss = 0.01533419\n",
      "Iteration 15604, loss = 0.01533293\n",
      "Iteration 15605, loss = 0.01533166\n",
      "Iteration 15606, loss = 0.01533040\n",
      "Iteration 15607, loss = 0.01532913\n",
      "Iteration 15608, loss = 0.01532787\n",
      "Iteration 15609, loss = 0.01532660\n",
      "Iteration 15610, loss = 0.01532534\n",
      "Iteration 15611, loss = 0.01532408\n",
      "Iteration 15612, loss = 0.01532281\n",
      "Iteration 15613, loss = 0.01532155\n",
      "Iteration 15614, loss = 0.01532029\n",
      "Iteration 15615, loss = 0.01531903\n",
      "Iteration 15616, loss = 0.01531776\n",
      "Iteration 15617, loss = 0.01531650\n",
      "Iteration 15618, loss = 0.01531524\n",
      "Iteration 15619, loss = 0.01531398\n",
      "Iteration 15620, loss = 0.01531272\n",
      "Iteration 15621, loss = 0.01531146\n",
      "Iteration 15622, loss = 0.01531020\n",
      "Iteration 15623, loss = 0.01530894\n",
      "Iteration 15624, loss = 0.01530767\n",
      "Iteration 15625, loss = 0.01530641\n",
      "Iteration 15626, loss = 0.01530516\n",
      "Iteration 15627, loss = 0.01530390\n",
      "Iteration 15628, loss = 0.01530264\n",
      "Iteration 15629, loss = 0.01530138\n",
      "Iteration 15630, loss = 0.01530012\n",
      "Iteration 15631, loss = 0.01529886\n",
      "Iteration 15632, loss = 0.01529760\n",
      "Iteration 15633, loss = 0.01529634\n",
      "Iteration 15634, loss = 0.01529509\n",
      "Iteration 15635, loss = 0.01529383\n",
      "Iteration 15636, loss = 0.01529257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15637, loss = 0.01529131\n",
      "Iteration 15638, loss = 0.01529006\n",
      "Iteration 15639, loss = 0.01528880\n",
      "Iteration 15640, loss = 0.01528754\n",
      "Iteration 15641, loss = 0.01528629\n",
      "Iteration 15642, loss = 0.01528503\n",
      "Iteration 15643, loss = 0.01528378\n",
      "Iteration 15644, loss = 0.01528252\n",
      "Iteration 15645, loss = 0.01528126\n",
      "Iteration 15646, loss = 0.01528001\n",
      "Iteration 15647, loss = 0.01527876\n",
      "Iteration 15648, loss = 0.01527750\n",
      "Iteration 15649, loss = 0.01527625\n",
      "Iteration 15650, loss = 0.01527499\n",
      "Iteration 15651, loss = 0.01527374\n",
      "Iteration 15652, loss = 0.01527249\n",
      "Iteration 15653, loss = 0.01527123\n",
      "Iteration 15654, loss = 0.01526998\n",
      "Iteration 15655, loss = 0.01526873\n",
      "Iteration 15656, loss = 0.01526747\n",
      "Iteration 15657, loss = 0.01526622\n",
      "Iteration 15658, loss = 0.01526497\n",
      "Iteration 15659, loss = 0.01526372\n",
      "Iteration 15660, loss = 0.01526247\n",
      "Iteration 15661, loss = 0.01526121\n",
      "Iteration 15662, loss = 0.01525996\n",
      "Iteration 15663, loss = 0.01525871\n",
      "Iteration 15664, loss = 0.01525746\n",
      "Iteration 15665, loss = 0.01525621\n",
      "Iteration 15666, loss = 0.01525496\n",
      "Iteration 15667, loss = 0.01525371\n",
      "Iteration 15668, loss = 0.01525246\n",
      "Iteration 15669, loss = 0.01525121\n",
      "Iteration 15670, loss = 0.01524996\n",
      "Iteration 15671, loss = 0.01524871\n",
      "Iteration 15672, loss = 0.01524746\n",
      "Iteration 15673, loss = 0.01524622\n",
      "Iteration 15674, loss = 0.01524497\n",
      "Iteration 15675, loss = 0.01524372\n",
      "Iteration 15676, loss = 0.01524247\n",
      "Iteration 15677, loss = 0.01524122\n",
      "Iteration 15678, loss = 0.01523998\n",
      "Iteration 15679, loss = 0.01523873\n",
      "Iteration 15680, loss = 0.01523748\n",
      "Iteration 15681, loss = 0.01523624\n",
      "Iteration 15682, loss = 0.01523499\n",
      "Iteration 15683, loss = 0.01523374\n",
      "Iteration 15684, loss = 0.01523250\n",
      "Iteration 15685, loss = 0.01523125\n",
      "Iteration 15686, loss = 0.01523001\n",
      "Iteration 15687, loss = 0.01522876\n",
      "Iteration 15688, loss = 0.01522752\n",
      "Iteration 15689, loss = 0.01522627\n",
      "Iteration 15690, loss = 0.01522503\n",
      "Iteration 15691, loss = 0.01522378\n",
      "Iteration 15692, loss = 0.01522254\n",
      "Iteration 15693, loss = 0.01522129\n",
      "Iteration 15694, loss = 0.01522005\n",
      "Iteration 15695, loss = 0.01521881\n",
      "Iteration 15696, loss = 0.01521756\n",
      "Iteration 15697, loss = 0.01521632\n",
      "Iteration 15698, loss = 0.01521508\n",
      "Iteration 15699, loss = 0.01521384\n",
      "Iteration 15700, loss = 0.01521259\n",
      "Iteration 15701, loss = 0.01521135\n",
      "Iteration 15702, loss = 0.01521011\n",
      "Iteration 15703, loss = 0.01520887\n",
      "Iteration 15704, loss = 0.01520763\n",
      "Iteration 15705, loss = 0.01520639\n",
      "Iteration 15706, loss = 0.01520515\n",
      "Iteration 15707, loss = 0.01520391\n",
      "Iteration 15708, loss = 0.01520267\n",
      "Iteration 15709, loss = 0.01520143\n",
      "Iteration 15710, loss = 0.01520019\n",
      "Iteration 15711, loss = 0.01519895\n",
      "Iteration 15712, loss = 0.01519771\n",
      "Iteration 15713, loss = 0.01519647\n",
      "Iteration 15714, loss = 0.01519523\n",
      "Iteration 15715, loss = 0.01519399\n",
      "Iteration 15716, loss = 0.01519275\n",
      "Iteration 15717, loss = 0.01519151\n",
      "Iteration 15718, loss = 0.01519028\n",
      "Iteration 15719, loss = 0.01518904\n",
      "Iteration 15720, loss = 0.01518780\n",
      "Iteration 15721, loss = 0.01518656\n",
      "Iteration 15722, loss = 0.01518533\n",
      "Iteration 15723, loss = 0.01518409\n",
      "Iteration 15724, loss = 0.01518285\n",
      "Iteration 15725, loss = 0.01518162\n",
      "Iteration 15726, loss = 0.01518038\n",
      "Iteration 15727, loss = 0.01517915\n",
      "Iteration 15728, loss = 0.01517791\n",
      "Iteration 15729, loss = 0.01517668\n",
      "Iteration 15730, loss = 0.01517544\n",
      "Iteration 15731, loss = 0.01517421\n",
      "Iteration 15732, loss = 0.01517297\n",
      "Iteration 15733, loss = 0.01517174\n",
      "Iteration 15734, loss = 0.01517050\n",
      "Iteration 15735, loss = 0.01516927\n",
      "Iteration 15736, loss = 0.01516803\n",
      "Iteration 15737, loss = 0.01516680\n",
      "Iteration 15738, loss = 0.01516557\n",
      "Iteration 15739, loss = 0.01516434\n",
      "Iteration 15740, loss = 0.01516310\n",
      "Iteration 15741, loss = 0.01516187\n",
      "Iteration 15742, loss = 0.01516064\n",
      "Iteration 15743, loss = 0.01515941\n",
      "Iteration 15744, loss = 0.01515817\n",
      "Iteration 15745, loss = 0.01515694\n",
      "Iteration 15746, loss = 0.01515571\n",
      "Iteration 15747, loss = 0.01515448\n",
      "Iteration 15748, loss = 0.01515325\n",
      "Iteration 15749, loss = 0.01515202\n",
      "Iteration 15750, loss = 0.01515079\n",
      "Iteration 15751, loss = 0.01514956\n",
      "Iteration 15752, loss = 0.01514833\n",
      "Iteration 15753, loss = 0.01514710\n",
      "Iteration 15754, loss = 0.01514587\n",
      "Iteration 15755, loss = 0.01514464\n",
      "Iteration 15756, loss = 0.01514341\n",
      "Iteration 15757, loss = 0.01514218\n",
      "Iteration 15758, loss = 0.01514095\n",
      "Iteration 15759, loss = 0.01513972\n",
      "Iteration 15760, loss = 0.01513850\n",
      "Iteration 15761, loss = 0.01513727\n",
      "Iteration 15762, loss = 0.01513604\n",
      "Iteration 15763, loss = 0.01513481\n",
      "Iteration 15764, loss = 0.01513359\n",
      "Iteration 15765, loss = 0.01513236\n",
      "Iteration 15766, loss = 0.01513113\n",
      "Iteration 15767, loss = 0.01512991\n",
      "Iteration 15768, loss = 0.01512868\n",
      "Iteration 15769, loss = 0.01512745\n",
      "Iteration 15770, loss = 0.01512623\n",
      "Iteration 15771, loss = 0.01512500\n",
      "Iteration 15772, loss = 0.01512378\n",
      "Iteration 15773, loss = 0.01512255\n",
      "Iteration 15774, loss = 0.01512133\n",
      "Iteration 15775, loss = 0.01512010\n",
      "Iteration 15776, loss = 0.01511888\n",
      "Iteration 15777, loss = 0.01511766\n",
      "Iteration 15778, loss = 0.01511643\n",
      "Iteration 15779, loss = 0.01511521\n",
      "Iteration 15780, loss = 0.01511398\n",
      "Iteration 15781, loss = 0.01511276\n",
      "Iteration 15782, loss = 0.01511154\n",
      "Iteration 15783, loss = 0.01511032\n",
      "Iteration 15784, loss = 0.01510909\n",
      "Iteration 15785, loss = 0.01510787\n",
      "Iteration 15786, loss = 0.01510665\n",
      "Iteration 15787, loss = 0.01510543\n",
      "Iteration 15788, loss = 0.01510420\n",
      "Iteration 15789, loss = 0.01510298\n",
      "Iteration 15790, loss = 0.01510176\n",
      "Iteration 15791, loss = 0.01510054\n",
      "Iteration 15792, loss = 0.01509932\n",
      "Iteration 15793, loss = 0.01509810\n",
      "Iteration 15794, loss = 0.01509688\n",
      "Iteration 15795, loss = 0.01509566\n",
      "Iteration 15796, loss = 0.01509444\n",
      "Iteration 15797, loss = 0.01509322\n",
      "Iteration 15798, loss = 0.01509200\n",
      "Iteration 15799, loss = 0.01509078\n",
      "Iteration 15800, loss = 0.01508956\n",
      "Iteration 15801, loss = 0.01508835\n",
      "Iteration 15802, loss = 0.01508713\n",
      "Iteration 15803, loss = 0.01508591\n",
      "Iteration 15804, loss = 0.01508469\n",
      "Iteration 15805, loss = 0.01508347\n",
      "Iteration 15806, loss = 0.01508226\n",
      "Iteration 15807, loss = 0.01508104\n",
      "Iteration 15808, loss = 0.01507982\n",
      "Iteration 15809, loss = 0.01507860\n",
      "Iteration 15810, loss = 0.01507739\n",
      "Iteration 15811, loss = 0.01507617\n",
      "Iteration 15812, loss = 0.01507496\n",
      "Iteration 15813, loss = 0.01507374\n",
      "Iteration 15814, loss = 0.01507252\n",
      "Iteration 15815, loss = 0.01507131\n",
      "Iteration 15816, loss = 0.01507009\n",
      "Iteration 15817, loss = 0.01506888\n",
      "Iteration 15818, loss = 0.01506766\n",
      "Iteration 15819, loss = 0.01506645\n",
      "Iteration 15820, loss = 0.01506524\n",
      "Iteration 15821, loss = 0.01506402\n",
      "Iteration 15822, loss = 0.01506281\n",
      "Iteration 15823, loss = 0.01506159\n",
      "Iteration 15824, loss = 0.01506038\n",
      "Iteration 15825, loss = 0.01505917\n",
      "Iteration 15826, loss = 0.01505796\n",
      "Iteration 15827, loss = 0.01505674\n",
      "Iteration 15828, loss = 0.01505553\n",
      "Iteration 15829, loss = 0.01505432\n",
      "Iteration 15830, loss = 0.01505311\n",
      "Iteration 15831, loss = 0.01505189\n",
      "Iteration 15832, loss = 0.01505068\n",
      "Iteration 15833, loss = 0.01504947\n",
      "Iteration 15834, loss = 0.01504826\n",
      "Iteration 15835, loss = 0.01504705\n",
      "Iteration 15836, loss = 0.01504584\n",
      "Iteration 15837, loss = 0.01504463\n",
      "Iteration 15838, loss = 0.01504342\n",
      "Iteration 15839, loss = 0.01504221\n",
      "Iteration 15840, loss = 0.01504100\n",
      "Iteration 15841, loss = 0.01503979\n",
      "Iteration 15842, loss = 0.01503858\n",
      "Iteration 15843, loss = 0.01503737\n",
      "Iteration 15844, loss = 0.01503616\n",
      "Iteration 15845, loss = 0.01503495\n",
      "Iteration 15846, loss = 0.01503375\n",
      "Iteration 15847, loss = 0.01503254\n",
      "Iteration 15848, loss = 0.01503133\n",
      "Iteration 15849, loss = 0.01503012\n",
      "Iteration 15850, loss = 0.01502891\n",
      "Iteration 15851, loss = 0.01502771\n",
      "Iteration 15852, loss = 0.01502650\n",
      "Iteration 15853, loss = 0.01502529\n",
      "Iteration 15854, loss = 0.01502409\n",
      "Iteration 15855, loss = 0.01502288\n",
      "Iteration 15856, loss = 0.01502168\n",
      "Iteration 15857, loss = 0.01502047\n",
      "Iteration 15858, loss = 0.01501926\n",
      "Iteration 15859, loss = 0.01501806\n",
      "Iteration 15860, loss = 0.01501685\n",
      "Iteration 15861, loss = 0.01501565\n",
      "Iteration 15862, loss = 0.01501444\n",
      "Iteration 15863, loss = 0.01501324\n",
      "Iteration 15864, loss = 0.01501203\n",
      "Iteration 15865, loss = 0.01501083\n",
      "Iteration 15866, loss = 0.01500963\n",
      "Iteration 15867, loss = 0.01500842\n",
      "Iteration 15868, loss = 0.01500722\n",
      "Iteration 15869, loss = 0.01500602\n",
      "Iteration 15870, loss = 0.01500481\n",
      "Iteration 15871, loss = 0.01500361\n",
      "Iteration 15872, loss = 0.01500241\n",
      "Iteration 15873, loss = 0.01500121\n",
      "Iteration 15874, loss = 0.01500001\n",
      "Iteration 15875, loss = 0.01499880\n",
      "Iteration 15876, loss = 0.01499760\n",
      "Iteration 15877, loss = 0.01499640\n",
      "Iteration 15878, loss = 0.01499520\n",
      "Iteration 15879, loss = 0.01499400\n",
      "Iteration 15880, loss = 0.01499280\n",
      "Iteration 15881, loss = 0.01499160\n",
      "Iteration 15882, loss = 0.01499040\n",
      "Iteration 15883, loss = 0.01498920\n",
      "Iteration 15884, loss = 0.01498800\n",
      "Iteration 15885, loss = 0.01498680\n",
      "Iteration 15886, loss = 0.01498560\n",
      "Iteration 15887, loss = 0.01498440\n",
      "Iteration 15888, loss = 0.01498320\n",
      "Iteration 15889, loss = 0.01498200\n",
      "Iteration 15890, loss = 0.01498080\n",
      "Iteration 15891, loss = 0.01497961\n",
      "Iteration 15892, loss = 0.01497841\n",
      "Iteration 15893, loss = 0.01497721\n",
      "Iteration 15894, loss = 0.01497601\n",
      "Iteration 15895, loss = 0.01497482\n",
      "Iteration 15896, loss = 0.01497362\n",
      "Iteration 15897, loss = 0.01497242\n",
      "Iteration 15898, loss = 0.01497123\n",
      "Iteration 15899, loss = 0.01497003\n",
      "Iteration 15900, loss = 0.01496883\n",
      "Iteration 15901, loss = 0.01496764\n",
      "Iteration 15902, loss = 0.01496644\n",
      "Iteration 15903, loss = 0.01496525\n",
      "Iteration 15904, loss = 0.01496405\n",
      "Iteration 15905, loss = 0.01496286\n",
      "Iteration 15906, loss = 0.01496166\n",
      "Iteration 15907, loss = 0.01496047\n",
      "Iteration 15908, loss = 0.01495927\n",
      "Iteration 15909, loss = 0.01495808\n",
      "Iteration 15910, loss = 0.01495688\n",
      "Iteration 15911, loss = 0.01495569\n",
      "Iteration 15912, loss = 0.01495450\n",
      "Iteration 15913, loss = 0.01495330\n",
      "Iteration 15914, loss = 0.01495211\n",
      "Iteration 15915, loss = 0.01495092\n",
      "Iteration 15916, loss = 0.01494973\n",
      "Iteration 15917, loss = 0.01494853\n",
      "Iteration 15918, loss = 0.01494734\n",
      "Iteration 15919, loss = 0.01494615\n",
      "Iteration 15920, loss = 0.01494496\n",
      "Iteration 15921, loss = 0.01494377\n",
      "Iteration 15922, loss = 0.01494257\n",
      "Iteration 15923, loss = 0.01494138\n",
      "Iteration 15924, loss = 0.01494019\n",
      "Iteration 15925, loss = 0.01493900\n",
      "Iteration 15926, loss = 0.01493781\n",
      "Iteration 15927, loss = 0.01493662\n",
      "Iteration 15928, loss = 0.01493543\n",
      "Iteration 15929, loss = 0.01493424\n",
      "Iteration 15930, loss = 0.01493305\n",
      "Iteration 15931, loss = 0.01493186\n",
      "Iteration 15932, loss = 0.01493068\n",
      "Iteration 15933, loss = 0.01492949\n",
      "Iteration 15934, loss = 0.01492830\n",
      "Iteration 15935, loss = 0.01492711\n",
      "Iteration 15936, loss = 0.01492592\n",
      "Iteration 15937, loss = 0.01492473\n",
      "Iteration 15938, loss = 0.01492355\n",
      "Iteration 15939, loss = 0.01492236\n",
      "Iteration 15940, loss = 0.01492117\n",
      "Iteration 15941, loss = 0.01491998\n",
      "Iteration 15942, loss = 0.01491880\n",
      "Iteration 15943, loss = 0.01491761\n",
      "Iteration 15944, loss = 0.01491642\n",
      "Iteration 15945, loss = 0.01491524\n",
      "Iteration 15946, loss = 0.01491405\n",
      "Iteration 15947, loss = 0.01491287\n",
      "Iteration 15948, loss = 0.01491168\n",
      "Iteration 15949, loss = 0.01491050\n",
      "Iteration 15950, loss = 0.01490931\n",
      "Iteration 15951, loss = 0.01490813\n",
      "Iteration 15952, loss = 0.01490694\n",
      "Iteration 15953, loss = 0.01490576\n",
      "Iteration 15954, loss = 0.01490457\n",
      "Iteration 15955, loss = 0.01490339\n",
      "Iteration 15956, loss = 0.01490221\n",
      "Iteration 15957, loss = 0.01490102\n",
      "Iteration 15958, loss = 0.01489984\n",
      "Iteration 15959, loss = 0.01489866\n",
      "Iteration 15960, loss = 0.01489747\n",
      "Iteration 15961, loss = 0.01489629\n",
      "Iteration 15962, loss = 0.01489511\n",
      "Iteration 15963, loss = 0.01489393\n",
      "Iteration 15964, loss = 0.01489274\n",
      "Iteration 15965, loss = 0.01489156\n",
      "Iteration 15966, loss = 0.01489038\n",
      "Iteration 15967, loss = 0.01488920\n",
      "Iteration 15968, loss = 0.01488802\n",
      "Iteration 15969, loss = 0.01488684\n",
      "Iteration 15970, loss = 0.01488566\n",
      "Iteration 15971, loss = 0.01488448\n",
      "Iteration 15972, loss = 0.01488330\n",
      "Iteration 15973, loss = 0.01488212\n",
      "Iteration 15974, loss = 0.01488094\n",
      "Iteration 15975, loss = 0.01487976\n",
      "Iteration 15976, loss = 0.01487858\n",
      "Iteration 15977, loss = 0.01487740\n",
      "Iteration 15978, loss = 0.01487622\n",
      "Iteration 15979, loss = 0.01487504\n",
      "Iteration 15980, loss = 0.01487386\n",
      "Iteration 15981, loss = 0.01487269\n",
      "Iteration 15982, loss = 0.01487151\n",
      "Iteration 15983, loss = 0.01487033\n",
      "Iteration 15984, loss = 0.01486915\n",
      "Iteration 15985, loss = 0.01486798\n",
      "Iteration 15986, loss = 0.01486680\n",
      "Iteration 15987, loss = 0.01486562\n",
      "Iteration 15988, loss = 0.01486444\n",
      "Iteration 15989, loss = 0.01486327\n",
      "Iteration 15990, loss = 0.01486209\n",
      "Iteration 15991, loss = 0.01486092\n",
      "Iteration 15992, loss = 0.01485974\n",
      "Iteration 15993, loss = 0.01485857\n",
      "Iteration 15994, loss = 0.01485739\n",
      "Iteration 15995, loss = 0.01485621\n",
      "Iteration 15996, loss = 0.01485504\n",
      "Iteration 15997, loss = 0.01485387\n",
      "Iteration 15998, loss = 0.01485269\n",
      "Iteration 15999, loss = 0.01485152\n",
      "Iteration 16000, loss = 0.01485034\n",
      "Iteration 16001, loss = 0.01484917\n",
      "Iteration 16002, loss = 0.01484800\n",
      "Iteration 16003, loss = 0.01484682\n",
      "Iteration 16004, loss = 0.01484565\n",
      "Iteration 16005, loss = 0.01484448\n",
      "Iteration 16006, loss = 0.01484330\n",
      "Iteration 16007, loss = 0.01484213\n",
      "Iteration 16008, loss = 0.01484096\n",
      "Iteration 16009, loss = 0.01483979\n",
      "Iteration 16010, loss = 0.01483861\n",
      "Iteration 16011, loss = 0.01483744\n",
      "Iteration 16012, loss = 0.01483627\n",
      "Iteration 16013, loss = 0.01483510\n",
      "Iteration 16014, loss = 0.01483393\n",
      "Iteration 16015, loss = 0.01483276\n",
      "Iteration 16016, loss = 0.01483159\n",
      "Iteration 16017, loss = 0.01483042\n",
      "Iteration 16018, loss = 0.01482925\n",
      "Iteration 16019, loss = 0.01482808\n",
      "Iteration 16020, loss = 0.01482691\n",
      "Iteration 16021, loss = 0.01482574\n",
      "Iteration 16022, loss = 0.01482457\n",
      "Iteration 16023, loss = 0.01482340\n",
      "Iteration 16024, loss = 0.01482223\n",
      "Iteration 16025, loss = 0.01482106\n",
      "Iteration 16026, loss = 0.01481989\n",
      "Iteration 16027, loss = 0.01481873\n",
      "Iteration 16028, loss = 0.01481756\n",
      "Iteration 16029, loss = 0.01481639\n",
      "Iteration 16030, loss = 0.01481522\n",
      "Iteration 16031, loss = 0.01481406\n",
      "Iteration 16032, loss = 0.01481289\n",
      "Iteration 16033, loss = 0.01481172\n",
      "Iteration 16034, loss = 0.01481056\n",
      "Iteration 16035, loss = 0.01480939\n",
      "Iteration 16036, loss = 0.01480822\n",
      "Iteration 16037, loss = 0.01480706\n",
      "Iteration 16038, loss = 0.01480589\n",
      "Iteration 16039, loss = 0.01480473\n",
      "Iteration 16040, loss = 0.01480356\n",
      "Iteration 16041, loss = 0.01480240\n",
      "Iteration 16042, loss = 0.01480123\n",
      "Iteration 16043, loss = 0.01480007\n",
      "Iteration 16044, loss = 0.01479890\n",
      "Iteration 16045, loss = 0.01479774\n",
      "Iteration 16046, loss = 0.01479657\n",
      "Iteration 16047, loss = 0.01479541\n",
      "Iteration 16048, loss = 0.01479425\n",
      "Iteration 16049, loss = 0.01479308\n",
      "Iteration 16050, loss = 0.01479192\n",
      "Iteration 16051, loss = 0.01479076\n",
      "Iteration 16052, loss = 0.01478959\n",
      "Iteration 16053, loss = 0.01478843\n",
      "Iteration 16054, loss = 0.01478727\n",
      "Iteration 16055, loss = 0.01478611\n",
      "Iteration 16056, loss = 0.01478494\n",
      "Iteration 16057, loss = 0.01478378\n",
      "Iteration 16058, loss = 0.01478262\n",
      "Iteration 16059, loss = 0.01478146\n",
      "Iteration 16060, loss = 0.01478030\n",
      "Iteration 16061, loss = 0.01477914\n",
      "Iteration 16062, loss = 0.01477798\n",
      "Iteration 16063, loss = 0.01477682\n",
      "Iteration 16064, loss = 0.01477566\n",
      "Iteration 16065, loss = 0.01477450\n",
      "Iteration 16066, loss = 0.01477334\n",
      "Iteration 16067, loss = 0.01477218\n",
      "Iteration 16068, loss = 0.01477102\n",
      "Iteration 16069, loss = 0.01476986\n",
      "Iteration 16070, loss = 0.01476870\n",
      "Iteration 16071, loss = 0.01476754\n",
      "Iteration 16072, loss = 0.01476638\n",
      "Iteration 16073, loss = 0.01476523\n",
      "Iteration 16074, loss = 0.01476407\n",
      "Iteration 16075, loss = 0.01476291\n",
      "Iteration 16076, loss = 0.01476175\n",
      "Iteration 16077, loss = 0.01476059\n",
      "Iteration 16078, loss = 0.01475944\n",
      "Iteration 16079, loss = 0.01475828\n",
      "Iteration 16080, loss = 0.01475712\n",
      "Iteration 16081, loss = 0.01475597\n",
      "Iteration 16082, loss = 0.01475481\n",
      "Iteration 16083, loss = 0.01475366\n",
      "Iteration 16084, loss = 0.01475250\n",
      "Iteration 16085, loss = 0.01475134\n",
      "Iteration 16086, loss = 0.01475019\n",
      "Iteration 16087, loss = 0.01474903\n",
      "Iteration 16088, loss = 0.01474788\n",
      "Iteration 16089, loss = 0.01474672\n",
      "Iteration 16090, loss = 0.01474557\n",
      "Iteration 16091, loss = 0.01474441\n",
      "Iteration 16092, loss = 0.01474326\n",
      "Iteration 16093, loss = 0.01474211\n",
      "Iteration 16094, loss = 0.01474095\n",
      "Iteration 16095, loss = 0.01473980\n",
      "Iteration 16096, loss = 0.01473865\n",
      "Iteration 16097, loss = 0.01473749\n",
      "Iteration 16098, loss = 0.01473634\n",
      "Iteration 16099, loss = 0.01473519\n",
      "Iteration 16100, loss = 0.01473403\n",
      "Iteration 16101, loss = 0.01473288\n",
      "Iteration 16102, loss = 0.01473173\n",
      "Iteration 16103, loss = 0.01473058\n",
      "Iteration 16104, loss = 0.01472943\n",
      "Iteration 16105, loss = 0.01472828\n",
      "Iteration 16106, loss = 0.01472712\n",
      "Iteration 16107, loss = 0.01472597\n",
      "Iteration 16108, loss = 0.01472482\n",
      "Iteration 16109, loss = 0.01472367\n",
      "Iteration 16110, loss = 0.01472252\n",
      "Iteration 16111, loss = 0.01472137\n",
      "Iteration 16112, loss = 0.01472022\n",
      "Iteration 16113, loss = 0.01471907\n",
      "Iteration 16114, loss = 0.01471792\n",
      "Iteration 16115, loss = 0.01471677\n",
      "Iteration 16116, loss = 0.01471562\n",
      "Iteration 16117, loss = 0.01471448\n",
      "Iteration 16118, loss = 0.01471333\n",
      "Iteration 16119, loss = 0.01471218\n",
      "Iteration 16120, loss = 0.01471103\n",
      "Iteration 16121, loss = 0.01470988\n",
      "Iteration 16122, loss = 0.01470873\n",
      "Iteration 16123, loss = 0.01470759\n",
      "Iteration 16124, loss = 0.01470644\n",
      "Iteration 16125, loss = 0.01470529\n",
      "Iteration 16126, loss = 0.01470414\n",
      "Iteration 16127, loss = 0.01470300\n",
      "Iteration 16128, loss = 0.01470185\n",
      "Iteration 16129, loss = 0.01470071\n",
      "Iteration 16130, loss = 0.01469956\n",
      "Iteration 16131, loss = 0.01469841\n",
      "Iteration 16132, loss = 0.01469727\n",
      "Iteration 16133, loss = 0.01469612\n",
      "Iteration 16134, loss = 0.01469498\n",
      "Iteration 16135, loss = 0.01469383\n",
      "Iteration 16136, loss = 0.01469269\n",
      "Iteration 16137, loss = 0.01469154\n",
      "Iteration 16138, loss = 0.01469040\n",
      "Iteration 16139, loss = 0.01468925\n",
      "Iteration 16140, loss = 0.01468811\n",
      "Iteration 16141, loss = 0.01468697\n",
      "Iteration 16142, loss = 0.01468582\n",
      "Iteration 16143, loss = 0.01468468\n",
      "Iteration 16144, loss = 0.01468354\n",
      "Iteration 16145, loss = 0.01468239\n",
      "Iteration 16146, loss = 0.01468125\n",
      "Iteration 16147, loss = 0.01468011\n",
      "Iteration 16148, loss = 0.01467897\n",
      "Iteration 16149, loss = 0.01467782\n",
      "Iteration 16150, loss = 0.01467668\n",
      "Iteration 16151, loss = 0.01467554\n",
      "Iteration 16152, loss = 0.01467440\n",
      "Iteration 16153, loss = 0.01467326\n",
      "Iteration 16154, loss = 0.01467212\n",
      "Iteration 16155, loss = 0.01467097\n",
      "Iteration 16156, loss = 0.01466983\n",
      "Iteration 16157, loss = 0.01466869\n",
      "Iteration 16158, loss = 0.01466755\n",
      "Iteration 16159, loss = 0.01466641\n",
      "Iteration 16160, loss = 0.01466527\n",
      "Iteration 16161, loss = 0.01466413\n",
      "Iteration 16162, loss = 0.01466299\n",
      "Iteration 16163, loss = 0.01466186\n",
      "Iteration 16164, loss = 0.01466072\n",
      "Iteration 16165, loss = 0.01465958\n",
      "Iteration 16166, loss = 0.01465844\n",
      "Iteration 16167, loss = 0.01465730\n",
      "Iteration 16168, loss = 0.01465616\n",
      "Iteration 16169, loss = 0.01465502\n",
      "Iteration 16170, loss = 0.01465389\n",
      "Iteration 16171, loss = 0.01465275\n",
      "Iteration 16172, loss = 0.01465161\n",
      "Iteration 16173, loss = 0.01465048\n",
      "Iteration 16174, loss = 0.01464934\n",
      "Iteration 16175, loss = 0.01464820\n",
      "Iteration 16176, loss = 0.01464707\n",
      "Iteration 16177, loss = 0.01464593\n",
      "Iteration 16178, loss = 0.01464479\n",
      "Iteration 16179, loss = 0.01464366\n",
      "Iteration 16180, loss = 0.01464252\n",
      "Iteration 16181, loss = 0.01464139\n",
      "Iteration 16182, loss = 0.01464025\n",
      "Iteration 16183, loss = 0.01463912\n",
      "Iteration 16184, loss = 0.01463798\n",
      "Iteration 16185, loss = 0.01463685\n",
      "Iteration 16186, loss = 0.01463571\n",
      "Iteration 16187, loss = 0.01463458\n",
      "Iteration 16188, loss = 0.01463344\n",
      "Iteration 16189, loss = 0.01463231\n",
      "Iteration 16190, loss = 0.01463118\n",
      "Iteration 16191, loss = 0.01463004\n",
      "Iteration 16192, loss = 0.01462891\n",
      "Iteration 16193, loss = 0.01462778\n",
      "Iteration 16194, loss = 0.01462664\n",
      "Iteration 16195, loss = 0.01462551\n",
      "Iteration 16196, loss = 0.01462438\n",
      "Iteration 16197, loss = 0.01462325\n",
      "Iteration 16198, loss = 0.01462212\n",
      "Iteration 16199, loss = 0.01462098\n",
      "Iteration 16200, loss = 0.01461985\n",
      "Iteration 16201, loss = 0.01461872\n",
      "Iteration 16202, loss = 0.01461759\n",
      "Iteration 16203, loss = 0.01461646\n",
      "Iteration 16204, loss = 0.01461533\n",
      "Iteration 16205, loss = 0.01461420\n",
      "Iteration 16206, loss = 0.01461307\n",
      "Iteration 16207, loss = 0.01461194\n",
      "Iteration 16208, loss = 0.01461081\n",
      "Iteration 16209, loss = 0.01460968\n",
      "Iteration 16210, loss = 0.01460855\n",
      "Iteration 16211, loss = 0.01460742\n",
      "Iteration 16212, loss = 0.01460629\n",
      "Iteration 16213, loss = 0.01460516\n",
      "Iteration 16214, loss = 0.01460403\n",
      "Iteration 16215, loss = 0.01460291\n",
      "Iteration 16216, loss = 0.01460178\n",
      "Iteration 16217, loss = 0.01460065\n",
      "Iteration 16218, loss = 0.01459952\n",
      "Iteration 16219, loss = 0.01459839\n",
      "Iteration 16220, loss = 0.01459727\n",
      "Iteration 16221, loss = 0.01459614\n",
      "Iteration 16222, loss = 0.01459501\n",
      "Iteration 16223, loss = 0.01459389\n",
      "Iteration 16224, loss = 0.01459276\n",
      "Iteration 16225, loss = 0.01459163\n",
      "Iteration 16226, loss = 0.01459051\n",
      "Iteration 16227, loss = 0.01458938\n",
      "Iteration 16228, loss = 0.01458826\n",
      "Iteration 16229, loss = 0.01458713\n",
      "Iteration 16230, loss = 0.01458600\n",
      "Iteration 16231, loss = 0.01458488\n",
      "Iteration 16232, loss = 0.01458375\n",
      "Iteration 16233, loss = 0.01458263\n",
      "Iteration 16234, loss = 0.01458151\n",
      "Iteration 16235, loss = 0.01458038\n",
      "Iteration 16236, loss = 0.01457926\n",
      "Iteration 16237, loss = 0.01457813\n",
      "Iteration 16238, loss = 0.01457701\n",
      "Iteration 16239, loss = 0.01457589\n",
      "Iteration 16240, loss = 0.01457476\n",
      "Iteration 16241, loss = 0.01457364\n",
      "Iteration 16242, loss = 0.01457252\n",
      "Iteration 16243, loss = 0.01457140\n",
      "Iteration 16244, loss = 0.01457027\n",
      "Iteration 16245, loss = 0.01456915\n",
      "Iteration 16246, loss = 0.01456803\n",
      "Iteration 16247, loss = 0.01456691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16248, loss = 0.01456579\n",
      "Iteration 16249, loss = 0.01456466\n",
      "Iteration 16250, loss = 0.01456354\n",
      "Iteration 16251, loss = 0.01456242\n",
      "Iteration 16252, loss = 0.01456130\n",
      "Iteration 16253, loss = 0.01456018\n",
      "Iteration 16254, loss = 0.01455906\n",
      "Iteration 16255, loss = 0.01455794\n",
      "Iteration 16256, loss = 0.01455682\n",
      "Iteration 16257, loss = 0.01455570\n",
      "Iteration 16258, loss = 0.01455458\n",
      "Iteration 16259, loss = 0.01455346\n",
      "Iteration 16260, loss = 0.01455234\n",
      "Iteration 16261, loss = 0.01455122\n",
      "Iteration 16262, loss = 0.01455010\n",
      "Iteration 16263, loss = 0.01454899\n",
      "Iteration 16264, loss = 0.01454787\n",
      "Iteration 16265, loss = 0.01454675\n",
      "Iteration 16266, loss = 0.01454563\n",
      "Iteration 16267, loss = 0.01454451\n",
      "Iteration 16268, loss = 0.01454340\n",
      "Iteration 16269, loss = 0.01454228\n",
      "Iteration 16270, loss = 0.01454116\n",
      "Iteration 16271, loss = 0.01454005\n",
      "Iteration 16272, loss = 0.01453893\n",
      "Iteration 16273, loss = 0.01453781\n",
      "Iteration 16274, loss = 0.01453670\n",
      "Iteration 16275, loss = 0.01453558\n",
      "Iteration 16276, loss = 0.01453446\n",
      "Iteration 16277, loss = 0.01453335\n",
      "Iteration 16278, loss = 0.01453223\n",
      "Iteration 16279, loss = 0.01453112\n",
      "Iteration 16280, loss = 0.01453000\n",
      "Iteration 16281, loss = 0.01452889\n",
      "Iteration 16282, loss = 0.01452777\n",
      "Iteration 16283, loss = 0.01452666\n",
      "Iteration 16284, loss = 0.01452554\n",
      "Iteration 16285, loss = 0.01452443\n",
      "Iteration 16286, loss = 0.01452332\n",
      "Iteration 16287, loss = 0.01452220\n",
      "Iteration 16288, loss = 0.01452109\n",
      "Iteration 16289, loss = 0.01451998\n",
      "Iteration 16290, loss = 0.01451886\n",
      "Iteration 16291, loss = 0.01451775\n",
      "Iteration 16292, loss = 0.01451664\n",
      "Iteration 16293, loss = 0.01451553\n",
      "Iteration 16294, loss = 0.01451441\n",
      "Iteration 16295, loss = 0.01451330\n",
      "Iteration 16296, loss = 0.01451219\n",
      "Iteration 16297, loss = 0.01451108\n",
      "Iteration 16298, loss = 0.01450997\n",
      "Iteration 16299, loss = 0.01450885\n",
      "Iteration 16300, loss = 0.01450774\n",
      "Iteration 16301, loss = 0.01450663\n",
      "Iteration 16302, loss = 0.01450552\n",
      "Iteration 16303, loss = 0.01450441\n",
      "Iteration 16304, loss = 0.01450330\n",
      "Iteration 16305, loss = 0.01450219\n",
      "Iteration 16306, loss = 0.01450108\n",
      "Iteration 16307, loss = 0.01449997\n",
      "Iteration 16308, loss = 0.01449886\n",
      "Iteration 16309, loss = 0.01449775\n",
      "Iteration 16310, loss = 0.01449664\n",
      "Iteration 16311, loss = 0.01449554\n",
      "Iteration 16312, loss = 0.01449443\n",
      "Iteration 16313, loss = 0.01449332\n",
      "Iteration 16314, loss = 0.01449221\n",
      "Iteration 16315, loss = 0.01449110\n",
      "Iteration 16316, loss = 0.01449000\n",
      "Iteration 16317, loss = 0.01448889\n",
      "Iteration 16318, loss = 0.01448778\n",
      "Iteration 16319, loss = 0.01448667\n",
      "Iteration 16320, loss = 0.01448557\n",
      "Iteration 16321, loss = 0.01448446\n",
      "Iteration 16322, loss = 0.01448335\n",
      "Iteration 16323, loss = 0.01448225\n",
      "Iteration 16324, loss = 0.01448114\n",
      "Iteration 16325, loss = 0.01448003\n",
      "Iteration 16326, loss = 0.01447893\n",
      "Iteration 16327, loss = 0.01447782\n",
      "Iteration 16328, loss = 0.01447672\n",
      "Iteration 16329, loss = 0.01447561\n",
      "Iteration 16330, loss = 0.01447451\n",
      "Iteration 16331, loss = 0.01447340\n",
      "Iteration 16332, loss = 0.01447230\n",
      "Iteration 16333, loss = 0.01447119\n",
      "Iteration 16334, loss = 0.01447009\n",
      "Iteration 16335, loss = 0.01446899\n",
      "Iteration 16336, loss = 0.01446788\n",
      "Iteration 16337, loss = 0.01446678\n",
      "Iteration 16338, loss = 0.01446567\n",
      "Iteration 16339, loss = 0.01446457\n",
      "Iteration 16340, loss = 0.01446347\n",
      "Iteration 16341, loss = 0.01446237\n",
      "Iteration 16342, loss = 0.01446126\n",
      "Iteration 16343, loss = 0.01446016\n",
      "Iteration 16344, loss = 0.01445906\n",
      "Iteration 16345, loss = 0.01445796\n",
      "Iteration 16346, loss = 0.01445686\n",
      "Iteration 16347, loss = 0.01445575\n",
      "Iteration 16348, loss = 0.01445465\n",
      "Iteration 16349, loss = 0.01445355\n",
      "Iteration 16350, loss = 0.01445245\n",
      "Iteration 16351, loss = 0.01445135\n",
      "Iteration 16352, loss = 0.01445025\n",
      "Iteration 16353, loss = 0.01444915\n",
      "Iteration 16354, loss = 0.01444805\n",
      "Iteration 16355, loss = 0.01444695\n",
      "Iteration 16356, loss = 0.01444585\n",
      "Iteration 16357, loss = 0.01444475\n",
      "Iteration 16358, loss = 0.01444365\n",
      "Iteration 16359, loss = 0.01444255\n",
      "Iteration 16360, loss = 0.01444145\n",
      "Iteration 16361, loss = 0.01444035\n",
      "Iteration 16362, loss = 0.01443925\n",
      "Iteration 16363, loss = 0.01443816\n",
      "Iteration 16364, loss = 0.01443706\n",
      "Iteration 16365, loss = 0.01443596\n",
      "Iteration 16366, loss = 0.01443486\n",
      "Iteration 16367, loss = 0.01443376\n",
      "Iteration 16368, loss = 0.01443267\n",
      "Iteration 16369, loss = 0.01443157\n",
      "Iteration 16370, loss = 0.01443047\n",
      "Iteration 16371, loss = 0.01442938\n",
      "Iteration 16372, loss = 0.01442828\n",
      "Iteration 16373, loss = 0.01442718\n",
      "Iteration 16374, loss = 0.01442609\n",
      "Iteration 16375, loss = 0.01442499\n",
      "Iteration 16376, loss = 0.01442389\n",
      "Iteration 16377, loss = 0.01442280\n",
      "Iteration 16378, loss = 0.01442170\n",
      "Iteration 16379, loss = 0.01442061\n",
      "Iteration 16380, loss = 0.01441951\n",
      "Iteration 16381, loss = 0.01441842\n",
      "Iteration 16382, loss = 0.01441732\n",
      "Iteration 16383, loss = 0.01441623\n",
      "Iteration 16384, loss = 0.01441514\n",
      "Iteration 16385, loss = 0.01441404\n",
      "Iteration 16386, loss = 0.01441295\n",
      "Iteration 16387, loss = 0.01441185\n",
      "Iteration 16388, loss = 0.01441076\n",
      "Iteration 16389, loss = 0.01440967\n",
      "Iteration 16390, loss = 0.01440857\n",
      "Iteration 16391, loss = 0.01440748\n",
      "Iteration 16392, loss = 0.01440639\n",
      "Iteration 16393, loss = 0.01440530\n",
      "Iteration 16394, loss = 0.01440420\n",
      "Iteration 16395, loss = 0.01440311\n",
      "Iteration 16396, loss = 0.01440202\n",
      "Iteration 16397, loss = 0.01440093\n",
      "Iteration 16398, loss = 0.01439984\n",
      "Iteration 16399, loss = 0.01439875\n",
      "Iteration 16400, loss = 0.01439765\n",
      "Iteration 16401, loss = 0.01439656\n",
      "Iteration 16402, loss = 0.01439547\n",
      "Iteration 16403, loss = 0.01439438\n",
      "Iteration 16404, loss = 0.01439329\n",
      "Iteration 16405, loss = 0.01439220\n",
      "Iteration 16406, loss = 0.01439111\n",
      "Iteration 16407, loss = 0.01439002\n",
      "Iteration 16408, loss = 0.01438893\n",
      "Iteration 16409, loss = 0.01438784\n",
      "Iteration 16410, loss = 0.01438675\n",
      "Iteration 16411, loss = 0.01438567\n",
      "Iteration 16412, loss = 0.01438458\n",
      "Iteration 16413, loss = 0.01438349\n",
      "Iteration 16414, loss = 0.01438240\n",
      "Iteration 16415, loss = 0.01438131\n",
      "Iteration 16416, loss = 0.01438022\n",
      "Iteration 16417, loss = 0.01437914\n",
      "Iteration 16418, loss = 0.01437805\n",
      "Iteration 16419, loss = 0.01437696\n",
      "Iteration 16420, loss = 0.01437587\n",
      "Iteration 16421, loss = 0.01437479\n",
      "Iteration 16422, loss = 0.01437370\n",
      "Iteration 16423, loss = 0.01437261\n",
      "Iteration 16424, loss = 0.01437153\n",
      "Iteration 16425, loss = 0.01437044\n",
      "Iteration 16426, loss = 0.01436936\n",
      "Iteration 16427, loss = 0.01436827\n",
      "Iteration 16428, loss = 0.01436718\n",
      "Iteration 16429, loss = 0.01436610\n",
      "Iteration 16430, loss = 0.01436501\n",
      "Iteration 16431, loss = 0.01436393\n",
      "Iteration 16432, loss = 0.01436284\n",
      "Iteration 16433, loss = 0.01436176\n",
      "Iteration 16434, loss = 0.01436068\n",
      "Iteration 16435, loss = 0.01435959\n",
      "Iteration 16436, loss = 0.01435851\n",
      "Iteration 16437, loss = 0.01435742\n",
      "Iteration 16438, loss = 0.01435634\n",
      "Iteration 16439, loss = 0.01435526\n",
      "Iteration 16440, loss = 0.01435417\n",
      "Iteration 16441, loss = 0.01435309\n",
      "Iteration 16442, loss = 0.01435201\n",
      "Iteration 16443, loss = 0.01435092\n",
      "Iteration 16444, loss = 0.01434984\n",
      "Iteration 16445, loss = 0.01434876\n",
      "Iteration 16446, loss = 0.01434768\n",
      "Iteration 16447, loss = 0.01434660\n",
      "Iteration 16448, loss = 0.01434551\n",
      "Iteration 16449, loss = 0.01434443\n",
      "Iteration 16450, loss = 0.01434335\n",
      "Iteration 16451, loss = 0.01434227\n",
      "Iteration 16452, loss = 0.01434119\n",
      "Iteration 16453, loss = 0.01434011\n",
      "Iteration 16454, loss = 0.01433903\n",
      "Iteration 16455, loss = 0.01433795\n",
      "Iteration 16456, loss = 0.01433687\n",
      "Iteration 16457, loss = 0.01433579\n",
      "Iteration 16458, loss = 0.01433471\n",
      "Iteration 16459, loss = 0.01433363\n",
      "Iteration 16460, loss = 0.01433255\n",
      "Iteration 16461, loss = 0.01433147\n",
      "Iteration 16462, loss = 0.01433039\n",
      "Iteration 16463, loss = 0.01432931\n",
      "Iteration 16464, loss = 0.01432823\n",
      "Iteration 16465, loss = 0.01432716\n",
      "Iteration 16466, loss = 0.01432608\n",
      "Iteration 16467, loss = 0.01432500\n",
      "Iteration 16468, loss = 0.01432392\n",
      "Iteration 16469, loss = 0.01432284\n",
      "Iteration 16470, loss = 0.01432177\n",
      "Iteration 16471, loss = 0.01432069\n",
      "Iteration 16472, loss = 0.01431961\n",
      "Iteration 16473, loss = 0.01431854\n",
      "Iteration 16474, loss = 0.01431746\n",
      "Iteration 16475, loss = 0.01431638\n",
      "Iteration 16476, loss = 0.01431531\n",
      "Iteration 16477, loss = 0.01431423\n",
      "Iteration 16478, loss = 0.01431316\n",
      "Iteration 16479, loss = 0.01431208\n",
      "Iteration 16480, loss = 0.01431100\n",
      "Iteration 16481, loss = 0.01430993\n",
      "Iteration 16482, loss = 0.01430885\n",
      "Iteration 16483, loss = 0.01430778\n",
      "Iteration 16484, loss = 0.01430670\n",
      "Iteration 16485, loss = 0.01430563\n",
      "Iteration 16486, loss = 0.01430456\n",
      "Iteration 16487, loss = 0.01430348\n",
      "Iteration 16488, loss = 0.01430241\n",
      "Iteration 16489, loss = 0.01430133\n",
      "Iteration 16490, loss = 0.01430026\n",
      "Iteration 16491, loss = 0.01429919\n",
      "Iteration 16492, loss = 0.01429811\n",
      "Iteration 16493, loss = 0.01429704\n",
      "Iteration 16494, loss = 0.01429597\n",
      "Iteration 16495, loss = 0.01429490\n",
      "Iteration 16496, loss = 0.01429382\n",
      "Iteration 16497, loss = 0.01429275\n",
      "Iteration 16498, loss = 0.01429168\n",
      "Iteration 16499, loss = 0.01429061\n",
      "Iteration 16500, loss = 0.01428954\n",
      "Iteration 16501, loss = 0.01428846\n",
      "Iteration 16502, loss = 0.01428739\n",
      "Iteration 16503, loss = 0.01428632\n",
      "Iteration 16504, loss = 0.01428525\n",
      "Iteration 16505, loss = 0.01428418\n",
      "Iteration 16506, loss = 0.01428311\n",
      "Iteration 16507, loss = 0.01428204\n",
      "Iteration 16508, loss = 0.01428097\n",
      "Iteration 16509, loss = 0.01427990\n",
      "Iteration 16510, loss = 0.01427883\n",
      "Iteration 16511, loss = 0.01427776\n",
      "Iteration 16512, loss = 0.01427669\n",
      "Iteration 16513, loss = 0.01427562\n",
      "Iteration 16514, loss = 0.01427455\n",
      "Iteration 16515, loss = 0.01427348\n",
      "Iteration 16516, loss = 0.01427242\n",
      "Iteration 16517, loss = 0.01427135\n",
      "Iteration 16518, loss = 0.01427028\n",
      "Iteration 16519, loss = 0.01426921\n",
      "Iteration 16520, loss = 0.01426814\n",
      "Iteration 16521, loss = 0.01426708\n",
      "Iteration 16522, loss = 0.01426601\n",
      "Iteration 16523, loss = 0.01426494\n",
      "Iteration 16524, loss = 0.01426387\n",
      "Iteration 16525, loss = 0.01426281\n",
      "Iteration 16526, loss = 0.01426174\n",
      "Iteration 16527, loss = 0.01426067\n",
      "Iteration 16528, loss = 0.01425961\n",
      "Iteration 16529, loss = 0.01425854\n",
      "Iteration 16530, loss = 0.01425748\n",
      "Iteration 16531, loss = 0.01425641\n",
      "Iteration 16532, loss = 0.01425534\n",
      "Iteration 16533, loss = 0.01425428\n",
      "Iteration 16534, loss = 0.01425321\n",
      "Iteration 16535, loss = 0.01425215\n",
      "Iteration 16536, loss = 0.01425108\n",
      "Iteration 16537, loss = 0.01425002\n",
      "Iteration 16538, loss = 0.01424896\n",
      "Iteration 16539, loss = 0.01424789\n",
      "Iteration 16540, loss = 0.01424683\n",
      "Iteration 16541, loss = 0.01424576\n",
      "Iteration 16542, loss = 0.01424470\n",
      "Iteration 16543, loss = 0.01424364\n",
      "Iteration 16544, loss = 0.01424257\n",
      "Iteration 16545, loss = 0.01424151\n",
      "Iteration 16546, loss = 0.01424045\n",
      "Iteration 16547, loss = 0.01423939\n",
      "Iteration 16548, loss = 0.01423832\n",
      "Iteration 16549, loss = 0.01423726\n",
      "Iteration 16550, loss = 0.01423620\n",
      "Iteration 16551, loss = 0.01423514\n",
      "Iteration 16552, loss = 0.01423407\n",
      "Iteration 16553, loss = 0.01423301\n",
      "Iteration 16554, loss = 0.01423195\n",
      "Iteration 16555, loss = 0.01423089\n",
      "Iteration 16556, loss = 0.01422983\n",
      "Iteration 16557, loss = 0.01422877\n",
      "Iteration 16558, loss = 0.01422771\n",
      "Iteration 16559, loss = 0.01422665\n",
      "Iteration 16560, loss = 0.01422559\n",
      "Iteration 16561, loss = 0.01422453\n",
      "Iteration 16562, loss = 0.01422347\n",
      "Iteration 16563, loss = 0.01422241\n",
      "Iteration 16564, loss = 0.01422135\n",
      "Iteration 16565, loss = 0.01422029\n",
      "Iteration 16566, loss = 0.01421923\n",
      "Iteration 16567, loss = 0.01421817\n",
      "Iteration 16568, loss = 0.01421711\n",
      "Iteration 16569, loss = 0.01421606\n",
      "Iteration 16570, loss = 0.01421500\n",
      "Iteration 16571, loss = 0.01421394\n",
      "Iteration 16572, loss = 0.01421288\n",
      "Iteration 16573, loss = 0.01421182\n",
      "Iteration 16574, loss = 0.01421077\n",
      "Iteration 16575, loss = 0.01420971\n",
      "Iteration 16576, loss = 0.01420865\n",
      "Iteration 16577, loss = 0.01420759\n",
      "Iteration 16578, loss = 0.01420654\n",
      "Iteration 16579, loss = 0.01420548\n",
      "Iteration 16580, loss = 0.01420442\n",
      "Iteration 16581, loss = 0.01420337\n",
      "Iteration 16582, loss = 0.01420231\n",
      "Iteration 16583, loss = 0.01420126\n",
      "Iteration 16584, loss = 0.01420020\n",
      "Iteration 16585, loss = 0.01419915\n",
      "Iteration 16586, loss = 0.01419809\n",
      "Iteration 16587, loss = 0.01419703\n",
      "Iteration 16588, loss = 0.01419598\n",
      "Iteration 16589, loss = 0.01419493\n",
      "Iteration 16590, loss = 0.01419387\n",
      "Iteration 16591, loss = 0.01419282\n",
      "Iteration 16592, loss = 0.01419176\n",
      "Iteration 16593, loss = 0.01419071\n",
      "Iteration 16594, loss = 0.01418965\n",
      "Iteration 16595, loss = 0.01418860\n",
      "Iteration 16596, loss = 0.01418755\n",
      "Iteration 16597, loss = 0.01418649\n",
      "Iteration 16598, loss = 0.01418544\n",
      "Iteration 16599, loss = 0.01418439\n",
      "Iteration 16600, loss = 0.01418334\n",
      "Iteration 16601, loss = 0.01418228\n",
      "Iteration 16602, loss = 0.01418123\n",
      "Iteration 16603, loss = 0.01418018\n",
      "Iteration 16604, loss = 0.01417913\n",
      "Iteration 16605, loss = 0.01417808\n",
      "Iteration 16606, loss = 0.01417702\n",
      "Iteration 16607, loss = 0.01417597\n",
      "Iteration 16608, loss = 0.01417492\n",
      "Iteration 16609, loss = 0.01417387\n",
      "Iteration 16610, loss = 0.01417282\n",
      "Iteration 16611, loss = 0.01417177\n",
      "Iteration 16612, loss = 0.01417072\n",
      "Iteration 16613, loss = 0.01416967\n",
      "Iteration 16614, loss = 0.01416862\n",
      "Iteration 16615, loss = 0.01416757\n",
      "Iteration 16616, loss = 0.01416652\n",
      "Iteration 16617, loss = 0.01416547\n",
      "Iteration 16618, loss = 0.01416442\n",
      "Iteration 16619, loss = 0.01416337\n",
      "Iteration 16620, loss = 0.01416232\n",
      "Iteration 16621, loss = 0.01416127\n",
      "Iteration 16622, loss = 0.01416023\n",
      "Iteration 16623, loss = 0.01415918\n",
      "Iteration 16624, loss = 0.01415813\n",
      "Iteration 16625, loss = 0.01415708\n",
      "Iteration 16626, loss = 0.01415603\n",
      "Iteration 16627, loss = 0.01415499\n",
      "Iteration 16628, loss = 0.01415394\n",
      "Iteration 16629, loss = 0.01415289\n",
      "Iteration 16630, loss = 0.01415184\n",
      "Iteration 16631, loss = 0.01415080\n",
      "Iteration 16632, loss = 0.01414975\n",
      "Iteration 16633, loss = 0.01414870\n",
      "Iteration 16634, loss = 0.01414766\n",
      "Iteration 16635, loss = 0.01414661\n",
      "Iteration 16636, loss = 0.01414557\n",
      "Iteration 16637, loss = 0.01414452\n",
      "Iteration 16638, loss = 0.01414347\n",
      "Iteration 16639, loss = 0.01414243\n",
      "Iteration 16640, loss = 0.01414138\n",
      "Iteration 16641, loss = 0.01414034\n",
      "Iteration 16642, loss = 0.01413929\n",
      "Iteration 16643, loss = 0.01413825\n",
      "Iteration 16644, loss = 0.01413720\n",
      "Iteration 16645, loss = 0.01413616\n",
      "Iteration 16646, loss = 0.01413512\n",
      "Iteration 16647, loss = 0.01413407\n",
      "Iteration 16648, loss = 0.01413303\n",
      "Iteration 16649, loss = 0.01413198\n",
      "Iteration 16650, loss = 0.01413094\n",
      "Iteration 16651, loss = 0.01412990\n",
      "Iteration 16652, loss = 0.01412886\n",
      "Iteration 16653, loss = 0.01412781\n",
      "Iteration 16654, loss = 0.01412677\n",
      "Iteration 16655, loss = 0.01412573\n",
      "Iteration 16656, loss = 0.01412469\n",
      "Iteration 16657, loss = 0.01412364\n",
      "Iteration 16658, loss = 0.01412260\n",
      "Iteration 16659, loss = 0.01412156\n",
      "Iteration 16660, loss = 0.01412052\n",
      "Iteration 16661, loss = 0.01411948\n",
      "Iteration 16662, loss = 0.01411844\n",
      "Iteration 16663, loss = 0.01411740\n",
      "Iteration 16664, loss = 0.01411635\n",
      "Iteration 16665, loss = 0.01411531\n",
      "Iteration 16666, loss = 0.01411427\n",
      "Iteration 16667, loss = 0.01411323\n",
      "Iteration 16668, loss = 0.01411219\n",
      "Iteration 16669, loss = 0.01411115\n",
      "Iteration 16670, loss = 0.01411011\n",
      "Iteration 16671, loss = 0.01410907\n",
      "Iteration 16672, loss = 0.01410803\n",
      "Iteration 16673, loss = 0.01410700\n",
      "Iteration 16674, loss = 0.01410596\n",
      "Iteration 16675, loss = 0.01410492\n",
      "Iteration 16676, loss = 0.01410388\n",
      "Iteration 16677, loss = 0.01410284\n",
      "Iteration 16678, loss = 0.01410180\n",
      "Iteration 16679, loss = 0.01410077\n",
      "Iteration 16680, loss = 0.01409973\n",
      "Iteration 16681, loss = 0.01409869\n",
      "Iteration 16682, loss = 0.01409765\n",
      "Iteration 16683, loss = 0.01409662\n",
      "Iteration 16684, loss = 0.01409558\n",
      "Iteration 16685, loss = 0.01409454\n",
      "Iteration 16686, loss = 0.01409350\n",
      "Iteration 16687, loss = 0.01409247\n",
      "Iteration 16688, loss = 0.01409143\n",
      "Iteration 16689, loss = 0.01409040\n",
      "Iteration 16690, loss = 0.01408936\n",
      "Iteration 16691, loss = 0.01408832\n",
      "Iteration 16692, loss = 0.01408729\n",
      "Iteration 16693, loss = 0.01408625\n",
      "Iteration 16694, loss = 0.01408522\n",
      "Iteration 16695, loss = 0.01408418\n",
      "Iteration 16696, loss = 0.01408315\n",
      "Iteration 16697, loss = 0.01408211\n",
      "Iteration 16698, loss = 0.01408108\n",
      "Iteration 16699, loss = 0.01408004\n",
      "Iteration 16700, loss = 0.01407901\n",
      "Iteration 16701, loss = 0.01407798\n",
      "Iteration 16702, loss = 0.01407694\n",
      "Iteration 16703, loss = 0.01407591\n",
      "Iteration 16704, loss = 0.01407487\n",
      "Iteration 16705, loss = 0.01407384\n",
      "Iteration 16706, loss = 0.01407281\n",
      "Iteration 16707, loss = 0.01407178\n",
      "Iteration 16708, loss = 0.01407074\n",
      "Iteration 16709, loss = 0.01406971\n",
      "Iteration 16710, loss = 0.01406868\n",
      "Iteration 16711, loss = 0.01406765\n",
      "Iteration 16712, loss = 0.01406661\n",
      "Iteration 16713, loss = 0.01406558\n",
      "Iteration 16714, loss = 0.01406455\n",
      "Iteration 16715, loss = 0.01406352\n",
      "Iteration 16716, loss = 0.01406249\n",
      "Iteration 16717, loss = 0.01406146\n",
      "Iteration 16718, loss = 0.01406043\n",
      "Iteration 16719, loss = 0.01405940\n",
      "Iteration 16720, loss = 0.01405837\n",
      "Iteration 16721, loss = 0.01405733\n",
      "Iteration 16722, loss = 0.01405630\n",
      "Iteration 16723, loss = 0.01405527\n",
      "Iteration 16724, loss = 0.01405424\n",
      "Iteration 16725, loss = 0.01405322\n",
      "Iteration 16726, loss = 0.01405219\n",
      "Iteration 16727, loss = 0.01405116\n",
      "Iteration 16728, loss = 0.01405013\n",
      "Iteration 16729, loss = 0.01404910\n",
      "Iteration 16730, loss = 0.01404807\n",
      "Iteration 16731, loss = 0.01404704\n",
      "Iteration 16732, loss = 0.01404601\n",
      "Iteration 16733, loss = 0.01404499\n",
      "Iteration 16734, loss = 0.01404396\n",
      "Iteration 16735, loss = 0.01404293\n",
      "Iteration 16736, loss = 0.01404190\n",
      "Iteration 16737, loss = 0.01404087\n",
      "Iteration 16738, loss = 0.01403985\n",
      "Iteration 16739, loss = 0.01403882\n",
      "Iteration 16740, loss = 0.01403779\n",
      "Iteration 16741, loss = 0.01403677\n",
      "Iteration 16742, loss = 0.01403574\n",
      "Iteration 16743, loss = 0.01403471\n",
      "Iteration 16744, loss = 0.01403369\n",
      "Iteration 16745, loss = 0.01403266\n",
      "Iteration 16746, loss = 0.01403164\n",
      "Iteration 16747, loss = 0.01403061\n",
      "Iteration 16748, loss = 0.01402958\n",
      "Iteration 16749, loss = 0.01402856\n",
      "Iteration 16750, loss = 0.01402753\n",
      "Iteration 16751, loss = 0.01402651\n",
      "Iteration 16752, loss = 0.01402548\n",
      "Iteration 16753, loss = 0.01402446\n",
      "Iteration 16754, loss = 0.01402344\n",
      "Iteration 16755, loss = 0.01402241\n",
      "Iteration 16756, loss = 0.01402139\n",
      "Iteration 16757, loss = 0.01402036\n",
      "Iteration 16758, loss = 0.01401934\n",
      "Iteration 16759, loss = 0.01401832\n",
      "Iteration 16760, loss = 0.01401729\n",
      "Iteration 16761, loss = 0.01401627\n",
      "Iteration 16762, loss = 0.01401525\n",
      "Iteration 16763, loss = 0.01401423\n",
      "Iteration 16764, loss = 0.01401320\n",
      "Iteration 16765, loss = 0.01401218\n",
      "Iteration 16766, loss = 0.01401116\n",
      "Iteration 16767, loss = 0.01401014\n",
      "Iteration 16768, loss = 0.01400911\n",
      "Iteration 16769, loss = 0.01400809\n",
      "Iteration 16770, loss = 0.01400707\n",
      "Iteration 16771, loss = 0.01400605\n",
      "Iteration 16772, loss = 0.01400503\n",
      "Iteration 16773, loss = 0.01400401\n",
      "Iteration 16774, loss = 0.01400299\n",
      "Iteration 16775, loss = 0.01400197\n",
      "Iteration 16776, loss = 0.01400095\n",
      "Iteration 16777, loss = 0.01399993\n",
      "Iteration 16778, loss = 0.01399891\n",
      "Iteration 16779, loss = 0.01399789\n",
      "Iteration 16780, loss = 0.01399687\n",
      "Iteration 16781, loss = 0.01399585\n",
      "Iteration 16782, loss = 0.01399483\n",
      "Iteration 16783, loss = 0.01399381\n",
      "Iteration 16784, loss = 0.01399279\n",
      "Iteration 16785, loss = 0.01399177\n",
      "Iteration 16786, loss = 0.01399075\n",
      "Iteration 16787, loss = 0.01398973\n",
      "Iteration 16788, loss = 0.01398872\n",
      "Iteration 16789, loss = 0.01398770\n",
      "Iteration 16790, loss = 0.01398668\n",
      "Iteration 16791, loss = 0.01398566\n",
      "Iteration 16792, loss = 0.01398464\n",
      "Iteration 16793, loss = 0.01398363\n",
      "Iteration 16794, loss = 0.01398261\n",
      "Iteration 16795, loss = 0.01398159\n",
      "Iteration 16796, loss = 0.01398058\n",
      "Iteration 16797, loss = 0.01397956\n",
      "Iteration 16798, loss = 0.01397854\n",
      "Iteration 16799, loss = 0.01397753\n",
      "Iteration 16800, loss = 0.01397651\n",
      "Iteration 16801, loss = 0.01397549\n",
      "Iteration 16802, loss = 0.01397448\n",
      "Iteration 16803, loss = 0.01397346\n",
      "Iteration 16804, loss = 0.01397245\n",
      "Iteration 16805, loss = 0.01397143\n",
      "Iteration 16806, loss = 0.01397042\n",
      "Iteration 16807, loss = 0.01396940\n",
      "Iteration 16808, loss = 0.01396839\n",
      "Iteration 16809, loss = 0.01396737\n",
      "Iteration 16810, loss = 0.01396636\n",
      "Iteration 16811, loss = 0.01396535\n",
      "Iteration 16812, loss = 0.01396433\n",
      "Iteration 16813, loss = 0.01396332\n",
      "Iteration 16814, loss = 0.01396230\n",
      "Iteration 16815, loss = 0.01396129\n",
      "Iteration 16816, loss = 0.01396028\n",
      "Iteration 16817, loss = 0.01395926\n",
      "Iteration 16818, loss = 0.01395825\n",
      "Iteration 16819, loss = 0.01395724\n",
      "Iteration 16820, loss = 0.01395623\n",
      "Iteration 16821, loss = 0.01395521\n",
      "Iteration 16822, loss = 0.01395420\n",
      "Iteration 16823, loss = 0.01395319\n",
      "Iteration 16824, loss = 0.01395218\n",
      "Iteration 16825, loss = 0.01395117\n",
      "Iteration 16826, loss = 0.01395015\n",
      "Iteration 16827, loss = 0.01394914\n",
      "Iteration 16828, loss = 0.01394813\n",
      "Iteration 16829, loss = 0.01394712\n",
      "Iteration 16830, loss = 0.01394611\n",
      "Iteration 16831, loss = 0.01394510\n",
      "Iteration 16832, loss = 0.01394409\n",
      "Iteration 16833, loss = 0.01394308\n",
      "Iteration 16834, loss = 0.01394207\n",
      "Iteration 16835, loss = 0.01394106\n",
      "Iteration 16836, loss = 0.01394005\n",
      "Iteration 16837, loss = 0.01393904\n",
      "Iteration 16838, loss = 0.01393803\n",
      "Iteration 16839, loss = 0.01393702\n",
      "Iteration 16840, loss = 0.01393601\n",
      "Iteration 16841, loss = 0.01393500\n",
      "Iteration 16842, loss = 0.01393399\n",
      "Iteration 16843, loss = 0.01393299\n",
      "Iteration 16844, loss = 0.01393198\n",
      "Iteration 16845, loss = 0.01393097\n",
      "Iteration 16846, loss = 0.01392996\n",
      "Iteration 16847, loss = 0.01392895\n",
      "Iteration 16848, loss = 0.01392795\n",
      "Iteration 16849, loss = 0.01392694\n",
      "Iteration 16850, loss = 0.01392593\n",
      "Iteration 16851, loss = 0.01392492\n",
      "Iteration 16852, loss = 0.01392392\n",
      "Iteration 16853, loss = 0.01392291\n",
      "Iteration 16854, loss = 0.01392190\n",
      "Iteration 16855, loss = 0.01392090\n",
      "Iteration 16856, loss = 0.01391989\n",
      "Iteration 16857, loss = 0.01391889\n",
      "Iteration 16858, loss = 0.01391788\n",
      "Iteration 16859, loss = 0.01391687\n",
      "Iteration 16860, loss = 0.01391587\n",
      "Iteration 16861, loss = 0.01391486\n",
      "Iteration 16862, loss = 0.01391386\n",
      "Iteration 16863, loss = 0.01391285\n",
      "Iteration 16864, loss = 0.01391185\n",
      "Iteration 16865, loss = 0.01391084\n",
      "Iteration 16866, loss = 0.01390984\n",
      "Iteration 16867, loss = 0.01390884\n",
      "Iteration 16868, loss = 0.01390783\n",
      "Iteration 16869, loss = 0.01390683\n",
      "Iteration 16870, loss = 0.01390582\n",
      "Iteration 16871, loss = 0.01390482\n",
      "Iteration 16872, loss = 0.01390382\n",
      "Iteration 16873, loss = 0.01390281\n",
      "Iteration 16874, loss = 0.01390181\n",
      "Iteration 16875, loss = 0.01390081\n",
      "Iteration 16876, loss = 0.01389981\n",
      "Iteration 16877, loss = 0.01389880\n",
      "Iteration 16878, loss = 0.01389780\n",
      "Iteration 16879, loss = 0.01389680\n",
      "Iteration 16880, loss = 0.01389580\n",
      "Iteration 16881, loss = 0.01389479\n",
      "Iteration 16882, loss = 0.01389379\n",
      "Iteration 16883, loss = 0.01389279\n",
      "Iteration 16884, loss = 0.01389179\n",
      "Iteration 16885, loss = 0.01389079\n",
      "Iteration 16886, loss = 0.01388979\n",
      "Iteration 16887, loss = 0.01388879\n",
      "Iteration 16888, loss = 0.01388779\n",
      "Iteration 16889, loss = 0.01388679\n",
      "Iteration 16890, loss = 0.01388579\n",
      "Iteration 16891, loss = 0.01388479\n",
      "Iteration 16892, loss = 0.01388379\n",
      "Iteration 16893, loss = 0.01388279\n",
      "Iteration 16894, loss = 0.01388179\n",
      "Iteration 16895, loss = 0.01388079\n",
      "Iteration 16896, loss = 0.01387979\n",
      "Iteration 16897, loss = 0.01387879\n",
      "Iteration 16898, loss = 0.01387779\n",
      "Iteration 16899, loss = 0.01387679\n",
      "Iteration 16900, loss = 0.01387579\n",
      "Iteration 16901, loss = 0.01387480\n",
      "Iteration 16902, loss = 0.01387380\n",
      "Iteration 16903, loss = 0.01387280\n",
      "Iteration 16904, loss = 0.01387180\n",
      "Iteration 16905, loss = 0.01387080\n",
      "Iteration 16906, loss = 0.01386981\n",
      "Iteration 16907, loss = 0.01386881\n",
      "Iteration 16908, loss = 0.01386781\n",
      "Iteration 16909, loss = 0.01386681\n",
      "Iteration 16910, loss = 0.01386582\n",
      "Iteration 16911, loss = 0.01386482\n",
      "Iteration 16912, loss = 0.01386383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16913, loss = 0.01386283\n",
      "Iteration 16914, loss = 0.01386183\n",
      "Iteration 16915, loss = 0.01386084\n",
      "Iteration 16916, loss = 0.01385984\n",
      "Iteration 16917, loss = 0.01385885\n",
      "Iteration 16918, loss = 0.01385785\n",
      "Iteration 16919, loss = 0.01385686\n",
      "Iteration 16920, loss = 0.01385586\n",
      "Iteration 16921, loss = 0.01385487\n",
      "Iteration 16922, loss = 0.01385387\n",
      "Iteration 16923, loss = 0.01385288\n",
      "Iteration 16924, loss = 0.01385188\n",
      "Iteration 16925, loss = 0.01385089\n",
      "Iteration 16926, loss = 0.01384989\n",
      "Iteration 16927, loss = 0.01384890\n",
      "Iteration 16928, loss = 0.01384791\n",
      "Iteration 16929, loss = 0.01384691\n",
      "Iteration 16930, loss = 0.01384592\n",
      "Iteration 16931, loss = 0.01384493\n",
      "Iteration 16932, loss = 0.01384393\n",
      "Iteration 16933, loss = 0.01384294\n",
      "Iteration 16934, loss = 0.01384195\n",
      "Iteration 16935, loss = 0.01384096\n",
      "Iteration 16936, loss = 0.01383996\n",
      "Iteration 16937, loss = 0.01383897\n",
      "Iteration 16938, loss = 0.01383798\n",
      "Iteration 16939, loss = 0.01383699\n",
      "Iteration 16940, loss = 0.01383600\n",
      "Iteration 16941, loss = 0.01383500\n",
      "Iteration 16942, loss = 0.01383401\n",
      "Iteration 16943, loss = 0.01383302\n",
      "Iteration 16944, loss = 0.01383203\n",
      "Iteration 16945, loss = 0.01383104\n",
      "Iteration 16946, loss = 0.01383005\n",
      "Iteration 16947, loss = 0.01382906\n",
      "Iteration 16948, loss = 0.01382807\n",
      "Iteration 16949, loss = 0.01382708\n",
      "Iteration 16950, loss = 0.01382609\n",
      "Iteration 16951, loss = 0.01382510\n",
      "Iteration 16952, loss = 0.01382411\n",
      "Iteration 16953, loss = 0.01382312\n",
      "Iteration 16954, loss = 0.01382213\n",
      "Iteration 16955, loss = 0.01382114\n",
      "Iteration 16956, loss = 0.01382015\n",
      "Iteration 16957, loss = 0.01381917\n",
      "Iteration 16958, loss = 0.01381818\n",
      "Iteration 16959, loss = 0.01381719\n",
      "Iteration 16960, loss = 0.01381620\n",
      "Iteration 16961, loss = 0.01381521\n",
      "Iteration 16962, loss = 0.01381422\n",
      "Iteration 16963, loss = 0.01381324\n",
      "Iteration 16964, loss = 0.01381225\n",
      "Iteration 16965, loss = 0.01381126\n",
      "Iteration 16966, loss = 0.01381028\n",
      "Iteration 16967, loss = 0.01380929\n",
      "Iteration 16968, loss = 0.01380830\n",
      "Iteration 16969, loss = 0.01380732\n",
      "Iteration 16970, loss = 0.01380633\n",
      "Iteration 16971, loss = 0.01380534\n",
      "Iteration 16972, loss = 0.01380436\n",
      "Iteration 16973, loss = 0.01380337\n",
      "Iteration 16974, loss = 0.01380238\n",
      "Iteration 16975, loss = 0.01380140\n",
      "Iteration 16976, loss = 0.01380041\n",
      "Iteration 16977, loss = 0.01379943\n",
      "Iteration 16978, loss = 0.01379844\n",
      "Iteration 16979, loss = 0.01379746\n",
      "Iteration 16980, loss = 0.01379647\n",
      "Iteration 16981, loss = 0.01379549\n",
      "Iteration 16982, loss = 0.01379451\n",
      "Iteration 16983, loss = 0.01379352\n",
      "Iteration 16984, loss = 0.01379254\n",
      "Iteration 16985, loss = 0.01379155\n",
      "Iteration 16986, loss = 0.01379057\n",
      "Iteration 16987, loss = 0.01378959\n",
      "Iteration 16988, loss = 0.01378860\n",
      "Iteration 16989, loss = 0.01378762\n",
      "Iteration 16990, loss = 0.01378664\n",
      "Iteration 16991, loss = 0.01378565\n",
      "Iteration 16992, loss = 0.01378467\n",
      "Iteration 16993, loss = 0.01378369\n",
      "Iteration 16994, loss = 0.01378271\n",
      "Iteration 16995, loss = 0.01378172\n",
      "Iteration 16996, loss = 0.01378074\n",
      "Iteration 16997, loss = 0.01377976\n",
      "Iteration 16998, loss = 0.01377878\n",
      "Iteration 16999, loss = 0.01377780\n",
      "Iteration 17000, loss = 0.01377682\n",
      "Iteration 17001, loss = 0.01377583\n",
      "Iteration 17002, loss = 0.01377485\n",
      "Iteration 17003, loss = 0.01377387\n",
      "Iteration 17004, loss = 0.01377289\n",
      "Iteration 17005, loss = 0.01377191\n",
      "Iteration 17006, loss = 0.01377093\n",
      "Iteration 17007, loss = 0.01376995\n",
      "Iteration 17008, loss = 0.01376897\n",
      "Iteration 17009, loss = 0.01376799\n",
      "Iteration 17010, loss = 0.01376701\n",
      "Iteration 17011, loss = 0.01376603\n",
      "Iteration 17012, loss = 0.01376505\n",
      "Iteration 17013, loss = 0.01376407\n",
      "Iteration 17014, loss = 0.01376310\n",
      "Iteration 17015, loss = 0.01376212\n",
      "Iteration 17016, loss = 0.01376114\n",
      "Iteration 17017, loss = 0.01376016\n",
      "Iteration 17018, loss = 0.01375918\n",
      "Iteration 17019, loss = 0.01375820\n",
      "Iteration 17020, loss = 0.01375723\n",
      "Iteration 17021, loss = 0.01375625\n",
      "Iteration 17022, loss = 0.01375527\n",
      "Iteration 17023, loss = 0.01375429\n",
      "Iteration 17024, loss = 0.01375332\n",
      "Iteration 17025, loss = 0.01375234\n",
      "Iteration 17026, loss = 0.01375136\n",
      "Iteration 17027, loss = 0.01375038\n",
      "Iteration 17028, loss = 0.01374941\n",
      "Iteration 17029, loss = 0.01374843\n",
      "Iteration 17030, loss = 0.01374746\n",
      "Iteration 17031, loss = 0.01374648\n",
      "Iteration 17032, loss = 0.01374550\n",
      "Iteration 17033, loss = 0.01374453\n",
      "Iteration 17034, loss = 0.01374355\n",
      "Iteration 17035, loss = 0.01374258\n",
      "Iteration 17036, loss = 0.01374160\n",
      "Iteration 17037, loss = 0.01374063\n",
      "Iteration 17038, loss = 0.01373965\n",
      "Iteration 17039, loss = 0.01373868\n",
      "Iteration 17040, loss = 0.01373770\n",
      "Iteration 17041, loss = 0.01373673\n",
      "Iteration 17042, loss = 0.01373575\n",
      "Iteration 17043, loss = 0.01373478\n",
      "Iteration 17044, loss = 0.01373381\n",
      "Iteration 17045, loss = 0.01373283\n",
      "Iteration 17046, loss = 0.01373186\n",
      "Iteration 17047, loss = 0.01373089\n",
      "Iteration 17048, loss = 0.01372991\n",
      "Iteration 17049, loss = 0.01372894\n",
      "Iteration 17050, loss = 0.01372797\n",
      "Iteration 17051, loss = 0.01372699\n",
      "Iteration 17052, loss = 0.01372602\n",
      "Iteration 17053, loss = 0.01372505\n",
      "Iteration 17054, loss = 0.01372408\n",
      "Iteration 17055, loss = 0.01372310\n",
      "Iteration 17056, loss = 0.01372213\n",
      "Iteration 17057, loss = 0.01372116\n",
      "Iteration 17058, loss = 0.01372019\n",
      "Iteration 17059, loss = 0.01371922\n",
      "Iteration 17060, loss = 0.01371825\n",
      "Iteration 17061, loss = 0.01371728\n",
      "Iteration 17062, loss = 0.01371630\n",
      "Iteration 17063, loss = 0.01371533\n",
      "Iteration 17064, loss = 0.01371436\n",
      "Iteration 17065, loss = 0.01371339\n",
      "Iteration 17066, loss = 0.01371242\n",
      "Iteration 17067, loss = 0.01371145\n",
      "Iteration 17068, loss = 0.01371048\n",
      "Iteration 17069, loss = 0.01370951\n",
      "Iteration 17070, loss = 0.01370854\n",
      "Iteration 17071, loss = 0.01370757\n",
      "Iteration 17072, loss = 0.01370661\n",
      "Iteration 17073, loss = 0.01370564\n",
      "Iteration 17074, loss = 0.01370467\n",
      "Iteration 17075, loss = 0.01370370\n",
      "Iteration 17076, loss = 0.01370273\n",
      "Iteration 17077, loss = 0.01370176\n",
      "Iteration 17078, loss = 0.01370079\n",
      "Iteration 17079, loss = 0.01369983\n",
      "Iteration 17080, loss = 0.01369886\n",
      "Iteration 17081, loss = 0.01369789\n",
      "Iteration 17082, loss = 0.01369692\n",
      "Iteration 17083, loss = 0.01369596\n",
      "Iteration 17084, loss = 0.01369499\n",
      "Iteration 17085, loss = 0.01369402\n",
      "Iteration 17086, loss = 0.01369305\n",
      "Iteration 17087, loss = 0.01369209\n",
      "Iteration 17088, loss = 0.01369112\n",
      "Iteration 17089, loss = 0.01369015\n",
      "Iteration 17090, loss = 0.01368919\n",
      "Iteration 17091, loss = 0.01368822\n",
      "Iteration 17092, loss = 0.01368726\n",
      "Iteration 17093, loss = 0.01368629\n",
      "Iteration 17094, loss = 0.01368533\n",
      "Iteration 17095, loss = 0.01368436\n",
      "Iteration 17096, loss = 0.01368339\n",
      "Iteration 17097, loss = 0.01368243\n",
      "Iteration 17098, loss = 0.01368146\n",
      "Iteration 17099, loss = 0.01368050\n",
      "Iteration 17100, loss = 0.01367954\n",
      "Iteration 17101, loss = 0.01367857\n",
      "Iteration 17102, loss = 0.01367761\n",
      "Iteration 17103, loss = 0.01367664\n",
      "Iteration 17104, loss = 0.01367568\n",
      "Iteration 17105, loss = 0.01367472\n",
      "Iteration 17106, loss = 0.01367375\n",
      "Iteration 17107, loss = 0.01367279\n",
      "Iteration 17108, loss = 0.01367183\n",
      "Iteration 17109, loss = 0.01367086\n",
      "Iteration 17110, loss = 0.01366990\n",
      "Iteration 17111, loss = 0.01366894\n",
      "Iteration 17112, loss = 0.01366797\n",
      "Iteration 17113, loss = 0.01366701\n",
      "Iteration 17114, loss = 0.01366605\n",
      "Iteration 17115, loss = 0.01366509\n",
      "Iteration 17116, loss = 0.01366413\n",
      "Iteration 17117, loss = 0.01366316\n",
      "Iteration 17118, loss = 0.01366220\n",
      "Iteration 17119, loss = 0.01366124\n",
      "Iteration 17120, loss = 0.01366028\n",
      "Iteration 17121, loss = 0.01365932\n",
      "Iteration 17122, loss = 0.01365836\n",
      "Iteration 17123, loss = 0.01365740\n",
      "Iteration 17124, loss = 0.01365644\n",
      "Iteration 17125, loss = 0.01365548\n",
      "Iteration 17126, loss = 0.01365452\n",
      "Iteration 17127, loss = 0.01365356\n",
      "Iteration 17128, loss = 0.01365260\n",
      "Iteration 17129, loss = 0.01365164\n",
      "Iteration 17130, loss = 0.01365068\n",
      "Iteration 17131, loss = 0.01364972\n",
      "Iteration 17132, loss = 0.01364876\n",
      "Iteration 17133, loss = 0.01364780\n",
      "Iteration 17134, loss = 0.01364684\n",
      "Iteration 17135, loss = 0.01364588\n",
      "Iteration 17136, loss = 0.01364492\n",
      "Iteration 17137, loss = 0.01364396\n",
      "Iteration 17138, loss = 0.01364301\n",
      "Iteration 17139, loss = 0.01364205\n",
      "Iteration 17140, loss = 0.01364109\n",
      "Iteration 17141, loss = 0.01364013\n",
      "Iteration 17142, loss = 0.01363917\n",
      "Iteration 17143, loss = 0.01363822\n",
      "Iteration 17144, loss = 0.01363726\n",
      "Iteration 17145, loss = 0.01363630\n",
      "Iteration 17146, loss = 0.01363535\n",
      "Iteration 17147, loss = 0.01363439\n",
      "Iteration 17148, loss = 0.01363343\n",
      "Iteration 17149, loss = 0.01363248\n",
      "Iteration 17150, loss = 0.01363152\n",
      "Iteration 17151, loss = 0.01363056\n",
      "Iteration 17152, loss = 0.01362961\n",
      "Iteration 17153, loss = 0.01362865\n",
      "Iteration 17154, loss = 0.01362770\n",
      "Iteration 17155, loss = 0.01362674\n",
      "Iteration 17156, loss = 0.01362578\n",
      "Iteration 17157, loss = 0.01362483\n",
      "Iteration 17158, loss = 0.01362387\n",
      "Iteration 17159, loss = 0.01362292\n",
      "Iteration 17160, loss = 0.01362197\n",
      "Iteration 17161, loss = 0.01362101\n",
      "Iteration 17162, loss = 0.01362006\n",
      "Iteration 17163, loss = 0.01361910\n",
      "Iteration 17164, loss = 0.01361815\n",
      "Iteration 17165, loss = 0.01361719\n",
      "Iteration 17166, loss = 0.01361624\n",
      "Iteration 17167, loss = 0.01361529\n",
      "Iteration 17168, loss = 0.01361433\n",
      "Iteration 17169, loss = 0.01361338\n",
      "Iteration 17170, loss = 0.01361243\n",
      "Iteration 17171, loss = 0.01361147\n",
      "Iteration 17172, loss = 0.01361052\n",
      "Iteration 17173, loss = 0.01360957\n",
      "Iteration 17174, loss = 0.01360862\n",
      "Iteration 17175, loss = 0.01360766\n",
      "Iteration 17176, loss = 0.01360671\n",
      "Iteration 17177, loss = 0.01360576\n",
      "Iteration 17178, loss = 0.01360481\n",
      "Iteration 17179, loss = 0.01360386\n",
      "Iteration 17180, loss = 0.01360291\n",
      "Iteration 17181, loss = 0.01360195\n",
      "Iteration 17182, loss = 0.01360100\n",
      "Iteration 17183, loss = 0.01360005\n",
      "Iteration 17184, loss = 0.01359910\n",
      "Iteration 17185, loss = 0.01359815\n",
      "Iteration 17186, loss = 0.01359720\n",
      "Iteration 17187, loss = 0.01359625\n",
      "Iteration 17188, loss = 0.01359530\n",
      "Iteration 17189, loss = 0.01359435\n",
      "Iteration 17190, loss = 0.01359340\n",
      "Iteration 17191, loss = 0.01359245\n",
      "Iteration 17192, loss = 0.01359150\n",
      "Iteration 17193, loss = 0.01359055\n",
      "Iteration 17194, loss = 0.01358960\n",
      "Iteration 17195, loss = 0.01358865\n",
      "Iteration 17196, loss = 0.01358771\n",
      "Iteration 17197, loss = 0.01358676\n",
      "Iteration 17198, loss = 0.01358581\n",
      "Iteration 17199, loss = 0.01358486\n",
      "Iteration 17200, loss = 0.01358391\n",
      "Iteration 17201, loss = 0.01358296\n",
      "Iteration 17202, loss = 0.01358202\n",
      "Iteration 17203, loss = 0.01358107\n",
      "Iteration 17204, loss = 0.01358012\n",
      "Iteration 17205, loss = 0.01357917\n",
      "Iteration 17206, loss = 0.01357823\n",
      "Iteration 17207, loss = 0.01357728\n",
      "Iteration 17208, loss = 0.01357633\n",
      "Iteration 17209, loss = 0.01357539\n",
      "Iteration 17210, loss = 0.01357444\n",
      "Iteration 17211, loss = 0.01357349\n",
      "Iteration 17212, loss = 0.01357255\n",
      "Iteration 17213, loss = 0.01357160\n",
      "Iteration 17214, loss = 0.01357065\n",
      "Iteration 17215, loss = 0.01356971\n",
      "Iteration 17216, loss = 0.01356876\n",
      "Iteration 17217, loss = 0.01356782\n",
      "Iteration 17218, loss = 0.01356687\n",
      "Iteration 17219, loss = 0.01356593\n",
      "Iteration 17220, loss = 0.01356498\n",
      "Iteration 17221, loss = 0.01356404\n",
      "Iteration 17222, loss = 0.01356309\n",
      "Iteration 17223, loss = 0.01356215\n",
      "Iteration 17224, loss = 0.01356120\n",
      "Iteration 17225, loss = 0.01356026\n",
      "Iteration 17226, loss = 0.01355932\n",
      "Iteration 17227, loss = 0.01355837\n",
      "Iteration 17228, loss = 0.01355743\n",
      "Iteration 17229, loss = 0.01355649\n",
      "Iteration 17230, loss = 0.01355554\n",
      "Iteration 17231, loss = 0.01355460\n",
      "Iteration 17232, loss = 0.01355366\n",
      "Iteration 17233, loss = 0.01355271\n",
      "Iteration 17234, loss = 0.01355177\n",
      "Iteration 17235, loss = 0.01355083\n",
      "Iteration 17236, loss = 0.01354989\n",
      "Iteration 17237, loss = 0.01354894\n",
      "Iteration 17238, loss = 0.01354800\n",
      "Iteration 17239, loss = 0.01354706\n",
      "Iteration 17240, loss = 0.01354612\n",
      "Iteration 17241, loss = 0.01354518\n",
      "Iteration 17242, loss = 0.01354423\n",
      "Iteration 17243, loss = 0.01354329\n",
      "Iteration 17244, loss = 0.01354235\n",
      "Iteration 17245, loss = 0.01354141\n",
      "Iteration 17246, loss = 0.01354047\n",
      "Iteration 17247, loss = 0.01353953\n",
      "Iteration 17248, loss = 0.01353859\n",
      "Iteration 17249, loss = 0.01353765\n",
      "Iteration 17250, loss = 0.01353671\n",
      "Iteration 17251, loss = 0.01353577\n",
      "Iteration 17252, loss = 0.01353483\n",
      "Iteration 17253, loss = 0.01353389\n",
      "Iteration 17254, loss = 0.01353295\n",
      "Iteration 17255, loss = 0.01353201\n",
      "Iteration 17256, loss = 0.01353107\n",
      "Iteration 17257, loss = 0.01353013\n",
      "Iteration 17258, loss = 0.01352919\n",
      "Iteration 17259, loss = 0.01352826\n",
      "Iteration 17260, loss = 0.01352732\n",
      "Iteration 17261, loss = 0.01352638\n",
      "Iteration 17262, loss = 0.01352544\n",
      "Iteration 17263, loss = 0.01352450\n",
      "Iteration 17264, loss = 0.01352356\n",
      "Iteration 17265, loss = 0.01352263\n",
      "Iteration 17266, loss = 0.01352169\n",
      "Iteration 17267, loss = 0.01352075\n",
      "Iteration 17268, loss = 0.01351981\n",
      "Iteration 17269, loss = 0.01351888\n",
      "Iteration 17270, loss = 0.01351794\n",
      "Iteration 17271, loss = 0.01351700\n",
      "Iteration 17272, loss = 0.01351607\n",
      "Iteration 17273, loss = 0.01351513\n",
      "Iteration 17274, loss = 0.01351419\n",
      "Iteration 17275, loss = 0.01351326\n",
      "Iteration 17276, loss = 0.01351232\n",
      "Iteration 17277, loss = 0.01351139\n",
      "Iteration 17278, loss = 0.01351045\n",
      "Iteration 17279, loss = 0.01350952\n",
      "Iteration 17280, loss = 0.01350858\n",
      "Iteration 17281, loss = 0.01350764\n",
      "Iteration 17282, loss = 0.01350671\n",
      "Iteration 17283, loss = 0.01350577\n",
      "Iteration 17284, loss = 0.01350484\n",
      "Iteration 17285, loss = 0.01350391\n",
      "Iteration 17286, loss = 0.01350297\n",
      "Iteration 17287, loss = 0.01350204\n",
      "Iteration 17288, loss = 0.01350110\n",
      "Iteration 17289, loss = 0.01350017\n",
      "Iteration 17290, loss = 0.01349924\n",
      "Iteration 17291, loss = 0.01349830\n",
      "Iteration 17292, loss = 0.01349737\n",
      "Iteration 17293, loss = 0.01349643\n",
      "Iteration 17294, loss = 0.01349550\n",
      "Iteration 17295, loss = 0.01349457\n",
      "Iteration 17296, loss = 0.01349364\n",
      "Iteration 17297, loss = 0.01349270\n",
      "Iteration 17298, loss = 0.01349177\n",
      "Iteration 17299, loss = 0.01349084\n",
      "Iteration 17300, loss = 0.01348991\n",
      "Iteration 17301, loss = 0.01348897\n",
      "Iteration 17302, loss = 0.01348804\n",
      "Iteration 17303, loss = 0.01348711\n",
      "Iteration 17304, loss = 0.01348618\n",
      "Iteration 17305, loss = 0.01348525\n",
      "Iteration 17306, loss = 0.01348432\n",
      "Iteration 17307, loss = 0.01348339\n",
      "Iteration 17308, loss = 0.01348245\n",
      "Iteration 17309, loss = 0.01348152\n",
      "Iteration 17310, loss = 0.01348059\n",
      "Iteration 17311, loss = 0.01347966\n",
      "Iteration 17312, loss = 0.01347873\n",
      "Iteration 17313, loss = 0.01347780\n",
      "Iteration 17314, loss = 0.01347687\n",
      "Iteration 17315, loss = 0.01347594\n",
      "Iteration 17316, loss = 0.01347501\n",
      "Iteration 17317, loss = 0.01347408\n",
      "Iteration 17318, loss = 0.01347315\n",
      "Iteration 17319, loss = 0.01347222\n",
      "Iteration 17320, loss = 0.01347130\n",
      "Iteration 17321, loss = 0.01347037\n",
      "Iteration 17322, loss = 0.01346944\n",
      "Iteration 17323, loss = 0.01346851\n",
      "Iteration 17324, loss = 0.01346758\n",
      "Iteration 17325, loss = 0.01346665\n",
      "Iteration 17326, loss = 0.01346573\n",
      "Iteration 17327, loss = 0.01346480\n",
      "Iteration 17328, loss = 0.01346387\n",
      "Iteration 17329, loss = 0.01346294\n",
      "Iteration 17330, loss = 0.01346201\n",
      "Iteration 17331, loss = 0.01346109\n",
      "Iteration 17332, loss = 0.01346016\n",
      "Iteration 17333, loss = 0.01345923\n",
      "Iteration 17334, loss = 0.01345831\n",
      "Iteration 17335, loss = 0.01345738\n",
      "Iteration 17336, loss = 0.01345645\n",
      "Iteration 17337, loss = 0.01345553\n",
      "Iteration 17338, loss = 0.01345460\n",
      "Iteration 17339, loss = 0.01345367\n",
      "Iteration 17340, loss = 0.01345275\n",
      "Iteration 17341, loss = 0.01345182\n",
      "Iteration 17342, loss = 0.01345090\n",
      "Iteration 17343, loss = 0.01344997\n",
      "Iteration 17344, loss = 0.01344905\n",
      "Iteration 17345, loss = 0.01344812\n",
      "Iteration 17346, loss = 0.01344720\n",
      "Iteration 17347, loss = 0.01344627\n",
      "Iteration 17348, loss = 0.01344535\n",
      "Iteration 17349, loss = 0.01344442\n",
      "Iteration 17350, loss = 0.01344350\n",
      "Iteration 17351, loss = 0.01344257\n",
      "Iteration 17352, loss = 0.01344165\n",
      "Iteration 17353, loss = 0.01344073\n",
      "Iteration 17354, loss = 0.01343980\n",
      "Iteration 17355, loss = 0.01343888\n",
      "Iteration 17356, loss = 0.01343796\n",
      "Iteration 17357, loss = 0.01343703\n",
      "Iteration 17358, loss = 0.01343611\n",
      "Iteration 17359, loss = 0.01343519\n",
      "Iteration 17360, loss = 0.01343426\n",
      "Iteration 17361, loss = 0.01343334\n",
      "Iteration 17362, loss = 0.01343242\n",
      "Iteration 17363, loss = 0.01343150\n",
      "Iteration 17364, loss = 0.01343057\n",
      "Iteration 17365, loss = 0.01342965\n",
      "Iteration 17366, loss = 0.01342873\n",
      "Iteration 17367, loss = 0.01342781\n",
      "Iteration 17368, loss = 0.01342689\n",
      "Iteration 17369, loss = 0.01342597\n",
      "Iteration 17370, loss = 0.01342504\n",
      "Iteration 17371, loss = 0.01342412\n",
      "Iteration 17372, loss = 0.01342320\n",
      "Iteration 17373, loss = 0.01342228\n",
      "Iteration 17374, loss = 0.01342136\n",
      "Iteration 17375, loss = 0.01342044\n",
      "Iteration 17376, loss = 0.01341952\n",
      "Iteration 17377, loss = 0.01341860\n",
      "Iteration 17378, loss = 0.01341768\n",
      "Iteration 17379, loss = 0.01341676\n",
      "Iteration 17380, loss = 0.01341584\n",
      "Iteration 17381, loss = 0.01341492\n",
      "Iteration 17382, loss = 0.01341400\n",
      "Iteration 17383, loss = 0.01341308\n",
      "Iteration 17384, loss = 0.01341216\n",
      "Iteration 17385, loss = 0.01341124\n",
      "Iteration 17386, loss = 0.01341033\n",
      "Iteration 17387, loss = 0.01340941\n",
      "Iteration 17388, loss = 0.01340849\n",
      "Iteration 17389, loss = 0.01340757\n",
      "Iteration 17390, loss = 0.01340665\n",
      "Iteration 17391, loss = 0.01340573\n",
      "Iteration 17392, loss = 0.01340482\n",
      "Iteration 17393, loss = 0.01340390\n",
      "Iteration 17394, loss = 0.01340298\n",
      "Iteration 17395, loss = 0.01340206\n",
      "Iteration 17396, loss = 0.01340115\n",
      "Iteration 17397, loss = 0.01340023\n",
      "Iteration 17398, loss = 0.01339931\n",
      "Iteration 17399, loss = 0.01339840\n",
      "Iteration 17400, loss = 0.01339748\n",
      "Iteration 17401, loss = 0.01339656\n",
      "Iteration 17402, loss = 0.01339565\n",
      "Iteration 17403, loss = 0.01339473\n",
      "Iteration 17404, loss = 0.01339382\n",
      "Iteration 17405, loss = 0.01339290\n",
      "Iteration 17406, loss = 0.01339198\n",
      "Iteration 17407, loss = 0.01339107\n",
      "Iteration 17408, loss = 0.01339015\n",
      "Iteration 17409, loss = 0.01338924\n",
      "Iteration 17410, loss = 0.01338832\n",
      "Iteration 17411, loss = 0.01338741\n",
      "Iteration 17412, loss = 0.01338649\n",
      "Iteration 17413, loss = 0.01338558\n",
      "Iteration 17414, loss = 0.01338466\n",
      "Iteration 17415, loss = 0.01338375\n",
      "Iteration 17416, loss = 0.01338284\n",
      "Iteration 17417, loss = 0.01338192\n",
      "Iteration 17418, loss = 0.01338101\n",
      "Iteration 17419, loss = 0.01338009\n",
      "Iteration 17420, loss = 0.01337918\n",
      "Iteration 17421, loss = 0.01337827\n",
      "Iteration 17422, loss = 0.01337735\n",
      "Iteration 17423, loss = 0.01337644\n",
      "Iteration 17424, loss = 0.01337553\n",
      "Iteration 17425, loss = 0.01337462\n",
      "Iteration 17426, loss = 0.01337370\n",
      "Iteration 17427, loss = 0.01337279\n",
      "Iteration 17428, loss = 0.01337188\n",
      "Iteration 17429, loss = 0.01337097\n",
      "Iteration 17430, loss = 0.01337005\n",
      "Iteration 17431, loss = 0.01336914\n",
      "Iteration 17432, loss = 0.01336823\n",
      "Iteration 17433, loss = 0.01336732\n",
      "Iteration 17434, loss = 0.01336641\n",
      "Iteration 17435, loss = 0.01336550\n",
      "Iteration 17436, loss = 0.01336459\n",
      "Iteration 17437, loss = 0.01336368\n",
      "Iteration 17438, loss = 0.01336276\n",
      "Iteration 17439, loss = 0.01336185\n",
      "Iteration 17440, loss = 0.01336094\n",
      "Iteration 17441, loss = 0.01336003\n",
      "Iteration 17442, loss = 0.01335912\n",
      "Iteration 17443, loss = 0.01335821\n",
      "Iteration 17444, loss = 0.01335730\n",
      "Iteration 17445, loss = 0.01335639\n",
      "Iteration 17446, loss = 0.01335548\n",
      "Iteration 17447, loss = 0.01335457\n",
      "Iteration 17448, loss = 0.01335367\n",
      "Iteration 17449, loss = 0.01335276\n",
      "Iteration 17450, loss = 0.01335185\n",
      "Iteration 17451, loss = 0.01335094\n",
      "Iteration 17452, loss = 0.01335003\n",
      "Iteration 17453, loss = 0.01334912\n",
      "Iteration 17454, loss = 0.01334821\n",
      "Iteration 17455, loss = 0.01334731\n",
      "Iteration 17456, loss = 0.01334640\n",
      "Iteration 17457, loss = 0.01334549\n",
      "Iteration 17458, loss = 0.01334458\n",
      "Iteration 17459, loss = 0.01334367\n",
      "Iteration 17460, loss = 0.01334277\n",
      "Iteration 17461, loss = 0.01334186\n",
      "Iteration 17462, loss = 0.01334095\n",
      "Iteration 17463, loss = 0.01334005\n",
      "Iteration 17464, loss = 0.01333914\n",
      "Iteration 17465, loss = 0.01333823\n",
      "Iteration 17466, loss = 0.01333733\n",
      "Iteration 17467, loss = 0.01333642\n",
      "Iteration 17468, loss = 0.01333551\n",
      "Iteration 17469, loss = 0.01333461\n",
      "Iteration 17470, loss = 0.01333370\n",
      "Iteration 17471, loss = 0.01333280\n",
      "Iteration 17472, loss = 0.01333189\n",
      "Iteration 17473, loss = 0.01333099\n",
      "Iteration 17474, loss = 0.01333008\n",
      "Iteration 17475, loss = 0.01332917\n",
      "Iteration 17476, loss = 0.01332827\n",
      "Iteration 17477, loss = 0.01332737\n",
      "Iteration 17478, loss = 0.01332646\n",
      "Iteration 17479, loss = 0.01332556\n",
      "Iteration 17480, loss = 0.01332465\n",
      "Iteration 17481, loss = 0.01332375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17482, loss = 0.01332284\n",
      "Iteration 17483, loss = 0.01332194\n",
      "Iteration 17484, loss = 0.01332104\n",
      "Iteration 17485, loss = 0.01332013\n",
      "Iteration 17486, loss = 0.01331923\n",
      "Iteration 17487, loss = 0.01331833\n",
      "Iteration 17488, loss = 0.01331742\n",
      "Iteration 17489, loss = 0.01331652\n",
      "Iteration 17490, loss = 0.01331562\n",
      "Iteration 17491, loss = 0.01331471\n",
      "Iteration 17492, loss = 0.01331381\n",
      "Iteration 17493, loss = 0.01331291\n",
      "Iteration 17494, loss = 0.01331201\n",
      "Iteration 17495, loss = 0.01331110\n",
      "Iteration 17496, loss = 0.01331020\n",
      "Iteration 17497, loss = 0.01330930\n",
      "Iteration 17498, loss = 0.01330840\n",
      "Iteration 17499, loss = 0.01330750\n",
      "Iteration 17500, loss = 0.01330660\n",
      "Iteration 17501, loss = 0.01330570\n",
      "Iteration 17502, loss = 0.01330479\n",
      "Iteration 17503, loss = 0.01330389\n",
      "Iteration 17504, loss = 0.01330299\n",
      "Iteration 17505, loss = 0.01330209\n",
      "Iteration 17506, loss = 0.01330119\n",
      "Iteration 17507, loss = 0.01330029\n",
      "Iteration 17508, loss = 0.01329939\n",
      "Iteration 17509, loss = 0.01329849\n",
      "Iteration 17510, loss = 0.01329759\n",
      "Iteration 17511, loss = 0.01329669\n",
      "Iteration 17512, loss = 0.01329579\n",
      "Iteration 17513, loss = 0.01329489\n",
      "Iteration 17514, loss = 0.01329399\n",
      "Iteration 17515, loss = 0.01329309\n",
      "Iteration 17516, loss = 0.01329220\n",
      "Iteration 17517, loss = 0.01329130\n",
      "Iteration 17518, loss = 0.01329040\n",
      "Iteration 17519, loss = 0.01328950\n",
      "Iteration 17520, loss = 0.01328860\n",
      "Iteration 17521, loss = 0.01328770\n",
      "Iteration 17522, loss = 0.01328681\n",
      "Iteration 17523, loss = 0.01328591\n",
      "Iteration 17524, loss = 0.01328501\n",
      "Iteration 17525, loss = 0.01328411\n",
      "Iteration 17526, loss = 0.01328322\n",
      "Iteration 17527, loss = 0.01328232\n",
      "Iteration 17528, loss = 0.01328142\n",
      "Iteration 17529, loss = 0.01328052\n",
      "Iteration 17530, loss = 0.01327963\n",
      "Iteration 17531, loss = 0.01327873\n",
      "Iteration 17532, loss = 0.01327783\n",
      "Iteration 17533, loss = 0.01327694\n",
      "Iteration 17534, loss = 0.01327604\n",
      "Iteration 17535, loss = 0.01327515\n",
      "Iteration 17536, loss = 0.01327425\n",
      "Iteration 17537, loss = 0.01327335\n",
      "Iteration 17538, loss = 0.01327246\n",
      "Iteration 17539, loss = 0.01327156\n",
      "Iteration 17540, loss = 0.01327067\n",
      "Iteration 17541, loss = 0.01326977\n",
      "Iteration 17542, loss = 0.01326888\n",
      "Iteration 17543, loss = 0.01326798\n",
      "Iteration 17544, loss = 0.01326709\n",
      "Iteration 17545, loss = 0.01326619\n",
      "Iteration 17546, loss = 0.01326530\n",
      "Iteration 17547, loss = 0.01326440\n",
      "Iteration 17548, loss = 0.01326351\n",
      "Iteration 17549, loss = 0.01326262\n",
      "Iteration 17550, loss = 0.01326172\n",
      "Iteration 17551, loss = 0.01326083\n",
      "Iteration 17552, loss = 0.01325993\n",
      "Iteration 17553, loss = 0.01325904\n",
      "Iteration 17554, loss = 0.01325815\n",
      "Iteration 17555, loss = 0.01325726\n",
      "Iteration 17556, loss = 0.01325636\n",
      "Iteration 17557, loss = 0.01325547\n",
      "Iteration 17558, loss = 0.01325458\n",
      "Iteration 17559, loss = 0.01325368\n",
      "Iteration 17560, loss = 0.01325279\n",
      "Iteration 17561, loss = 0.01325190\n",
      "Iteration 17562, loss = 0.01325101\n",
      "Iteration 17563, loss = 0.01325012\n",
      "Iteration 17564, loss = 0.01324922\n",
      "Iteration 17565, loss = 0.01324833\n",
      "Iteration 17566, loss = 0.01324744\n",
      "Iteration 17567, loss = 0.01324655\n",
      "Iteration 17568, loss = 0.01324566\n",
      "Iteration 17569, loss = 0.01324477\n",
      "Iteration 17570, loss = 0.01324388\n",
      "Iteration 17571, loss = 0.01324299\n",
      "Iteration 17572, loss = 0.01324210\n",
      "Iteration 17573, loss = 0.01324120\n",
      "Iteration 17574, loss = 0.01324031\n",
      "Iteration 17575, loss = 0.01323942\n",
      "Iteration 17576, loss = 0.01323853\n",
      "Iteration 17577, loss = 0.01323764\n",
      "Iteration 17578, loss = 0.01323676\n",
      "Iteration 17579, loss = 0.01323587\n",
      "Iteration 17580, loss = 0.01323498\n",
      "Iteration 17581, loss = 0.01323409\n",
      "Iteration 17582, loss = 0.01323320\n",
      "Iteration 17583, loss = 0.01323231\n",
      "Iteration 17584, loss = 0.01323142\n",
      "Iteration 17585, loss = 0.01323053\n",
      "Iteration 17586, loss = 0.01322964\n",
      "Iteration 17587, loss = 0.01322875\n",
      "Iteration 17588, loss = 0.01322787\n",
      "Iteration 17589, loss = 0.01322698\n",
      "Iteration 17590, loss = 0.01322609\n",
      "Iteration 17591, loss = 0.01322520\n",
      "Iteration 17592, loss = 0.01322432\n",
      "Iteration 17593, loss = 0.01322343\n",
      "Iteration 17594, loss = 0.01322254\n",
      "Iteration 17595, loss = 0.01322165\n",
      "Iteration 17596, loss = 0.01322077\n",
      "Iteration 17597, loss = 0.01321988\n",
      "Iteration 17598, loss = 0.01321899\n",
      "Iteration 17599, loss = 0.01321811\n",
      "Iteration 17600, loss = 0.01321722\n",
      "Iteration 17601, loss = 0.01321633\n",
      "Iteration 17602, loss = 0.01321545\n",
      "Iteration 17603, loss = 0.01321456\n",
      "Iteration 17604, loss = 0.01321368\n",
      "Iteration 17605, loss = 0.01321279\n",
      "Iteration 17606, loss = 0.01321190\n",
      "Iteration 17607, loss = 0.01321102\n",
      "Iteration 17608, loss = 0.01321013\n",
      "Iteration 17609, loss = 0.01320925\n",
      "Iteration 17610, loss = 0.01320836\n",
      "Iteration 17611, loss = 0.01320748\n",
      "Iteration 17612, loss = 0.01320659\n",
      "Iteration 17613, loss = 0.01320571\n",
      "Iteration 17614, loss = 0.01320483\n",
      "Iteration 17615, loss = 0.01320394\n",
      "Iteration 17616, loss = 0.01320306\n",
      "Iteration 17617, loss = 0.01320217\n",
      "Iteration 17618, loss = 0.01320129\n",
      "Iteration 17619, loss = 0.01320041\n",
      "Iteration 17620, loss = 0.01319952\n",
      "Iteration 17621, loss = 0.01319864\n",
      "Iteration 17622, loss = 0.01319776\n",
      "Iteration 17623, loss = 0.01319687\n",
      "Iteration 17624, loss = 0.01319599\n",
      "Iteration 17625, loss = 0.01319511\n",
      "Iteration 17626, loss = 0.01319422\n",
      "Iteration 17627, loss = 0.01319334\n",
      "Iteration 17628, loss = 0.01319246\n",
      "Iteration 17629, loss = 0.01319158\n",
      "Iteration 17630, loss = 0.01319070\n",
      "Iteration 17631, loss = 0.01318981\n",
      "Iteration 17632, loss = 0.01318893\n",
      "Iteration 17633, loss = 0.01318805\n",
      "Iteration 17634, loss = 0.01318717\n",
      "Iteration 17635, loss = 0.01318629\n",
      "Iteration 17636, loss = 0.01318541\n",
      "Iteration 17637, loss = 0.01318453\n",
      "Iteration 17638, loss = 0.01318364\n",
      "Iteration 17639, loss = 0.01318276\n",
      "Iteration 17640, loss = 0.01318188\n",
      "Iteration 17641, loss = 0.01318100\n",
      "Iteration 17642, loss = 0.01318012\n",
      "Iteration 17643, loss = 0.01317924\n",
      "Iteration 17644, loss = 0.01317836\n",
      "Iteration 17645, loss = 0.01317748\n",
      "Iteration 17646, loss = 0.01317660\n",
      "Iteration 17647, loss = 0.01317572\n",
      "Iteration 17648, loss = 0.01317484\n",
      "Iteration 17649, loss = 0.01317396\n",
      "Iteration 17650, loss = 0.01317309\n",
      "Iteration 17651, loss = 0.01317221\n",
      "Iteration 17652, loss = 0.01317133\n",
      "Iteration 17653, loss = 0.01317045\n",
      "Iteration 17654, loss = 0.01316957\n",
      "Iteration 17655, loss = 0.01316869\n",
      "Iteration 17656, loss = 0.01316781\n",
      "Iteration 17657, loss = 0.01316694\n",
      "Iteration 17658, loss = 0.01316606\n",
      "Iteration 17659, loss = 0.01316518\n",
      "Iteration 17660, loss = 0.01316430\n",
      "Iteration 17661, loss = 0.01316342\n",
      "Iteration 17662, loss = 0.01316255\n",
      "Iteration 17663, loss = 0.01316167\n",
      "Iteration 17664, loss = 0.01316079\n",
      "Iteration 17665, loss = 0.01315992\n",
      "Iteration 17666, loss = 0.01315904\n",
      "Iteration 17667, loss = 0.01315816\n",
      "Iteration 17668, loss = 0.01315729\n",
      "Iteration 17669, loss = 0.01315641\n",
      "Iteration 17670, loss = 0.01315553\n",
      "Iteration 17671, loss = 0.01315466\n",
      "Iteration 17672, loss = 0.01315378\n",
      "Iteration 17673, loss = 0.01315291\n",
      "Iteration 17674, loss = 0.01315203\n",
      "Iteration 17675, loss = 0.01315115\n",
      "Iteration 17676, loss = 0.01315028\n",
      "Iteration 17677, loss = 0.01314940\n",
      "Iteration 17678, loss = 0.01314853\n",
      "Iteration 17679, loss = 0.01314765\n",
      "Iteration 17680, loss = 0.01314678\n",
      "Iteration 17681, loss = 0.01314590\n",
      "Iteration 17682, loss = 0.01314503\n",
      "Iteration 17683, loss = 0.01314416\n",
      "Iteration 17684, loss = 0.01314328\n",
      "Iteration 17685, loss = 0.01314241\n",
      "Iteration 17686, loss = 0.01314153\n",
      "Iteration 17687, loss = 0.01314066\n",
      "Iteration 17688, loss = 0.01313979\n",
      "Iteration 17689, loss = 0.01313891\n",
      "Iteration 17690, loss = 0.01313804\n",
      "Iteration 17691, loss = 0.01313717\n",
      "Iteration 17692, loss = 0.01313629\n",
      "Iteration 17693, loss = 0.01313542\n",
      "Iteration 17694, loss = 0.01313455\n",
      "Iteration 17695, loss = 0.01313368\n",
      "Iteration 17696, loss = 0.01313280\n",
      "Iteration 17697, loss = 0.01313193\n",
      "Iteration 17698, loss = 0.01313106\n",
      "Iteration 17699, loss = 0.01313019\n",
      "Iteration 17700, loss = 0.01312931\n",
      "Iteration 17701, loss = 0.01312844\n",
      "Iteration 17702, loss = 0.01312757\n",
      "Iteration 17703, loss = 0.01312670\n",
      "Iteration 17704, loss = 0.01312583\n",
      "Iteration 17705, loss = 0.01312496\n",
      "Iteration 17706, loss = 0.01312409\n",
      "Iteration 17707, loss = 0.01312322\n",
      "Iteration 17708, loss = 0.01312234\n",
      "Iteration 17709, loss = 0.01312147\n",
      "Iteration 17710, loss = 0.01312060\n",
      "Iteration 17711, loss = 0.01311973\n",
      "Iteration 17712, loss = 0.01311886\n",
      "Iteration 17713, loss = 0.01311799\n",
      "Iteration 17714, loss = 0.01311712\n",
      "Iteration 17715, loss = 0.01311625\n",
      "Iteration 17716, loss = 0.01311538\n",
      "Iteration 17717, loss = 0.01311451\n",
      "Iteration 17718, loss = 0.01311365\n",
      "Iteration 17719, loss = 0.01311278\n",
      "Iteration 17720, loss = 0.01311191\n",
      "Iteration 17721, loss = 0.01311104\n",
      "Iteration 17722, loss = 0.01311017\n",
      "Iteration 17723, loss = 0.01310930\n",
      "Iteration 17724, loss = 0.01310843\n",
      "Iteration 17725, loss = 0.01310756\n",
      "Iteration 17726, loss = 0.01310670\n",
      "Iteration 17727, loss = 0.01310583\n",
      "Iteration 17728, loss = 0.01310496\n",
      "Iteration 17729, loss = 0.01310409\n",
      "Iteration 17730, loss = 0.01310323\n",
      "Iteration 17731, loss = 0.01310236\n",
      "Iteration 17732, loss = 0.01310149\n",
      "Iteration 17733, loss = 0.01310062\n",
      "Iteration 17734, loss = 0.01309976\n",
      "Iteration 17735, loss = 0.01309889\n",
      "Iteration 17736, loss = 0.01309802\n",
      "Iteration 17737, loss = 0.01309716\n",
      "Iteration 17738, loss = 0.01309629\n",
      "Iteration 17739, loss = 0.01309542\n",
      "Iteration 17740, loss = 0.01309456\n",
      "Iteration 17741, loss = 0.01309369\n",
      "Iteration 17742, loss = 0.01309283\n",
      "Iteration 17743, loss = 0.01309196\n",
      "Iteration 17744, loss = 0.01309109\n",
      "Iteration 17745, loss = 0.01309023\n",
      "Iteration 17746, loss = 0.01308936\n",
      "Iteration 17747, loss = 0.01308850\n",
      "Iteration 17748, loss = 0.01308763\n",
      "Iteration 17749, loss = 0.01308677\n",
      "Iteration 17750, loss = 0.01308590\n",
      "Iteration 17751, loss = 0.01308504\n",
      "Iteration 17752, loss = 0.01308417\n",
      "Iteration 17753, loss = 0.01308331\n",
      "Iteration 17754, loss = 0.01308245\n",
      "Iteration 17755, loss = 0.01308158\n",
      "Iteration 17756, loss = 0.01308072\n",
      "Iteration 17757, loss = 0.01307985\n",
      "Iteration 17758, loss = 0.01307899\n",
      "Iteration 17759, loss = 0.01307813\n",
      "Iteration 17760, loss = 0.01307726\n",
      "Iteration 17761, loss = 0.01307640\n",
      "Iteration 17762, loss = 0.01307554\n",
      "Iteration 17763, loss = 0.01307468\n",
      "Iteration 17764, loss = 0.01307381\n",
      "Iteration 17765, loss = 0.01307295\n",
      "Iteration 17766, loss = 0.01307209\n",
      "Iteration 17767, loss = 0.01307123\n",
      "Iteration 17768, loss = 0.01307036\n",
      "Iteration 17769, loss = 0.01306950\n",
      "Iteration 17770, loss = 0.01306864\n",
      "Iteration 17771, loss = 0.01306778\n",
      "Iteration 17772, loss = 0.01306692\n",
      "Iteration 17773, loss = 0.01306605\n",
      "Iteration 17774, loss = 0.01306519\n",
      "Iteration 17775, loss = 0.01306433\n",
      "Iteration 17776, loss = 0.01306347\n",
      "Iteration 17777, loss = 0.01306261\n",
      "Iteration 17778, loss = 0.01306175\n",
      "Iteration 17779, loss = 0.01306089\n",
      "Iteration 17780, loss = 0.01306003\n",
      "Iteration 17781, loss = 0.01305917\n",
      "Iteration 17782, loss = 0.01305831\n",
      "Iteration 17783, loss = 0.01305745\n",
      "Iteration 17784, loss = 0.01305659\n",
      "Iteration 17785, loss = 0.01305573\n",
      "Iteration 17786, loss = 0.01305487\n",
      "Iteration 17787, loss = 0.01305401\n",
      "Iteration 17788, loss = 0.01305315\n",
      "Iteration 17789, loss = 0.01305229\n",
      "Iteration 17790, loss = 0.01305143\n",
      "Iteration 17791, loss = 0.01305057\n",
      "Iteration 17792, loss = 0.01304971\n",
      "Iteration 17793, loss = 0.01304885\n",
      "Iteration 17794, loss = 0.01304800\n",
      "Iteration 17795, loss = 0.01304714\n",
      "Iteration 17796, loss = 0.01304628\n",
      "Iteration 17797, loss = 0.01304542\n",
      "Iteration 17798, loss = 0.01304456\n",
      "Iteration 17799, loss = 0.01304371\n",
      "Iteration 17800, loss = 0.01304285\n",
      "Iteration 17801, loss = 0.01304199\n",
      "Iteration 17802, loss = 0.01304113\n",
      "Iteration 17803, loss = 0.01304028\n",
      "Iteration 17804, loss = 0.01303942\n",
      "Iteration 17805, loss = 0.01303856\n",
      "Iteration 17806, loss = 0.01303771\n",
      "Iteration 17807, loss = 0.01303685\n",
      "Iteration 17808, loss = 0.01303599\n",
      "Iteration 17809, loss = 0.01303514\n",
      "Iteration 17810, loss = 0.01303428\n",
      "Iteration 17811, loss = 0.01303342\n",
      "Iteration 17812, loss = 0.01303257\n",
      "Iteration 17813, loss = 0.01303171\n",
      "Iteration 17814, loss = 0.01303086\n",
      "Iteration 17815, loss = 0.01303000\n",
      "Iteration 17816, loss = 0.01302914\n",
      "Iteration 17817, loss = 0.01302829\n",
      "Iteration 17818, loss = 0.01302743\n",
      "Iteration 17819, loss = 0.01302658\n",
      "Iteration 17820, loss = 0.01302572\n",
      "Iteration 17821, loss = 0.01302487\n",
      "Iteration 17822, loss = 0.01302402\n",
      "Iteration 17823, loss = 0.01302316\n",
      "Iteration 17824, loss = 0.01302231\n",
      "Iteration 17825, loss = 0.01302145\n",
      "Iteration 17826, loss = 0.01302060\n",
      "Iteration 17827, loss = 0.01301974\n",
      "Iteration 17828, loss = 0.01301889\n",
      "Iteration 17829, loss = 0.01301804\n",
      "Iteration 17830, loss = 0.01301718\n",
      "Iteration 17831, loss = 0.01301633\n",
      "Iteration 17832, loss = 0.01301548\n",
      "Iteration 17833, loss = 0.01301462\n",
      "Iteration 17834, loss = 0.01301377\n",
      "Iteration 17835, loss = 0.01301292\n",
      "Iteration 17836, loss = 0.01301207\n",
      "Iteration 17837, loss = 0.01301121\n",
      "Iteration 17838, loss = 0.01301036\n",
      "Iteration 17839, loss = 0.01300951\n",
      "Iteration 17840, loss = 0.01300866\n",
      "Iteration 17841, loss = 0.01300780\n",
      "Iteration 17842, loss = 0.01300695\n",
      "Iteration 17843, loss = 0.01300610\n",
      "Iteration 17844, loss = 0.01300525\n",
      "Iteration 17845, loss = 0.01300440\n",
      "Iteration 17846, loss = 0.01300355\n",
      "Iteration 17847, loss = 0.01300270\n",
      "Iteration 17848, loss = 0.01300185\n",
      "Iteration 17849, loss = 0.01300099\n",
      "Iteration 17850, loss = 0.01300014\n",
      "Iteration 17851, loss = 0.01299929\n",
      "Iteration 17852, loss = 0.01299844\n",
      "Iteration 17853, loss = 0.01299759\n",
      "Iteration 17854, loss = 0.01299674\n",
      "Iteration 17855, loss = 0.01299589\n",
      "Iteration 17856, loss = 0.01299504\n",
      "Iteration 17857, loss = 0.01299419\n",
      "Iteration 17858, loss = 0.01299334\n",
      "Iteration 17859, loss = 0.01299249\n",
      "Iteration 17860, loss = 0.01299164\n",
      "Iteration 17861, loss = 0.01299080\n",
      "Iteration 17862, loss = 0.01298995\n",
      "Iteration 17863, loss = 0.01298910\n",
      "Iteration 17864, loss = 0.01298825\n",
      "Iteration 17865, loss = 0.01298740\n",
      "Iteration 17866, loss = 0.01298655\n",
      "Iteration 17867, loss = 0.01298570\n",
      "Iteration 17868, loss = 0.01298486\n",
      "Iteration 17869, loss = 0.01298401\n",
      "Iteration 17870, loss = 0.01298316\n",
      "Iteration 17871, loss = 0.01298231\n",
      "Iteration 17872, loss = 0.01298146\n",
      "Iteration 17873, loss = 0.01298062\n",
      "Iteration 17874, loss = 0.01297977\n",
      "Iteration 17875, loss = 0.01297892\n",
      "Iteration 17876, loss = 0.01297808\n",
      "Iteration 17877, loss = 0.01297723\n",
      "Iteration 17878, loss = 0.01297638\n",
      "Iteration 17879, loss = 0.01297553\n",
      "Iteration 17880, loss = 0.01297469\n",
      "Iteration 17881, loss = 0.01297384\n",
      "Iteration 17882, loss = 0.01297300\n",
      "Iteration 17883, loss = 0.01297215\n",
      "Iteration 17884, loss = 0.01297130\n",
      "Iteration 17885, loss = 0.01297046\n",
      "Iteration 17886, loss = 0.01296961\n",
      "Iteration 17887, loss = 0.01296877\n",
      "Iteration 17888, loss = 0.01296792\n",
      "Iteration 17889, loss = 0.01296708\n",
      "Iteration 17890, loss = 0.01296623\n",
      "Iteration 17891, loss = 0.01296539\n",
      "Iteration 17892, loss = 0.01296454\n",
      "Iteration 17893, loss = 0.01296370\n",
      "Iteration 17894, loss = 0.01296285\n",
      "Iteration 17895, loss = 0.01296201\n",
      "Iteration 17896, loss = 0.01296116\n",
      "Iteration 17897, loss = 0.01296032\n",
      "Iteration 17898, loss = 0.01295947\n",
      "Iteration 17899, loss = 0.01295863\n",
      "Iteration 17900, loss = 0.01295779\n",
      "Iteration 17901, loss = 0.01295694\n",
      "Iteration 17902, loss = 0.01295610\n",
      "Iteration 17903, loss = 0.01295526\n",
      "Iteration 17904, loss = 0.01295441\n",
      "Iteration 17905, loss = 0.01295357\n",
      "Iteration 17906, loss = 0.01295273\n",
      "Iteration 17907, loss = 0.01295188\n",
      "Iteration 17908, loss = 0.01295104\n",
      "Iteration 17909, loss = 0.01295020\n",
      "Iteration 17910, loss = 0.01294936\n",
      "Iteration 17911, loss = 0.01294851\n",
      "Iteration 17912, loss = 0.01294767\n",
      "Iteration 17913, loss = 0.01294683\n",
      "Iteration 17914, loss = 0.01294599\n",
      "Iteration 17915, loss = 0.01294515\n",
      "Iteration 17916, loss = 0.01294431\n",
      "Iteration 17917, loss = 0.01294346\n",
      "Iteration 17918, loss = 0.01294262\n",
      "Iteration 17919, loss = 0.01294178\n",
      "Iteration 17920, loss = 0.01294094\n",
      "Iteration 17921, loss = 0.01294010\n",
      "Iteration 17922, loss = 0.01293926\n",
      "Iteration 17923, loss = 0.01293842\n",
      "Iteration 17924, loss = 0.01293758\n",
      "Iteration 17925, loss = 0.01293674\n",
      "Iteration 17926, loss = 0.01293590\n",
      "Iteration 17927, loss = 0.01293506\n",
      "Iteration 17928, loss = 0.01293422\n",
      "Iteration 17929, loss = 0.01293338\n",
      "Iteration 17930, loss = 0.01293254\n",
      "Iteration 17931, loss = 0.01293170\n",
      "Iteration 17932, loss = 0.01293086\n",
      "Iteration 17933, loss = 0.01293002\n",
      "Iteration 17934, loss = 0.01292918\n",
      "Iteration 17935, loss = 0.01292834\n",
      "Iteration 17936, loss = 0.01292750\n",
      "Iteration 17937, loss = 0.01292666\n",
      "Iteration 17938, loss = 0.01292583\n",
      "Iteration 17939, loss = 0.01292499\n",
      "Iteration 17940, loss = 0.01292415\n",
      "Iteration 17941, loss = 0.01292331\n",
      "Iteration 17942, loss = 0.01292247\n",
      "Iteration 17943, loss = 0.01292163\n",
      "Iteration 17944, loss = 0.01292080\n",
      "Iteration 17945, loss = 0.01291996\n",
      "Iteration 17946, loss = 0.01291912\n",
      "Iteration 17947, loss = 0.01291828\n",
      "Iteration 17948, loss = 0.01291745\n",
      "Iteration 17949, loss = 0.01291661\n",
      "Iteration 17950, loss = 0.01291577\n",
      "Iteration 17951, loss = 0.01291494\n",
      "Iteration 17952, loss = 0.01291410\n",
      "Iteration 17953, loss = 0.01291326\n",
      "Iteration 17954, loss = 0.01291243\n",
      "Iteration 17955, loss = 0.01291159\n",
      "Iteration 17956, loss = 0.01291075\n",
      "Iteration 17957, loss = 0.01290992\n",
      "Iteration 17958, loss = 0.01290908\n",
      "Iteration 17959, loss = 0.01290825\n",
      "Iteration 17960, loss = 0.01290741\n",
      "Iteration 17961, loss = 0.01290658\n",
      "Iteration 17962, loss = 0.01290574\n",
      "Iteration 17963, loss = 0.01290491\n",
      "Iteration 17964, loss = 0.01290407\n",
      "Iteration 17965, loss = 0.01290324\n",
      "Iteration 17966, loss = 0.01290240\n",
      "Iteration 17967, loss = 0.01290157\n",
      "Iteration 17968, loss = 0.01290073\n",
      "Iteration 17969, loss = 0.01289990\n",
      "Iteration 17970, loss = 0.01289906\n",
      "Iteration 17971, loss = 0.01289823\n",
      "Iteration 17972, loss = 0.01289740\n",
      "Iteration 17973, loss = 0.01289656\n",
      "Iteration 17974, loss = 0.01289573\n",
      "Iteration 17975, loss = 0.01289489\n",
      "Iteration 17976, loss = 0.01289406\n",
      "Iteration 17977, loss = 0.01289323\n",
      "Iteration 17978, loss = 0.01289239\n",
      "Iteration 17979, loss = 0.01289156\n",
      "Iteration 17980, loss = 0.01289073\n",
      "Iteration 17981, loss = 0.01288990\n",
      "Iteration 17982, loss = 0.01288906\n",
      "Iteration 17983, loss = 0.01288823\n",
      "Iteration 17984, loss = 0.01288740\n",
      "Iteration 17985, loss = 0.01288657\n",
      "Iteration 17986, loss = 0.01288573\n",
      "Iteration 17987, loss = 0.01288490\n",
      "Iteration 17988, loss = 0.01288407\n",
      "Iteration 17989, loss = 0.01288324\n",
      "Iteration 17990, loss = 0.01288241\n",
      "Iteration 17991, loss = 0.01288158\n",
      "Iteration 17992, loss = 0.01288074\n",
      "Iteration 17993, loss = 0.01287991\n",
      "Iteration 17994, loss = 0.01287908\n",
      "Iteration 17995, loss = 0.01287825\n",
      "Iteration 17996, loss = 0.01287742\n",
      "Iteration 17997, loss = 0.01287659\n",
      "Iteration 17998, loss = 0.01287576\n",
      "Iteration 17999, loss = 0.01287493\n",
      "Iteration 18000, loss = 0.01287410\n",
      "Iteration 18001, loss = 0.01287327\n",
      "Iteration 18002, loss = 0.01287244\n",
      "Iteration 18003, loss = 0.01287161\n",
      "Iteration 18004, loss = 0.01287078\n",
      "Iteration 18005, loss = 0.01286995\n",
      "Iteration 18006, loss = 0.01286912\n",
      "Iteration 18007, loss = 0.01286829\n",
      "Iteration 18008, loss = 0.01286746\n",
      "Iteration 18009, loss = 0.01286663\n",
      "Iteration 18010, loss = 0.01286580\n",
      "Iteration 18011, loss = 0.01286498\n",
      "Iteration 18012, loss = 0.01286415\n",
      "Iteration 18013, loss = 0.01286332\n",
      "Iteration 18014, loss = 0.01286249\n",
      "Iteration 18015, loss = 0.01286166\n",
      "Iteration 18016, loss = 0.01286083\n",
      "Iteration 18017, loss = 0.01286001\n",
      "Iteration 18018, loss = 0.01285918\n",
      "Iteration 18019, loss = 0.01285835\n",
      "Iteration 18020, loss = 0.01285752\n",
      "Iteration 18021, loss = 0.01285670\n",
      "Iteration 18022, loss = 0.01285587\n",
      "Iteration 18023, loss = 0.01285504\n",
      "Iteration 18024, loss = 0.01285422\n",
      "Iteration 18025, loss = 0.01285339\n",
      "Iteration 18026, loss = 0.01285256\n",
      "Iteration 18027, loss = 0.01285174\n",
      "Iteration 18028, loss = 0.01285091\n",
      "Iteration 18029, loss = 0.01285008\n",
      "Iteration 18030, loss = 0.01284926\n",
      "Iteration 18031, loss = 0.01284843\n",
      "Iteration 18032, loss = 0.01284760\n",
      "Iteration 18033, loss = 0.01284678\n",
      "Iteration 18034, loss = 0.01284595\n",
      "Iteration 18035, loss = 0.01284513\n",
      "Iteration 18036, loss = 0.01284430\n",
      "Iteration 18037, loss = 0.01284348\n",
      "Iteration 18038, loss = 0.01284265\n",
      "Iteration 18039, loss = 0.01284183\n",
      "Iteration 18040, loss = 0.01284100\n",
      "Iteration 18041, loss = 0.01284018\n",
      "Iteration 18042, loss = 0.01283935\n",
      "Iteration 18043, loss = 0.01283853\n",
      "Iteration 18044, loss = 0.01283770\n",
      "Iteration 18045, loss = 0.01283688\n",
      "Iteration 18046, loss = 0.01283606\n",
      "Iteration 18047, loss = 0.01283523\n",
      "Iteration 18048, loss = 0.01283441\n",
      "Iteration 18049, loss = 0.01283358\n",
      "Iteration 18050, loss = 0.01283276\n",
      "Iteration 18051, loss = 0.01283194\n",
      "Iteration 18052, loss = 0.01283111\n",
      "Iteration 18053, loss = 0.01283029\n",
      "Iteration 18054, loss = 0.01282947\n",
      "Iteration 18055, loss = 0.01282865\n",
      "Iteration 18056, loss = 0.01282782\n",
      "Iteration 18057, loss = 0.01282700\n",
      "Iteration 18058, loss = 0.01282618\n",
      "Iteration 18059, loss = 0.01282536\n",
      "Iteration 18060, loss = 0.01282453\n",
      "Iteration 18061, loss = 0.01282371\n",
      "Iteration 18062, loss = 0.01282289\n",
      "Iteration 18063, loss = 0.01282207\n",
      "Iteration 18064, loss = 0.01282125\n",
      "Iteration 18065, loss = 0.01282042\n",
      "Iteration 18066, loss = 0.01281960\n",
      "Iteration 18067, loss = 0.01281878\n",
      "Iteration 18068, loss = 0.01281796\n",
      "Iteration 18069, loss = 0.01281714\n",
      "Iteration 18070, loss = 0.01281632\n",
      "Iteration 18071, loss = 0.01281550\n",
      "Iteration 18072, loss = 0.01281468\n",
      "Iteration 18073, loss = 0.01281386\n",
      "Iteration 18074, loss = 0.01281304\n",
      "Iteration 18075, loss = 0.01281222\n",
      "Iteration 18076, loss = 0.01281140\n",
      "Iteration 18077, loss = 0.01281058\n",
      "Iteration 18078, loss = 0.01280976\n",
      "Iteration 18079, loss = 0.01280894\n",
      "Iteration 18080, loss = 0.01280812\n",
      "Iteration 18081, loss = 0.01280730\n",
      "Iteration 18082, loss = 0.01280648\n",
      "Iteration 18083, loss = 0.01280566\n",
      "Iteration 18084, loss = 0.01280484\n",
      "Iteration 18085, loss = 0.01280402\n",
      "Iteration 18086, loss = 0.01280320\n",
      "Iteration 18087, loss = 0.01280238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18088, loss = 0.01280157\n",
      "Iteration 18089, loss = 0.01280075\n",
      "Iteration 18090, loss = 0.01279993\n",
      "Iteration 18091, loss = 0.01279911\n",
      "Iteration 18092, loss = 0.01279829\n",
      "Iteration 18093, loss = 0.01279748\n",
      "Iteration 18094, loss = 0.01279666\n",
      "Iteration 18095, loss = 0.01279584\n",
      "Iteration 18096, loss = 0.01279502\n",
      "Iteration 18097, loss = 0.01279421\n",
      "Iteration 18098, loss = 0.01279339\n",
      "Iteration 18099, loss = 0.01279257\n",
      "Iteration 18100, loss = 0.01279175\n",
      "Iteration 18101, loss = 0.01279094\n",
      "Iteration 18102, loss = 0.01279012\n",
      "Iteration 18103, loss = 0.01278930\n",
      "Iteration 18104, loss = 0.01278849\n",
      "Iteration 18105, loss = 0.01278767\n",
      "Iteration 18106, loss = 0.01278686\n",
      "Iteration 18107, loss = 0.01278604\n",
      "Iteration 18108, loss = 0.01278522\n",
      "Iteration 18109, loss = 0.01278441\n",
      "Iteration 18110, loss = 0.01278359\n",
      "Iteration 18111, loss = 0.01278278\n",
      "Iteration 18112, loss = 0.01278196\n",
      "Iteration 18113, loss = 0.01278115\n",
      "Iteration 18114, loss = 0.01278033\n",
      "Iteration 18115, loss = 0.01277952\n",
      "Iteration 18116, loss = 0.01277870\n",
      "Iteration 18117, loss = 0.01277789\n",
      "Iteration 18118, loss = 0.01277707\n",
      "Iteration 18119, loss = 0.01277626\n",
      "Iteration 18120, loss = 0.01277544\n",
      "Iteration 18121, loss = 0.01277463\n",
      "Iteration 18122, loss = 0.01277382\n",
      "Iteration 18123, loss = 0.01277300\n",
      "Iteration 18124, loss = 0.01277219\n",
      "Iteration 18125, loss = 0.01277137\n",
      "Iteration 18126, loss = 0.01277056\n",
      "Iteration 18127, loss = 0.01276975\n",
      "Iteration 18128, loss = 0.01276893\n",
      "Iteration 18129, loss = 0.01276812\n",
      "Iteration 18130, loss = 0.01276731\n",
      "Iteration 18131, loss = 0.01276650\n",
      "Iteration 18132, loss = 0.01276568\n",
      "Iteration 18133, loss = 0.01276487\n",
      "Iteration 18134, loss = 0.01276406\n",
      "Iteration 18135, loss = 0.01276325\n",
      "Iteration 18136, loss = 0.01276243\n",
      "Iteration 18137, loss = 0.01276162\n",
      "Iteration 18138, loss = 0.01276081\n",
      "Iteration 18139, loss = 0.01276000\n",
      "Iteration 18140, loss = 0.01275919\n",
      "Iteration 18141, loss = 0.01275837\n",
      "Iteration 18142, loss = 0.01275756\n",
      "Iteration 18143, loss = 0.01275675\n",
      "Iteration 18144, loss = 0.01275594\n",
      "Iteration 18145, loss = 0.01275513\n",
      "Iteration 18146, loss = 0.01275432\n",
      "Iteration 18147, loss = 0.01275351\n",
      "Iteration 18148, loss = 0.01275270\n",
      "Iteration 18149, loss = 0.01275189\n",
      "Iteration 18150, loss = 0.01275108\n",
      "Iteration 18151, loss = 0.01275027\n",
      "Iteration 18152, loss = 0.01274946\n",
      "Iteration 18153, loss = 0.01274865\n",
      "Iteration 18154, loss = 0.01274784\n",
      "Iteration 18155, loss = 0.01274703\n",
      "Iteration 18156, loss = 0.01274622\n",
      "Iteration 18157, loss = 0.01274541\n",
      "Iteration 18158, loss = 0.01274460\n",
      "Iteration 18159, loss = 0.01274379\n",
      "Iteration 18160, loss = 0.01274298\n",
      "Iteration 18161, loss = 0.01274217\n",
      "Iteration 18162, loss = 0.01274136\n",
      "Iteration 18163, loss = 0.01274055\n",
      "Iteration 18164, loss = 0.01273974\n",
      "Iteration 18165, loss = 0.01273894\n",
      "Iteration 18166, loss = 0.01273813\n",
      "Iteration 18167, loss = 0.01273732\n",
      "Iteration 18168, loss = 0.01273651\n",
      "Iteration 18169, loss = 0.01273570\n",
      "Iteration 18170, loss = 0.01273490\n",
      "Iteration 18171, loss = 0.01273409\n",
      "Iteration 18172, loss = 0.01273328\n",
      "Iteration 18173, loss = 0.01273247\n",
      "Iteration 18174, loss = 0.01273167\n",
      "Iteration 18175, loss = 0.01273086\n",
      "Iteration 18176, loss = 0.01273005\n",
      "Iteration 18177, loss = 0.01272924\n",
      "Iteration 18178, loss = 0.01272844\n",
      "Iteration 18179, loss = 0.01272763\n",
      "Iteration 18180, loss = 0.01272683\n",
      "Iteration 18181, loss = 0.01272602\n",
      "Iteration 18182, loss = 0.01272521\n",
      "Iteration 18183, loss = 0.01272441\n",
      "Iteration 18184, loss = 0.01272360\n",
      "Iteration 18185, loss = 0.01272279\n",
      "Iteration 18186, loss = 0.01272199\n",
      "Iteration 18187, loss = 0.01272118\n",
      "Iteration 18188, loss = 0.01272038\n",
      "Iteration 18189, loss = 0.01271957\n",
      "Iteration 18190, loss = 0.01271877\n",
      "Iteration 18191, loss = 0.01271796\n",
      "Iteration 18192, loss = 0.01271716\n",
      "Iteration 18193, loss = 0.01271635\n",
      "Iteration 18194, loss = 0.01271555\n",
      "Iteration 18195, loss = 0.01271474\n",
      "Iteration 18196, loss = 0.01271394\n",
      "Iteration 18197, loss = 0.01271313\n",
      "Iteration 18198, loss = 0.01271233\n",
      "Iteration 18199, loss = 0.01271153\n",
      "Iteration 18200, loss = 0.01271072\n",
      "Iteration 18201, loss = 0.01270992\n",
      "Iteration 18202, loss = 0.01270911\n",
      "Iteration 18203, loss = 0.01270831\n",
      "Iteration 18204, loss = 0.01270751\n",
      "Iteration 18205, loss = 0.01270670\n",
      "Iteration 18206, loss = 0.01270590\n",
      "Iteration 18207, loss = 0.01270510\n",
      "Iteration 18208, loss = 0.01270430\n",
      "Iteration 18209, loss = 0.01270349\n",
      "Iteration 18210, loss = 0.01270269\n",
      "Iteration 18211, loss = 0.01270189\n",
      "Iteration 18212, loss = 0.01270109\n",
      "Iteration 18213, loss = 0.01270028\n",
      "Iteration 18214, loss = 0.01269948\n",
      "Iteration 18215, loss = 0.01269868\n",
      "Iteration 18216, loss = 0.01269788\n",
      "Iteration 18217, loss = 0.01269708\n",
      "Iteration 18218, loss = 0.01269627\n",
      "Iteration 18219, loss = 0.01269547\n",
      "Iteration 18220, loss = 0.01269467\n",
      "Iteration 18221, loss = 0.01269387\n",
      "Iteration 18222, loss = 0.01269307\n",
      "Iteration 18223, loss = 0.01269227\n",
      "Iteration 18224, loss = 0.01269147\n",
      "Iteration 18225, loss = 0.01269067\n",
      "Iteration 18226, loss = 0.01268987\n",
      "Iteration 18227, loss = 0.01268906\n",
      "Iteration 18228, loss = 0.01268826\n",
      "Iteration 18229, loss = 0.01268746\n",
      "Iteration 18230, loss = 0.01268666\n",
      "Iteration 18231, loss = 0.01268586\n",
      "Iteration 18232, loss = 0.01268506\n",
      "Iteration 18233, loss = 0.01268426\n",
      "Iteration 18234, loss = 0.01268347\n",
      "Iteration 18235, loss = 0.01268267\n",
      "Iteration 18236, loss = 0.01268187\n",
      "Iteration 18237, loss = 0.01268107\n",
      "Iteration 18238, loss = 0.01268027\n",
      "Iteration 18239, loss = 0.01267947\n",
      "Iteration 18240, loss = 0.01267867\n",
      "Iteration 18241, loss = 0.01267787\n",
      "Iteration 18242, loss = 0.01267707\n",
      "Iteration 18243, loss = 0.01267627\n",
      "Iteration 18244, loss = 0.01267548\n",
      "Iteration 18245, loss = 0.01267468\n",
      "Iteration 18246, loss = 0.01267388\n",
      "Iteration 18247, loss = 0.01267308\n",
      "Iteration 18248, loss = 0.01267228\n",
      "Iteration 18249, loss = 0.01267149\n",
      "Iteration 18250, loss = 0.01267069\n",
      "Iteration 18251, loss = 0.01266989\n",
      "Iteration 18252, loss = 0.01266909\n",
      "Iteration 18253, loss = 0.01266830\n",
      "Iteration 18254, loss = 0.01266750\n",
      "Iteration 18255, loss = 0.01266670\n",
      "Iteration 18256, loss = 0.01266591\n",
      "Iteration 18257, loss = 0.01266511\n",
      "Iteration 18258, loss = 0.01266431\n",
      "Iteration 18259, loss = 0.01266352\n",
      "Iteration 18260, loss = 0.01266272\n",
      "Iteration 18261, loss = 0.01266192\n",
      "Iteration 18262, loss = 0.01266113\n",
      "Iteration 18263, loss = 0.01266033\n",
      "Iteration 18264, loss = 0.01265954\n",
      "Iteration 18265, loss = 0.01265874\n",
      "Iteration 18266, loss = 0.01265795\n",
      "Iteration 18267, loss = 0.01265715\n",
      "Iteration 18268, loss = 0.01265636\n",
      "Iteration 18269, loss = 0.01265556\n",
      "Iteration 18270, loss = 0.01265477\n",
      "Iteration 18271, loss = 0.01265397\n",
      "Iteration 18272, loss = 0.01265318\n",
      "Iteration 18273, loss = 0.01265238\n",
      "Iteration 18274, loss = 0.01265159\n",
      "Iteration 18275, loss = 0.01265079\n",
      "Iteration 18276, loss = 0.01265000\n",
      "Iteration 18277, loss = 0.01264920\n",
      "Iteration 18278, loss = 0.01264841\n",
      "Iteration 18279, loss = 0.01264762\n",
      "Iteration 18280, loss = 0.01264682\n",
      "Iteration 18281, loss = 0.01264603\n",
      "Iteration 18282, loss = 0.01264524\n",
      "Iteration 18283, loss = 0.01264444\n",
      "Iteration 18284, loss = 0.01264365\n",
      "Iteration 18285, loss = 0.01264286\n",
      "Iteration 18286, loss = 0.01264206\n",
      "Iteration 18287, loss = 0.01264127\n",
      "Iteration 18288, loss = 0.01264048\n",
      "Iteration 18289, loss = 0.01263968\n",
      "Iteration 18290, loss = 0.01263889\n",
      "Iteration 18291, loss = 0.01263810\n",
      "Iteration 18292, loss = 0.01263731\n",
      "Iteration 18293, loss = 0.01263652\n",
      "Iteration 18294, loss = 0.01263572\n",
      "Iteration 18295, loss = 0.01263493\n",
      "Iteration 18296, loss = 0.01263414\n",
      "Iteration 18297, loss = 0.01263335\n",
      "Iteration 18298, loss = 0.01263256\n",
      "Iteration 18299, loss = 0.01263177\n",
      "Iteration 18300, loss = 0.01263097\n",
      "Iteration 18301, loss = 0.01263018\n",
      "Iteration 18302, loss = 0.01262939\n",
      "Iteration 18303, loss = 0.01262860\n",
      "Iteration 18304, loss = 0.01262781\n",
      "Iteration 18305, loss = 0.01262702\n",
      "Iteration 18306, loss = 0.01262623\n",
      "Iteration 18307, loss = 0.01262544\n",
      "Iteration 18308, loss = 0.01262465\n",
      "Iteration 18309, loss = 0.01262386\n",
      "Iteration 18310, loss = 0.01262307\n",
      "Iteration 18311, loss = 0.01262228\n",
      "Iteration 18312, loss = 0.01262149\n",
      "Iteration 18313, loss = 0.01262070\n",
      "Iteration 18314, loss = 0.01261991\n",
      "Iteration 18315, loss = 0.01261912\n",
      "Iteration 18316, loss = 0.01261833\n",
      "Iteration 18317, loss = 0.01261754\n",
      "Iteration 18318, loss = 0.01261675\n",
      "Iteration 18319, loss = 0.01261596\n",
      "Iteration 18320, loss = 0.01261518\n",
      "Iteration 18321, loss = 0.01261439\n",
      "Iteration 18322, loss = 0.01261360\n",
      "Iteration 18323, loss = 0.01261281\n",
      "Iteration 18324, loss = 0.01261202\n",
      "Iteration 18325, loss = 0.01261123\n",
      "Iteration 18326, loss = 0.01261045\n",
      "Iteration 18327, loss = 0.01260966\n",
      "Iteration 18328, loss = 0.01260887\n",
      "Iteration 18329, loss = 0.01260808\n",
      "Iteration 18330, loss = 0.01260730\n",
      "Iteration 18331, loss = 0.01260651\n",
      "Iteration 18332, loss = 0.01260572\n",
      "Iteration 18333, loss = 0.01260493\n",
      "Iteration 18334, loss = 0.01260415\n",
      "Iteration 18335, loss = 0.01260336\n",
      "Iteration 18336, loss = 0.01260257\n",
      "Iteration 18337, loss = 0.01260179\n",
      "Iteration 18338, loss = 0.01260100\n",
      "Iteration 18339, loss = 0.01260021\n",
      "Iteration 18340, loss = 0.01259943\n",
      "Iteration 18341, loss = 0.01259864\n",
      "Iteration 18342, loss = 0.01259786\n",
      "Iteration 18343, loss = 0.01259707\n",
      "Iteration 18344, loss = 0.01259628\n",
      "Iteration 18345, loss = 0.01259550\n",
      "Iteration 18346, loss = 0.01259471\n",
      "Iteration 18347, loss = 0.01259393\n",
      "Iteration 18348, loss = 0.01259314\n",
      "Iteration 18349, loss = 0.01259236\n",
      "Iteration 18350, loss = 0.01259157\n",
      "Iteration 18351, loss = 0.01259079\n",
      "Iteration 18352, loss = 0.01259000\n",
      "Iteration 18353, loss = 0.01258922\n",
      "Iteration 18354, loss = 0.01258843\n",
      "Iteration 18355, loss = 0.01258765\n",
      "Iteration 18356, loss = 0.01258686\n",
      "Iteration 18357, loss = 0.01258608\n",
      "Iteration 18358, loss = 0.01258530\n",
      "Iteration 18359, loss = 0.01258451\n",
      "Iteration 18360, loss = 0.01258373\n",
      "Iteration 18361, loss = 0.01258295\n",
      "Iteration 18362, loss = 0.01258216\n",
      "Iteration 18363, loss = 0.01258138\n",
      "Iteration 18364, loss = 0.01258060\n",
      "Iteration 18365, loss = 0.01257981\n",
      "Iteration 18366, loss = 0.01257903\n",
      "Iteration 18367, loss = 0.01257825\n",
      "Iteration 18368, loss = 0.01257746\n",
      "Iteration 18369, loss = 0.01257668\n",
      "Iteration 18370, loss = 0.01257590\n",
      "Iteration 18371, loss = 0.01257512\n",
      "Iteration 18372, loss = 0.01257433\n",
      "Iteration 18373, loss = 0.01257355\n",
      "Iteration 18374, loss = 0.01257277\n",
      "Iteration 18375, loss = 0.01257199\n",
      "Iteration 18376, loss = 0.01257121\n",
      "Iteration 18377, loss = 0.01257042\n",
      "Iteration 18378, loss = 0.01256964\n",
      "Iteration 18379, loss = 0.01256886\n",
      "Iteration 18380, loss = 0.01256808\n",
      "Iteration 18381, loss = 0.01256730\n",
      "Iteration 18382, loss = 0.01256652\n",
      "Iteration 18383, loss = 0.01256574\n",
      "Iteration 18384, loss = 0.01256496\n",
      "Iteration 18385, loss = 0.01256418\n",
      "Iteration 18386, loss = 0.01256340\n",
      "Iteration 18387, loss = 0.01256261\n",
      "Iteration 18388, loss = 0.01256183\n",
      "Iteration 18389, loss = 0.01256105\n",
      "Iteration 18390, loss = 0.01256027\n",
      "Iteration 18391, loss = 0.01255949\n",
      "Iteration 18392, loss = 0.01255871\n",
      "Iteration 18393, loss = 0.01255793\n",
      "Iteration 18394, loss = 0.01255716\n",
      "Iteration 18395, loss = 0.01255638\n",
      "Iteration 18396, loss = 0.01255560\n",
      "Iteration 18397, loss = 0.01255482\n",
      "Iteration 18398, loss = 0.01255404\n",
      "Iteration 18399, loss = 0.01255326\n",
      "Iteration 18400, loss = 0.01255248\n",
      "Iteration 18401, loss = 0.01255170\n",
      "Iteration 18402, loss = 0.01255092\n",
      "Iteration 18403, loss = 0.01255015\n",
      "Iteration 18404, loss = 0.01254937\n",
      "Iteration 18405, loss = 0.01254859\n",
      "Iteration 18406, loss = 0.01254781\n",
      "Iteration 18407, loss = 0.01254703\n",
      "Iteration 18408, loss = 0.01254625\n",
      "Iteration 18409, loss = 0.01254548\n",
      "Iteration 18410, loss = 0.01254470\n",
      "Iteration 18411, loss = 0.01254392\n",
      "Iteration 18412, loss = 0.01254314\n",
      "Iteration 18413, loss = 0.01254237\n",
      "Iteration 18414, loss = 0.01254159\n",
      "Iteration 18415, loss = 0.01254081\n",
      "Iteration 18416, loss = 0.01254004\n",
      "Iteration 18417, loss = 0.01253926\n",
      "Iteration 18418, loss = 0.01253848\n",
      "Iteration 18419, loss = 0.01253771\n",
      "Iteration 18420, loss = 0.01253693\n",
      "Iteration 18421, loss = 0.01253615\n",
      "Iteration 18422, loss = 0.01253538\n",
      "Iteration 18423, loss = 0.01253460\n",
      "Iteration 18424, loss = 0.01253383\n",
      "Iteration 18425, loss = 0.01253305\n",
      "Iteration 18426, loss = 0.01253227\n",
      "Iteration 18427, loss = 0.01253150\n",
      "Iteration 18428, loss = 0.01253072\n",
      "Iteration 18429, loss = 0.01252995\n",
      "Iteration 18430, loss = 0.01252917\n",
      "Iteration 18431, loss = 0.01252840\n",
      "Iteration 18432, loss = 0.01252762\n",
      "Iteration 18433, loss = 0.01252685\n",
      "Iteration 18434, loss = 0.01252607\n",
      "Iteration 18435, loss = 0.01252530\n",
      "Iteration 18436, loss = 0.01252453\n",
      "Iteration 18437, loss = 0.01252375\n",
      "Iteration 18438, loss = 0.01252298\n",
      "Iteration 18439, loss = 0.01252220\n",
      "Iteration 18440, loss = 0.01252143\n",
      "Iteration 18441, loss = 0.01252065\n",
      "Iteration 18442, loss = 0.01251988\n",
      "Iteration 18443, loss = 0.01251911\n",
      "Iteration 18444, loss = 0.01251833\n",
      "Iteration 18445, loss = 0.01251756\n",
      "Iteration 18446, loss = 0.01251679\n",
      "Iteration 18447, loss = 0.01251601\n",
      "Iteration 18448, loss = 0.01251524\n",
      "Iteration 18449, loss = 0.01251447\n",
      "Iteration 18450, loss = 0.01251370\n",
      "Iteration 18451, loss = 0.01251292\n",
      "Iteration 18452, loss = 0.01251215\n",
      "Iteration 18453, loss = 0.01251138\n",
      "Iteration 18454, loss = 0.01251061\n",
      "Iteration 18455, loss = 0.01250983\n",
      "Iteration 18456, loss = 0.01250906\n",
      "Iteration 18457, loss = 0.01250829\n",
      "Iteration 18458, loss = 0.01250752\n",
      "Iteration 18459, loss = 0.01250675\n",
      "Iteration 18460, loss = 0.01250598\n",
      "Iteration 18461, loss = 0.01250520\n",
      "Iteration 18462, loss = 0.01250443\n",
      "Iteration 18463, loss = 0.01250366\n",
      "Iteration 18464, loss = 0.01250289\n",
      "Iteration 18465, loss = 0.01250212\n",
      "Iteration 18466, loss = 0.01250135\n",
      "Iteration 18467, loss = 0.01250058\n",
      "Iteration 18468, loss = 0.01249981\n",
      "Iteration 18469, loss = 0.01249904\n",
      "Iteration 18470, loss = 0.01249827\n",
      "Iteration 18471, loss = 0.01249750\n",
      "Iteration 18472, loss = 0.01249673\n",
      "Iteration 18473, loss = 0.01249596\n",
      "Iteration 18474, loss = 0.01249519\n",
      "Iteration 18475, loss = 0.01249442\n",
      "Iteration 18476, loss = 0.01249365\n",
      "Iteration 18477, loss = 0.01249288\n",
      "Iteration 18478, loss = 0.01249211\n",
      "Iteration 18479, loss = 0.01249134\n",
      "Iteration 18480, loss = 0.01249057\n",
      "Iteration 18481, loss = 0.01248980\n",
      "Iteration 18482, loss = 0.01248903\n",
      "Iteration 18483, loss = 0.01248827\n",
      "Iteration 18484, loss = 0.01248750\n",
      "Iteration 18485, loss = 0.01248673\n",
      "Iteration 18486, loss = 0.01248596\n",
      "Iteration 18487, loss = 0.01248519\n",
      "Iteration 18488, loss = 0.01248442\n",
      "Iteration 18489, loss = 0.01248366\n",
      "Iteration 18490, loss = 0.01248289\n",
      "Iteration 18491, loss = 0.01248212\n",
      "Iteration 18492, loss = 0.01248135\n",
      "Iteration 18493, loss = 0.01248058\n",
      "Iteration 18494, loss = 0.01247982\n",
      "Iteration 18495, loss = 0.01247905\n",
      "Iteration 18496, loss = 0.01247828\n",
      "Iteration 18497, loss = 0.01247752\n",
      "Iteration 18498, loss = 0.01247675\n",
      "Iteration 18499, loss = 0.01247598\n",
      "Iteration 18500, loss = 0.01247522\n",
      "Iteration 18501, loss = 0.01247445\n",
      "Iteration 18502, loss = 0.01247368\n",
      "Iteration 18503, loss = 0.01247292\n",
      "Iteration 18504, loss = 0.01247215\n",
      "Iteration 18505, loss = 0.01247138\n",
      "Iteration 18506, loss = 0.01247062\n",
      "Iteration 18507, loss = 0.01246985\n",
      "Iteration 18508, loss = 0.01246909\n",
      "Iteration 18509, loss = 0.01246832\n",
      "Iteration 18510, loss = 0.01246756\n",
      "Iteration 18511, loss = 0.01246679\n",
      "Iteration 18512, loss = 0.01246602\n",
      "Iteration 18513, loss = 0.01246526\n",
      "Iteration 18514, loss = 0.01246449\n",
      "Iteration 18515, loss = 0.01246373\n",
      "Iteration 18516, loss = 0.01246297\n",
      "Iteration 18517, loss = 0.01246220\n",
      "Iteration 18518, loss = 0.01246144\n",
      "Iteration 18519, loss = 0.01246067\n",
      "Iteration 18520, loss = 0.01245991\n",
      "Iteration 18521, loss = 0.01245914\n",
      "Iteration 18522, loss = 0.01245838\n",
      "Iteration 18523, loss = 0.01245762\n",
      "Iteration 18524, loss = 0.01245685\n",
      "Iteration 18525, loss = 0.01245609\n",
      "Iteration 18526, loss = 0.01245532\n",
      "Iteration 18527, loss = 0.01245456\n",
      "Iteration 18528, loss = 0.01245380\n",
      "Iteration 18529, loss = 0.01245303\n",
      "Iteration 18530, loss = 0.01245227\n",
      "Iteration 18531, loss = 0.01245151\n",
      "Iteration 18532, loss = 0.01245075\n",
      "Iteration 18533, loss = 0.01244998\n",
      "Iteration 18534, loss = 0.01244922\n",
      "Iteration 18535, loss = 0.01244846\n",
      "Iteration 18536, loss = 0.01244770\n",
      "Iteration 18537, loss = 0.01244693\n",
      "Iteration 18538, loss = 0.01244617\n",
      "Iteration 18539, loss = 0.01244541\n",
      "Iteration 18540, loss = 0.01244465\n",
      "Iteration 18541, loss = 0.01244389\n",
      "Iteration 18542, loss = 0.01244312\n",
      "Iteration 18543, loss = 0.01244236\n",
      "Iteration 18544, loss = 0.01244160\n",
      "Iteration 18545, loss = 0.01244084\n",
      "Iteration 18546, loss = 0.01244008\n",
      "Iteration 18547, loss = 0.01243932\n",
      "Iteration 18548, loss = 0.01243856\n",
      "Iteration 18549, loss = 0.01243780\n",
      "Iteration 18550, loss = 0.01243703\n",
      "Iteration 18551, loss = 0.01243627\n",
      "Iteration 18552, loss = 0.01243551\n",
      "Iteration 18553, loss = 0.01243475\n",
      "Iteration 18554, loss = 0.01243399\n",
      "Iteration 18555, loss = 0.01243323\n",
      "Iteration 18556, loss = 0.01243247\n",
      "Iteration 18557, loss = 0.01243171\n",
      "Iteration 18558, loss = 0.01243095\n",
      "Iteration 18559, loss = 0.01243019\n",
      "Iteration 18560, loss = 0.01242943\n",
      "Iteration 18561, loss = 0.01242868\n",
      "Iteration 18562, loss = 0.01242792\n",
      "Iteration 18563, loss = 0.01242716\n",
      "Iteration 18564, loss = 0.01242640\n",
      "Iteration 18565, loss = 0.01242564\n",
      "Iteration 18566, loss = 0.01242488\n",
      "Iteration 18567, loss = 0.01242412\n",
      "Iteration 18568, loss = 0.01242336\n",
      "Iteration 18569, loss = 0.01242260\n",
      "Iteration 18570, loss = 0.01242185\n",
      "Iteration 18571, loss = 0.01242109\n",
      "Iteration 18572, loss = 0.01242033\n",
      "Iteration 18573, loss = 0.01241957\n",
      "Iteration 18574, loss = 0.01241881\n",
      "Iteration 18575, loss = 0.01241806\n",
      "Iteration 18576, loss = 0.01241730\n",
      "Iteration 18577, loss = 0.01241654\n",
      "Iteration 18578, loss = 0.01241578\n",
      "Iteration 18579, loss = 0.01241503\n",
      "Iteration 18580, loss = 0.01241427\n",
      "Iteration 18581, loss = 0.01241351\n",
      "Iteration 18582, loss = 0.01241276\n",
      "Iteration 18583, loss = 0.01241200\n",
      "Iteration 18584, loss = 0.01241124\n",
      "Iteration 18585, loss = 0.01241049\n",
      "Iteration 18586, loss = 0.01240973\n",
      "Iteration 18587, loss = 0.01240897\n",
      "Iteration 18588, loss = 0.01240822\n",
      "Iteration 18589, loss = 0.01240746\n",
      "Iteration 18590, loss = 0.01240670\n",
      "Iteration 18591, loss = 0.01240595\n",
      "Iteration 18592, loss = 0.01240519\n",
      "Iteration 18593, loss = 0.01240444\n",
      "Iteration 18594, loss = 0.01240368\n",
      "Iteration 18595, loss = 0.01240293\n",
      "Iteration 18596, loss = 0.01240217\n",
      "Iteration 18597, loss = 0.01240142\n",
      "Iteration 18598, loss = 0.01240066\n",
      "Iteration 18599, loss = 0.01239991\n",
      "Iteration 18600, loss = 0.01239915\n",
      "Iteration 18601, loss = 0.01239840\n",
      "Iteration 18602, loss = 0.01239764\n",
      "Iteration 18603, loss = 0.01239689\n",
      "Iteration 18604, loss = 0.01239613\n",
      "Iteration 18605, loss = 0.01239538\n",
      "Iteration 18606, loss = 0.01239462\n",
      "Iteration 18607, loss = 0.01239387\n",
      "Iteration 18608, loss = 0.01239312\n",
      "Iteration 18609, loss = 0.01239236\n",
      "Iteration 18610, loss = 0.01239161\n",
      "Iteration 18611, loss = 0.01239086\n",
      "Iteration 18612, loss = 0.01239010\n",
      "Iteration 18613, loss = 0.01238935\n",
      "Iteration 18614, loss = 0.01238860\n",
      "Iteration 18615, loss = 0.01238784\n",
      "Iteration 18616, loss = 0.01238709\n",
      "Iteration 18617, loss = 0.01238634\n",
      "Iteration 18618, loss = 0.01238559\n",
      "Iteration 18619, loss = 0.01238483\n",
      "Iteration 18620, loss = 0.01238408\n",
      "Iteration 18621, loss = 0.01238333\n",
      "Iteration 18622, loss = 0.01238258\n",
      "Iteration 18623, loss = 0.01238182\n",
      "Iteration 18624, loss = 0.01238107\n",
      "Iteration 18625, loss = 0.01238032\n",
      "Iteration 18626, loss = 0.01237957\n",
      "Iteration 18627, loss = 0.01237882\n",
      "Iteration 18628, loss = 0.01237806\n",
      "Iteration 18629, loss = 0.01237731\n",
      "Iteration 18630, loss = 0.01237656\n",
      "Iteration 18631, loss = 0.01237581\n",
      "Iteration 18632, loss = 0.01237506\n",
      "Iteration 18633, loss = 0.01237431\n",
      "Iteration 18634, loss = 0.01237356\n",
      "Iteration 18635, loss = 0.01237281\n",
      "Iteration 18636, loss = 0.01237206\n",
      "Iteration 18637, loss = 0.01237131\n",
      "Iteration 18638, loss = 0.01237056\n",
      "Iteration 18639, loss = 0.01236981\n",
      "Iteration 18640, loss = 0.01236906\n",
      "Iteration 18641, loss = 0.01236831\n",
      "Iteration 18642, loss = 0.01236756\n",
      "Iteration 18643, loss = 0.01236681\n",
      "Iteration 18644, loss = 0.01236606\n",
      "Iteration 18645, loss = 0.01236531\n",
      "Iteration 18646, loss = 0.01236456\n",
      "Iteration 18647, loss = 0.01236381\n",
      "Iteration 18648, loss = 0.01236306\n",
      "Iteration 18649, loss = 0.01236231\n",
      "Iteration 18650, loss = 0.01236156\n",
      "Iteration 18651, loss = 0.01236081\n",
      "Iteration 18652, loss = 0.01236006\n",
      "Iteration 18653, loss = 0.01235932\n",
      "Iteration 18654, loss = 0.01235857\n",
      "Iteration 18655, loss = 0.01235782\n",
      "Iteration 18656, loss = 0.01235707\n",
      "Iteration 18657, loss = 0.01235632\n",
      "Iteration 18658, loss = 0.01235557\n",
      "Iteration 18659, loss = 0.01235483\n",
      "Iteration 18660, loss = 0.01235408\n",
      "Iteration 18661, loss = 0.01235333\n",
      "Iteration 18662, loss = 0.01235258\n",
      "Iteration 18663, loss = 0.01235184\n",
      "Iteration 18664, loss = 0.01235109\n",
      "Iteration 18665, loss = 0.01235034\n",
      "Iteration 18666, loss = 0.01234959\n",
      "Iteration 18667, loss = 0.01234885\n",
      "Iteration 18668, loss = 0.01234810\n",
      "Iteration 18669, loss = 0.01234735\n",
      "Iteration 18670, loss = 0.01234661\n",
      "Iteration 18671, loss = 0.01234586\n",
      "Iteration 18672, loss = 0.01234511\n",
      "Iteration 18673, loss = 0.01234437\n",
      "Iteration 18674, loss = 0.01234362\n",
      "Iteration 18675, loss = 0.01234288\n",
      "Iteration 18676, loss = 0.01234213\n",
      "Iteration 18677, loss = 0.01234138\n",
      "Iteration 18678, loss = 0.01234064\n",
      "Iteration 18679, loss = 0.01233989\n",
      "Iteration 18680, loss = 0.01233915\n",
      "Iteration 18681, loss = 0.01233840\n",
      "Iteration 18682, loss = 0.01233766\n",
      "Iteration 18683, loss = 0.01233691\n",
      "Iteration 18684, loss = 0.01233617\n",
      "Iteration 18685, loss = 0.01233542\n",
      "Iteration 18686, loss = 0.01233468\n",
      "Iteration 18687, loss = 0.01233393\n",
      "Iteration 18688, loss = 0.01233319\n",
      "Iteration 18689, loss = 0.01233244\n",
      "Iteration 18690, loss = 0.01233170\n",
      "Iteration 18691, loss = 0.01233095\n",
      "Iteration 18692, loss = 0.01233021\n",
      "Iteration 18693, loss = 0.01232947\n",
      "Iteration 18694, loss = 0.01232872\n",
      "Iteration 18695, loss = 0.01232798\n",
      "Iteration 18696, loss = 0.01232724\n",
      "Iteration 18697, loss = 0.01232649\n",
      "Iteration 18698, loss = 0.01232575\n",
      "Iteration 18699, loss = 0.01232501\n",
      "Iteration 18700, loss = 0.01232426\n",
      "Iteration 18701, loss = 0.01232352\n",
      "Iteration 18702, loss = 0.01232278\n",
      "Iteration 18703, loss = 0.01232203\n",
      "Iteration 18704, loss = 0.01232129\n",
      "Iteration 18705, loss = 0.01232055\n",
      "Iteration 18706, loss = 0.01231981\n",
      "Iteration 18707, loss = 0.01231906\n",
      "Iteration 18708, loss = 0.01231832\n",
      "Iteration 18709, loss = 0.01231758\n",
      "Iteration 18710, loss = 0.01231684\n",
      "Iteration 18711, loss = 0.01231609\n",
      "Iteration 18712, loss = 0.01231535\n",
      "Iteration 18713, loss = 0.01231461\n",
      "Iteration 18714, loss = 0.01231387\n",
      "Iteration 18715, loss = 0.01231313\n",
      "Iteration 18716, loss = 0.01231239\n",
      "Iteration 18717, loss = 0.01231165\n",
      "Iteration 18718, loss = 0.01231090\n",
      "Iteration 18719, loss = 0.01231016\n",
      "Iteration 18720, loss = 0.01230942\n",
      "Iteration 18721, loss = 0.01230868\n",
      "Iteration 18722, loss = 0.01230794\n",
      "Iteration 18723, loss = 0.01230720\n",
      "Iteration 18724, loss = 0.01230646\n",
      "Iteration 18725, loss = 0.01230572\n",
      "Iteration 18726, loss = 0.01230498\n",
      "Iteration 18727, loss = 0.01230424\n",
      "Iteration 18728, loss = 0.01230350\n",
      "Iteration 18729, loss = 0.01230276\n",
      "Iteration 18730, loss = 0.01230202\n",
      "Iteration 18731, loss = 0.01230128\n",
      "Iteration 18732, loss = 0.01230054\n",
      "Iteration 18733, loss = 0.01229980\n",
      "Iteration 18734, loss = 0.01229906\n",
      "Iteration 18735, loss = 0.01229832\n",
      "Iteration 18736, loss = 0.01229758\n",
      "Iteration 18737, loss = 0.01229685\n",
      "Iteration 18738, loss = 0.01229611\n",
      "Iteration 18739, loss = 0.01229537\n",
      "Iteration 18740, loss = 0.01229463\n",
      "Iteration 18741, loss = 0.01229389\n",
      "Iteration 18742, loss = 0.01229315\n",
      "Iteration 18743, loss = 0.01229241\n",
      "Iteration 18744, loss = 0.01229168\n",
      "Iteration 18745, loss = 0.01229094\n",
      "Iteration 18746, loss = 0.01229020\n",
      "Iteration 18747, loss = 0.01228946\n",
      "Iteration 18748, loss = 0.01228873\n",
      "Iteration 18749, loss = 0.01228799\n",
      "Iteration 18750, loss = 0.01228725\n",
      "Iteration 18751, loss = 0.01228651\n",
      "Iteration 18752, loss = 0.01228578\n",
      "Iteration 18753, loss = 0.01228504\n",
      "Iteration 18754, loss = 0.01228430\n",
      "Iteration 18755, loss = 0.01228356\n",
      "Iteration 18756, loss = 0.01228283\n",
      "Iteration 18757, loss = 0.01228209\n",
      "Iteration 18758, loss = 0.01228135\n",
      "Iteration 18759, loss = 0.01228062\n",
      "Iteration 18760, loss = 0.01227988\n",
      "Iteration 18761, loss = 0.01227915\n",
      "Iteration 18762, loss = 0.01227841\n",
      "Iteration 18763, loss = 0.01227767\n",
      "Iteration 18764, loss = 0.01227694\n",
      "Iteration 18765, loss = 0.01227620\n",
      "Iteration 18766, loss = 0.01227547\n",
      "Iteration 18767, loss = 0.01227473\n",
      "Iteration 18768, loss = 0.01227400\n",
      "Iteration 18769, loss = 0.01227326\n",
      "Iteration 18770, loss = 0.01227253\n",
      "Iteration 18771, loss = 0.01227179\n",
      "Iteration 18772, loss = 0.01227106\n",
      "Iteration 18773, loss = 0.01227032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18774, loss = 0.01226959\n",
      "Iteration 18775, loss = 0.01226885\n",
      "Iteration 18776, loss = 0.01226812\n",
      "Iteration 18777, loss = 0.01226738\n",
      "Iteration 18778, loss = 0.01226665\n",
      "Iteration 18779, loss = 0.01226591\n",
      "Iteration 18780, loss = 0.01226518\n",
      "Iteration 18781, loss = 0.01226445\n",
      "Iteration 18782, loss = 0.01226371\n",
      "Iteration 18783, loss = 0.01226298\n",
      "Iteration 18784, loss = 0.01226224\n",
      "Iteration 18785, loss = 0.01226151\n",
      "Iteration 18786, loss = 0.01226078\n",
      "Iteration 18787, loss = 0.01226004\n",
      "Iteration 18788, loss = 0.01225931\n",
      "Iteration 18789, loss = 0.01225858\n",
      "Iteration 18790, loss = 0.01225785\n",
      "Iteration 18791, loss = 0.01225711\n",
      "Iteration 18792, loss = 0.01225638\n",
      "Iteration 18793, loss = 0.01225565\n",
      "Iteration 18794, loss = 0.01225492\n",
      "Iteration 18795, loss = 0.01225418\n",
      "Iteration 18796, loss = 0.01225345\n",
      "Iteration 18797, loss = 0.01225272\n",
      "Iteration 18798, loss = 0.01225199\n",
      "Iteration 18799, loss = 0.01225125\n",
      "Iteration 18800, loss = 0.01225052\n",
      "Iteration 18801, loss = 0.01224979\n",
      "Iteration 18802, loss = 0.01224906\n",
      "Iteration 18803, loss = 0.01224833\n",
      "Iteration 18804, loss = 0.01224760\n",
      "Iteration 18805, loss = 0.01224687\n",
      "Iteration 18806, loss = 0.01224613\n",
      "Iteration 18807, loss = 0.01224540\n",
      "Iteration 18808, loss = 0.01224467\n",
      "Iteration 18809, loss = 0.01224394\n",
      "Iteration 18810, loss = 0.01224321\n",
      "Iteration 18811, loss = 0.01224248\n",
      "Iteration 18812, loss = 0.01224175\n",
      "Iteration 18813, loss = 0.01224102\n",
      "Iteration 18814, loss = 0.01224029\n",
      "Iteration 18815, loss = 0.01223956\n",
      "Iteration 18816, loss = 0.01223883\n",
      "Iteration 18817, loss = 0.01223810\n",
      "Iteration 18818, loss = 0.01223737\n",
      "Iteration 18819, loss = 0.01223664\n",
      "Iteration 18820, loss = 0.01223591\n",
      "Iteration 18821, loss = 0.01223518\n",
      "Iteration 18822, loss = 0.01223445\n",
      "Iteration 18823, loss = 0.01223372\n",
      "Iteration 18824, loss = 0.01223299\n",
      "Iteration 18825, loss = 0.01223226\n",
      "Iteration 18826, loss = 0.01223154\n",
      "Iteration 18827, loss = 0.01223081\n",
      "Iteration 18828, loss = 0.01223008\n",
      "Iteration 18829, loss = 0.01222935\n",
      "Iteration 18830, loss = 0.01222862\n",
      "Iteration 18831, loss = 0.01222789\n",
      "Iteration 18832, loss = 0.01222716\n",
      "Iteration 18833, loss = 0.01222644\n",
      "Iteration 18834, loss = 0.01222571\n",
      "Iteration 18835, loss = 0.01222498\n",
      "Iteration 18836, loss = 0.01222425\n",
      "Iteration 18837, loss = 0.01222353\n",
      "Iteration 18838, loss = 0.01222280\n",
      "Iteration 18839, loss = 0.01222207\n",
      "Iteration 18840, loss = 0.01222134\n",
      "Iteration 18841, loss = 0.01222062\n",
      "Iteration 18842, loss = 0.01221989\n",
      "Iteration 18843, loss = 0.01221916\n",
      "Iteration 18844, loss = 0.01221844\n",
      "Iteration 18845, loss = 0.01221771\n",
      "Iteration 18846, loss = 0.01221698\n",
      "Iteration 18847, loss = 0.01221626\n",
      "Iteration 18848, loss = 0.01221553\n",
      "Iteration 18849, loss = 0.01221480\n",
      "Iteration 18850, loss = 0.01221408\n",
      "Iteration 18851, loss = 0.01221335\n",
      "Iteration 18852, loss = 0.01221262\n",
      "Iteration 18853, loss = 0.01221190\n",
      "Iteration 18854, loss = 0.01221117\n",
      "Iteration 18855, loss = 0.01221045\n",
      "Iteration 18856, loss = 0.01220972\n",
      "Iteration 18857, loss = 0.01220900\n",
      "Iteration 18858, loss = 0.01220827\n",
      "Iteration 18859, loss = 0.01220755\n",
      "Iteration 18860, loss = 0.01220682\n",
      "Iteration 18861, loss = 0.01220610\n",
      "Iteration 18862, loss = 0.01220537\n",
      "Iteration 18863, loss = 0.01220465\n",
      "Iteration 18864, loss = 0.01220392\n",
      "Iteration 18865, loss = 0.01220320\n",
      "Iteration 18866, loss = 0.01220247\n",
      "Iteration 18867, loss = 0.01220175\n",
      "Iteration 18868, loss = 0.01220102\n",
      "Iteration 18869, loss = 0.01220030\n",
      "Iteration 18870, loss = 0.01219958\n",
      "Iteration 18871, loss = 0.01219885\n",
      "Iteration 18872, loss = 0.01219813\n",
      "Iteration 18873, loss = 0.01219740\n",
      "Iteration 18874, loss = 0.01219668\n",
      "Iteration 18875, loss = 0.01219596\n",
      "Iteration 18876, loss = 0.01219523\n",
      "Iteration 18877, loss = 0.01219451\n",
      "Iteration 18878, loss = 0.01219379\n",
      "Iteration 18879, loss = 0.01219307\n",
      "Iteration 18880, loss = 0.01219234\n",
      "Iteration 18881, loss = 0.01219162\n",
      "Iteration 18882, loss = 0.01219090\n",
      "Iteration 18883, loss = 0.01219017\n",
      "Iteration 18884, loss = 0.01218945\n",
      "Iteration 18885, loss = 0.01218873\n",
      "Iteration 18886, loss = 0.01218801\n",
      "Iteration 18887, loss = 0.01218729\n",
      "Iteration 18888, loss = 0.01218656\n",
      "Iteration 18889, loss = 0.01218584\n",
      "Iteration 18890, loss = 0.01218512\n",
      "Iteration 18891, loss = 0.01218440\n",
      "Iteration 18892, loss = 0.01218368\n",
      "Iteration 18893, loss = 0.01218296\n",
      "Iteration 18894, loss = 0.01218223\n",
      "Iteration 18895, loss = 0.01218151\n",
      "Iteration 18896, loss = 0.01218079\n",
      "Iteration 18897, loss = 0.01218007\n",
      "Iteration 18898, loss = 0.01217935\n",
      "Iteration 18899, loss = 0.01217863\n",
      "Iteration 18900, loss = 0.01217791\n",
      "Iteration 18901, loss = 0.01217719\n",
      "Iteration 18902, loss = 0.01217647\n",
      "Iteration 18903, loss = 0.01217575\n",
      "Iteration 18904, loss = 0.01217503\n",
      "Iteration 18905, loss = 0.01217431\n",
      "Iteration 18906, loss = 0.01217359\n",
      "Iteration 18907, loss = 0.01217287\n",
      "Iteration 18908, loss = 0.01217215\n",
      "Iteration 18909, loss = 0.01217143\n",
      "Iteration 18910, loss = 0.01217071\n",
      "Iteration 18911, loss = 0.01216999\n",
      "Iteration 18912, loss = 0.01216927\n",
      "Iteration 18913, loss = 0.01216855\n",
      "Iteration 18914, loss = 0.01216783\n",
      "Iteration 18915, loss = 0.01216711\n",
      "Iteration 18916, loss = 0.01216639\n",
      "Iteration 18917, loss = 0.01216567\n",
      "Iteration 18918, loss = 0.01216496\n",
      "Iteration 18919, loss = 0.01216424\n",
      "Iteration 18920, loss = 0.01216352\n",
      "Iteration 18921, loss = 0.01216280\n",
      "Iteration 18922, loss = 0.01216208\n",
      "Iteration 18923, loss = 0.01216136\n",
      "Iteration 18924, loss = 0.01216065\n",
      "Iteration 18925, loss = 0.01215993\n",
      "Iteration 18926, loss = 0.01215921\n",
      "Iteration 18927, loss = 0.01215849\n",
      "Iteration 18928, loss = 0.01215777\n",
      "Iteration 18929, loss = 0.01215706\n",
      "Iteration 18930, loss = 0.01215634\n",
      "Iteration 18931, loss = 0.01215562\n",
      "Iteration 18932, loss = 0.01215491\n",
      "Iteration 18933, loss = 0.01215419\n",
      "Iteration 18934, loss = 0.01215347\n",
      "Iteration 18935, loss = 0.01215275\n",
      "Iteration 18936, loss = 0.01215204\n",
      "Iteration 18937, loss = 0.01215132\n",
      "Iteration 18938, loss = 0.01215061\n",
      "Iteration 18939, loss = 0.01214989\n",
      "Iteration 18940, loss = 0.01214917\n",
      "Iteration 18941, loss = 0.01214846\n",
      "Iteration 18942, loss = 0.01214774\n",
      "Iteration 18943, loss = 0.01214702\n",
      "Iteration 18944, loss = 0.01214631\n",
      "Iteration 18945, loss = 0.01214559\n",
      "Iteration 18946, loss = 0.01214488\n",
      "Iteration 18947, loss = 0.01214416\n",
      "Iteration 18948, loss = 0.01214345\n",
      "Iteration 18949, loss = 0.01214273\n",
      "Iteration 18950, loss = 0.01214202\n",
      "Iteration 18951, loss = 0.01214130\n",
      "Iteration 18952, loss = 0.01214059\n",
      "Iteration 18953, loss = 0.01213987\n",
      "Iteration 18954, loss = 0.01213916\n",
      "Iteration 18955, loss = 0.01213844\n",
      "Iteration 18956, loss = 0.01213773\n",
      "Iteration 18957, loss = 0.01213701\n",
      "Iteration 18958, loss = 0.01213630\n",
      "Iteration 18959, loss = 0.01213558\n",
      "Iteration 18960, loss = 0.01213487\n",
      "Iteration 18961, loss = 0.01213416\n",
      "Iteration 18962, loss = 0.01213344\n",
      "Iteration 18963, loss = 0.01213273\n",
      "Iteration 18964, loss = 0.01213201\n",
      "Iteration 18965, loss = 0.01213130\n",
      "Iteration 18966, loss = 0.01213059\n",
      "Iteration 18967, loss = 0.01212987\n",
      "Iteration 18968, loss = 0.01212916\n",
      "Iteration 18969, loss = 0.01212845\n",
      "Iteration 18970, loss = 0.01212774\n",
      "Iteration 18971, loss = 0.01212702\n",
      "Iteration 18972, loss = 0.01212631\n",
      "Iteration 18973, loss = 0.01212560\n",
      "Iteration 18974, loss = 0.01212488\n",
      "Iteration 18975, loss = 0.01212417\n",
      "Iteration 18976, loss = 0.01212346\n",
      "Iteration 18977, loss = 0.01212275\n",
      "Iteration 18978, loss = 0.01212204\n",
      "Iteration 18979, loss = 0.01212132\n",
      "Iteration 18980, loss = 0.01212061\n",
      "Iteration 18981, loss = 0.01211990\n",
      "Iteration 18982, loss = 0.01211919\n",
      "Iteration 18983, loss = 0.01211848\n",
      "Iteration 18984, loss = 0.01211776\n",
      "Iteration 18985, loss = 0.01211705\n",
      "Iteration 18986, loss = 0.01211634\n",
      "Iteration 18987, loss = 0.01211563\n",
      "Iteration 18988, loss = 0.01211492\n",
      "Iteration 18989, loss = 0.01211421\n",
      "Iteration 18990, loss = 0.01211350\n",
      "Iteration 18991, loss = 0.01211279\n",
      "Iteration 18992, loss = 0.01211208\n",
      "Iteration 18993, loss = 0.01211137\n",
      "Iteration 18994, loss = 0.01211066\n",
      "Iteration 18995, loss = 0.01210995\n",
      "Iteration 18996, loss = 0.01210924\n",
      "Iteration 18997, loss = 0.01210853\n",
      "Iteration 18998, loss = 0.01210782\n",
      "Iteration 18999, loss = 0.01210711\n",
      "Iteration 19000, loss = 0.01210640\n",
      "Iteration 19001, loss = 0.01210569\n",
      "Iteration 19002, loss = 0.01210498\n",
      "Iteration 19003, loss = 0.01210427\n",
      "Iteration 19004, loss = 0.01210356\n",
      "Iteration 19005, loss = 0.01210285\n",
      "Iteration 19006, loss = 0.01210214\n",
      "Iteration 19007, loss = 0.01210143\n",
      "Iteration 19008, loss = 0.01210072\n",
      "Iteration 19009, loss = 0.01210001\n",
      "Iteration 19010, loss = 0.01209930\n",
      "Iteration 19011, loss = 0.01209860\n",
      "Iteration 19012, loss = 0.01209789\n",
      "Iteration 19013, loss = 0.01209718\n",
      "Iteration 19014, loss = 0.01209647\n",
      "Iteration 19015, loss = 0.01209576\n",
      "Iteration 19016, loss = 0.01209505\n",
      "Iteration 19017, loss = 0.01209435\n",
      "Iteration 19018, loss = 0.01209364\n",
      "Iteration 19019, loss = 0.01209293\n",
      "Iteration 19020, loss = 0.01209222\n",
      "Iteration 19021, loss = 0.01209152\n",
      "Iteration 19022, loss = 0.01209081\n",
      "Iteration 19023, loss = 0.01209010\n",
      "Iteration 19024, loss = 0.01208939\n",
      "Iteration 19025, loss = 0.01208869\n",
      "Iteration 19026, loss = 0.01208798\n",
      "Iteration 19027, loss = 0.01208727\n",
      "Iteration 19028, loss = 0.01208657\n",
      "Iteration 19029, loss = 0.01208586\n",
      "Iteration 19030, loss = 0.01208515\n",
      "Iteration 19031, loss = 0.01208445\n",
      "Iteration 19032, loss = 0.01208374\n",
      "Iteration 19033, loss = 0.01208303\n",
      "Iteration 19034, loss = 0.01208233\n",
      "Iteration 19035, loss = 0.01208162\n",
      "Iteration 19036, loss = 0.01208092\n",
      "Iteration 19037, loss = 0.01208021\n",
      "Iteration 19038, loss = 0.01207950\n",
      "Iteration 19039, loss = 0.01207880\n",
      "Iteration 19040, loss = 0.01207809\n",
      "Iteration 19041, loss = 0.01207739\n",
      "Iteration 19042, loss = 0.01207668\n",
      "Iteration 19043, loss = 0.01207598\n",
      "Iteration 19044, loss = 0.01207527\n",
      "Iteration 19045, loss = 0.01207457\n",
      "Iteration 19046, loss = 0.01207386\n",
      "Iteration 19047, loss = 0.01207316\n",
      "Iteration 19048, loss = 0.01207245\n",
      "Iteration 19049, loss = 0.01207175\n",
      "Iteration 19050, loss = 0.01207105\n",
      "Iteration 19051, loss = 0.01207034\n",
      "Iteration 19052, loss = 0.01206964\n",
      "Iteration 19053, loss = 0.01206893\n",
      "Iteration 19054, loss = 0.01206823\n",
      "Iteration 19055, loss = 0.01206752\n",
      "Iteration 19056, loss = 0.01206682\n",
      "Iteration 19057, loss = 0.01206612\n",
      "Iteration 19058, loss = 0.01206541\n",
      "Iteration 19059, loss = 0.01206471\n",
      "Iteration 19060, loss = 0.01206401\n",
      "Iteration 19061, loss = 0.01206330\n",
      "Iteration 19062, loss = 0.01206260\n",
      "Iteration 19063, loss = 0.01206190\n",
      "Iteration 19064, loss = 0.01206119\n",
      "Iteration 19065, loss = 0.01206049\n",
      "Iteration 19066, loss = 0.01205979\n",
      "Iteration 19067, loss = 0.01205909\n",
      "Iteration 19068, loss = 0.01205838\n",
      "Iteration 19069, loss = 0.01205768\n",
      "Iteration 19070, loss = 0.01205698\n",
      "Iteration 19071, loss = 0.01205628\n",
      "Iteration 19072, loss = 0.01205558\n",
      "Iteration 19073, loss = 0.01205487\n",
      "Iteration 19074, loss = 0.01205417\n",
      "Iteration 19075, loss = 0.01205347\n",
      "Iteration 19076, loss = 0.01205277\n",
      "Iteration 19077, loss = 0.01205207\n",
      "Iteration 19078, loss = 0.01205137\n",
      "Iteration 19079, loss = 0.01205066\n",
      "Iteration 19080, loss = 0.01204996\n",
      "Iteration 19081, loss = 0.01204926\n",
      "Iteration 19082, loss = 0.01204856\n",
      "Iteration 19083, loss = 0.01204786\n",
      "Iteration 19084, loss = 0.01204716\n",
      "Iteration 19085, loss = 0.01204646\n",
      "Iteration 19086, loss = 0.01204576\n",
      "Iteration 19087, loss = 0.01204506\n",
      "Iteration 19088, loss = 0.01204436\n",
      "Iteration 19089, loss = 0.01204366\n",
      "Iteration 19090, loss = 0.01204296\n",
      "Iteration 19091, loss = 0.01204226\n",
      "Iteration 19092, loss = 0.01204156\n",
      "Iteration 19093, loss = 0.01204086\n",
      "Iteration 19094, loss = 0.01204016\n",
      "Iteration 19095, loss = 0.01203946\n",
      "Iteration 19096, loss = 0.01203876\n",
      "Iteration 19097, loss = 0.01203806\n",
      "Iteration 19098, loss = 0.01203736\n",
      "Iteration 19099, loss = 0.01203666\n",
      "Iteration 19100, loss = 0.01203596\n",
      "Iteration 19101, loss = 0.01203526\n",
      "Iteration 19102, loss = 0.01203456\n",
      "Iteration 19103, loss = 0.01203386\n",
      "Iteration 19104, loss = 0.01203317\n",
      "Iteration 19105, loss = 0.01203247\n",
      "Iteration 19106, loss = 0.01203177\n",
      "Iteration 19107, loss = 0.01203107\n",
      "Iteration 19108, loss = 0.01203037\n",
      "Iteration 19109, loss = 0.01202967\n",
      "Iteration 19110, loss = 0.01202898\n",
      "Iteration 19111, loss = 0.01202828\n",
      "Iteration 19112, loss = 0.01202758\n",
      "Iteration 19113, loss = 0.01202688\n",
      "Iteration 19114, loss = 0.01202618\n",
      "Iteration 19115, loss = 0.01202549\n",
      "Iteration 19116, loss = 0.01202479\n",
      "Iteration 19117, loss = 0.01202409\n",
      "Iteration 19118, loss = 0.01202340\n",
      "Iteration 19119, loss = 0.01202270\n",
      "Iteration 19120, loss = 0.01202200\n",
      "Iteration 19121, loss = 0.01202130\n",
      "Iteration 19122, loss = 0.01202061\n",
      "Iteration 19123, loss = 0.01201991\n",
      "Iteration 19124, loss = 0.01201921\n",
      "Iteration 19125, loss = 0.01201852\n",
      "Iteration 19126, loss = 0.01201782\n",
      "Iteration 19127, loss = 0.01201713\n",
      "Iteration 19128, loss = 0.01201643\n",
      "Iteration 19129, loss = 0.01201573\n",
      "Iteration 19130, loss = 0.01201504\n",
      "Iteration 19131, loss = 0.01201434\n",
      "Iteration 19132, loss = 0.01201365\n",
      "Iteration 19133, loss = 0.01201295\n",
      "Iteration 19134, loss = 0.01201225\n",
      "Iteration 19135, loss = 0.01201156\n",
      "Iteration 19136, loss = 0.01201086\n",
      "Iteration 19137, loss = 0.01201017\n",
      "Iteration 19138, loss = 0.01200947\n",
      "Iteration 19139, loss = 0.01200878\n",
      "Iteration 19140, loss = 0.01200808\n",
      "Iteration 19141, loss = 0.01200739\n",
      "Iteration 19142, loss = 0.01200669\n",
      "Iteration 19143, loss = 0.01200600\n",
      "Iteration 19144, loss = 0.01200531\n",
      "Iteration 19145, loss = 0.01200461\n",
      "Iteration 19146, loss = 0.01200392\n",
      "Iteration 19147, loss = 0.01200322\n",
      "Iteration 19148, loss = 0.01200253\n",
      "Iteration 19149, loss = 0.01200183\n",
      "Iteration 19150, loss = 0.01200114\n",
      "Iteration 19151, loss = 0.01200045\n",
      "Iteration 19152, loss = 0.01199975\n",
      "Iteration 19153, loss = 0.01199906\n",
      "Iteration 19154, loss = 0.01199837\n",
      "Iteration 19155, loss = 0.01199767\n",
      "Iteration 19156, loss = 0.01199698\n",
      "Iteration 19157, loss = 0.01199629\n",
      "Iteration 19158, loss = 0.01199559\n",
      "Iteration 19159, loss = 0.01199490\n",
      "Iteration 19160, loss = 0.01199421\n",
      "Iteration 19161, loss = 0.01199352\n",
      "Iteration 19162, loss = 0.01199282\n",
      "Iteration 19163, loss = 0.01199213\n",
      "Iteration 19164, loss = 0.01199144\n",
      "Iteration 19165, loss = 0.01199075\n",
      "Iteration 19166, loss = 0.01199005\n",
      "Iteration 19167, loss = 0.01198936\n",
      "Iteration 19168, loss = 0.01198867\n",
      "Iteration 19169, loss = 0.01198798\n",
      "Iteration 19170, loss = 0.01198729\n",
      "Iteration 19171, loss = 0.01198659\n",
      "Iteration 19172, loss = 0.01198590\n",
      "Iteration 19173, loss = 0.01198521\n",
      "Iteration 19174, loss = 0.01198452\n",
      "Iteration 19175, loss = 0.01198383\n",
      "Iteration 19176, loss = 0.01198314\n",
      "Iteration 19177, loss = 0.01198245\n",
      "Iteration 19178, loss = 0.01198176\n",
      "Iteration 19179, loss = 0.01198106\n",
      "Iteration 19180, loss = 0.01198037\n",
      "Iteration 19181, loss = 0.01197968\n",
      "Iteration 19182, loss = 0.01197899\n",
      "Iteration 19183, loss = 0.01197830\n",
      "Iteration 19184, loss = 0.01197761\n",
      "Iteration 19185, loss = 0.01197692\n",
      "Iteration 19186, loss = 0.01197623\n",
      "Iteration 19187, loss = 0.01197554\n",
      "Iteration 19188, loss = 0.01197485\n",
      "Iteration 19189, loss = 0.01197416\n",
      "Iteration 19190, loss = 0.01197347\n",
      "Iteration 19191, loss = 0.01197278\n",
      "Iteration 19192, loss = 0.01197209\n",
      "Iteration 19193, loss = 0.01197140\n",
      "Iteration 19194, loss = 0.01197072\n",
      "Iteration 19195, loss = 0.01197003\n",
      "Iteration 19196, loss = 0.01196934\n",
      "Iteration 19197, loss = 0.01196865\n",
      "Iteration 19198, loss = 0.01196796\n",
      "Iteration 19199, loss = 0.01196727\n",
      "Iteration 19200, loss = 0.01196658\n",
      "Iteration 19201, loss = 0.01196589\n",
      "Iteration 19202, loss = 0.01196520\n",
      "Iteration 19203, loss = 0.01196452\n",
      "Iteration 19204, loss = 0.01196383\n",
      "Iteration 19205, loss = 0.01196314\n",
      "Iteration 19206, loss = 0.01196245\n",
      "Iteration 19207, loss = 0.01196176\n",
      "Iteration 19208, loss = 0.01196108\n",
      "Iteration 19209, loss = 0.01196039\n",
      "Iteration 19210, loss = 0.01195970\n",
      "Iteration 19211, loss = 0.01195901\n",
      "Iteration 19212, loss = 0.01195833\n",
      "Iteration 19213, loss = 0.01195764\n",
      "Iteration 19214, loss = 0.01195695\n",
      "Iteration 19215, loss = 0.01195626\n",
      "Iteration 19216, loss = 0.01195558\n",
      "Iteration 19217, loss = 0.01195489\n",
      "Iteration 19218, loss = 0.01195420\n",
      "Iteration 19219, loss = 0.01195352\n",
      "Iteration 19220, loss = 0.01195283\n",
      "Iteration 19221, loss = 0.01195214\n",
      "Iteration 19222, loss = 0.01195146\n",
      "Iteration 19223, loss = 0.01195077\n",
      "Iteration 19224, loss = 0.01195009\n",
      "Iteration 19225, loss = 0.01194940\n",
      "Iteration 19226, loss = 0.01194871\n",
      "Iteration 19227, loss = 0.01194803\n",
      "Iteration 19228, loss = 0.01194734\n",
      "Iteration 19229, loss = 0.01194666\n",
      "Iteration 19230, loss = 0.01194597\n",
      "Iteration 19231, loss = 0.01194529\n",
      "Iteration 19232, loss = 0.01194460\n",
      "Iteration 19233, loss = 0.01194391\n",
      "Iteration 19234, loss = 0.01194323\n",
      "Iteration 19235, loss = 0.01194254\n",
      "Iteration 19236, loss = 0.01194186\n",
      "Iteration 19237, loss = 0.01194117\n",
      "Iteration 19238, loss = 0.01194049\n",
      "Iteration 19239, loss = 0.01193981\n",
      "Iteration 19240, loss = 0.01193912\n",
      "Iteration 19241, loss = 0.01193844\n",
      "Iteration 19242, loss = 0.01193775\n",
      "Iteration 19243, loss = 0.01193707\n",
      "Iteration 19244, loss = 0.01193638\n",
      "Iteration 19245, loss = 0.01193570\n",
      "Iteration 19246, loss = 0.01193502\n",
      "Iteration 19247, loss = 0.01193433\n",
      "Iteration 19248, loss = 0.01193365\n",
      "Iteration 19249, loss = 0.01193296\n",
      "Iteration 19250, loss = 0.01193228\n",
      "Iteration 19251, loss = 0.01193160\n",
      "Iteration 19252, loss = 0.01193091\n",
      "Iteration 19253, loss = 0.01193023\n",
      "Iteration 19254, loss = 0.01192955\n",
      "Iteration 19255, loss = 0.01192887\n",
      "Iteration 19256, loss = 0.01192818\n",
      "Iteration 19257, loss = 0.01192750\n",
      "Iteration 19258, loss = 0.01192682\n",
      "Iteration 19259, loss = 0.01192613\n",
      "Iteration 19260, loss = 0.01192545\n",
      "Iteration 19261, loss = 0.01192477\n",
      "Iteration 19262, loss = 0.01192409\n",
      "Iteration 19263, loss = 0.01192340\n",
      "Iteration 19264, loss = 0.01192272\n",
      "Iteration 19265, loss = 0.01192204\n",
      "Iteration 19266, loss = 0.01192136\n",
      "Iteration 19267, loss = 0.01192068\n",
      "Iteration 19268, loss = 0.01192000\n",
      "Iteration 19269, loss = 0.01191931\n",
      "Iteration 19270, loss = 0.01191863\n",
      "Iteration 19271, loss = 0.01191795\n",
      "Iteration 19272, loss = 0.01191727\n",
      "Iteration 19273, loss = 0.01191659\n",
      "Iteration 19274, loss = 0.01191591\n",
      "Iteration 19275, loss = 0.01191523\n",
      "Iteration 19276, loss = 0.01191455\n",
      "Iteration 19277, loss = 0.01191387\n",
      "Iteration 19278, loss = 0.01191318\n",
      "Iteration 19279, loss = 0.01191250\n",
      "Iteration 19280, loss = 0.01191182\n",
      "Iteration 19281, loss = 0.01191114\n",
      "Iteration 19282, loss = 0.01191046\n",
      "Iteration 19283, loss = 0.01190978\n",
      "Iteration 19284, loss = 0.01190910\n",
      "Iteration 19285, loss = 0.01190842\n",
      "Iteration 19286, loss = 0.01190774\n",
      "Iteration 19287, loss = 0.01190706\n",
      "Iteration 19288, loss = 0.01190638\n",
      "Iteration 19289, loss = 0.01190570\n",
      "Iteration 19290, loss = 0.01190502\n",
      "Iteration 19291, loss = 0.01190435\n",
      "Iteration 19292, loss = 0.01190367\n",
      "Iteration 19293, loss = 0.01190299\n",
      "Iteration 19294, loss = 0.01190231\n",
      "Iteration 19295, loss = 0.01190163\n",
      "Iteration 19296, loss = 0.01190095\n",
      "Iteration 19297, loss = 0.01190027\n",
      "Iteration 19298, loss = 0.01189959\n",
      "Iteration 19299, loss = 0.01189891\n",
      "Iteration 19300, loss = 0.01189824\n",
      "Iteration 19301, loss = 0.01189756\n",
      "Iteration 19302, loss = 0.01189688\n",
      "Iteration 19303, loss = 0.01189620\n",
      "Iteration 19304, loss = 0.01189552\n",
      "Iteration 19305, loss = 0.01189485\n",
      "Iteration 19306, loss = 0.01189417\n",
      "Iteration 19307, loss = 0.01189349\n",
      "Iteration 19308, loss = 0.01189281\n",
      "Iteration 19309, loss = 0.01189213\n",
      "Iteration 19310, loss = 0.01189146\n",
      "Iteration 19311, loss = 0.01189078\n",
      "Iteration 19312, loss = 0.01189010\n",
      "Iteration 19313, loss = 0.01188943\n",
      "Iteration 19314, loss = 0.01188875\n",
      "Iteration 19315, loss = 0.01188807\n",
      "Iteration 19316, loss = 0.01188740\n",
      "Iteration 19317, loss = 0.01188672\n",
      "Iteration 19318, loss = 0.01188604\n",
      "Iteration 19319, loss = 0.01188537\n",
      "Iteration 19320, loss = 0.01188469\n",
      "Iteration 19321, loss = 0.01188401\n",
      "Iteration 19322, loss = 0.01188334\n",
      "Iteration 19323, loss = 0.01188266\n",
      "Iteration 19324, loss = 0.01188198\n",
      "Iteration 19325, loss = 0.01188131\n",
      "Iteration 19326, loss = 0.01188063\n",
      "Iteration 19327, loss = 0.01187996\n",
      "Iteration 19328, loss = 0.01187928\n",
      "Iteration 19329, loss = 0.01187861\n",
      "Iteration 19330, loss = 0.01187793\n",
      "Iteration 19331, loss = 0.01187726\n",
      "Iteration 19332, loss = 0.01187658\n",
      "Iteration 19333, loss = 0.01187590\n",
      "Iteration 19334, loss = 0.01187523\n",
      "Iteration 19335, loss = 0.01187456\n",
      "Iteration 19336, loss = 0.01187388\n",
      "Iteration 19337, loss = 0.01187321\n",
      "Iteration 19338, loss = 0.01187253\n",
      "Iteration 19339, loss = 0.01187186\n",
      "Iteration 19340, loss = 0.01187118\n",
      "Iteration 19341, loss = 0.01187051\n",
      "Iteration 19342, loss = 0.01186983\n",
      "Iteration 19343, loss = 0.01186916\n",
      "Iteration 19344, loss = 0.01186849\n",
      "Iteration 19345, loss = 0.01186781\n",
      "Iteration 19346, loss = 0.01186714\n",
      "Iteration 19347, loss = 0.01186646\n",
      "Iteration 19348, loss = 0.01186579\n",
      "Iteration 19349, loss = 0.01186512\n",
      "Iteration 19350, loss = 0.01186444\n",
      "Iteration 19351, loss = 0.01186377\n",
      "Iteration 19352, loss = 0.01186310\n",
      "Iteration 19353, loss = 0.01186242\n",
      "Iteration 19354, loss = 0.01186175\n",
      "Iteration 19355, loss = 0.01186108\n",
      "Iteration 19356, loss = 0.01186041\n",
      "Iteration 19357, loss = 0.01185973\n",
      "Iteration 19358, loss = 0.01185906\n",
      "Iteration 19359, loss = 0.01185839\n",
      "Iteration 19360, loss = 0.01185772\n",
      "Iteration 19361, loss = 0.01185704\n",
      "Iteration 19362, loss = 0.01185637\n",
      "Iteration 19363, loss = 0.01185570\n",
      "Iteration 19364, loss = 0.01185503\n",
      "Iteration 19365, loss = 0.01185436\n",
      "Iteration 19366, loss = 0.01185368\n",
      "Iteration 19367, loss = 0.01185301\n",
      "Iteration 19368, loss = 0.01185234\n",
      "Iteration 19369, loss = 0.01185167\n",
      "Iteration 19370, loss = 0.01185100\n",
      "Iteration 19371, loss = 0.01185033\n",
      "Iteration 19372, loss = 0.01184965\n",
      "Iteration 19373, loss = 0.01184898\n",
      "Iteration 19374, loss = 0.01184831\n",
      "Iteration 19375, loss = 0.01184764\n",
      "Iteration 19376, loss = 0.01184697\n",
      "Iteration 19377, loss = 0.01184630\n",
      "Iteration 19378, loss = 0.01184563\n",
      "Iteration 19379, loss = 0.01184496\n",
      "Iteration 19380, loss = 0.01184429\n",
      "Iteration 19381, loss = 0.01184362\n",
      "Iteration 19382, loss = 0.01184295\n",
      "Iteration 19383, loss = 0.01184228\n",
      "Iteration 19384, loss = 0.01184161\n",
      "Iteration 19385, loss = 0.01184094\n",
      "Iteration 19386, loss = 0.01184027\n",
      "Iteration 19387, loss = 0.01183960\n",
      "Iteration 19388, loss = 0.01183893\n",
      "Iteration 19389, loss = 0.01183826\n",
      "Iteration 19390, loss = 0.01183759\n",
      "Iteration 19391, loss = 0.01183692\n",
      "Iteration 19392, loss = 0.01183625\n",
      "Iteration 19393, loss = 0.01183558\n",
      "Iteration 19394, loss = 0.01183491\n",
      "Iteration 19395, loss = 0.01183425\n",
      "Iteration 19396, loss = 0.01183358\n",
      "Iteration 19397, loss = 0.01183291\n",
      "Iteration 19398, loss = 0.01183224\n",
      "Iteration 19399, loss = 0.01183157\n",
      "Iteration 19400, loss = 0.01183090\n",
      "Iteration 19401, loss = 0.01183023\n",
      "Iteration 19402, loss = 0.01182957\n",
      "Iteration 19403, loss = 0.01182890\n",
      "Iteration 19404, loss = 0.01182823\n",
      "Iteration 19405, loss = 0.01182756\n",
      "Iteration 19406, loss = 0.01182689\n",
      "Iteration 19407, loss = 0.01182623\n",
      "Iteration 19408, loss = 0.01182556\n",
      "Iteration 19409, loss = 0.01182489\n",
      "Iteration 19410, loss = 0.01182422\n",
      "Iteration 19411, loss = 0.01182356\n",
      "Iteration 19412, loss = 0.01182289\n",
      "Iteration 19413, loss = 0.01182222\n",
      "Iteration 19414, loss = 0.01182156\n",
      "Iteration 19415, loss = 0.01182089\n",
      "Iteration 19416, loss = 0.01182022\n",
      "Iteration 19417, loss = 0.01181955\n",
      "Iteration 19418, loss = 0.01181889\n",
      "Iteration 19419, loss = 0.01181822\n",
      "Iteration 19420, loss = 0.01181756\n",
      "Iteration 19421, loss = 0.01181689\n",
      "Iteration 19422, loss = 0.01181622\n",
      "Iteration 19423, loss = 0.01181556\n",
      "Iteration 19424, loss = 0.01181489\n",
      "Iteration 19425, loss = 0.01181422\n",
      "Iteration 19426, loss = 0.01181356\n",
      "Iteration 19427, loss = 0.01181289\n",
      "Iteration 19428, loss = 0.01181223\n",
      "Iteration 19429, loss = 0.01181156\n",
      "Iteration 19430, loss = 0.01181090\n",
      "Iteration 19431, loss = 0.01181023\n",
      "Iteration 19432, loss = 0.01180957\n",
      "Iteration 19433, loss = 0.01180890\n",
      "Iteration 19434, loss = 0.01180824\n",
      "Iteration 19435, loss = 0.01180757\n",
      "Iteration 19436, loss = 0.01180691\n",
      "Iteration 19437, loss = 0.01180624\n",
      "Iteration 19438, loss = 0.01180558\n",
      "Iteration 19439, loss = 0.01180491\n",
      "Iteration 19440, loss = 0.01180425\n",
      "Iteration 19441, loss = 0.01180358\n",
      "Iteration 19442, loss = 0.01180292\n",
      "Iteration 19443, loss = 0.01180225\n",
      "Iteration 19444, loss = 0.01180159\n",
      "Iteration 19445, loss = 0.01180093\n",
      "Iteration 19446, loss = 0.01180026\n",
      "Iteration 19447, loss = 0.01179960\n",
      "Iteration 19448, loss = 0.01179894\n",
      "Iteration 19449, loss = 0.01179827\n",
      "Iteration 19450, loss = 0.01179761\n",
      "Iteration 19451, loss = 0.01179694\n",
      "Iteration 19452, loss = 0.01179628\n",
      "Iteration 19453, loss = 0.01179562\n",
      "Iteration 19454, loss = 0.01179496\n",
      "Iteration 19455, loss = 0.01179429\n",
      "Iteration 19456, loss = 0.01179363\n",
      "Iteration 19457, loss = 0.01179297\n",
      "Iteration 19458, loss = 0.01179230\n",
      "Iteration 19459, loss = 0.01179164\n",
      "Iteration 19460, loss = 0.01179098\n",
      "Iteration 19461, loss = 0.01179032\n",
      "Iteration 19462, loss = 0.01178965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19463, loss = 0.01178899\n",
      "Iteration 19464, loss = 0.01178833\n",
      "Iteration 19465, loss = 0.01178767\n",
      "Iteration 19466, loss = 0.01178701\n",
      "Iteration 19467, loss = 0.01178634\n",
      "Iteration 19468, loss = 0.01178568\n",
      "Iteration 19469, loss = 0.01178502\n",
      "Iteration 19470, loss = 0.01178436\n",
      "Iteration 19471, loss = 0.01178370\n",
      "Iteration 19472, loss = 0.01178304\n",
      "Iteration 19473, loss = 0.01178238\n",
      "Iteration 19474, loss = 0.01178171\n",
      "Iteration 19475, loss = 0.01178105\n",
      "Iteration 19476, loss = 0.01178039\n",
      "Iteration 19477, loss = 0.01177973\n",
      "Iteration 19478, loss = 0.01177907\n",
      "Iteration 19479, loss = 0.01177841\n",
      "Iteration 19480, loss = 0.01177775\n",
      "Iteration 19481, loss = 0.01177709\n",
      "Iteration 19482, loss = 0.01177643\n",
      "Iteration 19483, loss = 0.01177577\n",
      "Iteration 19484, loss = 0.01177511\n",
      "Iteration 19485, loss = 0.01177445\n",
      "Iteration 19486, loss = 0.01177379\n",
      "Iteration 19487, loss = 0.01177313\n",
      "Iteration 19488, loss = 0.01177247\n",
      "Iteration 19489, loss = 0.01177181\n",
      "Iteration 19490, loss = 0.01177115\n",
      "Iteration 19491, loss = 0.01177049\n",
      "Iteration 19492, loss = 0.01176983\n",
      "Iteration 19493, loss = 0.01176917\n",
      "Iteration 19494, loss = 0.01176851\n",
      "Iteration 19495, loss = 0.01176785\n",
      "Iteration 19496, loss = 0.01176719\n",
      "Iteration 19497, loss = 0.01176654\n",
      "Iteration 19498, loss = 0.01176588\n",
      "Iteration 19499, loss = 0.01176522\n",
      "Iteration 19500, loss = 0.01176456\n",
      "Iteration 19501, loss = 0.01176390\n",
      "Iteration 19502, loss = 0.01176324\n",
      "Iteration 19503, loss = 0.01176258\n",
      "Iteration 19504, loss = 0.01176193\n",
      "Iteration 19505, loss = 0.01176127\n",
      "Iteration 19506, loss = 0.01176061\n",
      "Iteration 19507, loss = 0.01175995\n",
      "Iteration 19508, loss = 0.01175929\n",
      "Iteration 19509, loss = 0.01175864\n",
      "Iteration 19510, loss = 0.01175798\n",
      "Iteration 19511, loss = 0.01175732\n",
      "Iteration 19512, loss = 0.01175666\n",
      "Iteration 19513, loss = 0.01175601\n",
      "Iteration 19514, loss = 0.01175535\n",
      "Iteration 19515, loss = 0.01175469\n",
      "Iteration 19516, loss = 0.01175404\n",
      "Iteration 19517, loss = 0.01175338\n",
      "Iteration 19518, loss = 0.01175272\n",
      "Iteration 19519, loss = 0.01175206\n",
      "Iteration 19520, loss = 0.01175141\n",
      "Iteration 19521, loss = 0.01175075\n",
      "Iteration 19522, loss = 0.01175010\n",
      "Iteration 19523, loss = 0.01174944\n",
      "Iteration 19524, loss = 0.01174878\n",
      "Iteration 19525, loss = 0.01174813\n",
      "Iteration 19526, loss = 0.01174747\n",
      "Iteration 19527, loss = 0.01174681\n",
      "Iteration 19528, loss = 0.01174616\n",
      "Iteration 19529, loss = 0.01174550\n",
      "Iteration 19530, loss = 0.01174485\n",
      "Iteration 19531, loss = 0.01174419\n",
      "Iteration 19532, loss = 0.01174354\n",
      "Iteration 19533, loss = 0.01174288\n",
      "Iteration 19534, loss = 0.01174223\n",
      "Iteration 19535, loss = 0.01174157\n",
      "Iteration 19536, loss = 0.01174091\n",
      "Iteration 19537, loss = 0.01174026\n",
      "Iteration 19538, loss = 0.01173960\n",
      "Iteration 19539, loss = 0.01173895\n",
      "Iteration 19540, loss = 0.01173830\n",
      "Iteration 19541, loss = 0.01173764\n",
      "Iteration 19542, loss = 0.01173699\n",
      "Iteration 19543, loss = 0.01173633\n",
      "Iteration 19544, loss = 0.01173568\n",
      "Iteration 19545, loss = 0.01173502\n",
      "Iteration 19546, loss = 0.01173437\n",
      "Iteration 19547, loss = 0.01173372\n",
      "Iteration 19548, loss = 0.01173306\n",
      "Iteration 19549, loss = 0.01173241\n",
      "Iteration 19550, loss = 0.01173175\n",
      "Iteration 19551, loss = 0.01173110\n",
      "Iteration 19552, loss = 0.01173045\n",
      "Iteration 19553, loss = 0.01172979\n",
      "Iteration 19554, loss = 0.01172914\n",
      "Iteration 19555, loss = 0.01172849\n",
      "Iteration 19556, loss = 0.01172783\n",
      "Iteration 19557, loss = 0.01172718\n",
      "Iteration 19558, loss = 0.01172653\n",
      "Iteration 19559, loss = 0.01172587\n",
      "Iteration 19560, loss = 0.01172522\n",
      "Iteration 19561, loss = 0.01172457\n",
      "Iteration 19562, loss = 0.01172392\n",
      "Iteration 19563, loss = 0.01172326\n",
      "Iteration 19564, loss = 0.01172261\n",
      "Iteration 19565, loss = 0.01172196\n",
      "Iteration 19566, loss = 0.01172131\n",
      "Iteration 19567, loss = 0.01172065\n",
      "Iteration 19568, loss = 0.01172000\n",
      "Iteration 19569, loss = 0.01171935\n",
      "Iteration 19570, loss = 0.01171870\n",
      "Iteration 19571, loss = 0.01171805\n",
      "Iteration 19572, loss = 0.01171740\n",
      "Iteration 19573, loss = 0.01171674\n",
      "Iteration 19574, loss = 0.01171609\n",
      "Iteration 19575, loss = 0.01171544\n",
      "Iteration 19576, loss = 0.01171479\n",
      "Iteration 19577, loss = 0.01171414\n",
      "Iteration 19578, loss = 0.01171349\n",
      "Iteration 19579, loss = 0.01171284\n",
      "Iteration 19580, loss = 0.01171219\n",
      "Iteration 19581, loss = 0.01171153\n",
      "Iteration 19582, loss = 0.01171088\n",
      "Iteration 19583, loss = 0.01171023\n",
      "Iteration 19584, loss = 0.01170958\n",
      "Iteration 19585, loss = 0.01170893\n",
      "Iteration 19586, loss = 0.01170828\n",
      "Iteration 19587, loss = 0.01170763\n",
      "Iteration 19588, loss = 0.01170698\n",
      "Iteration 19589, loss = 0.01170633\n",
      "Iteration 19590, loss = 0.01170568\n",
      "Iteration 19591, loss = 0.01170503\n",
      "Iteration 19592, loss = 0.01170438\n",
      "Iteration 19593, loss = 0.01170373\n",
      "Iteration 19594, loss = 0.01170308\n",
      "Iteration 19595, loss = 0.01170243\n",
      "Iteration 19596, loss = 0.01170178\n",
      "Iteration 19597, loss = 0.01170113\n",
      "Iteration 19598, loss = 0.01170049\n",
      "Iteration 19599, loss = 0.01169984\n",
      "Iteration 19600, loss = 0.01169919\n",
      "Iteration 19601, loss = 0.01169854\n",
      "Iteration 19602, loss = 0.01169789\n",
      "Iteration 19603, loss = 0.01169724\n",
      "Iteration 19604, loss = 0.01169659\n",
      "Iteration 19605, loss = 0.01169594\n",
      "Iteration 19606, loss = 0.01169530\n",
      "Iteration 19607, loss = 0.01169465\n",
      "Iteration 19608, loss = 0.01169400\n",
      "Iteration 19609, loss = 0.01169335\n",
      "Iteration 19610, loss = 0.01169270\n",
      "Iteration 19611, loss = 0.01169206\n",
      "Iteration 19612, loss = 0.01169141\n",
      "Iteration 19613, loss = 0.01169076\n",
      "Iteration 19614, loss = 0.01169011\n",
      "Iteration 19615, loss = 0.01168946\n",
      "Iteration 19616, loss = 0.01168882\n",
      "Iteration 19617, loss = 0.01168817\n",
      "Iteration 19618, loss = 0.01168752\n",
      "Iteration 19619, loss = 0.01168688\n",
      "Iteration 19620, loss = 0.01168623\n",
      "Iteration 19621, loss = 0.01168558\n",
      "Iteration 19622, loss = 0.01168493\n",
      "Iteration 19623, loss = 0.01168429\n",
      "Iteration 19624, loss = 0.01168364\n",
      "Iteration 19625, loss = 0.01168299\n",
      "Iteration 19626, loss = 0.01168235\n",
      "Iteration 19627, loss = 0.01168170\n",
      "Iteration 19628, loss = 0.01168105\n",
      "Iteration 19629, loss = 0.01168041\n",
      "Iteration 19630, loss = 0.01167976\n",
      "Iteration 19631, loss = 0.01167912\n",
      "Iteration 19632, loss = 0.01167847\n",
      "Iteration 19633, loss = 0.01167782\n",
      "Iteration 19634, loss = 0.01167718\n",
      "Iteration 19635, loss = 0.01167653\n",
      "Iteration 19636, loss = 0.01167589\n",
      "Iteration 19637, loss = 0.01167524\n",
      "Iteration 19638, loss = 0.01167460\n",
      "Iteration 19639, loss = 0.01167395\n",
      "Iteration 19640, loss = 0.01167331\n",
      "Iteration 19641, loss = 0.01167266\n",
      "Iteration 19642, loss = 0.01167202\n",
      "Iteration 19643, loss = 0.01167137\n",
      "Iteration 19644, loss = 0.01167073\n",
      "Iteration 19645, loss = 0.01167008\n",
      "Iteration 19646, loss = 0.01166944\n",
      "Iteration 19647, loss = 0.01166879\n",
      "Iteration 19648, loss = 0.01166815\n",
      "Iteration 19649, loss = 0.01166750\n",
      "Iteration 19650, loss = 0.01166686\n",
      "Iteration 19651, loss = 0.01166622\n",
      "Iteration 19652, loss = 0.01166557\n",
      "Iteration 19653, loss = 0.01166493\n",
      "Iteration 19654, loss = 0.01166428\n",
      "Iteration 19655, loss = 0.01166364\n",
      "Iteration 19656, loss = 0.01166300\n",
      "Iteration 19657, loss = 0.01166235\n",
      "Iteration 19658, loss = 0.01166171\n",
      "Iteration 19659, loss = 0.01166107\n",
      "Iteration 19660, loss = 0.01166042\n",
      "Iteration 19661, loss = 0.01165978\n",
      "Iteration 19662, loss = 0.01165914\n",
      "Iteration 19663, loss = 0.01165849\n",
      "Iteration 19664, loss = 0.01165785\n",
      "Iteration 19665, loss = 0.01165721\n",
      "Iteration 19666, loss = 0.01165656\n",
      "Iteration 19667, loss = 0.01165592\n",
      "Iteration 19668, loss = 0.01165528\n",
      "Iteration 19669, loss = 0.01165464\n",
      "Iteration 19670, loss = 0.01165399\n",
      "Iteration 19671, loss = 0.01165335\n",
      "Iteration 19672, loss = 0.01165271\n",
      "Iteration 19673, loss = 0.01165207\n",
      "Iteration 19674, loss = 0.01165143\n",
      "Iteration 19675, loss = 0.01165078\n",
      "Iteration 19676, loss = 0.01165014\n",
      "Iteration 19677, loss = 0.01164950\n",
      "Iteration 19678, loss = 0.01164886\n",
      "Iteration 19679, loss = 0.01164822\n",
      "Iteration 19680, loss = 0.01164758\n",
      "Iteration 19681, loss = 0.01164693\n",
      "Iteration 19682, loss = 0.01164629\n",
      "Iteration 19683, loss = 0.01164565\n",
      "Iteration 19684, loss = 0.01164501\n",
      "Iteration 19685, loss = 0.01164437\n",
      "Iteration 19686, loss = 0.01164373\n",
      "Iteration 19687, loss = 0.01164309\n",
      "Iteration 19688, loss = 0.01164245\n",
      "Iteration 19689, loss = 0.01164181\n",
      "Iteration 19690, loss = 0.01164117\n",
      "Iteration 19691, loss = 0.01164053\n",
      "Iteration 19692, loss = 0.01163989\n",
      "Iteration 19693, loss = 0.01163925\n",
      "Iteration 19694, loss = 0.01163861\n",
      "Iteration 19695, loss = 0.01163797\n",
      "Iteration 19696, loss = 0.01163733\n",
      "Iteration 19697, loss = 0.01163669\n",
      "Iteration 19698, loss = 0.01163605\n",
      "Iteration 19699, loss = 0.01163541\n",
      "Iteration 19700, loss = 0.01163477\n",
      "Iteration 19701, loss = 0.01163413\n",
      "Iteration 19702, loss = 0.01163349\n",
      "Iteration 19703, loss = 0.01163285\n",
      "Iteration 19704, loss = 0.01163221\n",
      "Iteration 19705, loss = 0.01163157\n",
      "Iteration 19706, loss = 0.01163093\n",
      "Iteration 19707, loss = 0.01163029\n",
      "Iteration 19708, loss = 0.01162965\n",
      "Iteration 19709, loss = 0.01162902\n",
      "Iteration 19710, loss = 0.01162838\n",
      "Iteration 19711, loss = 0.01162774\n",
      "Iteration 19712, loss = 0.01162710\n",
      "Iteration 19713, loss = 0.01162646\n",
      "Iteration 19714, loss = 0.01162582\n",
      "Iteration 19715, loss = 0.01162518\n",
      "Iteration 19716, loss = 0.01162455\n",
      "Iteration 19717, loss = 0.01162391\n",
      "Iteration 19718, loss = 0.01162327\n",
      "Iteration 19719, loss = 0.01162263\n",
      "Iteration 19720, loss = 0.01162200\n",
      "Iteration 19721, loss = 0.01162136\n",
      "Iteration 19722, loss = 0.01162072\n",
      "Iteration 19723, loss = 0.01162008\n",
      "Iteration 19724, loss = 0.01161945\n",
      "Iteration 19725, loss = 0.01161881\n",
      "Iteration 19726, loss = 0.01161817\n",
      "Iteration 19727, loss = 0.01161753\n",
      "Iteration 19728, loss = 0.01161690\n",
      "Iteration 19729, loss = 0.01161626\n",
      "Iteration 19730, loss = 0.01161562\n",
      "Iteration 19731, loss = 0.01161499\n",
      "Iteration 19732, loss = 0.01161435\n",
      "Iteration 19733, loss = 0.01161371\n",
      "Iteration 19734, loss = 0.01161308\n",
      "Iteration 19735, loss = 0.01161244\n",
      "Iteration 19736, loss = 0.01161180\n",
      "Iteration 19737, loss = 0.01161117\n",
      "Iteration 19738, loss = 0.01161053\n",
      "Iteration 19739, loss = 0.01160990\n",
      "Iteration 19740, loss = 0.01160926\n",
      "Iteration 19741, loss = 0.01160862\n",
      "Iteration 19742, loss = 0.01160799\n",
      "Iteration 19743, loss = 0.01160735\n",
      "Iteration 19744, loss = 0.01160672\n",
      "Iteration 19745, loss = 0.01160608\n",
      "Iteration 19746, loss = 0.01160545\n",
      "Iteration 19747, loss = 0.01160481\n",
      "Iteration 19748, loss = 0.01160418\n",
      "Iteration 19749, loss = 0.01160354\n",
      "Iteration 19750, loss = 0.01160291\n",
      "Iteration 19751, loss = 0.01160227\n",
      "Iteration 19752, loss = 0.01160164\n",
      "Iteration 19753, loss = 0.01160100\n",
      "Iteration 19754, loss = 0.01160037\n",
      "Iteration 19755, loss = 0.01159973\n",
      "Iteration 19756, loss = 0.01159910\n",
      "Iteration 19757, loss = 0.01159847\n",
      "Iteration 19758, loss = 0.01159783\n",
      "Iteration 19759, loss = 0.01159720\n",
      "Iteration 19760, loss = 0.01159656\n",
      "Iteration 19761, loss = 0.01159593\n",
      "Iteration 19762, loss = 0.01159530\n",
      "Iteration 19763, loss = 0.01159466\n",
      "Iteration 19764, loss = 0.01159403\n",
      "Iteration 19765, loss = 0.01159339\n",
      "Iteration 19766, loss = 0.01159276\n",
      "Iteration 19767, loss = 0.01159213\n",
      "Iteration 19768, loss = 0.01159149\n",
      "Iteration 19769, loss = 0.01159086\n",
      "Iteration 19770, loss = 0.01159023\n",
      "Iteration 19771, loss = 0.01158960\n",
      "Iteration 19772, loss = 0.01158896\n",
      "Iteration 19773, loss = 0.01158833\n",
      "Iteration 19774, loss = 0.01158770\n",
      "Iteration 19775, loss = 0.01158706\n",
      "Iteration 19776, loss = 0.01158643\n",
      "Iteration 19777, loss = 0.01158580\n",
      "Iteration 19778, loss = 0.01158517\n",
      "Iteration 19779, loss = 0.01158453\n",
      "Iteration 19780, loss = 0.01158390\n",
      "Iteration 19781, loss = 0.01158327\n",
      "Iteration 19782, loss = 0.01158264\n",
      "Iteration 19783, loss = 0.01158201\n",
      "Iteration 19784, loss = 0.01158137\n",
      "Iteration 19785, loss = 0.01158074\n",
      "Iteration 19786, loss = 0.01158011\n",
      "Iteration 19787, loss = 0.01157948\n",
      "Iteration 19788, loss = 0.01157885\n",
      "Iteration 19789, loss = 0.01157822\n",
      "Iteration 19790, loss = 0.01157759\n",
      "Iteration 19791, loss = 0.01157695\n",
      "Iteration 19792, loss = 0.01157632\n",
      "Iteration 19793, loss = 0.01157569\n",
      "Iteration 19794, loss = 0.01157506\n",
      "Iteration 19795, loss = 0.01157443\n",
      "Iteration 19796, loss = 0.01157380\n",
      "Iteration 19797, loss = 0.01157317\n",
      "Iteration 19798, loss = 0.01157254\n",
      "Iteration 19799, loss = 0.01157191\n",
      "Iteration 19800, loss = 0.01157128\n",
      "Iteration 19801, loss = 0.01157065\n",
      "Iteration 19802, loss = 0.01157002\n",
      "Iteration 19803, loss = 0.01156939\n",
      "Iteration 19804, loss = 0.01156876\n",
      "Iteration 19805, loss = 0.01156813\n",
      "Iteration 19806, loss = 0.01156750\n",
      "Iteration 19807, loss = 0.01156687\n",
      "Iteration 19808, loss = 0.01156624\n",
      "Iteration 19809, loss = 0.01156561\n",
      "Iteration 19810, loss = 0.01156498\n",
      "Iteration 19811, loss = 0.01156435\n",
      "Iteration 19812, loss = 0.01156372\n",
      "Iteration 19813, loss = 0.01156309\n",
      "Iteration 19814, loss = 0.01156246\n",
      "Iteration 19815, loss = 0.01156183\n",
      "Iteration 19816, loss = 0.01156121\n",
      "Iteration 19817, loss = 0.01156058\n",
      "Iteration 19818, loss = 0.01155995\n",
      "Iteration 19819, loss = 0.01155932\n",
      "Iteration 19820, loss = 0.01155869\n",
      "Iteration 19821, loss = 0.01155806\n",
      "Iteration 19822, loss = 0.01155743\n",
      "Iteration 19823, loss = 0.01155681\n",
      "Iteration 19824, loss = 0.01155618\n",
      "Iteration 19825, loss = 0.01155555\n",
      "Iteration 19826, loss = 0.01155492\n",
      "Iteration 19827, loss = 0.01155429\n",
      "Iteration 19828, loss = 0.01155367\n",
      "Iteration 19829, loss = 0.01155304\n",
      "Iteration 19830, loss = 0.01155241\n",
      "Iteration 19831, loss = 0.01155178\n",
      "Iteration 19832, loss = 0.01155116\n",
      "Iteration 19833, loss = 0.01155053\n",
      "Iteration 19834, loss = 0.01154990\n",
      "Iteration 19835, loss = 0.01154927\n",
      "Iteration 19836, loss = 0.01154865\n",
      "Iteration 19837, loss = 0.01154802\n",
      "Iteration 19838, loss = 0.01154739\n",
      "Iteration 19839, loss = 0.01154677\n",
      "Iteration 19840, loss = 0.01154614\n",
      "Iteration 19841, loss = 0.01154551\n",
      "Iteration 19842, loss = 0.01154489\n",
      "Iteration 19843, loss = 0.01154426\n",
      "Iteration 19844, loss = 0.01154363\n",
      "Iteration 19845, loss = 0.01154301\n",
      "Iteration 19846, loss = 0.01154238\n",
      "Iteration 19847, loss = 0.01154175\n",
      "Iteration 19848, loss = 0.01154113\n",
      "Iteration 19849, loss = 0.01154050\n",
      "Iteration 19850, loss = 0.01153988\n",
      "Iteration 19851, loss = 0.01153925\n",
      "Iteration 19852, loss = 0.01153863\n",
      "Iteration 19853, loss = 0.01153800\n",
      "Iteration 19854, loss = 0.01153737\n",
      "Iteration 19855, loss = 0.01153675\n",
      "Iteration 19856, loss = 0.01153612\n",
      "Iteration 19857, loss = 0.01153550\n",
      "Iteration 19858, loss = 0.01153487\n",
      "Iteration 19859, loss = 0.01153425\n",
      "Iteration 19860, loss = 0.01153362\n",
      "Iteration 19861, loss = 0.01153300\n",
      "Iteration 19862, loss = 0.01153237\n",
      "Iteration 19863, loss = 0.01153175\n",
      "Iteration 19864, loss = 0.01153113\n",
      "Iteration 19865, loss = 0.01153050\n",
      "Iteration 19866, loss = 0.01152988\n",
      "Iteration 19867, loss = 0.01152925\n",
      "Iteration 19868, loss = 0.01152863\n",
      "Iteration 19869, loss = 0.01152800\n",
      "Iteration 19870, loss = 0.01152738\n",
      "Iteration 19871, loss = 0.01152676\n",
      "Iteration 19872, loss = 0.01152613\n",
      "Iteration 19873, loss = 0.01152551\n",
      "Iteration 19874, loss = 0.01152489\n",
      "Iteration 19875, loss = 0.01152426\n",
      "Iteration 19876, loss = 0.01152364\n",
      "Iteration 19877, loss = 0.01152301\n",
      "Iteration 19878, loss = 0.01152239\n",
      "Iteration 19879, loss = 0.01152177\n",
      "Iteration 19880, loss = 0.01152115\n",
      "Iteration 19881, loss = 0.01152052\n",
      "Iteration 19882, loss = 0.01151990\n",
      "Iteration 19883, loss = 0.01151928\n",
      "Iteration 19884, loss = 0.01151865\n",
      "Iteration 19885, loss = 0.01151803\n",
      "Iteration 19886, loss = 0.01151741\n",
      "Iteration 19887, loss = 0.01151679\n",
      "Iteration 19888, loss = 0.01151616\n",
      "Iteration 19889, loss = 0.01151554\n",
      "Iteration 19890, loss = 0.01151492\n",
      "Iteration 19891, loss = 0.01151430\n",
      "Iteration 19892, loss = 0.01151368\n",
      "Iteration 19893, loss = 0.01151305\n",
      "Iteration 19894, loss = 0.01151243\n",
      "Iteration 19895, loss = 0.01151181\n",
      "Iteration 19896, loss = 0.01151119\n",
      "Iteration 19897, loss = 0.01151057\n",
      "Iteration 19898, loss = 0.01150994\n",
      "Iteration 19899, loss = 0.01150932\n",
      "Iteration 19900, loss = 0.01150870\n",
      "Iteration 19901, loss = 0.01150808\n",
      "Iteration 19902, loss = 0.01150746\n",
      "Iteration 19903, loss = 0.01150684\n",
      "Iteration 19904, loss = 0.01150622\n",
      "Iteration 19905, loss = 0.01150560\n",
      "Iteration 19906, loss = 0.01150498\n",
      "Iteration 19907, loss = 0.01150436\n",
      "Iteration 19908, loss = 0.01150374\n",
      "Iteration 19909, loss = 0.01150311\n",
      "Iteration 19910, loss = 0.01150249\n",
      "Iteration 19911, loss = 0.01150187\n",
      "Iteration 19912, loss = 0.01150125\n",
      "Iteration 19913, loss = 0.01150063\n",
      "Iteration 19914, loss = 0.01150001\n",
      "Iteration 19915, loss = 0.01149939\n",
      "Iteration 19916, loss = 0.01149877\n",
      "Iteration 19917, loss = 0.01149815\n",
      "Iteration 19918, loss = 0.01149753\n",
      "Iteration 19919, loss = 0.01149691\n",
      "Iteration 19920, loss = 0.01149630\n",
      "Iteration 19921, loss = 0.01149568\n",
      "Iteration 19922, loss = 0.01149506\n",
      "Iteration 19923, loss = 0.01149444\n",
      "Iteration 19924, loss = 0.01149382\n",
      "Iteration 19925, loss = 0.01149320\n",
      "Iteration 19926, loss = 0.01149258\n",
      "Iteration 19927, loss = 0.01149196\n",
      "Iteration 19928, loss = 0.01149134\n",
      "Iteration 19929, loss = 0.01149072\n",
      "Iteration 19930, loss = 0.01149010\n",
      "Iteration 19931, loss = 0.01148949\n",
      "Iteration 19932, loss = 0.01148887\n",
      "Iteration 19933, loss = 0.01148825\n",
      "Iteration 19934, loss = 0.01148763\n",
      "Iteration 19935, loss = 0.01148701\n",
      "Iteration 19936, loss = 0.01148639\n",
      "Iteration 19937, loss = 0.01148578\n",
      "Iteration 19938, loss = 0.01148516\n",
      "Iteration 19939, loss = 0.01148454\n",
      "Iteration 19940, loss = 0.01148392\n",
      "Iteration 19941, loss = 0.01148331\n",
      "Iteration 19942, loss = 0.01148269\n",
      "Iteration 19943, loss = 0.01148207\n",
      "Iteration 19944, loss = 0.01148145\n",
      "Iteration 19945, loss = 0.01148084\n",
      "Iteration 19946, loss = 0.01148022\n",
      "Iteration 19947, loss = 0.01147960\n",
      "Iteration 19948, loss = 0.01147898\n",
      "Iteration 19949, loss = 0.01147837\n",
      "Iteration 19950, loss = 0.01147775\n",
      "Iteration 19951, loss = 0.01147713\n",
      "Iteration 19952, loss = 0.01147652\n",
      "Iteration 19953, loss = 0.01147590\n",
      "Iteration 19954, loss = 0.01147528\n",
      "Iteration 19955, loss = 0.01147467\n",
      "Iteration 19956, loss = 0.01147405\n",
      "Iteration 19957, loss = 0.01147344\n",
      "Iteration 19958, loss = 0.01147282\n",
      "Iteration 19959, loss = 0.01147220\n",
      "Iteration 19960, loss = 0.01147159\n",
      "Iteration 19961, loss = 0.01147097\n",
      "Iteration 19962, loss = 0.01147036\n",
      "Iteration 19963, loss = 0.01146974\n",
      "Iteration 19964, loss = 0.01146912\n",
      "Iteration 19965, loss = 0.01146851\n",
      "Iteration 19966, loss = 0.01146789\n",
      "Iteration 19967, loss = 0.01146728\n",
      "Iteration 19968, loss = 0.01146666\n",
      "Iteration 19969, loss = 0.01146605\n",
      "Iteration 19970, loss = 0.01146543\n",
      "Iteration 19971, loss = 0.01146482\n",
      "Iteration 19972, loss = 0.01146420\n",
      "Iteration 19973, loss = 0.01146359\n",
      "Iteration 19974, loss = 0.01146297\n",
      "Iteration 19975, loss = 0.01146236\n",
      "Iteration 19976, loss = 0.01146174\n",
      "Iteration 19977, loss = 0.01146113\n",
      "Iteration 19978, loss = 0.01146052\n",
      "Iteration 19979, loss = 0.01145990\n",
      "Iteration 19980, loss = 0.01145929\n",
      "Iteration 19981, loss = 0.01145867\n",
      "Iteration 19982, loss = 0.01145806\n",
      "Iteration 19983, loss = 0.01145744\n",
      "Iteration 19984, loss = 0.01145683\n",
      "Iteration 19985, loss = 0.01145622\n",
      "Iteration 19986, loss = 0.01145560\n",
      "Iteration 19987, loss = 0.01145499\n",
      "Iteration 19988, loss = 0.01145438\n",
      "Iteration 19989, loss = 0.01145376\n",
      "Iteration 19990, loss = 0.01145315\n",
      "Iteration 19991, loss = 0.01145254\n",
      "Iteration 19992, loss = 0.01145192\n",
      "Iteration 19993, loss = 0.01145131\n",
      "Iteration 19994, loss = 0.01145070\n",
      "Iteration 19995, loss = 0.01145008\n",
      "Iteration 19996, loss = 0.01144947\n",
      "Iteration 19997, loss = 0.01144886\n",
      "Iteration 19998, loss = 0.01144825\n",
      "Iteration 19999, loss = 0.01144763\n",
      "Iteration 20000, loss = 0.01144702\n",
      "Iteration 20001, loss = 0.01144641\n",
      "Iteration 20002, loss = 0.01144580\n",
      "Iteration 20003, loss = 0.01144518\n",
      "Iteration 20004, loss = 0.01144457\n",
      "Iteration 20005, loss = 0.01144396\n",
      "Iteration 20006, loss = 0.01144335\n",
      "Iteration 20007, loss = 0.01144274\n",
      "Iteration 20008, loss = 0.01144212\n",
      "Iteration 20009, loss = 0.01144151\n",
      "Iteration 20010, loss = 0.01144090\n",
      "Iteration 20011, loss = 0.01144029\n",
      "Iteration 20012, loss = 0.01143968\n",
      "Iteration 20013, loss = 0.01143907\n",
      "Iteration 20014, loss = 0.01143846\n",
      "Iteration 20015, loss = 0.01143785\n",
      "Iteration 20016, loss = 0.01143723\n",
      "Iteration 20017, loss = 0.01143662\n",
      "Iteration 20018, loss = 0.01143601\n",
      "Iteration 20019, loss = 0.01143540\n",
      "Iteration 20020, loss = 0.01143479\n",
      "Iteration 20021, loss = 0.01143418\n",
      "Iteration 20022, loss = 0.01143357\n",
      "Iteration 20023, loss = 0.01143296\n",
      "Iteration 20024, loss = 0.01143235\n",
      "Iteration 20025, loss = 0.01143174\n",
      "Iteration 20026, loss = 0.01143113\n",
      "Iteration 20027, loss = 0.01143052\n",
      "Iteration 20028, loss = 0.01142991\n",
      "Iteration 20029, loss = 0.01142930\n",
      "Iteration 20030, loss = 0.01142869\n",
      "Iteration 20031, loss = 0.01142808\n",
      "Iteration 20032, loss = 0.01142747\n",
      "Iteration 20033, loss = 0.01142686\n",
      "Iteration 20034, loss = 0.01142625\n",
      "Iteration 20035, loss = 0.01142564\n",
      "Iteration 20036, loss = 0.01142503\n",
      "Iteration 20037, loss = 0.01142442\n",
      "Iteration 20038, loss = 0.01142381\n",
      "Iteration 20039, loss = 0.01142320\n",
      "Iteration 20040, loss = 0.01142260\n",
      "Iteration 20041, loss = 0.01142199\n",
      "Iteration 20042, loss = 0.01142138\n",
      "Iteration 20043, loss = 0.01142077\n",
      "Iteration 20044, loss = 0.01142016\n",
      "Iteration 20045, loss = 0.01141955\n",
      "Iteration 20046, loss = 0.01141894\n",
      "Iteration 20047, loss = 0.01141834\n",
      "Iteration 20048, loss = 0.01141773\n",
      "Iteration 20049, loss = 0.01141712\n",
      "Iteration 20050, loss = 0.01141651\n",
      "Iteration 20051, loss = 0.01141590\n",
      "Iteration 20052, loss = 0.01141529\n",
      "Iteration 20053, loss = 0.01141469\n",
      "Iteration 20054, loss = 0.01141408\n",
      "Iteration 20055, loss = 0.01141347\n",
      "Iteration 20056, loss = 0.01141286\n",
      "Iteration 20057, loss = 0.01141226\n",
      "Iteration 20058, loss = 0.01141165\n",
      "Iteration 20059, loss = 0.01141104\n",
      "Iteration 20060, loss = 0.01141043\n",
      "Iteration 20061, loss = 0.01140983\n",
      "Iteration 20062, loss = 0.01140922\n",
      "Iteration 20063, loss = 0.01140861\n",
      "Iteration 20064, loss = 0.01140801\n",
      "Iteration 20065, loss = 0.01140740\n",
      "Iteration 20066, loss = 0.01140679\n",
      "Iteration 20067, loss = 0.01140619\n",
      "Iteration 20068, loss = 0.01140558\n",
      "Iteration 20069, loss = 0.01140497\n",
      "Iteration 20070, loss = 0.01140437\n",
      "Iteration 20071, loss = 0.01140376\n",
      "Iteration 20072, loss = 0.01140315\n",
      "Iteration 20073, loss = 0.01140255\n",
      "Iteration 20074, loss = 0.01140194\n",
      "Iteration 20075, loss = 0.01140134\n",
      "Iteration 20076, loss = 0.01140073\n",
      "Iteration 20077, loss = 0.01140013\n",
      "Iteration 20078, loss = 0.01139952\n",
      "Iteration 20079, loss = 0.01139891\n",
      "Iteration 20080, loss = 0.01139831\n",
      "Iteration 20081, loss = 0.01139770\n",
      "Iteration 20082, loss = 0.01139710\n",
      "Iteration 20083, loss = 0.01139649\n",
      "Iteration 20084, loss = 0.01139589\n",
      "Iteration 20085, loss = 0.01139528\n",
      "Iteration 20086, loss = 0.01139468\n",
      "Iteration 20087, loss = 0.01139407\n",
      "Iteration 20088, loss = 0.01139347\n",
      "Iteration 20089, loss = 0.01139286\n",
      "Iteration 20090, loss = 0.01139226\n",
      "Iteration 20091, loss = 0.01139165\n",
      "Iteration 20092, loss = 0.01139105\n",
      "Iteration 20093, loss = 0.01139045\n",
      "Iteration 20094, loss = 0.01138984\n",
      "Iteration 20095, loss = 0.01138924\n",
      "Iteration 20096, loss = 0.01138863\n",
      "Iteration 20097, loss = 0.01138803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20098, loss = 0.01138742\n",
      "Iteration 20099, loss = 0.01138682\n",
      "Iteration 20100, loss = 0.01138622\n",
      "Iteration 20101, loss = 0.01138561\n",
      "Iteration 20102, loss = 0.01138501\n",
      "Iteration 20103, loss = 0.01138441\n",
      "Iteration 20104, loss = 0.01138380\n",
      "Iteration 20105, loss = 0.01138320\n",
      "Iteration 20106, loss = 0.01138260\n",
      "Iteration 20107, loss = 0.01138199\n",
      "Iteration 20108, loss = 0.01138139\n",
      "Iteration 20109, loss = 0.01138079\n",
      "Iteration 20110, loss = 0.01138018\n",
      "Iteration 20111, loss = 0.01137958\n",
      "Iteration 20112, loss = 0.01137898\n",
      "Iteration 20113, loss = 0.01137838\n",
      "Iteration 20114, loss = 0.01137777\n",
      "Iteration 20115, loss = 0.01137717\n",
      "Iteration 20116, loss = 0.01137657\n",
      "Iteration 20117, loss = 0.01137597\n",
      "Iteration 20118, loss = 0.01137536\n",
      "Iteration 20119, loss = 0.01137476\n",
      "Iteration 20120, loss = 0.01137416\n",
      "Iteration 20121, loss = 0.01137356\n",
      "Iteration 20122, loss = 0.01137296\n",
      "Iteration 20123, loss = 0.01137235\n",
      "Iteration 20124, loss = 0.01137175\n",
      "Iteration 20125, loss = 0.01137115\n",
      "Iteration 20126, loss = 0.01137055\n",
      "Iteration 20127, loss = 0.01136995\n",
      "Iteration 20128, loss = 0.01136935\n",
      "Iteration 20129, loss = 0.01136875\n",
      "Iteration 20130, loss = 0.01136814\n",
      "Iteration 20131, loss = 0.01136754\n",
      "Iteration 20132, loss = 0.01136694\n",
      "Iteration 20133, loss = 0.01136634\n",
      "Iteration 20134, loss = 0.01136574\n",
      "Iteration 20135, loss = 0.01136514\n",
      "Iteration 20136, loss = 0.01136454\n",
      "Iteration 20137, loss = 0.01136394\n",
      "Iteration 20138, loss = 0.01136334\n",
      "Iteration 20139, loss = 0.01136274\n",
      "Iteration 20140, loss = 0.01136214\n",
      "Iteration 20141, loss = 0.01136154\n",
      "Iteration 20142, loss = 0.01136094\n",
      "Iteration 20143, loss = 0.01136034\n",
      "Iteration 20144, loss = 0.01135974\n",
      "Iteration 20145, loss = 0.01135914\n",
      "Iteration 20146, loss = 0.01135854\n",
      "Iteration 20147, loss = 0.01135794\n",
      "Iteration 20148, loss = 0.01135734\n",
      "Iteration 20149, loss = 0.01135674\n",
      "Iteration 20150, loss = 0.01135614\n",
      "Iteration 20151, loss = 0.01135554\n",
      "Iteration 20152, loss = 0.01135494\n",
      "Iteration 20153, loss = 0.01135434\n",
      "Iteration 20154, loss = 0.01135374\n",
      "Iteration 20155, loss = 0.01135314\n",
      "Iteration 20156, loss = 0.01135254\n",
      "Iteration 20157, loss = 0.01135194\n",
      "Iteration 20158, loss = 0.01135135\n",
      "Iteration 20159, loss = 0.01135075\n",
      "Iteration 20160, loss = 0.01135015\n",
      "Iteration 20161, loss = 0.01134955\n",
      "Iteration 20162, loss = 0.01134895\n",
      "Iteration 20163, loss = 0.01134835\n",
      "Iteration 20164, loss = 0.01134775\n",
      "Iteration 20165, loss = 0.01134716\n",
      "Iteration 20166, loss = 0.01134656\n",
      "Iteration 20167, loss = 0.01134596\n",
      "Iteration 20168, loss = 0.01134536\n",
      "Iteration 20169, loss = 0.01134476\n",
      "Iteration 20170, loss = 0.01134417\n",
      "Iteration 20171, loss = 0.01134357\n",
      "Iteration 20172, loss = 0.01134297\n",
      "Iteration 20173, loss = 0.01134237\n",
      "Iteration 20174, loss = 0.01134178\n",
      "Iteration 20175, loss = 0.01134118\n",
      "Iteration 20176, loss = 0.01134058\n",
      "Iteration 20177, loss = 0.01133998\n",
      "Iteration 20178, loss = 0.01133939\n",
      "Iteration 20179, loss = 0.01133879\n",
      "Iteration 20180, loss = 0.01133819\n",
      "Iteration 20181, loss = 0.01133760\n",
      "Iteration 20182, loss = 0.01133700\n",
      "Iteration 20183, loss = 0.01133640\n",
      "Iteration 20184, loss = 0.01133581\n",
      "Iteration 20185, loss = 0.01133521\n",
      "Iteration 20186, loss = 0.01133461\n",
      "Iteration 20187, loss = 0.01133402\n",
      "Iteration 20188, loss = 0.01133342\n",
      "Iteration 20189, loss = 0.01133283\n",
      "Iteration 20190, loss = 0.01133223\n",
      "Iteration 20191, loss = 0.01133163\n",
      "Iteration 20192, loss = 0.01133104\n",
      "Iteration 20193, loss = 0.01133044\n",
      "Iteration 20194, loss = 0.01132985\n",
      "Iteration 20195, loss = 0.01132925\n",
      "Iteration 20196, loss = 0.01132865\n",
      "Iteration 20197, loss = 0.01132806\n",
      "Iteration 20198, loss = 0.01132746\n",
      "Iteration 20199, loss = 0.01132687\n",
      "Iteration 20200, loss = 0.01132627\n",
      "Iteration 20201, loss = 0.01132568\n",
      "Iteration 20202, loss = 0.01132508\n",
      "Iteration 20203, loss = 0.01132449\n",
      "Iteration 20204, loss = 0.01132389\n",
      "Iteration 20205, loss = 0.01132330\n",
      "Iteration 20206, loss = 0.01132270\n",
      "Iteration 20207, loss = 0.01132211\n",
      "Iteration 20208, loss = 0.01132151\n",
      "Iteration 20209, loss = 0.01132092\n",
      "Iteration 20210, loss = 0.01132032\n",
      "Iteration 20211, loss = 0.01131973\n",
      "Iteration 20212, loss = 0.01131914\n",
      "Iteration 20213, loss = 0.01131854\n",
      "Iteration 20214, loss = 0.01131795\n",
      "Iteration 20215, loss = 0.01131735\n",
      "Iteration 20216, loss = 0.01131676\n",
      "Iteration 20217, loss = 0.01131617\n",
      "Iteration 20218, loss = 0.01131557\n",
      "Iteration 20219, loss = 0.01131498\n",
      "Iteration 20220, loss = 0.01131439\n",
      "Iteration 20221, loss = 0.01131379\n",
      "Iteration 20222, loss = 0.01131320\n",
      "Iteration 20223, loss = 0.01131261\n",
      "Iteration 20224, loss = 0.01131201\n",
      "Iteration 20225, loss = 0.01131142\n",
      "Iteration 20226, loss = 0.01131083\n",
      "Iteration 20227, loss = 0.01131023\n",
      "Iteration 20228, loss = 0.01130964\n",
      "Iteration 20229, loss = 0.01130905\n",
      "Iteration 20230, loss = 0.01130845\n",
      "Iteration 20231, loss = 0.01130786\n",
      "Iteration 20232, loss = 0.01130727\n",
      "Iteration 20233, loss = 0.01130668\n",
      "Iteration 20234, loss = 0.01130608\n",
      "Iteration 20235, loss = 0.01130549\n",
      "Iteration 20236, loss = 0.01130490\n",
      "Iteration 20237, loss = 0.01130431\n",
      "Iteration 20238, loss = 0.01130372\n",
      "Iteration 20239, loss = 0.01130312\n",
      "Iteration 20240, loss = 0.01130253\n",
      "Iteration 20241, loss = 0.01130194\n",
      "Iteration 20242, loss = 0.01130135\n",
      "Iteration 20243, loss = 0.01130076\n",
      "Iteration 20244, loss = 0.01130017\n",
      "Iteration 20245, loss = 0.01129957\n",
      "Iteration 20246, loss = 0.01129898\n",
      "Iteration 20247, loss = 0.01129839\n",
      "Iteration 20248, loss = 0.01129780\n",
      "Iteration 20249, loss = 0.01129721\n",
      "Iteration 20250, loss = 0.01129662\n",
      "Iteration 20251, loss = 0.01129603\n",
      "Iteration 20252, loss = 0.01129544\n",
      "Iteration 20253, loss = 0.01129484\n",
      "Iteration 20254, loss = 0.01129425\n",
      "Iteration 20255, loss = 0.01129366\n",
      "Iteration 20256, loss = 0.01129307\n",
      "Iteration 20257, loss = 0.01129248\n",
      "Iteration 20258, loss = 0.01129189\n",
      "Iteration 20259, loss = 0.01129130\n",
      "Iteration 20260, loss = 0.01129071\n",
      "Iteration 20261, loss = 0.01129012\n",
      "Iteration 20262, loss = 0.01128953\n",
      "Iteration 20263, loss = 0.01128894\n",
      "Iteration 20264, loss = 0.01128835\n",
      "Iteration 20265, loss = 0.01128776\n",
      "Iteration 20266, loss = 0.01128717\n",
      "Iteration 20267, loss = 0.01128658\n",
      "Iteration 20268, loss = 0.01128599\n",
      "Iteration 20269, loss = 0.01128540\n",
      "Iteration 20270, loss = 0.01128481\n",
      "Iteration 20271, loss = 0.01128422\n",
      "Iteration 20272, loss = 0.01128364\n",
      "Iteration 20273, loss = 0.01128305\n",
      "Iteration 20274, loss = 0.01128246\n",
      "Iteration 20275, loss = 0.01128187\n",
      "Iteration 20276, loss = 0.01128128\n",
      "Iteration 20277, loss = 0.01128069\n",
      "Iteration 20278, loss = 0.01128010\n",
      "Iteration 20279, loss = 0.01127951\n",
      "Iteration 20280, loss = 0.01127892\n",
      "Iteration 20281, loss = 0.01127834\n",
      "Iteration 20282, loss = 0.01127775\n",
      "Iteration 20283, loss = 0.01127716\n",
      "Iteration 20284, loss = 0.01127657\n",
      "Iteration 20285, loss = 0.01127598\n",
      "Iteration 20286, loss = 0.01127540\n",
      "Iteration 20287, loss = 0.01127481\n",
      "Iteration 20288, loss = 0.01127422\n",
      "Iteration 20289, loss = 0.01127363\n",
      "Iteration 20290, loss = 0.01127304\n",
      "Iteration 20291, loss = 0.01127246\n",
      "Iteration 20292, loss = 0.01127187\n",
      "Iteration 20293, loss = 0.01127128\n",
      "Iteration 20294, loss = 0.01127069\n",
      "Iteration 20295, loss = 0.01127011\n",
      "Iteration 20296, loss = 0.01126952\n",
      "Iteration 20297, loss = 0.01126893\n",
      "Iteration 20298, loss = 0.01126834\n",
      "Iteration 20299, loss = 0.01126776\n",
      "Iteration 20300, loss = 0.01126717\n",
      "Iteration 20301, loss = 0.01126658\n",
      "Iteration 20302, loss = 0.01126600\n",
      "Iteration 20303, loss = 0.01126541\n",
      "Iteration 20304, loss = 0.01126482\n",
      "Iteration 20305, loss = 0.01126424\n",
      "Iteration 20306, loss = 0.01126365\n",
      "Iteration 20307, loss = 0.01126307\n",
      "Iteration 20308, loss = 0.01126248\n",
      "Iteration 20309, loss = 0.01126189\n",
      "Iteration 20310, loss = 0.01126131\n",
      "Iteration 20311, loss = 0.01126072\n",
      "Iteration 20312, loss = 0.01126013\n",
      "Iteration 20313, loss = 0.01125955\n",
      "Iteration 20314, loss = 0.01125896\n",
      "Iteration 20315, loss = 0.01125838\n",
      "Iteration 20316, loss = 0.01125779\n",
      "Iteration 20317, loss = 0.01125721\n",
      "Iteration 20318, loss = 0.01125662\n",
      "Iteration 20319, loss = 0.01125604\n",
      "Iteration 20320, loss = 0.01125545\n",
      "Iteration 20321, loss = 0.01125487\n",
      "Iteration 20322, loss = 0.01125428\n",
      "Iteration 20323, loss = 0.01125370\n",
      "Iteration 20324, loss = 0.01125311\n",
      "Iteration 20325, loss = 0.01125253\n",
      "Iteration 20326, loss = 0.01125194\n",
      "Iteration 20327, loss = 0.01125136\n",
      "Iteration 20328, loss = 0.01125077\n",
      "Iteration 20329, loss = 0.01125019\n",
      "Iteration 20330, loss = 0.01124960\n",
      "Iteration 20331, loss = 0.01124902\n",
      "Iteration 20332, loss = 0.01124843\n",
      "Iteration 20333, loss = 0.01124785\n",
      "Iteration 20334, loss = 0.01124727\n",
      "Iteration 20335, loss = 0.01124668\n",
      "Iteration 20336, loss = 0.01124610\n",
      "Iteration 20337, loss = 0.01124551\n",
      "Iteration 20338, loss = 0.01124493\n",
      "Iteration 20339, loss = 0.01124435\n",
      "Iteration 20340, loss = 0.01124376\n",
      "Iteration 20341, loss = 0.01124318\n",
      "Iteration 20342, loss = 0.01124260\n",
      "Iteration 20343, loss = 0.01124201\n",
      "Iteration 20344, loss = 0.01124143\n",
      "Iteration 20345, loss = 0.01124085\n",
      "Iteration 20346, loss = 0.01124026\n",
      "Iteration 20347, loss = 0.01123968\n",
      "Iteration 20348, loss = 0.01123910\n",
      "Iteration 20349, loss = 0.01123851\n",
      "Iteration 20350, loss = 0.01123793\n",
      "Iteration 20351, loss = 0.01123735\n",
      "Iteration 20352, loss = 0.01123677\n",
      "Iteration 20353, loss = 0.01123618\n",
      "Iteration 20354, loss = 0.01123560\n",
      "Iteration 20355, loss = 0.01123502\n",
      "Iteration 20356, loss = 0.01123444\n",
      "Iteration 20357, loss = 0.01123385\n",
      "Iteration 20358, loss = 0.01123327\n",
      "Iteration 20359, loss = 0.01123269\n",
      "Iteration 20360, loss = 0.01123211\n",
      "Iteration 20361, loss = 0.01123153\n",
      "Iteration 20362, loss = 0.01123094\n",
      "Iteration 20363, loss = 0.01123036\n",
      "Iteration 20364, loss = 0.01122978\n",
      "Iteration 20365, loss = 0.01122920\n",
      "Iteration 20366, loss = 0.01122862\n",
      "Iteration 20367, loss = 0.01122804\n",
      "Iteration 20368, loss = 0.01122745\n",
      "Iteration 20369, loss = 0.01122687\n",
      "Iteration 20370, loss = 0.01122629\n",
      "Iteration 20371, loss = 0.01122571\n",
      "Iteration 20372, loss = 0.01122513\n",
      "Iteration 20373, loss = 0.01122455\n",
      "Iteration 20374, loss = 0.01122397\n",
      "Iteration 20375, loss = 0.01122339\n",
      "Iteration 20376, loss = 0.01122281\n",
      "Iteration 20377, loss = 0.01122223\n",
      "Iteration 20378, loss = 0.01122165\n",
      "Iteration 20379, loss = 0.01122107\n",
      "Iteration 20380, loss = 0.01122049\n",
      "Iteration 20381, loss = 0.01121991\n",
      "Iteration 20382, loss = 0.01121932\n",
      "Iteration 20383, loss = 0.01121874\n",
      "Iteration 20384, loss = 0.01121816\n",
      "Iteration 20385, loss = 0.01121758\n",
      "Iteration 20386, loss = 0.01121700\n",
      "Iteration 20387, loss = 0.01121643\n",
      "Iteration 20388, loss = 0.01121585\n",
      "Iteration 20389, loss = 0.01121527\n",
      "Iteration 20390, loss = 0.01121469\n",
      "Iteration 20391, loss = 0.01121411\n",
      "Iteration 20392, loss = 0.01121353\n",
      "Iteration 20393, loss = 0.01121295\n",
      "Iteration 20394, loss = 0.01121237\n",
      "Iteration 20395, loss = 0.01121179\n",
      "Iteration 20396, loss = 0.01121121\n",
      "Iteration 20397, loss = 0.01121063\n",
      "Iteration 20398, loss = 0.01121005\n",
      "Iteration 20399, loss = 0.01120947\n",
      "Iteration 20400, loss = 0.01120890\n",
      "Iteration 20401, loss = 0.01120832\n",
      "Iteration 20402, loss = 0.01120774\n",
      "Iteration 20403, loss = 0.01120716\n",
      "Iteration 20404, loss = 0.01120658\n",
      "Iteration 20405, loss = 0.01120600\n",
      "Iteration 20406, loss = 0.01120542\n",
      "Iteration 20407, loss = 0.01120485\n",
      "Iteration 20408, loss = 0.01120427\n",
      "Iteration 20409, loss = 0.01120369\n",
      "Iteration 20410, loss = 0.01120311\n",
      "Iteration 20411, loss = 0.01120253\n",
      "Iteration 20412, loss = 0.01120196\n",
      "Iteration 20413, loss = 0.01120138\n",
      "Iteration 20414, loss = 0.01120080\n",
      "Iteration 20415, loss = 0.01120022\n",
      "Iteration 20416, loss = 0.01119965\n",
      "Iteration 20417, loss = 0.01119907\n",
      "Iteration 20418, loss = 0.01119849\n",
      "Iteration 20419, loss = 0.01119792\n",
      "Iteration 20420, loss = 0.01119734\n",
      "Iteration 20421, loss = 0.01119676\n",
      "Iteration 20422, loss = 0.01119618\n",
      "Iteration 20423, loss = 0.01119561\n",
      "Iteration 20424, loss = 0.01119503\n",
      "Iteration 20425, loss = 0.01119445\n",
      "Iteration 20426, loss = 0.01119388\n",
      "Iteration 20427, loss = 0.01119330\n",
      "Iteration 20428, loss = 0.01119272\n",
      "Iteration 20429, loss = 0.01119215\n",
      "Iteration 20430, loss = 0.01119157\n",
      "Iteration 20431, loss = 0.01119100\n",
      "Iteration 20432, loss = 0.01119042\n",
      "Iteration 20433, loss = 0.01118984\n",
      "Iteration 20434, loss = 0.01118927\n",
      "Iteration 20435, loss = 0.01118869\n",
      "Iteration 20436, loss = 0.01118812\n",
      "Iteration 20437, loss = 0.01118754\n",
      "Iteration 20438, loss = 0.01118696\n",
      "Iteration 20439, loss = 0.01118639\n",
      "Iteration 20440, loss = 0.01118581\n",
      "Iteration 20441, loss = 0.01118524\n",
      "Iteration 20442, loss = 0.01118466\n",
      "Iteration 20443, loss = 0.01118409\n",
      "Iteration 20444, loss = 0.01118351\n",
      "Iteration 20445, loss = 0.01118294\n",
      "Iteration 20446, loss = 0.01118236\n",
      "Iteration 20447, loss = 0.01118179\n",
      "Iteration 20448, loss = 0.01118121\n",
      "Iteration 20449, loss = 0.01118064\n",
      "Iteration 20450, loss = 0.01118006\n",
      "Iteration 20451, loss = 0.01117949\n",
      "Iteration 20452, loss = 0.01117891\n",
      "Iteration 20453, loss = 0.01117834\n",
      "Iteration 20454, loss = 0.01117776\n",
      "Iteration 20455, loss = 0.01117719\n",
      "Iteration 20456, loss = 0.01117662\n",
      "Iteration 20457, loss = 0.01117604\n",
      "Iteration 20458, loss = 0.01117547\n",
      "Iteration 20459, loss = 0.01117489\n",
      "Iteration 20460, loss = 0.01117432\n",
      "Iteration 20461, loss = 0.01117375\n",
      "Iteration 20462, loss = 0.01117317\n",
      "Iteration 20463, loss = 0.01117260\n",
      "Iteration 20464, loss = 0.01117203\n",
      "Iteration 20465, loss = 0.01117145\n",
      "Iteration 20466, loss = 0.01117088\n",
      "Iteration 20467, loss = 0.01117031\n",
      "Iteration 20468, loss = 0.01116973\n",
      "Iteration 20469, loss = 0.01116916\n",
      "Iteration 20470, loss = 0.01116859\n",
      "Iteration 20471, loss = 0.01116801\n",
      "Iteration 20472, loss = 0.01116744\n",
      "Iteration 20473, loss = 0.01116687\n",
      "Iteration 20474, loss = 0.01116629\n",
      "Iteration 20475, loss = 0.01116572\n",
      "Iteration 20476, loss = 0.01116515\n",
      "Iteration 20477, loss = 0.01116458\n",
      "Iteration 20478, loss = 0.01116400\n",
      "Iteration 20479, loss = 0.01116343\n",
      "Iteration 20480, loss = 0.01116286\n",
      "Iteration 20481, loss = 0.01116229\n",
      "Iteration 20482, loss = 0.01116171\n",
      "Iteration 20483, loss = 0.01116114\n",
      "Iteration 20484, loss = 0.01116057\n",
      "Iteration 20485, loss = 0.01116000\n",
      "Iteration 20486, loss = 0.01115943\n",
      "Iteration 20487, loss = 0.01115886\n",
      "Iteration 20488, loss = 0.01115828\n",
      "Iteration 20489, loss = 0.01115771\n",
      "Iteration 20490, loss = 0.01115714\n",
      "Iteration 20491, loss = 0.01115657\n",
      "Iteration 20492, loss = 0.01115600\n",
      "Iteration 20493, loss = 0.01115543\n",
      "Iteration 20494, loss = 0.01115486\n",
      "Iteration 20495, loss = 0.01115428\n",
      "Iteration 20496, loss = 0.01115371\n",
      "Iteration 20497, loss = 0.01115314\n",
      "Iteration 20498, loss = 0.01115257\n",
      "Iteration 20499, loss = 0.01115200\n",
      "Iteration 20500, loss = 0.01115143\n",
      "Iteration 20501, loss = 0.01115086\n",
      "Iteration 20502, loss = 0.01115029\n",
      "Iteration 20503, loss = 0.01114972\n",
      "Iteration 20504, loss = 0.01114915\n",
      "Iteration 20505, loss = 0.01114858\n",
      "Iteration 20506, loss = 0.01114801\n",
      "Iteration 20507, loss = 0.01114744\n",
      "Iteration 20508, loss = 0.01114687\n",
      "Iteration 20509, loss = 0.01114630\n",
      "Iteration 20510, loss = 0.01114573\n",
      "Iteration 20511, loss = 0.01114516\n",
      "Iteration 20512, loss = 0.01114459\n",
      "Iteration 20513, loss = 0.01114402\n",
      "Iteration 20514, loss = 0.01114345\n",
      "Iteration 20515, loss = 0.01114288\n",
      "Iteration 20516, loss = 0.01114231\n",
      "Iteration 20517, loss = 0.01114174\n",
      "Iteration 20518, loss = 0.01114117\n",
      "Iteration 20519, loss = 0.01114060\n",
      "Iteration 20520, loss = 0.01114003\n",
      "Iteration 20521, loss = 0.01113946\n",
      "Iteration 20522, loss = 0.01113890\n",
      "Iteration 20523, loss = 0.01113833\n",
      "Iteration 20524, loss = 0.01113776\n",
      "Iteration 20525, loss = 0.01113719\n",
      "Iteration 20526, loss = 0.01113662\n",
      "Iteration 20527, loss = 0.01113605\n",
      "Iteration 20528, loss = 0.01113548\n",
      "Iteration 20529, loss = 0.01113491\n",
      "Iteration 20530, loss = 0.01113435\n",
      "Iteration 20531, loss = 0.01113378\n",
      "Iteration 20532, loss = 0.01113321\n",
      "Iteration 20533, loss = 0.01113264\n",
      "Iteration 20534, loss = 0.01113207\n",
      "Iteration 20535, loss = 0.01113151\n",
      "Iteration 20536, loss = 0.01113094\n",
      "Iteration 20537, loss = 0.01113037\n",
      "Iteration 20538, loss = 0.01112980\n",
      "Iteration 20539, loss = 0.01112923\n",
      "Iteration 20540, loss = 0.01112867\n",
      "Iteration 20541, loss = 0.01112810\n",
      "Iteration 20542, loss = 0.01112753\n",
      "Iteration 20543, loss = 0.01112697\n",
      "Iteration 20544, loss = 0.01112640\n",
      "Iteration 20545, loss = 0.01112583\n",
      "Iteration 20546, loss = 0.01112526\n",
      "Iteration 20547, loss = 0.01112470\n",
      "Iteration 20548, loss = 0.01112413\n",
      "Iteration 20549, loss = 0.01112356\n",
      "Iteration 20550, loss = 0.01112300\n",
      "Iteration 20551, loss = 0.01112243\n",
      "Iteration 20552, loss = 0.01112186\n",
      "Iteration 20553, loss = 0.01112130\n",
      "Iteration 20554, loss = 0.01112073\n",
      "Iteration 20555, loss = 0.01112016\n",
      "Iteration 20556, loss = 0.01111960\n",
      "Iteration 20557, loss = 0.01111903\n",
      "Iteration 20558, loss = 0.01111847\n",
      "Iteration 20559, loss = 0.01111790\n",
      "Iteration 20560, loss = 0.01111733\n",
      "Iteration 20561, loss = 0.01111677\n",
      "Iteration 20562, loss = 0.01111620\n",
      "Iteration 20563, loss = 0.01111564\n",
      "Iteration 20564, loss = 0.01111507\n",
      "Iteration 20565, loss = 0.01111450\n",
      "Iteration 20566, loss = 0.01111394\n",
      "Iteration 20567, loss = 0.01111337\n",
      "Iteration 20568, loss = 0.01111281\n",
      "Iteration 20569, loss = 0.01111224\n",
      "Iteration 20570, loss = 0.01111168\n",
      "Iteration 20571, loss = 0.01111111\n",
      "Iteration 20572, loss = 0.01111055\n",
      "Iteration 20573, loss = 0.01110998\n",
      "Iteration 20574, loss = 0.01110942\n",
      "Iteration 20575, loss = 0.01110885\n",
      "Iteration 20576, loss = 0.01110829\n",
      "Iteration 20577, loss = 0.01110772\n",
      "Iteration 20578, loss = 0.01110716\n",
      "Iteration 20579, loss = 0.01110660\n",
      "Iteration 20580, loss = 0.01110603\n",
      "Iteration 20581, loss = 0.01110547\n",
      "Iteration 20582, loss = 0.01110490\n",
      "Iteration 20583, loss = 0.01110434\n",
      "Iteration 20584, loss = 0.01110377\n",
      "Iteration 20585, loss = 0.01110321\n",
      "Iteration 20586, loss = 0.01110265\n",
      "Iteration 20587, loss = 0.01110208\n",
      "Iteration 20588, loss = 0.01110152\n",
      "Iteration 20589, loss = 0.01110095\n",
      "Iteration 20590, loss = 0.01110039\n",
      "Iteration 20591, loss = 0.01109983\n",
      "Iteration 20592, loss = 0.01109926\n",
      "Iteration 20593, loss = 0.01109870\n",
      "Iteration 20594, loss = 0.01109814\n",
      "Iteration 20595, loss = 0.01109757\n",
      "Iteration 20596, loss = 0.01109701\n",
      "Iteration 20597, loss = 0.01109645\n",
      "Iteration 20598, loss = 0.01109589\n",
      "Iteration 20599, loss = 0.01109532\n",
      "Iteration 20600, loss = 0.01109476\n",
      "Iteration 20601, loss = 0.01109420\n",
      "Iteration 20602, loss = 0.01109363\n",
      "Iteration 20603, loss = 0.01109307\n",
      "Iteration 20604, loss = 0.01109251\n",
      "Iteration 20605, loss = 0.01109195\n",
      "Iteration 20606, loss = 0.01109138\n",
      "Iteration 20607, loss = 0.01109082\n",
      "Iteration 20608, loss = 0.01109026\n",
      "Iteration 20609, loss = 0.01108970\n",
      "Iteration 20610, loss = 0.01108914\n",
      "Iteration 20611, loss = 0.01108857\n",
      "Iteration 20612, loss = 0.01108801\n",
      "Iteration 20613, loss = 0.01108745\n",
      "Iteration 20614, loss = 0.01108689\n",
      "Iteration 20615, loss = 0.01108633\n",
      "Iteration 20616, loss = 0.01108576\n",
      "Iteration 20617, loss = 0.01108520\n",
      "Iteration 20618, loss = 0.01108464\n",
      "Iteration 20619, loss = 0.01108408\n",
      "Iteration 20620, loss = 0.01108352\n",
      "Iteration 20621, loss = 0.01108296\n",
      "Iteration 20622, loss = 0.01108240\n",
      "Iteration 20623, loss = 0.01108184\n",
      "Iteration 20624, loss = 0.01108127\n",
      "Iteration 20625, loss = 0.01108071\n",
      "Iteration 20626, loss = 0.01108015\n",
      "Iteration 20627, loss = 0.01107959\n",
      "Iteration 20628, loss = 0.01107903\n",
      "Iteration 20629, loss = 0.01107847\n",
      "Iteration 20630, loss = 0.01107791\n",
      "Iteration 20631, loss = 0.01107735\n",
      "Iteration 20632, loss = 0.01107679\n",
      "Iteration 20633, loss = 0.01107623\n",
      "Iteration 20634, loss = 0.01107567\n",
      "Iteration 20635, loss = 0.01107511\n",
      "Iteration 20636, loss = 0.01107455\n",
      "Iteration 20637, loss = 0.01107399\n",
      "Iteration 20638, loss = 0.01107343\n",
      "Iteration 20639, loss = 0.01107287\n",
      "Iteration 20640, loss = 0.01107231\n",
      "Iteration 20641, loss = 0.01107175\n",
      "Iteration 20642, loss = 0.01107119\n",
      "Iteration 20643, loss = 0.01107063\n",
      "Iteration 20644, loss = 0.01107007\n",
      "Iteration 20645, loss = 0.01106951\n",
      "Iteration 20646, loss = 0.01106895\n",
      "Iteration 20647, loss = 0.01106839\n",
      "Iteration 20648, loss = 0.01106784\n",
      "Iteration 20649, loss = 0.01106728\n",
      "Iteration 20650, loss = 0.01106672\n",
      "Iteration 20651, loss = 0.01106616\n",
      "Iteration 20652, loss = 0.01106560\n",
      "Iteration 20653, loss = 0.01106504\n",
      "Iteration 20654, loss = 0.01106448\n",
      "Iteration 20655, loss = 0.01106392\n",
      "Iteration 20656, loss = 0.01106337\n",
      "Iteration 20657, loss = 0.01106281\n",
      "Iteration 20658, loss = 0.01106225\n",
      "Iteration 20659, loss = 0.01106169\n",
      "Iteration 20660, loss = 0.01106113\n",
      "Iteration 20661, loss = 0.01106057\n",
      "Iteration 20662, loss = 0.01106002\n",
      "Iteration 20663, loss = 0.01105946\n",
      "Iteration 20664, loss = 0.01105890\n",
      "Iteration 20665, loss = 0.01105834\n",
      "Iteration 20666, loss = 0.01105778\n",
      "Iteration 20667, loss = 0.01105723\n",
      "Iteration 20668, loss = 0.01105667\n",
      "Iteration 20669, loss = 0.01105611\n",
      "Iteration 20670, loss = 0.01105555\n",
      "Iteration 20671, loss = 0.01105500\n",
      "Iteration 20672, loss = 0.01105444\n",
      "Iteration 20673, loss = 0.01105388\n",
      "Iteration 20674, loss = 0.01105333\n",
      "Iteration 20675, loss = 0.01105277\n",
      "Iteration 20676, loss = 0.01105221\n",
      "Iteration 20677, loss = 0.01105165\n",
      "Iteration 20678, loss = 0.01105110\n",
      "Iteration 20679, loss = 0.01105054\n",
      "Iteration 20680, loss = 0.01104998\n",
      "Iteration 20681, loss = 0.01104943\n",
      "Iteration 20682, loss = 0.01104887\n",
      "Iteration 20683, loss = 0.01104832\n",
      "Iteration 20684, loss = 0.01104776\n",
      "Iteration 20685, loss = 0.01104720\n",
      "Iteration 20686, loss = 0.01104665\n",
      "Iteration 20687, loss = 0.01104609\n",
      "Iteration 20688, loss = 0.01104553\n",
      "Iteration 20689, loss = 0.01104498\n",
      "Iteration 20690, loss = 0.01104442\n",
      "Iteration 20691, loss = 0.01104387\n",
      "Iteration 20692, loss = 0.01104331\n",
      "Iteration 20693, loss = 0.01104276\n",
      "Iteration 20694, loss = 0.01104220\n",
      "Iteration 20695, loss = 0.01104164\n",
      "Iteration 20696, loss = 0.01104109\n",
      "Iteration 20697, loss = 0.01104053\n",
      "Iteration 20698, loss = 0.01103998\n",
      "Iteration 20699, loss = 0.01103942\n",
      "Iteration 20700, loss = 0.01103887\n",
      "Iteration 20701, loss = 0.01103831\n",
      "Iteration 20702, loss = 0.01103776\n",
      "Iteration 20703, loss = 0.01103720\n",
      "Iteration 20704, loss = 0.01103665\n",
      "Iteration 20705, loss = 0.01103609\n",
      "Iteration 20706, loss = 0.01103554\n",
      "Iteration 20707, loss = 0.01103498\n",
      "Iteration 20708, loss = 0.01103443\n",
      "Iteration 20709, loss = 0.01103388\n",
      "Iteration 20710, loss = 0.01103332\n",
      "Iteration 20711, loss = 0.01103277\n",
      "Iteration 20712, loss = 0.01103221\n",
      "Iteration 20713, loss = 0.01103166\n",
      "Iteration 20714, loss = 0.01103110\n",
      "Iteration 20715, loss = 0.01103055\n",
      "Iteration 20716, loss = 0.01103000\n",
      "Iteration 20717, loss = 0.01102944\n",
      "Iteration 20718, loss = 0.01102889\n",
      "Iteration 20719, loss = 0.01102834\n",
      "Iteration 20720, loss = 0.01102778\n",
      "Iteration 20721, loss = 0.01102723\n",
      "Iteration 20722, loss = 0.01102667\n",
      "Iteration 20723, loss = 0.01102612\n",
      "Iteration 20724, loss = 0.01102557\n",
      "Iteration 20725, loss = 0.01102501\n",
      "Iteration 20726, loss = 0.01102446\n",
      "Iteration 20727, loss = 0.01102391\n",
      "Iteration 20728, loss = 0.01102336\n",
      "Iteration 20729, loss = 0.01102280\n",
      "Iteration 20730, loss = 0.01102225\n",
      "Iteration 20731, loss = 0.01102170\n",
      "Iteration 20732, loss = 0.01102114\n",
      "Iteration 20733, loss = 0.01102059\n",
      "Iteration 20734, loss = 0.01102004\n",
      "Iteration 20735, loss = 0.01101949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20736, loss = 0.01101893\n",
      "Iteration 20737, loss = 0.01101838\n",
      "Iteration 20738, loss = 0.01101783\n",
      "Iteration 20739, loss = 0.01101728\n",
      "Iteration 20740, loss = 0.01101673\n",
      "Iteration 20741, loss = 0.01101617\n",
      "Iteration 20742, loss = 0.01101562\n",
      "Iteration 20743, loss = 0.01101507\n",
      "Iteration 20744, loss = 0.01101452\n",
      "Iteration 20745, loss = 0.01101397\n",
      "Iteration 20746, loss = 0.01101342\n",
      "Iteration 20747, loss = 0.01101286\n",
      "Iteration 20748, loss = 0.01101231\n",
      "Iteration 20749, loss = 0.01101176\n",
      "Iteration 20750, loss = 0.01101121\n",
      "Iteration 20751, loss = 0.01101066\n",
      "Iteration 20752, loss = 0.01101011\n",
      "Iteration 20753, loss = 0.01100956\n",
      "Iteration 20754, loss = 0.01100901\n",
      "Iteration 20755, loss = 0.01100845\n",
      "Iteration 20756, loss = 0.01100790\n",
      "Iteration 20757, loss = 0.01100735\n",
      "Iteration 20758, loss = 0.01100680\n",
      "Iteration 20759, loss = 0.01100625\n",
      "Iteration 20760, loss = 0.01100570\n",
      "Iteration 20761, loss = 0.01100515\n",
      "Iteration 20762, loss = 0.01100460\n",
      "Iteration 20763, loss = 0.01100405\n",
      "Iteration 20764, loss = 0.01100350\n",
      "Iteration 20765, loss = 0.01100295\n",
      "Iteration 20766, loss = 0.01100240\n",
      "Iteration 20767, loss = 0.01100185\n",
      "Iteration 20768, loss = 0.01100130\n",
      "Iteration 20769, loss = 0.01100075\n",
      "Iteration 20770, loss = 0.01100020\n",
      "Iteration 20771, loss = 0.01099965\n",
      "Iteration 20772, loss = 0.01099910\n",
      "Iteration 20773, loss = 0.01099855\n",
      "Iteration 20774, loss = 0.01099800\n",
      "Iteration 20775, loss = 0.01099745\n",
      "Iteration 20776, loss = 0.01099690\n",
      "Iteration 20777, loss = 0.01099635\n",
      "Iteration 20778, loss = 0.01099580\n",
      "Iteration 20779, loss = 0.01099525\n",
      "Iteration 20780, loss = 0.01099471\n",
      "Iteration 20781, loss = 0.01099416\n",
      "Iteration 20782, loss = 0.01099361\n",
      "Iteration 20783, loss = 0.01099306\n",
      "Iteration 20784, loss = 0.01099251\n",
      "Iteration 20785, loss = 0.01099196\n",
      "Iteration 20786, loss = 0.01099141\n",
      "Iteration 20787, loss = 0.01099086\n",
      "Iteration 20788, loss = 0.01099032\n",
      "Iteration 20789, loss = 0.01098977\n",
      "Iteration 20790, loss = 0.01098922\n",
      "Iteration 20791, loss = 0.01098867\n",
      "Iteration 20792, loss = 0.01098812\n",
      "Iteration 20793, loss = 0.01098757\n",
      "Iteration 20794, loss = 0.01098703\n",
      "Iteration 20795, loss = 0.01098648\n",
      "Iteration 20796, loss = 0.01098593\n",
      "Iteration 20797, loss = 0.01098538\n",
      "Iteration 20798, loss = 0.01098484\n",
      "Iteration 20799, loss = 0.01098429\n",
      "Iteration 20800, loss = 0.01098374\n",
      "Iteration 20801, loss = 0.01098319\n",
      "Iteration 20802, loss = 0.01098265\n",
      "Iteration 20803, loss = 0.01098210\n",
      "Iteration 20804, loss = 0.01098155\n",
      "Iteration 20805, loss = 0.01098100\n",
      "Iteration 20806, loss = 0.01098046\n",
      "Iteration 20807, loss = 0.01097991\n",
      "Iteration 20808, loss = 0.01097936\n",
      "Iteration 20809, loss = 0.01097882\n",
      "Iteration 20810, loss = 0.01097827\n",
      "Iteration 20811, loss = 0.01097772\n",
      "Iteration 20812, loss = 0.01097718\n",
      "Iteration 20813, loss = 0.01097663\n",
      "Iteration 20814, loss = 0.01097608\n",
      "Iteration 20815, loss = 0.01097554\n",
      "Iteration 20816, loss = 0.01097499\n",
      "Iteration 20817, loss = 0.01097444\n",
      "Iteration 20818, loss = 0.01097390\n",
      "Iteration 20819, loss = 0.01097335\n",
      "Iteration 20820, loss = 0.01097281\n",
      "Iteration 20821, loss = 0.01097226\n",
      "Iteration 20822, loss = 0.01097171\n",
      "Iteration 20823, loss = 0.01097117\n",
      "Iteration 20824, loss = 0.01097062\n",
      "Iteration 20825, loss = 0.01097008\n",
      "Iteration 20826, loss = 0.01096953\n",
      "Iteration 20827, loss = 0.01096899\n",
      "Iteration 20828, loss = 0.01096844\n",
      "Iteration 20829, loss = 0.01096789\n",
      "Iteration 20830, loss = 0.01096735\n",
      "Iteration 20831, loss = 0.01096680\n",
      "Iteration 20832, loss = 0.01096626\n",
      "Iteration 20833, loss = 0.01096571\n",
      "Iteration 20834, loss = 0.01096517\n",
      "Iteration 20835, loss = 0.01096462\n",
      "Iteration 20836, loss = 0.01096408\n",
      "Iteration 20837, loss = 0.01096353\n",
      "Iteration 20838, loss = 0.01096299\n",
      "Iteration 20839, loss = 0.01096244\n",
      "Iteration 20840, loss = 0.01096190\n",
      "Iteration 20841, loss = 0.01096136\n",
      "Iteration 20842, loss = 0.01096081\n",
      "Iteration 20843, loss = 0.01096027\n",
      "Iteration 20844, loss = 0.01095972\n",
      "Iteration 20845, loss = 0.01095918\n",
      "Iteration 20846, loss = 0.01095863\n",
      "Iteration 20847, loss = 0.01095809\n",
      "Iteration 20848, loss = 0.01095755\n",
      "Iteration 20849, loss = 0.01095700\n",
      "Iteration 20850, loss = 0.01095646\n",
      "Iteration 20851, loss = 0.01095592\n",
      "Iteration 20852, loss = 0.01095537\n",
      "Iteration 20853, loss = 0.01095483\n",
      "Iteration 20854, loss = 0.01095428\n",
      "Iteration 20855, loss = 0.01095374\n",
      "Iteration 20856, loss = 0.01095320\n",
      "Iteration 20857, loss = 0.01095265\n",
      "Iteration 20858, loss = 0.01095211\n",
      "Iteration 20859, loss = 0.01095157\n",
      "Iteration 20860, loss = 0.01095102\n",
      "Iteration 20861, loss = 0.01095048\n",
      "Iteration 20862, loss = 0.01094994\n",
      "Iteration 20863, loss = 0.01094940\n",
      "Iteration 20864, loss = 0.01094885\n",
      "Iteration 20865, loss = 0.01094831\n",
      "Iteration 20866, loss = 0.01094777\n",
      "Iteration 20867, loss = 0.01094723\n",
      "Iteration 20868, loss = 0.01094668\n",
      "Iteration 20869, loss = 0.01094614\n",
      "Iteration 20870, loss = 0.01094560\n",
      "Iteration 20871, loss = 0.01094506\n",
      "Iteration 20872, loss = 0.01094451\n",
      "Iteration 20873, loss = 0.01094397\n",
      "Iteration 20874, loss = 0.01094343\n",
      "Iteration 20875, loss = 0.01094289\n",
      "Iteration 20876, loss = 0.01094235\n",
      "Iteration 20877, loss = 0.01094180\n",
      "Iteration 20878, loss = 0.01094126\n",
      "Iteration 20879, loss = 0.01094072\n",
      "Iteration 20880, loss = 0.01094018\n",
      "Iteration 20881, loss = 0.01093964\n",
      "Iteration 20882, loss = 0.01093910\n",
      "Iteration 20883, loss = 0.01093855\n",
      "Iteration 20884, loss = 0.01093801\n",
      "Iteration 20885, loss = 0.01093747\n",
      "Iteration 20886, loss = 0.01093693\n",
      "Iteration 20887, loss = 0.01093639\n",
      "Iteration 20888, loss = 0.01093585\n",
      "Iteration 20889, loss = 0.01093531\n",
      "Iteration 20890, loss = 0.01093477\n",
      "Iteration 20891, loss = 0.01093423\n",
      "Iteration 20892, loss = 0.01093369\n",
      "Iteration 20893, loss = 0.01093315\n",
      "Iteration 20894, loss = 0.01093260\n",
      "Iteration 20895, loss = 0.01093206\n",
      "Iteration 20896, loss = 0.01093152\n",
      "Iteration 20897, loss = 0.01093098\n",
      "Iteration 20898, loss = 0.01093044\n",
      "Iteration 20899, loss = 0.01092990\n",
      "Iteration 20900, loss = 0.01092936\n",
      "Iteration 20901, loss = 0.01092882\n",
      "Iteration 20902, loss = 0.01092828\n",
      "Iteration 20903, loss = 0.01092774\n",
      "Iteration 20904, loss = 0.01092720\n",
      "Iteration 20905, loss = 0.01092666\n",
      "Iteration 20906, loss = 0.01092612\n",
      "Iteration 20907, loss = 0.01092558\n",
      "Iteration 20908, loss = 0.01092504\n",
      "Iteration 20909, loss = 0.01092451\n",
      "Iteration 20910, loss = 0.01092397\n",
      "Iteration 20911, loss = 0.01092343\n",
      "Iteration 20912, loss = 0.01092289\n",
      "Iteration 20913, loss = 0.01092235\n",
      "Iteration 20914, loss = 0.01092181\n",
      "Iteration 20915, loss = 0.01092127\n",
      "Iteration 20916, loss = 0.01092073\n",
      "Iteration 20917, loss = 0.01092019\n",
      "Iteration 20918, loss = 0.01091965\n",
      "Iteration 20919, loss = 0.01091911\n",
      "Iteration 20920, loss = 0.01091858\n",
      "Iteration 20921, loss = 0.01091804\n",
      "Iteration 20922, loss = 0.01091750\n",
      "Iteration 20923, loss = 0.01091696\n",
      "Iteration 20924, loss = 0.01091642\n",
      "Iteration 20925, loss = 0.01091588\n",
      "Iteration 20926, loss = 0.01091535\n",
      "Iteration 20927, loss = 0.01091481\n",
      "Iteration 20928, loss = 0.01091427\n",
      "Iteration 20929, loss = 0.01091373\n",
      "Iteration 20930, loss = 0.01091319\n",
      "Iteration 20931, loss = 0.01091266\n",
      "Iteration 20932, loss = 0.01091212\n",
      "Iteration 20933, loss = 0.01091158\n",
      "Iteration 20934, loss = 0.01091104\n",
      "Iteration 20935, loss = 0.01091051\n",
      "Iteration 20936, loss = 0.01090997\n",
      "Iteration 20937, loss = 0.01090943\n",
      "Iteration 20938, loss = 0.01090889\n",
      "Iteration 20939, loss = 0.01090836\n",
      "Iteration 20940, loss = 0.01090782\n",
      "Iteration 20941, loss = 0.01090728\n",
      "Iteration 20942, loss = 0.01090674\n",
      "Iteration 20943, loss = 0.01090621\n",
      "Iteration 20944, loss = 0.01090567\n",
      "Iteration 20945, loss = 0.01090513\n",
      "Iteration 20946, loss = 0.01090460\n",
      "Iteration 20947, loss = 0.01090406\n",
      "Iteration 20948, loss = 0.01090352\n",
      "Iteration 20949, loss = 0.01090299\n",
      "Iteration 20950, loss = 0.01090245\n",
      "Iteration 20951, loss = 0.01090191\n",
      "Iteration 20952, loss = 0.01090138\n",
      "Iteration 20953, loss = 0.01090084\n",
      "Iteration 20954, loss = 0.01090031\n",
      "Iteration 20955, loss = 0.01089977\n",
      "Iteration 20956, loss = 0.01089923\n",
      "Iteration 20957, loss = 0.01089870\n",
      "Iteration 20958, loss = 0.01089816\n",
      "Iteration 20959, loss = 0.01089763\n",
      "Iteration 20960, loss = 0.01089709\n",
      "Iteration 20961, loss = 0.01089655\n",
      "Iteration 20962, loss = 0.01089602\n",
      "Iteration 20963, loss = 0.01089548\n",
      "Iteration 20964, loss = 0.01089495\n",
      "Iteration 20965, loss = 0.01089441\n",
      "Iteration 20966, loss = 0.01089388\n",
      "Iteration 20967, loss = 0.01089334\n",
      "Iteration 20968, loss = 0.01089281\n",
      "Iteration 20969, loss = 0.01089227\n",
      "Iteration 20970, loss = 0.01089174\n",
      "Iteration 20971, loss = 0.01089120\n",
      "Iteration 20972, loss = 0.01089067\n",
      "Iteration 20973, loss = 0.01089013\n",
      "Iteration 20974, loss = 0.01088960\n",
      "Iteration 20975, loss = 0.01088906\n",
      "Iteration 20976, loss = 0.01088853\n",
      "Iteration 20977, loss = 0.01088799\n",
      "Iteration 20978, loss = 0.01088746\n",
      "Iteration 20979, loss = 0.01088693\n",
      "Iteration 20980, loss = 0.01088639\n",
      "Iteration 20981, loss = 0.01088586\n",
      "Iteration 20982, loss = 0.01088532\n",
      "Iteration 20983, loss = 0.01088479\n",
      "Iteration 20984, loss = 0.01088425\n",
      "Iteration 20985, loss = 0.01088372\n",
      "Iteration 20986, loss = 0.01088319\n",
      "Iteration 20987, loss = 0.01088265\n",
      "Iteration 20988, loss = 0.01088212\n",
      "Iteration 20989, loss = 0.01088159\n",
      "Iteration 20990, loss = 0.01088105\n",
      "Iteration 20991, loss = 0.01088052\n",
      "Iteration 20992, loss = 0.01087999\n",
      "Iteration 20993, loss = 0.01087945\n",
      "Iteration 20994, loss = 0.01087892\n",
      "Iteration 20995, loss = 0.01087839\n",
      "Iteration 20996, loss = 0.01087785\n",
      "Iteration 20997, loss = 0.01087732\n",
      "Iteration 20998, loss = 0.01087679\n",
      "Iteration 20999, loss = 0.01087625\n",
      "Iteration 21000, loss = 0.01087572\n",
      "Iteration 21001, loss = 0.01087519\n",
      "Iteration 21002, loss = 0.01087466\n",
      "Iteration 21003, loss = 0.01087412\n",
      "Iteration 21004, loss = 0.01087359\n",
      "Iteration 21005, loss = 0.01087306\n",
      "Iteration 21006, loss = 0.01087253\n",
      "Iteration 21007, loss = 0.01087199\n",
      "Iteration 21008, loss = 0.01087146\n",
      "Iteration 21009, loss = 0.01087093\n",
      "Iteration 21010, loss = 0.01087040\n",
      "Iteration 21011, loss = 0.01086986\n",
      "Iteration 21012, loss = 0.01086933\n",
      "Iteration 21013, loss = 0.01086880\n",
      "Iteration 21014, loss = 0.01086827\n",
      "Iteration 21015, loss = 0.01086774\n",
      "Iteration 21016, loss = 0.01086721\n",
      "Iteration 21017, loss = 0.01086667\n",
      "Iteration 21018, loss = 0.01086614\n",
      "Iteration 21019, loss = 0.01086561\n",
      "Iteration 21020, loss = 0.01086508\n",
      "Iteration 21021, loss = 0.01086455\n",
      "Iteration 21022, loss = 0.01086402\n",
      "Iteration 21023, loss = 0.01086349\n",
      "Iteration 21024, loss = 0.01086295\n",
      "Iteration 21025, loss = 0.01086242\n",
      "Iteration 21026, loss = 0.01086189\n",
      "Iteration 21027, loss = 0.01086136\n",
      "Iteration 21028, loss = 0.01086083\n",
      "Iteration 21029, loss = 0.01086030\n",
      "Iteration 21030, loss = 0.01085977\n",
      "Iteration 21031, loss = 0.01085924\n",
      "Iteration 21032, loss = 0.01085871\n",
      "Iteration 21033, loss = 0.01085818\n",
      "Iteration 21034, loss = 0.01085765\n",
      "Iteration 21035, loss = 0.01085712\n",
      "Iteration 21036, loss = 0.01085659\n",
      "Iteration 21037, loss = 0.01085606\n",
      "Iteration 21038, loss = 0.01085553\n",
      "Iteration 21039, loss = 0.01085500\n",
      "Iteration 21040, loss = 0.01085447\n",
      "Iteration 21041, loss = 0.01085394\n",
      "Iteration 21042, loss = 0.01085341\n",
      "Iteration 21043, loss = 0.01085288\n",
      "Iteration 21044, loss = 0.01085235\n",
      "Iteration 21045, loss = 0.01085182\n",
      "Iteration 21046, loss = 0.01085129\n",
      "Iteration 21047, loss = 0.01085076\n",
      "Iteration 21048, loss = 0.01085023\n",
      "Iteration 21049, loss = 0.01084970\n",
      "Iteration 21050, loss = 0.01084917\n",
      "Iteration 21051, loss = 0.01084864\n",
      "Iteration 21052, loss = 0.01084811\n",
      "Iteration 21053, loss = 0.01084758\n",
      "Iteration 21054, loss = 0.01084706\n",
      "Iteration 21055, loss = 0.01084653\n",
      "Iteration 21056, loss = 0.01084600\n",
      "Iteration 21057, loss = 0.01084547\n",
      "Iteration 21058, loss = 0.01084494\n",
      "Iteration 21059, loss = 0.01084441\n",
      "Iteration 21060, loss = 0.01084388\n",
      "Iteration 21061, loss = 0.01084335\n",
      "Iteration 21062, loss = 0.01084283\n",
      "Iteration 21063, loss = 0.01084230\n",
      "Iteration 21064, loss = 0.01084177\n",
      "Iteration 21065, loss = 0.01084124\n",
      "Iteration 21066, loss = 0.01084071\n",
      "Iteration 21067, loss = 0.01084019\n",
      "Iteration 21068, loss = 0.01083966\n",
      "Iteration 21069, loss = 0.01083913\n",
      "Iteration 21070, loss = 0.01083860\n",
      "Iteration 21071, loss = 0.01083807\n",
      "Iteration 21072, loss = 0.01083755\n",
      "Iteration 21073, loss = 0.01083702\n",
      "Iteration 21074, loss = 0.01083649\n",
      "Iteration 21075, loss = 0.01083596\n",
      "Iteration 21076, loss = 0.01083544\n",
      "Iteration 21077, loss = 0.01083491\n",
      "Iteration 21078, loss = 0.01083438\n",
      "Iteration 21079, loss = 0.01083385\n",
      "Iteration 21080, loss = 0.01083333\n",
      "Iteration 21081, loss = 0.01083280\n",
      "Iteration 21082, loss = 0.01083227\n",
      "Iteration 21083, loss = 0.01083175\n",
      "Iteration 21084, loss = 0.01083122\n",
      "Iteration 21085, loss = 0.01083069\n",
      "Iteration 21086, loss = 0.01083017\n",
      "Iteration 21087, loss = 0.01082964\n",
      "Iteration 21088, loss = 0.01082911\n",
      "Iteration 21089, loss = 0.01082859\n",
      "Iteration 21090, loss = 0.01082806\n",
      "Iteration 21091, loss = 0.01082753\n",
      "Iteration 21092, loss = 0.01082701\n",
      "Iteration 21093, loss = 0.01082648\n",
      "Iteration 21094, loss = 0.01082596\n",
      "Iteration 21095, loss = 0.01082543\n",
      "Iteration 21096, loss = 0.01082490\n",
      "Iteration 21097, loss = 0.01082438\n",
      "Iteration 21098, loss = 0.01082385\n",
      "Iteration 21099, loss = 0.01082333\n",
      "Iteration 21100, loss = 0.01082280\n",
      "Iteration 21101, loss = 0.01082228\n",
      "Iteration 21102, loss = 0.01082175\n",
      "Iteration 21103, loss = 0.01082122\n",
      "Iteration 21104, loss = 0.01082070\n",
      "Iteration 21105, loss = 0.01082017\n",
      "Iteration 21106, loss = 0.01081965\n",
      "Iteration 21107, loss = 0.01081912\n",
      "Iteration 21108, loss = 0.01081860\n",
      "Iteration 21109, loss = 0.01081807\n",
      "Iteration 21110, loss = 0.01081755\n",
      "Iteration 21111, loss = 0.01081702\n",
      "Iteration 21112, loss = 0.01081650\n",
      "Iteration 21113, loss = 0.01081597\n",
      "Iteration 21114, loss = 0.01081545\n",
      "Iteration 21115, loss = 0.01081492\n",
      "Iteration 21116, loss = 0.01081440\n",
      "Iteration 21117, loss = 0.01081388\n",
      "Iteration 21118, loss = 0.01081335\n",
      "Iteration 21119, loss = 0.01081283\n",
      "Iteration 21120, loss = 0.01081230\n",
      "Iteration 21121, loss = 0.01081178\n",
      "Iteration 21122, loss = 0.01081125\n",
      "Iteration 21123, loss = 0.01081073\n",
      "Iteration 21124, loss = 0.01081021\n",
      "Iteration 21125, loss = 0.01080968\n",
      "Iteration 21126, loss = 0.01080916\n",
      "Iteration 21127, loss = 0.01080863\n",
      "Iteration 21128, loss = 0.01080811\n",
      "Iteration 21129, loss = 0.01080759\n",
      "Iteration 21130, loss = 0.01080706\n",
      "Iteration 21131, loss = 0.01080654\n",
      "Iteration 21132, loss = 0.01080602\n",
      "Iteration 21133, loss = 0.01080549\n",
      "Iteration 21134, loss = 0.01080497\n",
      "Iteration 21135, loss = 0.01080445\n",
      "Iteration 21136, loss = 0.01080392\n",
      "Iteration 21137, loss = 0.01080340\n",
      "Iteration 21138, loss = 0.01080288\n",
      "Iteration 21139, loss = 0.01080235\n",
      "Iteration 21140, loss = 0.01080183\n",
      "Iteration 21141, loss = 0.01080131\n",
      "Iteration 21142, loss = 0.01080079\n",
      "Iteration 21143, loss = 0.01080026\n",
      "Iteration 21144, loss = 0.01079974\n",
      "Iteration 21145, loss = 0.01079922\n",
      "Iteration 21146, loss = 0.01079870\n",
      "Iteration 21147, loss = 0.01079817\n",
      "Iteration 21148, loss = 0.01079765\n",
      "Iteration 21149, loss = 0.01079713\n",
      "Iteration 21150, loss = 0.01079661\n",
      "Iteration 21151, loss = 0.01079609\n",
      "Iteration 21152, loss = 0.01079556\n",
      "Iteration 21153, loss = 0.01079504\n",
      "Iteration 21154, loss = 0.01079452\n",
      "Iteration 21155, loss = 0.01079400\n",
      "Iteration 21156, loss = 0.01079348\n",
      "Iteration 21157, loss = 0.01079295\n",
      "Iteration 21158, loss = 0.01079243\n",
      "Iteration 21159, loss = 0.01079191\n",
      "Iteration 21160, loss = 0.01079139\n",
      "Iteration 21161, loss = 0.01079087\n",
      "Iteration 21162, loss = 0.01079035\n",
      "Iteration 21163, loss = 0.01078983\n",
      "Iteration 21164, loss = 0.01078930\n",
      "Iteration 21165, loss = 0.01078878\n",
      "Iteration 21166, loss = 0.01078826\n",
      "Iteration 21167, loss = 0.01078774\n",
      "Iteration 21168, loss = 0.01078722\n",
      "Iteration 21169, loss = 0.01078670\n",
      "Iteration 21170, loss = 0.01078618\n",
      "Iteration 21171, loss = 0.01078566\n",
      "Iteration 21172, loss = 0.01078514\n",
      "Iteration 21173, loss = 0.01078462\n",
      "Iteration 21174, loss = 0.01078410\n",
      "Iteration 21175, loss = 0.01078358\n",
      "Iteration 21176, loss = 0.01078306\n",
      "Iteration 21177, loss = 0.01078254\n",
      "Iteration 21178, loss = 0.01078202\n",
      "Iteration 21179, loss = 0.01078150\n",
      "Iteration 21180, loss = 0.01078098\n",
      "Iteration 21181, loss = 0.01078046\n",
      "Iteration 21182, loss = 0.01077994\n",
      "Iteration 21183, loss = 0.01077942\n",
      "Iteration 21184, loss = 0.01077890\n",
      "Iteration 21185, loss = 0.01077838\n",
      "Iteration 21186, loss = 0.01077786\n",
      "Iteration 21187, loss = 0.01077734\n",
      "Iteration 21188, loss = 0.01077682\n",
      "Iteration 21189, loss = 0.01077630\n",
      "Iteration 21190, loss = 0.01077578\n",
      "Iteration 21191, loss = 0.01077526\n",
      "Iteration 21192, loss = 0.01077474\n",
      "Iteration 21193, loss = 0.01077422\n",
      "Iteration 21194, loss = 0.01077370\n",
      "Iteration 21195, loss = 0.01077318\n",
      "Iteration 21196, loss = 0.01077266\n",
      "Iteration 21197, loss = 0.01077215\n",
      "Iteration 21198, loss = 0.01077163\n",
      "Iteration 21199, loss = 0.01077111\n",
      "Iteration 21200, loss = 0.01077059\n",
      "Iteration 21201, loss = 0.01077007\n",
      "Iteration 21202, loss = 0.01076955\n",
      "Iteration 21203, loss = 0.01076903\n",
      "Iteration 21204, loss = 0.01076852\n",
      "Iteration 21205, loss = 0.01076800\n",
      "Iteration 21206, loss = 0.01076748\n",
      "Iteration 21207, loss = 0.01076696\n",
      "Iteration 21208, loss = 0.01076644\n",
      "Iteration 21209, loss = 0.01076592\n",
      "Iteration 21210, loss = 0.01076541\n",
      "Iteration 21211, loss = 0.01076489\n",
      "Iteration 21212, loss = 0.01076437\n",
      "Iteration 21213, loss = 0.01076385\n",
      "Iteration 21214, loss = 0.01076334\n",
      "Iteration 21215, loss = 0.01076282\n",
      "Iteration 21216, loss = 0.01076230\n",
      "Iteration 21217, loss = 0.01076178\n",
      "Iteration 21218, loss = 0.01076127\n",
      "Iteration 21219, loss = 0.01076075\n",
      "Iteration 21220, loss = 0.01076023\n",
      "Iteration 21221, loss = 0.01075971\n",
      "Iteration 21222, loss = 0.01075920\n",
      "Iteration 21223, loss = 0.01075868\n",
      "Iteration 21224, loss = 0.01075816\n",
      "Iteration 21225, loss = 0.01075765\n",
      "Iteration 21226, loss = 0.01075713\n",
      "Iteration 21227, loss = 0.01075661\n",
      "Iteration 21228, loss = 0.01075609\n",
      "Iteration 21229, loss = 0.01075558\n",
      "Iteration 21230, loss = 0.01075506\n",
      "Iteration 21231, loss = 0.01075455\n",
      "Iteration 21232, loss = 0.01075403\n",
      "Iteration 21233, loss = 0.01075351\n",
      "Iteration 21234, loss = 0.01075300\n",
      "Iteration 21235, loss = 0.01075248\n",
      "Iteration 21236, loss = 0.01075196\n",
      "Iteration 21237, loss = 0.01075145\n",
      "Iteration 21238, loss = 0.01075093\n",
      "Iteration 21239, loss = 0.01075042\n",
      "Iteration 21240, loss = 0.01074990\n",
      "Iteration 21241, loss = 0.01074938\n",
      "Iteration 21242, loss = 0.01074887\n",
      "Iteration 21243, loss = 0.01074835\n",
      "Iteration 21244, loss = 0.01074784\n",
      "Iteration 21245, loss = 0.01074732\n",
      "Iteration 21246, loss = 0.01074681\n",
      "Iteration 21247, loss = 0.01074629\n",
      "Iteration 21248, loss = 0.01074577\n",
      "Iteration 21249, loss = 0.01074526\n",
      "Iteration 21250, loss = 0.01074474\n",
      "Iteration 21251, loss = 0.01074423\n",
      "Iteration 21252, loss = 0.01074371\n",
      "Iteration 21253, loss = 0.01074320\n",
      "Iteration 21254, loss = 0.01074268\n",
      "Iteration 21255, loss = 0.01074217\n",
      "Iteration 21256, loss = 0.01074165\n",
      "Iteration 21257, loss = 0.01074114\n",
      "Iteration 21258, loss = 0.01074063\n",
      "Iteration 21259, loss = 0.01074011\n",
      "Iteration 21260, loss = 0.01073960\n",
      "Iteration 21261, loss = 0.01073908\n",
      "Iteration 21262, loss = 0.01073857\n",
      "Iteration 21263, loss = 0.01073805\n",
      "Iteration 21264, loss = 0.01073754\n",
      "Iteration 21265, loss = 0.01073702\n",
      "Iteration 21266, loss = 0.01073651\n",
      "Iteration 21267, loss = 0.01073600\n",
      "Iteration 21268, loss = 0.01073548\n",
      "Iteration 21269, loss = 0.01073497\n",
      "Iteration 21270, loss = 0.01073446\n",
      "Iteration 21271, loss = 0.01073394\n",
      "Iteration 21272, loss = 0.01073343\n",
      "Iteration 21273, loss = 0.01073291\n",
      "Iteration 21274, loss = 0.01073240\n",
      "Iteration 21275, loss = 0.01073189\n",
      "Iteration 21276, loss = 0.01073137\n",
      "Iteration 21277, loss = 0.01073086\n",
      "Iteration 21278, loss = 0.01073035\n",
      "Iteration 21279, loss = 0.01072983\n",
      "Iteration 21280, loss = 0.01072932\n",
      "Iteration 21281, loss = 0.01072881\n",
      "Iteration 21282, loss = 0.01072829\n",
      "Iteration 21283, loss = 0.01072778\n",
      "Iteration 21284, loss = 0.01072727\n",
      "Iteration 21285, loss = 0.01072676\n",
      "Iteration 21286, loss = 0.01072624\n",
      "Iteration 21287, loss = 0.01072573\n",
      "Iteration 21288, loss = 0.01072522\n",
      "Iteration 21289, loss = 0.01072471\n",
      "Iteration 21290, loss = 0.01072419\n",
      "Iteration 21291, loss = 0.01072368\n",
      "Iteration 21292, loss = 0.01072317\n",
      "Iteration 21293, loss = 0.01072266\n",
      "Iteration 21294, loss = 0.01072214\n",
      "Iteration 21295, loss = 0.01072163\n",
      "Iteration 21296, loss = 0.01072112\n",
      "Iteration 21297, loss = 0.01072061\n",
      "Iteration 21298, loss = 0.01072010\n",
      "Iteration 21299, loss = 0.01071958\n",
      "Iteration 21300, loss = 0.01071907\n",
      "Iteration 21301, loss = 0.01071856\n",
      "Iteration 21302, loss = 0.01071805\n",
      "Iteration 21303, loss = 0.01071754\n",
      "Iteration 21304, loss = 0.01071703\n",
      "Iteration 21305, loss = 0.01071651\n",
      "Iteration 21306, loss = 0.01071600\n",
      "Iteration 21307, loss = 0.01071549\n",
      "Iteration 21308, loss = 0.01071498\n",
      "Iteration 21309, loss = 0.01071447\n",
      "Iteration 21310, loss = 0.01071396\n",
      "Iteration 21311, loss = 0.01071345\n",
      "Iteration 21312, loss = 0.01071294\n",
      "Iteration 21313, loss = 0.01071243\n",
      "Iteration 21314, loss = 0.01071191\n",
      "Iteration 21315, loss = 0.01071140\n",
      "Iteration 21316, loss = 0.01071089\n",
      "Iteration 21317, loss = 0.01071038\n",
      "Iteration 21318, loss = 0.01070987\n",
      "Iteration 21319, loss = 0.01070936\n",
      "Iteration 21320, loss = 0.01070885\n",
      "Iteration 21321, loss = 0.01070834\n",
      "Iteration 21322, loss = 0.01070783\n",
      "Iteration 21323, loss = 0.01070732\n",
      "Iteration 21324, loss = 0.01070681\n",
      "Iteration 21325, loss = 0.01070630\n",
      "Iteration 21326, loss = 0.01070579\n",
      "Iteration 21327, loss = 0.01070528\n",
      "Iteration 21328, loss = 0.01070477\n",
      "Iteration 21329, loss = 0.01070426\n",
      "Iteration 21330, loss = 0.01070375\n",
      "Iteration 21331, loss = 0.01070324\n",
      "Iteration 21332, loss = 0.01070273\n",
      "Iteration 21333, loss = 0.01070222\n",
      "Iteration 21334, loss = 0.01070171\n",
      "Iteration 21335, loss = 0.01070120\n",
      "Iteration 21336, loss = 0.01070069\n",
      "Iteration 21337, loss = 0.01070018\n",
      "Iteration 21338, loss = 0.01069968\n",
      "Iteration 21339, loss = 0.01069917\n",
      "Iteration 21340, loss = 0.01069866\n",
      "Iteration 21341, loss = 0.01069815\n",
      "Iteration 21342, loss = 0.01069764\n",
      "Iteration 21343, loss = 0.01069713\n",
      "Iteration 21344, loss = 0.01069662\n",
      "Iteration 21345, loss = 0.01069611\n",
      "Iteration 21346, loss = 0.01069560\n",
      "Iteration 21347, loss = 0.01069510\n",
      "Iteration 21348, loss = 0.01069459\n",
      "Iteration 21349, loss = 0.01069408\n",
      "Iteration 21350, loss = 0.01069357\n",
      "Iteration 21351, loss = 0.01069306\n",
      "Iteration 21352, loss = 0.01069255\n",
      "Iteration 21353, loss = 0.01069205\n",
      "Iteration 21354, loss = 0.01069154\n",
      "Iteration 21355, loss = 0.01069103\n",
      "Iteration 21356, loss = 0.01069052\n",
      "Iteration 21357, loss = 0.01069001\n",
      "Iteration 21358, loss = 0.01068951\n",
      "Iteration 21359, loss = 0.01068900\n",
      "Iteration 21360, loss = 0.01068849\n",
      "Iteration 21361, loss = 0.01068798\n",
      "Iteration 21362, loss = 0.01068748\n",
      "Iteration 21363, loss = 0.01068697\n",
      "Iteration 21364, loss = 0.01068646\n",
      "Iteration 21365, loss = 0.01068595\n",
      "Iteration 21366, loss = 0.01068545\n",
      "Iteration 21367, loss = 0.01068494\n",
      "Iteration 21368, loss = 0.01068443\n",
      "Iteration 21369, loss = 0.01068392\n",
      "Iteration 21370, loss = 0.01068342\n",
      "Iteration 21371, loss = 0.01068291\n",
      "Iteration 21372, loss = 0.01068240\n",
      "Iteration 21373, loss = 0.01068190\n",
      "Iteration 21374, loss = 0.01068139\n",
      "Iteration 21375, loss = 0.01068088\n",
      "Iteration 21376, loss = 0.01068038\n",
      "Iteration 21377, loss = 0.01067987\n",
      "Iteration 21378, loss = 0.01067936\n",
      "Iteration 21379, loss = 0.01067886\n",
      "Iteration 21380, loss = 0.01067835\n",
      "Iteration 21381, loss = 0.01067784\n",
      "Iteration 21382, loss = 0.01067734\n",
      "Iteration 21383, loss = 0.01067683\n",
      "Iteration 21384, loss = 0.01067633\n",
      "Iteration 21385, loss = 0.01067582\n",
      "Iteration 21386, loss = 0.01067531\n",
      "Iteration 21387, loss = 0.01067481\n",
      "Iteration 21388, loss = 0.01067430\n",
      "Iteration 21389, loss = 0.01067380\n",
      "Iteration 21390, loss = 0.01067329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21391, loss = 0.01067279\n",
      "Iteration 21392, loss = 0.01067228\n",
      "Iteration 21393, loss = 0.01067177\n",
      "Iteration 21394, loss = 0.01067127\n",
      "Iteration 21395, loss = 0.01067076\n",
      "Iteration 21396, loss = 0.01067026\n",
      "Iteration 21397, loss = 0.01066975\n",
      "Iteration 21398, loss = 0.01066925\n",
      "Iteration 21399, loss = 0.01066874\n",
      "Iteration 21400, loss = 0.01066824\n",
      "Iteration 21401, loss = 0.01066773\n",
      "Iteration 21402, loss = 0.01066723\n",
      "Iteration 21403, loss = 0.01066672\n",
      "Iteration 21404, loss = 0.01066622\n",
      "Iteration 21405, loss = 0.01066571\n",
      "Iteration 21406, loss = 0.01066521\n",
      "Iteration 21407, loss = 0.01066470\n",
      "Iteration 21408, loss = 0.01066420\n",
      "Iteration 21409, loss = 0.01066370\n",
      "Iteration 21410, loss = 0.01066319\n",
      "Iteration 21411, loss = 0.01066269\n",
      "Iteration 21412, loss = 0.01066218\n",
      "Iteration 21413, loss = 0.01066168\n",
      "Iteration 21414, loss = 0.01066117\n",
      "Iteration 21415, loss = 0.01066067\n",
      "Iteration 21416, loss = 0.01066017\n",
      "Iteration 21417, loss = 0.01065966\n",
      "Iteration 21418, loss = 0.01065916\n",
      "Iteration 21419, loss = 0.01065865\n",
      "Iteration 21420, loss = 0.01065815\n",
      "Iteration 21421, loss = 0.01065765\n",
      "Iteration 21422, loss = 0.01065714\n",
      "Iteration 21423, loss = 0.01065664\n",
      "Iteration 21424, loss = 0.01065614\n",
      "Iteration 21425, loss = 0.01065563\n",
      "Iteration 21426, loss = 0.01065513\n",
      "Iteration 21427, loss = 0.01065463\n",
      "Iteration 21428, loss = 0.01065412\n",
      "Iteration 21429, loss = 0.01065362\n",
      "Iteration 21430, loss = 0.01065312\n",
      "Iteration 21431, loss = 0.01065262\n",
      "Iteration 21432, loss = 0.01065211\n",
      "Iteration 21433, loss = 0.01065161\n",
      "Iteration 21434, loss = 0.01065111\n",
      "Iteration 21435, loss = 0.01065060\n",
      "Iteration 21436, loss = 0.01065010\n",
      "Iteration 21437, loss = 0.01064960\n",
      "Iteration 21438, loss = 0.01064910\n",
      "Iteration 21439, loss = 0.01064859\n",
      "Iteration 21440, loss = 0.01064809\n",
      "Iteration 21441, loss = 0.01064759\n",
      "Iteration 21442, loss = 0.01064709\n",
      "Iteration 21443, loss = 0.01064659\n",
      "Iteration 21444, loss = 0.01064608\n",
      "Iteration 21445, loss = 0.01064558\n",
      "Iteration 21446, loss = 0.01064508\n",
      "Iteration 21447, loss = 0.01064458\n",
      "Iteration 21448, loss = 0.01064408\n",
      "Iteration 21449, loss = 0.01064357\n",
      "Iteration 21450, loss = 0.01064307\n",
      "Iteration 21451, loss = 0.01064257\n",
      "Iteration 21452, loss = 0.01064207\n",
      "Iteration 21453, loss = 0.01064157\n",
      "Iteration 21454, loss = 0.01064107\n",
      "Iteration 21455, loss = 0.01064056\n",
      "Iteration 21456, loss = 0.01064006\n",
      "Iteration 21457, loss = 0.01063956\n",
      "Iteration 21458, loss = 0.01063906\n",
      "Iteration 21459, loss = 0.01063856\n",
      "Iteration 21460, loss = 0.01063806\n",
      "Iteration 21461, loss = 0.01063756\n",
      "Iteration 21462, loss = 0.01063706\n",
      "Iteration 21463, loss = 0.01063656\n",
      "Iteration 21464, loss = 0.01063606\n",
      "Iteration 21465, loss = 0.01063555\n",
      "Iteration 21466, loss = 0.01063505\n",
      "Iteration 21467, loss = 0.01063455\n",
      "Iteration 21468, loss = 0.01063405\n",
      "Iteration 21469, loss = 0.01063355\n",
      "Iteration 21470, loss = 0.01063305\n",
      "Iteration 21471, loss = 0.01063255\n",
      "Iteration 21472, loss = 0.01063205\n",
      "Iteration 21473, loss = 0.01063155\n",
      "Iteration 21474, loss = 0.01063105\n",
      "Iteration 21475, loss = 0.01063055\n",
      "Iteration 21476, loss = 0.01063005\n",
      "Iteration 21477, loss = 0.01062955\n",
      "Iteration 21478, loss = 0.01062905\n",
      "Iteration 21479, loss = 0.01062855\n",
      "Iteration 21480, loss = 0.01062805\n",
      "Iteration 21481, loss = 0.01062755\n",
      "Iteration 21482, loss = 0.01062705\n",
      "Iteration 21483, loss = 0.01062655\n",
      "Iteration 21484, loss = 0.01062605\n",
      "Iteration 21485, loss = 0.01062556\n",
      "Iteration 21486, loss = 0.01062506\n",
      "Iteration 21487, loss = 0.01062456\n",
      "Iteration 21488, loss = 0.01062406\n",
      "Iteration 21489, loss = 0.01062356\n",
      "Iteration 21490, loss = 0.01062306\n",
      "Iteration 21491, loss = 0.01062256\n",
      "Iteration 21492, loss = 0.01062206\n",
      "Iteration 21493, loss = 0.01062156\n",
      "Iteration 21494, loss = 0.01062106\n",
      "Iteration 21495, loss = 0.01062057\n",
      "Iteration 21496, loss = 0.01062007\n",
      "Iteration 21497, loss = 0.01061957\n",
      "Iteration 21498, loss = 0.01061907\n",
      "Iteration 21499, loss = 0.01061857\n",
      "Iteration 21500, loss = 0.01061807\n",
      "Iteration 21501, loss = 0.01061757\n",
      "Iteration 21502, loss = 0.01061708\n",
      "Iteration 21503, loss = 0.01061658\n",
      "Iteration 21504, loss = 0.01061608\n",
      "Iteration 21505, loss = 0.01061558\n",
      "Iteration 21506, loss = 0.01061508\n",
      "Iteration 21507, loss = 0.01061459\n",
      "Iteration 21508, loss = 0.01061409\n",
      "Iteration 21509, loss = 0.01061359\n",
      "Iteration 21510, loss = 0.01061309\n",
      "Iteration 21511, loss = 0.01061259\n",
      "Iteration 21512, loss = 0.01061210\n",
      "Iteration 21513, loss = 0.01061160\n",
      "Iteration 21514, loss = 0.01061110\n",
      "Iteration 21515, loss = 0.01061060\n",
      "Iteration 21516, loss = 0.01061011\n",
      "Iteration 21517, loss = 0.01060961\n",
      "Iteration 21518, loss = 0.01060911\n",
      "Iteration 21519, loss = 0.01060862\n",
      "Iteration 21520, loss = 0.01060812\n",
      "Iteration 21521, loss = 0.01060762\n",
      "Iteration 21522, loss = 0.01060712\n",
      "Iteration 21523, loss = 0.01060663\n",
      "Iteration 21524, loss = 0.01060613\n",
      "Iteration 21525, loss = 0.01060563\n",
      "Iteration 21526, loss = 0.01060514\n",
      "Iteration 21527, loss = 0.01060464\n",
      "Iteration 21528, loss = 0.01060414\n",
      "Iteration 21529, loss = 0.01060365\n",
      "Iteration 21530, loss = 0.01060315\n",
      "Iteration 21531, loss = 0.01060266\n",
      "Iteration 21532, loss = 0.01060216\n",
      "Iteration 21533, loss = 0.01060166\n",
      "Iteration 21534, loss = 0.01060117\n",
      "Iteration 21535, loss = 0.01060067\n",
      "Iteration 21536, loss = 0.01060017\n",
      "Iteration 21537, loss = 0.01059968\n",
      "Iteration 21538, loss = 0.01059918\n",
      "Iteration 21539, loss = 0.01059869\n",
      "Iteration 21540, loss = 0.01059819\n",
      "Iteration 21541, loss = 0.01059770\n",
      "Iteration 21542, loss = 0.01059720\n",
      "Iteration 21543, loss = 0.01059670\n",
      "Iteration 21544, loss = 0.01059621\n",
      "Iteration 21545, loss = 0.01059571\n",
      "Iteration 21546, loss = 0.01059522\n",
      "Iteration 21547, loss = 0.01059472\n",
      "Iteration 21548, loss = 0.01059423\n",
      "Iteration 21549, loss = 0.01059373\n",
      "Iteration 21550, loss = 0.01059324\n",
      "Iteration 21551, loss = 0.01059274\n",
      "Iteration 21552, loss = 0.01059225\n",
      "Iteration 21553, loss = 0.01059175\n",
      "Iteration 21554, loss = 0.01059126\n",
      "Iteration 21555, loss = 0.01059076\n",
      "Iteration 21556, loss = 0.01059027\n",
      "Iteration 21557, loss = 0.01058977\n",
      "Iteration 21558, loss = 0.01058928\n",
      "Iteration 21559, loss = 0.01058878\n",
      "Iteration 21560, loss = 0.01058829\n",
      "Iteration 21561, loss = 0.01058780\n",
      "Iteration 21562, loss = 0.01058730\n",
      "Iteration 21563, loss = 0.01058681\n",
      "Iteration 21564, loss = 0.01058631\n",
      "Iteration 21565, loss = 0.01058582\n",
      "Iteration 21566, loss = 0.01058532\n",
      "Iteration 21567, loss = 0.01058483\n",
      "Iteration 21568, loss = 0.01058434\n",
      "Iteration 21569, loss = 0.01058384\n",
      "Iteration 21570, loss = 0.01058335\n",
      "Iteration 21571, loss = 0.01058286\n",
      "Iteration 21572, loss = 0.01058236\n",
      "Iteration 21573, loss = 0.01058187\n",
      "Iteration 21574, loss = 0.01058137\n",
      "Iteration 21575, loss = 0.01058088\n",
      "Iteration 21576, loss = 0.01058039\n",
      "Iteration 21577, loss = 0.01057989\n",
      "Iteration 21578, loss = 0.01057940\n",
      "Iteration 21579, loss = 0.01057891\n",
      "Iteration 21580, loss = 0.01057841\n",
      "Iteration 21581, loss = 0.01057792\n",
      "Iteration 21582, loss = 0.01057743\n",
      "Iteration 21583, loss = 0.01057694\n",
      "Iteration 21584, loss = 0.01057644\n",
      "Iteration 21585, loss = 0.01057595\n",
      "Iteration 21586, loss = 0.01057546\n",
      "Iteration 21587, loss = 0.01057496\n",
      "Iteration 21588, loss = 0.01057447\n",
      "Iteration 21589, loss = 0.01057398\n",
      "Iteration 21590, loss = 0.01057349\n",
      "Iteration 21591, loss = 0.01057299\n",
      "Iteration 21592, loss = 0.01057250\n",
      "Iteration 21593, loss = 0.01057201\n",
      "Iteration 21594, loss = 0.01057152\n",
      "Iteration 21595, loss = 0.01057102\n",
      "Iteration 21596, loss = 0.01057053\n",
      "Iteration 21597, loss = 0.01057004\n",
      "Iteration 21598, loss = 0.01056955\n",
      "Iteration 21599, loss = 0.01056906\n",
      "Iteration 21600, loss = 0.01056856\n",
      "Iteration 21601, loss = 0.01056807\n",
      "Iteration 21602, loss = 0.01056758\n",
      "Iteration 21603, loss = 0.01056709\n",
      "Iteration 21604, loss = 0.01056660\n",
      "Iteration 21605, loss = 0.01056611\n",
      "Iteration 21606, loss = 0.01056562\n",
      "Iteration 21607, loss = 0.01056512\n",
      "Iteration 21608, loss = 0.01056463\n",
      "Iteration 21609, loss = 0.01056414\n",
      "Iteration 21610, loss = 0.01056365\n",
      "Iteration 21611, loss = 0.01056316\n",
      "Iteration 21612, loss = 0.01056267\n",
      "Iteration 21613, loss = 0.01056218\n",
      "Iteration 21614, loss = 0.01056169\n",
      "Iteration 21615, loss = 0.01056119\n",
      "Iteration 21616, loss = 0.01056070\n",
      "Iteration 21617, loss = 0.01056021\n",
      "Iteration 21618, loss = 0.01055972\n",
      "Iteration 21619, loss = 0.01055923\n",
      "Iteration 21620, loss = 0.01055874\n",
      "Iteration 21621, loss = 0.01055825\n",
      "Iteration 21622, loss = 0.01055776\n",
      "Iteration 21623, loss = 0.01055727\n",
      "Iteration 21624, loss = 0.01055678\n",
      "Iteration 21625, loss = 0.01055629\n",
      "Iteration 21626, loss = 0.01055580\n",
      "Iteration 21627, loss = 0.01055531\n",
      "Iteration 21628, loss = 0.01055482\n",
      "Iteration 21629, loss = 0.01055433\n",
      "Iteration 21630, loss = 0.01055384\n",
      "Iteration 21631, loss = 0.01055335\n",
      "Iteration 21632, loss = 0.01055286\n",
      "Iteration 21633, loss = 0.01055237\n",
      "Iteration 21634, loss = 0.01055188\n",
      "Iteration 21635, loss = 0.01055139\n",
      "Iteration 21636, loss = 0.01055090\n",
      "Iteration 21637, loss = 0.01055041\n",
      "Iteration 21638, loss = 0.01054992\n",
      "Iteration 21639, loss = 0.01054943\n",
      "Iteration 21640, loss = 0.01054894\n",
      "Iteration 21641, loss = 0.01054845\n",
      "Iteration 21642, loss = 0.01054796\n",
      "Iteration 21643, loss = 0.01054748\n",
      "Iteration 21644, loss = 0.01054699\n",
      "Iteration 21645, loss = 0.01054650\n",
      "Iteration 21646, loss = 0.01054601\n",
      "Iteration 21647, loss = 0.01054552\n",
      "Iteration 21648, loss = 0.01054503\n",
      "Iteration 21649, loss = 0.01054454\n",
      "Iteration 21650, loss = 0.01054405\n",
      "Iteration 21651, loss = 0.01054357\n",
      "Iteration 21652, loss = 0.01054308\n",
      "Iteration 21653, loss = 0.01054259\n",
      "Iteration 21654, loss = 0.01054210\n",
      "Iteration 21655, loss = 0.01054161\n",
      "Iteration 21656, loss = 0.01054112\n",
      "Iteration 21657, loss = 0.01054063\n",
      "Iteration 21658, loss = 0.01054015\n",
      "Iteration 21659, loss = 0.01053966\n",
      "Iteration 21660, loss = 0.01053917\n",
      "Iteration 21661, loss = 0.01053868\n",
      "Iteration 21662, loss = 0.01053819\n",
      "Iteration 21663, loss = 0.01053771\n",
      "Iteration 21664, loss = 0.01053722\n",
      "Iteration 21665, loss = 0.01053673\n",
      "Iteration 21666, loss = 0.01053624\n",
      "Iteration 21667, loss = 0.01053576\n",
      "Iteration 21668, loss = 0.01053527\n",
      "Iteration 21669, loss = 0.01053478\n",
      "Iteration 21670, loss = 0.01053429\n",
      "Iteration 21671, loss = 0.01053381\n",
      "Iteration 21672, loss = 0.01053332\n",
      "Iteration 21673, loss = 0.01053283\n",
      "Iteration 21674, loss = 0.01053235\n",
      "Iteration 21675, loss = 0.01053186\n",
      "Iteration 21676, loss = 0.01053137\n",
      "Iteration 21677, loss = 0.01053088\n",
      "Iteration 21678, loss = 0.01053040\n",
      "Iteration 21679, loss = 0.01052991\n",
      "Iteration 21680, loss = 0.01052942\n",
      "Iteration 21681, loss = 0.01052894\n",
      "Iteration 21682, loss = 0.01052845\n",
      "Iteration 21683, loss = 0.01052796\n",
      "Iteration 21684, loss = 0.01052748\n",
      "Iteration 21685, loss = 0.01052699\n",
      "Iteration 21686, loss = 0.01052650\n",
      "Iteration 21687, loss = 0.01052602\n",
      "Iteration 21688, loss = 0.01052553\n",
      "Iteration 21689, loss = 0.01052505\n",
      "Iteration 21690, loss = 0.01052456\n",
      "Iteration 21691, loss = 0.01052407\n",
      "Iteration 21692, loss = 0.01052359\n",
      "Iteration 21693, loss = 0.01052310\n",
      "Iteration 21694, loss = 0.01052262\n",
      "Iteration 21695, loss = 0.01052213\n",
      "Iteration 21696, loss = 0.01052164\n",
      "Iteration 21697, loss = 0.01052116\n",
      "Iteration 21698, loss = 0.01052067\n",
      "Iteration 21699, loss = 0.01052019\n",
      "Iteration 21700, loss = 0.01051970\n",
      "Iteration 21701, loss = 0.01051922\n",
      "Iteration 21702, loss = 0.01051873\n",
      "Iteration 21703, loss = 0.01051825\n",
      "Iteration 21704, loss = 0.01051776\n",
      "Iteration 21705, loss = 0.01051728\n",
      "Iteration 21706, loss = 0.01051679\n",
      "Iteration 21707, loss = 0.01051631\n",
      "Iteration 21708, loss = 0.01051582\n",
      "Iteration 21709, loss = 0.01051534\n",
      "Iteration 21710, loss = 0.01051485\n",
      "Iteration 21711, loss = 0.01051437\n",
      "Iteration 21712, loss = 0.01051388\n",
      "Iteration 21713, loss = 0.01051340\n",
      "Iteration 21714, loss = 0.01051291\n",
      "Iteration 21715, loss = 0.01051243\n",
      "Iteration 21716, loss = 0.01051194\n",
      "Iteration 21717, loss = 0.01051146\n",
      "Iteration 21718, loss = 0.01051097\n",
      "Iteration 21719, loss = 0.01051049\n",
      "Iteration 21720, loss = 0.01051001\n",
      "Iteration 21721, loss = 0.01050952\n",
      "Iteration 21722, loss = 0.01050904\n",
      "Iteration 21723, loss = 0.01050855\n",
      "Iteration 21724, loss = 0.01050807\n",
      "Iteration 21725, loss = 0.01050759\n",
      "Iteration 21726, loss = 0.01050710\n",
      "Iteration 21727, loss = 0.01050662\n",
      "Iteration 21728, loss = 0.01050614\n",
      "Iteration 21729, loss = 0.01050565\n",
      "Iteration 21730, loss = 0.01050517\n",
      "Iteration 21731, loss = 0.01050468\n",
      "Iteration 21732, loss = 0.01050420\n",
      "Iteration 21733, loss = 0.01050372\n",
      "Iteration 21734, loss = 0.01050323\n",
      "Iteration 21735, loss = 0.01050275\n",
      "Iteration 21736, loss = 0.01050227\n",
      "Iteration 21737, loss = 0.01050178\n",
      "Iteration 21738, loss = 0.01050130\n",
      "Iteration 21739, loss = 0.01050082\n",
      "Iteration 21740, loss = 0.01050034\n",
      "Iteration 21741, loss = 0.01049985\n",
      "Iteration 21742, loss = 0.01049937\n",
      "Iteration 21743, loss = 0.01049889\n",
      "Iteration 21744, loss = 0.01049840\n",
      "Iteration 21745, loss = 0.01049792\n",
      "Iteration 21746, loss = 0.01049744\n",
      "Iteration 21747, loss = 0.01049696\n",
      "Iteration 21748, loss = 0.01049647\n",
      "Iteration 21749, loss = 0.01049599\n",
      "Iteration 21750, loss = 0.01049551\n",
      "Iteration 21751, loss = 0.01049503\n",
      "Iteration 21752, loss = 0.01049455\n",
      "Iteration 21753, loss = 0.01049406\n",
      "Iteration 21754, loss = 0.01049358\n",
      "Iteration 21755, loss = 0.01049310\n",
      "Iteration 21756, loss = 0.01049262\n",
      "Iteration 21757, loss = 0.01049214\n",
      "Iteration 21758, loss = 0.01049165\n",
      "Iteration 21759, loss = 0.01049117\n",
      "Iteration 21760, loss = 0.01049069\n",
      "Iteration 21761, loss = 0.01049021\n",
      "Iteration 21762, loss = 0.01048973\n",
      "Iteration 21763, loss = 0.01048925\n",
      "Iteration 21764, loss = 0.01048876\n",
      "Iteration 21765, loss = 0.01048828\n",
      "Iteration 21766, loss = 0.01048780\n",
      "Iteration 21767, loss = 0.01048732\n",
      "Iteration 21768, loss = 0.01048684\n",
      "Iteration 21769, loss = 0.01048636\n",
      "Iteration 21770, loss = 0.01048588\n",
      "Iteration 21771, loss = 0.01048540\n",
      "Iteration 21772, loss = 0.01048491\n",
      "Iteration 21773, loss = 0.01048443\n",
      "Iteration 21774, loss = 0.01048395\n",
      "Iteration 21775, loss = 0.01048347\n",
      "Iteration 21776, loss = 0.01048299\n",
      "Iteration 21777, loss = 0.01048251\n",
      "Iteration 21778, loss = 0.01048203\n",
      "Iteration 21779, loss = 0.01048155\n",
      "Iteration 21780, loss = 0.01048107\n",
      "Iteration 21781, loss = 0.01048059\n",
      "Iteration 21782, loss = 0.01048011\n",
      "Iteration 21783, loss = 0.01047963\n",
      "Iteration 21784, loss = 0.01047915\n",
      "Iteration 21785, loss = 0.01047867\n",
      "Iteration 21786, loss = 0.01047819\n",
      "Iteration 21787, loss = 0.01047771\n",
      "Iteration 21788, loss = 0.01047723\n",
      "Iteration 21789, loss = 0.01047675\n",
      "Iteration 21790, loss = 0.01047627\n",
      "Iteration 21791, loss = 0.01047579\n",
      "Iteration 21792, loss = 0.01047531\n",
      "Iteration 21793, loss = 0.01047483\n",
      "Iteration 21794, loss = 0.01047435\n",
      "Iteration 21795, loss = 0.01047387\n",
      "Iteration 21796, loss = 0.01047339\n",
      "Iteration 21797, loss = 0.01047291\n",
      "Iteration 21798, loss = 0.01047243\n",
      "Iteration 21799, loss = 0.01047195\n",
      "Iteration 21800, loss = 0.01047147\n",
      "Iteration 21801, loss = 0.01047099\n",
      "Iteration 21802, loss = 0.01047052\n",
      "Iteration 21803, loss = 0.01047004\n",
      "Iteration 21804, loss = 0.01046956\n",
      "Iteration 21805, loss = 0.01046908\n",
      "Iteration 21806, loss = 0.01046860\n",
      "Iteration 21807, loss = 0.01046812\n",
      "Iteration 21808, loss = 0.01046764\n",
      "Iteration 21809, loss = 0.01046716\n",
      "Iteration 21810, loss = 0.01046669\n",
      "Iteration 21811, loss = 0.01046621\n",
      "Iteration 21812, loss = 0.01046573\n",
      "Iteration 21813, loss = 0.01046525\n",
      "Iteration 21814, loss = 0.01046477\n",
      "Iteration 21815, loss = 0.01046429\n",
      "Iteration 21816, loss = 0.01046382\n",
      "Iteration 21817, loss = 0.01046334\n",
      "Iteration 21818, loss = 0.01046286\n",
      "Iteration 21819, loss = 0.01046238\n",
      "Iteration 21820, loss = 0.01046190\n",
      "Iteration 21821, loss = 0.01046143\n",
      "Iteration 21822, loss = 0.01046095\n",
      "Iteration 21823, loss = 0.01046047\n",
      "Iteration 21824, loss = 0.01045999\n",
      "Iteration 21825, loss = 0.01045951\n",
      "Iteration 21826, loss = 0.01045904\n",
      "Iteration 21827, loss = 0.01045856\n",
      "Iteration 21828, loss = 0.01045808\n",
      "Iteration 21829, loss = 0.01045760\n",
      "Iteration 21830, loss = 0.01045713\n",
      "Iteration 21831, loss = 0.01045665\n",
      "Iteration 21832, loss = 0.01045617\n",
      "Iteration 21833, loss = 0.01045570\n",
      "Iteration 21834, loss = 0.01045522\n",
      "Iteration 21835, loss = 0.01045474\n",
      "Iteration 21836, loss = 0.01045426\n",
      "Iteration 21837, loss = 0.01045379\n",
      "Iteration 21838, loss = 0.01045331\n",
      "Iteration 21839, loss = 0.01045283\n",
      "Iteration 21840, loss = 0.01045236\n",
      "Iteration 21841, loss = 0.01045188\n",
      "Iteration 21842, loss = 0.01045140\n",
      "Iteration 21843, loss = 0.01045093\n",
      "Iteration 21844, loss = 0.01045045\n",
      "Iteration 21845, loss = 0.01044997\n",
      "Iteration 21846, loss = 0.01044950\n",
      "Iteration 21847, loss = 0.01044902\n",
      "Iteration 21848, loss = 0.01044855\n",
      "Iteration 21849, loss = 0.01044807\n",
      "Iteration 21850, loss = 0.01044759\n",
      "Iteration 21851, loss = 0.01044712\n",
      "Iteration 21852, loss = 0.01044664\n",
      "Iteration 21853, loss = 0.01044617\n",
      "Iteration 21854, loss = 0.01044569\n",
      "Iteration 21855, loss = 0.01044521\n",
      "Iteration 21856, loss = 0.01044474\n",
      "Iteration 21857, loss = 0.01044426\n",
      "Iteration 21858, loss = 0.01044379\n",
      "Iteration 21859, loss = 0.01044331\n",
      "Iteration 21860, loss = 0.01044284\n",
      "Iteration 21861, loss = 0.01044236\n",
      "Iteration 21862, loss = 0.01044189\n",
      "Iteration 21863, loss = 0.01044141\n",
      "Iteration 21864, loss = 0.01044093\n",
      "Iteration 21865, loss = 0.01044046\n",
      "Iteration 21866, loss = 0.01043998\n",
      "Iteration 21867, loss = 0.01043951\n",
      "Iteration 21868, loss = 0.01043903\n",
      "Iteration 21869, loss = 0.01043856\n",
      "Iteration 21870, loss = 0.01043808\n",
      "Iteration 21871, loss = 0.01043761\n",
      "Iteration 21872, loss = 0.01043714\n",
      "Iteration 21873, loss = 0.01043666\n",
      "Iteration 21874, loss = 0.01043619\n",
      "Iteration 21875, loss = 0.01043571\n",
      "Iteration 21876, loss = 0.01043524\n",
      "Iteration 21877, loss = 0.01043476\n",
      "Iteration 21878, loss = 0.01043429\n",
      "Iteration 21879, loss = 0.01043381\n",
      "Iteration 21880, loss = 0.01043334\n",
      "Iteration 21881, loss = 0.01043287\n",
      "Iteration 21882, loss = 0.01043239\n",
      "Iteration 21883, loss = 0.01043192\n",
      "Iteration 21884, loss = 0.01043144\n",
      "Iteration 21885, loss = 0.01043097\n",
      "Iteration 21886, loss = 0.01043050\n",
      "Iteration 21887, loss = 0.01043002\n",
      "Iteration 21888, loss = 0.01042955\n",
      "Iteration 21889, loss = 0.01042907\n",
      "Iteration 21890, loss = 0.01042860\n",
      "Iteration 21891, loss = 0.01042813\n",
      "Iteration 21892, loss = 0.01042765\n",
      "Iteration 21893, loss = 0.01042718\n",
      "Iteration 21894, loss = 0.01042671\n",
      "Iteration 21895, loss = 0.01042623\n",
      "Iteration 21896, loss = 0.01042576\n",
      "Iteration 21897, loss = 0.01042529\n",
      "Iteration 21898, loss = 0.01042481\n",
      "Iteration 21899, loss = 0.01042434\n",
      "Iteration 21900, loss = 0.01042387\n",
      "Iteration 21901, loss = 0.01042339\n",
      "Iteration 21902, loss = 0.01042292\n",
      "Iteration 21903, loss = 0.01042245\n",
      "Iteration 21904, loss = 0.01042198\n",
      "Iteration 21905, loss = 0.01042150\n",
      "Iteration 21906, loss = 0.01042103\n",
      "Iteration 21907, loss = 0.01042056\n",
      "Iteration 21908, loss = 0.01042009\n",
      "Iteration 21909, loss = 0.01041961\n",
      "Iteration 21910, loss = 0.01041914\n",
      "Iteration 21911, loss = 0.01041867\n",
      "Iteration 21912, loss = 0.01041820\n",
      "Iteration 21913, loss = 0.01041772\n",
      "Iteration 21914, loss = 0.01041725\n",
      "Iteration 21915, loss = 0.01041678\n",
      "Iteration 21916, loss = 0.01041631\n",
      "Iteration 21917, loss = 0.01041584\n",
      "Iteration 21918, loss = 0.01041536\n",
      "Iteration 21919, loss = 0.01041489\n",
      "Iteration 21920, loss = 0.01041442\n",
      "Iteration 21921, loss = 0.01041395\n",
      "Iteration 21922, loss = 0.01041348\n",
      "Iteration 21923, loss = 0.01041301\n",
      "Iteration 21924, loss = 0.01041253\n",
      "Iteration 21925, loss = 0.01041206\n",
      "Iteration 21926, loss = 0.01041159\n",
      "Iteration 21927, loss = 0.01041112\n",
      "Iteration 21928, loss = 0.01041065\n",
      "Iteration 21929, loss = 0.01041018\n",
      "Iteration 21930, loss = 0.01040971\n",
      "Iteration 21931, loss = 0.01040923\n",
      "Iteration 21932, loss = 0.01040876\n",
      "Iteration 21933, loss = 0.01040829\n",
      "Iteration 21934, loss = 0.01040782\n",
      "Iteration 21935, loss = 0.01040735\n",
      "Iteration 21936, loss = 0.01040688\n",
      "Iteration 21937, loss = 0.01040641\n",
      "Iteration 21938, loss = 0.01040594\n",
      "Iteration 21939, loss = 0.01040547\n",
      "Iteration 21940, loss = 0.01040500\n",
      "Iteration 21941, loss = 0.01040453\n",
      "Iteration 21942, loss = 0.01040406\n",
      "Iteration 21943, loss = 0.01040359\n",
      "Iteration 21944, loss = 0.01040312\n",
      "Iteration 21945, loss = 0.01040264\n",
      "Iteration 21946, loss = 0.01040217\n",
      "Iteration 21947, loss = 0.01040170\n",
      "Iteration 21948, loss = 0.01040123\n",
      "Iteration 21949, loss = 0.01040076\n",
      "Iteration 21950, loss = 0.01040029\n",
      "Iteration 21951, loss = 0.01039982\n",
      "Iteration 21952, loss = 0.01039935\n",
      "Iteration 21953, loss = 0.01039888\n",
      "Iteration 21954, loss = 0.01039842\n",
      "Iteration 21955, loss = 0.01039795\n",
      "Iteration 21956, loss = 0.01039748\n",
      "Iteration 21957, loss = 0.01039701\n",
      "Iteration 21958, loss = 0.01039654\n",
      "Iteration 21959, loss = 0.01039607\n",
      "Iteration 21960, loss = 0.01039560\n",
      "Iteration 21961, loss = 0.01039513\n",
      "Iteration 21962, loss = 0.01039466\n",
      "Iteration 21963, loss = 0.01039419\n",
      "Iteration 21964, loss = 0.01039372\n",
      "Iteration 21965, loss = 0.01039325\n",
      "Iteration 21966, loss = 0.01039278\n",
      "Iteration 21967, loss = 0.01039231\n",
      "Iteration 21968, loss = 0.01039185\n",
      "Iteration 21969, loss = 0.01039138\n",
      "Iteration 21970, loss = 0.01039091\n",
      "Iteration 21971, loss = 0.01039044\n",
      "Iteration 21972, loss = 0.01038997\n",
      "Iteration 21973, loss = 0.01038950\n",
      "Iteration 21974, loss = 0.01038903\n",
      "Iteration 21975, loss = 0.01038856\n",
      "Iteration 21976, loss = 0.01038810\n",
      "Iteration 21977, loss = 0.01038763\n",
      "Iteration 21978, loss = 0.01038716\n",
      "Iteration 21979, loss = 0.01038669\n",
      "Iteration 21980, loss = 0.01038622\n",
      "Iteration 21981, loss = 0.01038576\n",
      "Iteration 21982, loss = 0.01038529\n",
      "Iteration 21983, loss = 0.01038482\n",
      "Iteration 21984, loss = 0.01038435\n",
      "Iteration 21985, loss = 0.01038388\n",
      "Iteration 21986, loss = 0.01038342\n",
      "Iteration 21987, loss = 0.01038295\n",
      "Iteration 21988, loss = 0.01038248\n",
      "Iteration 21989, loss = 0.01038201\n",
      "Iteration 21990, loss = 0.01038154\n",
      "Iteration 21991, loss = 0.01038108\n",
      "Iteration 21992, loss = 0.01038061\n",
      "Iteration 21993, loss = 0.01038014\n",
      "Iteration 21994, loss = 0.01037968\n",
      "Iteration 21995, loss = 0.01037921\n",
      "Iteration 21996, loss = 0.01037874\n",
      "Iteration 21997, loss = 0.01037827\n",
      "Iteration 21998, loss = 0.01037781\n",
      "Iteration 21999, loss = 0.01037734\n",
      "Iteration 22000, loss = 0.01037687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22001, loss = 0.01037641\n",
      "Iteration 22002, loss = 0.01037594\n",
      "Iteration 22003, loss = 0.01037547\n",
      "Iteration 22004, loss = 0.01037501\n",
      "Iteration 22005, loss = 0.01037454\n",
      "Iteration 22006, loss = 0.01037407\n",
      "Iteration 22007, loss = 0.01037361\n",
      "Iteration 22008, loss = 0.01037314\n",
      "Iteration 22009, loss = 0.01037267\n",
      "Iteration 22010, loss = 0.01037221\n",
      "Iteration 22011, loss = 0.01037174\n",
      "Iteration 22012, loss = 0.01037127\n",
      "Iteration 22013, loss = 0.01037081\n",
      "Iteration 22014, loss = 0.01037034\n",
      "Iteration 22015, loss = 0.01036988\n",
      "Iteration 22016, loss = 0.01036941\n",
      "Iteration 22017, loss = 0.01036894\n",
      "Iteration 22018, loss = 0.01036848\n",
      "Iteration 22019, loss = 0.01036801\n",
      "Iteration 22020, loss = 0.01036755\n",
      "Iteration 22021, loss = 0.01036708\n",
      "Iteration 22022, loss = 0.01036661\n",
      "Iteration 22023, loss = 0.01036615\n",
      "Iteration 22024, loss = 0.01036568\n",
      "Iteration 22025, loss = 0.01036522\n",
      "Iteration 22026, loss = 0.01036475\n",
      "Iteration 22027, loss = 0.01036429\n",
      "Iteration 22028, loss = 0.01036382\n",
      "Iteration 22029, loss = 0.01036336\n",
      "Iteration 22030, loss = 0.01036289\n",
      "Iteration 22031, loss = 0.01036243\n",
      "Iteration 22032, loss = 0.01036196\n",
      "Iteration 22033, loss = 0.01036150\n",
      "Iteration 22034, loss = 0.01036103\n",
      "Iteration 22035, loss = 0.01036057\n",
      "Iteration 22036, loss = 0.01036010\n",
      "Iteration 22037, loss = 0.01035964\n",
      "Iteration 22038, loss = 0.01035917\n",
      "Iteration 22039, loss = 0.01035871\n",
      "Iteration 22040, loss = 0.01035824\n",
      "Iteration 22041, loss = 0.01035778\n",
      "Iteration 22042, loss = 0.01035731\n",
      "Iteration 22043, loss = 0.01035685\n",
      "Iteration 22044, loss = 0.01035639\n",
      "Iteration 22045, loss = 0.01035592\n",
      "Iteration 22046, loss = 0.01035546\n",
      "Iteration 22047, loss = 0.01035499\n",
      "Iteration 22048, loss = 0.01035453\n",
      "Iteration 22049, loss = 0.01035406\n",
      "Iteration 22050, loss = 0.01035360\n",
      "Iteration 22051, loss = 0.01035314\n",
      "Iteration 22052, loss = 0.01035267\n",
      "Iteration 22053, loss = 0.01035221\n",
      "Iteration 22054, loss = 0.01035175\n",
      "Iteration 22055, loss = 0.01035128\n",
      "Iteration 22056, loss = 0.01035082\n",
      "Iteration 22057, loss = 0.01035035\n",
      "Iteration 22058, loss = 0.01034989\n",
      "Iteration 22059, loss = 0.01034943\n",
      "Iteration 22060, loss = 0.01034896\n",
      "Iteration 22061, loss = 0.01034850\n",
      "Iteration 22062, loss = 0.01034804\n",
      "Iteration 22063, loss = 0.01034757\n",
      "Iteration 22064, loss = 0.01034711\n",
      "Iteration 22065, loss = 0.01034665\n",
      "Iteration 22066, loss = 0.01034619\n",
      "Iteration 22067, loss = 0.01034572\n",
      "Iteration 22068, loss = 0.01034526\n",
      "Iteration 22069, loss = 0.01034480\n",
      "Iteration 22070, loss = 0.01034433\n",
      "Iteration 22071, loss = 0.01034387\n",
      "Iteration 22072, loss = 0.01034341\n",
      "Iteration 22073, loss = 0.01034295\n",
      "Iteration 22074, loss = 0.01034248\n",
      "Iteration 22075, loss = 0.01034202\n",
      "Iteration 22076, loss = 0.01034156\n",
      "Iteration 22077, loss = 0.01034110\n",
      "Iteration 22078, loss = 0.01034063\n",
      "Iteration 22079, loss = 0.01034017\n",
      "Iteration 22080, loss = 0.01033971\n",
      "Iteration 22081, loss = 0.01033925\n",
      "Iteration 22082, loss = 0.01033878\n",
      "Iteration 22083, loss = 0.01033832\n",
      "Iteration 22084, loss = 0.01033786\n",
      "Iteration 22085, loss = 0.01033740\n",
      "Iteration 22086, loss = 0.01033694\n",
      "Iteration 22087, loss = 0.01033648\n",
      "Iteration 22088, loss = 0.01033601\n",
      "Iteration 22089, loss = 0.01033555\n",
      "Iteration 22090, loss = 0.01033509\n",
      "Iteration 22091, loss = 0.01033463\n",
      "Iteration 22092, loss = 0.01033417\n",
      "Iteration 22093, loss = 0.01033371\n",
      "Iteration 22094, loss = 0.01033324\n",
      "Iteration 22095, loss = 0.01033278\n",
      "Iteration 22096, loss = 0.01033232\n",
      "Iteration 22097, loss = 0.01033186\n",
      "Iteration 22098, loss = 0.01033140\n",
      "Iteration 22099, loss = 0.01033094\n",
      "Iteration 22100, loss = 0.01033048\n",
      "Iteration 22101, loss = 0.01033002\n",
      "Iteration 22102, loss = 0.01032956\n",
      "Iteration 22103, loss = 0.01032909\n",
      "Iteration 22104, loss = 0.01032863\n",
      "Iteration 22105, loss = 0.01032817\n",
      "Iteration 22106, loss = 0.01032771\n",
      "Iteration 22107, loss = 0.01032725\n",
      "Iteration 22108, loss = 0.01032679\n",
      "Iteration 22109, loss = 0.01032633\n",
      "Iteration 22110, loss = 0.01032587\n",
      "Iteration 22111, loss = 0.01032541\n",
      "Iteration 22112, loss = 0.01032495\n",
      "Iteration 22113, loss = 0.01032449\n",
      "Iteration 22114, loss = 0.01032403\n",
      "Iteration 22115, loss = 0.01032357\n",
      "Iteration 22116, loss = 0.01032311\n",
      "Iteration 22117, loss = 0.01032265\n",
      "Iteration 22118, loss = 0.01032219\n",
      "Iteration 22119, loss = 0.01032173\n",
      "Iteration 22120, loss = 0.01032127\n",
      "Iteration 22121, loss = 0.01032081\n",
      "Iteration 22122, loss = 0.01032035\n",
      "Iteration 22123, loss = 0.01031989\n",
      "Iteration 22124, loss = 0.01031943\n",
      "Iteration 22125, loss = 0.01031897\n",
      "Iteration 22126, loss = 0.01031851\n",
      "Iteration 22127, loss = 0.01031805\n",
      "Iteration 22128, loss = 0.01031759\n",
      "Iteration 22129, loss = 0.01031713\n",
      "Iteration 22130, loss = 0.01031667\n",
      "Iteration 22131, loss = 0.01031622\n",
      "Iteration 22132, loss = 0.01031576\n",
      "Iteration 22133, loss = 0.01031530\n",
      "Iteration 22134, loss = 0.01031484\n",
      "Iteration 22135, loss = 0.01031438\n",
      "Iteration 22136, loss = 0.01031392\n",
      "Iteration 22137, loss = 0.01031346\n",
      "Iteration 22138, loss = 0.01031300\n",
      "Iteration 22139, loss = 0.01031254\n",
      "Iteration 22140, loss = 0.01031209\n",
      "Iteration 22141, loss = 0.01031163\n",
      "Iteration 22142, loss = 0.01031117\n",
      "Iteration 22143, loss = 0.01031071\n",
      "Iteration 22144, loss = 0.01031025\n",
      "Iteration 22145, loss = 0.01030979\n",
      "Iteration 22146, loss = 0.01030933\n",
      "Iteration 22147, loss = 0.01030888\n",
      "Iteration 22148, loss = 0.01030842\n",
      "Iteration 22149, loss = 0.01030796\n",
      "Iteration 22150, loss = 0.01030750\n",
      "Iteration 22151, loss = 0.01030704\n",
      "Iteration 22152, loss = 0.01030659\n",
      "Iteration 22153, loss = 0.01030613\n",
      "Iteration 22154, loss = 0.01030567\n",
      "Iteration 22155, loss = 0.01030521\n",
      "Iteration 22156, loss = 0.01030475\n",
      "Iteration 22157, loss = 0.01030430\n",
      "Iteration 22158, loss = 0.01030384\n",
      "Iteration 22159, loss = 0.01030338\n",
      "Iteration 22160, loss = 0.01030292\n",
      "Iteration 22161, loss = 0.01030247\n",
      "Iteration 22162, loss = 0.01030201\n",
      "Iteration 22163, loss = 0.01030155\n",
      "Iteration 22164, loss = 0.01030109\n",
      "Iteration 22165, loss = 0.01030064\n",
      "Iteration 22166, loss = 0.01030018\n",
      "Iteration 22167, loss = 0.01029972\n",
      "Iteration 22168, loss = 0.01029927\n",
      "Iteration 22169, loss = 0.01029881\n",
      "Iteration 22170, loss = 0.01029835\n",
      "Iteration 22171, loss = 0.01029790\n",
      "Iteration 22172, loss = 0.01029744\n",
      "Iteration 22173, loss = 0.01029698\n",
      "Iteration 22174, loss = 0.01029653\n",
      "Iteration 22175, loss = 0.01029607\n",
      "Iteration 22176, loss = 0.01029561\n",
      "Iteration 22177, loss = 0.01029516\n",
      "Iteration 22178, loss = 0.01029470\n",
      "Iteration 22179, loss = 0.01029424\n",
      "Iteration 22180, loss = 0.01029379\n",
      "Iteration 22181, loss = 0.01029333\n",
      "Iteration 22182, loss = 0.01029287\n",
      "Iteration 22183, loss = 0.01029242\n",
      "Iteration 22184, loss = 0.01029196\n",
      "Iteration 22185, loss = 0.01029151\n",
      "Iteration 22186, loss = 0.01029105\n",
      "Iteration 22187, loss = 0.01029059\n",
      "Iteration 22188, loss = 0.01029014\n",
      "Iteration 22189, loss = 0.01028968\n",
      "Iteration 22190, loss = 0.01028923\n",
      "Iteration 22191, loss = 0.01028877\n",
      "Iteration 22192, loss = 0.01028832\n",
      "Iteration 22193, loss = 0.01028786\n",
      "Iteration 22194, loss = 0.01028740\n",
      "Iteration 22195, loss = 0.01028695\n",
      "Iteration 22196, loss = 0.01028649\n",
      "Iteration 22197, loss = 0.01028604\n",
      "Iteration 22198, loss = 0.01028558\n",
      "Iteration 22199, loss = 0.01028513\n",
      "Iteration 22200, loss = 0.01028467\n",
      "Iteration 22201, loss = 0.01028422\n",
      "Iteration 22202, loss = 0.01028376\n",
      "Iteration 22203, loss = 0.01028331\n",
      "Iteration 22204, loss = 0.01028285\n",
      "Iteration 22205, loss = 0.01028240\n",
      "Iteration 22206, loss = 0.01028194\n",
      "Iteration 22207, loss = 0.01028149\n",
      "Iteration 22208, loss = 0.01028103\n",
      "Iteration 22209, loss = 0.01028058\n",
      "Iteration 22210, loss = 0.01028012\n",
      "Iteration 22211, loss = 0.01027967\n",
      "Iteration 22212, loss = 0.01027921\n",
      "Iteration 22213, loss = 0.01027876\n",
      "Iteration 22214, loss = 0.01027831\n",
      "Iteration 22215, loss = 0.01027785\n",
      "Iteration 22216, loss = 0.01027740\n",
      "Iteration 22217, loss = 0.01027694\n",
      "Iteration 22218, loss = 0.01027649\n",
      "Iteration 22219, loss = 0.01027603\n",
      "Iteration 22220, loss = 0.01027558\n",
      "Iteration 22221, loss = 0.01027513\n",
      "Iteration 22222, loss = 0.01027467\n",
      "Iteration 22223, loss = 0.01027422\n",
      "Iteration 22224, loss = 0.01027377\n",
      "Iteration 22225, loss = 0.01027331\n",
      "Iteration 22226, loss = 0.01027286\n",
      "Iteration 22227, loss = 0.01027240\n",
      "Iteration 22228, loss = 0.01027195\n",
      "Iteration 22229, loss = 0.01027150\n",
      "Iteration 22230, loss = 0.01027104\n",
      "Iteration 22231, loss = 0.01027059\n",
      "Iteration 22232, loss = 0.01027014\n",
      "Iteration 22233, loss = 0.01026968\n",
      "Iteration 22234, loss = 0.01026923\n",
      "Iteration 22235, loss = 0.01026878\n",
      "Iteration 22236, loss = 0.01026832\n",
      "Iteration 22237, loss = 0.01026787\n",
      "Iteration 22238, loss = 0.01026742\n",
      "Iteration 22239, loss = 0.01026697\n",
      "Iteration 22240, loss = 0.01026651\n",
      "Iteration 22241, loss = 0.01026606\n",
      "Iteration 22242, loss = 0.01026561\n",
      "Iteration 22243, loss = 0.01026515\n",
      "Iteration 22244, loss = 0.01026470\n",
      "Iteration 22245, loss = 0.01026425\n",
      "Iteration 22246, loss = 0.01026380\n",
      "Iteration 22247, loss = 0.01026334\n",
      "Iteration 22248, loss = 0.01026289\n",
      "Iteration 22249, loss = 0.01026244\n",
      "Iteration 22250, loss = 0.01026199\n",
      "Iteration 22251, loss = 0.01026154\n",
      "Iteration 22252, loss = 0.01026108\n",
      "Iteration 22253, loss = 0.01026063\n",
      "Iteration 22254, loss = 0.01026018\n",
      "Iteration 22255, loss = 0.01025973\n",
      "Iteration 22256, loss = 0.01025927\n",
      "Iteration 22257, loss = 0.01025882\n",
      "Iteration 22258, loss = 0.01025837\n",
      "Iteration 22259, loss = 0.01025792\n",
      "Iteration 22260, loss = 0.01025747\n",
      "Iteration 22261, loss = 0.01025702\n",
      "Iteration 22262, loss = 0.01025656\n",
      "Iteration 22263, loss = 0.01025611\n",
      "Iteration 22264, loss = 0.01025566\n",
      "Iteration 22265, loss = 0.01025521\n",
      "Iteration 22266, loss = 0.01025476\n",
      "Iteration 22267, loss = 0.01025431\n",
      "Iteration 22268, loss = 0.01025386\n",
      "Iteration 22269, loss = 0.01025340\n",
      "Iteration 22270, loss = 0.01025295\n",
      "Iteration 22271, loss = 0.01025250\n",
      "Iteration 22272, loss = 0.01025205\n",
      "Iteration 22273, loss = 0.01025160\n",
      "Iteration 22274, loss = 0.01025115\n",
      "Iteration 22275, loss = 0.01025070\n",
      "Iteration 22276, loss = 0.01025025\n",
      "Iteration 22277, loss = 0.01024980\n",
      "Iteration 22278, loss = 0.01024935\n",
      "Iteration 22279, loss = 0.01024890\n",
      "Iteration 22280, loss = 0.01024845\n",
      "Iteration 22281, loss = 0.01024799\n",
      "Iteration 22282, loss = 0.01024754\n",
      "Iteration 22283, loss = 0.01024709\n",
      "Iteration 22284, loss = 0.01024664\n",
      "Iteration 22285, loss = 0.01024619\n",
      "Iteration 22286, loss = 0.01024574\n",
      "Iteration 22287, loss = 0.01024529\n",
      "Iteration 22288, loss = 0.01024484\n",
      "Iteration 22289, loss = 0.01024439\n",
      "Iteration 22290, loss = 0.01024394\n",
      "Iteration 22291, loss = 0.01024349\n",
      "Iteration 22292, loss = 0.01024304\n",
      "Iteration 22293, loss = 0.01024259\n",
      "Iteration 22294, loss = 0.01024214\n",
      "Iteration 22295, loss = 0.01024169\n",
      "Iteration 22296, loss = 0.01024124\n",
      "Iteration 22297, loss = 0.01024079\n",
      "Iteration 22298, loss = 0.01024034\n",
      "Iteration 22299, loss = 0.01023990\n",
      "Iteration 22300, loss = 0.01023945\n",
      "Iteration 22301, loss = 0.01023900\n",
      "Iteration 22302, loss = 0.01023855\n",
      "Iteration 22303, loss = 0.01023810\n",
      "Iteration 22304, loss = 0.01023765\n",
      "Iteration 22305, loss = 0.01023720\n",
      "Iteration 22306, loss = 0.01023675\n",
      "Iteration 22307, loss = 0.01023630\n",
      "Iteration 22308, loss = 0.01023585\n",
      "Iteration 22309, loss = 0.01023540\n",
      "Iteration 22310, loss = 0.01023495\n",
      "Iteration 22311, loss = 0.01023451\n",
      "Iteration 22312, loss = 0.01023406\n",
      "Iteration 22313, loss = 0.01023361\n",
      "Iteration 22314, loss = 0.01023316\n",
      "Iteration 22315, loss = 0.01023271\n",
      "Iteration 22316, loss = 0.01023226\n",
      "Iteration 22317, loss = 0.01023181\n",
      "Iteration 22318, loss = 0.01023137\n",
      "Iteration 22319, loss = 0.01023092\n",
      "Iteration 22320, loss = 0.01023047\n",
      "Iteration 22321, loss = 0.01023002\n",
      "Iteration 22322, loss = 0.01022957\n",
      "Iteration 22323, loss = 0.01022912\n",
      "Iteration 22324, loss = 0.01022868\n",
      "Iteration 22325, loss = 0.01022823\n",
      "Iteration 22326, loss = 0.01022778\n",
      "Iteration 22327, loss = 0.01022733\n",
      "Iteration 22328, loss = 0.01022688\n",
      "Iteration 22329, loss = 0.01022644\n",
      "Iteration 22330, loss = 0.01022599\n",
      "Iteration 22331, loss = 0.01022554\n",
      "Iteration 22332, loss = 0.01022509\n",
      "Iteration 22333, loss = 0.01022465\n",
      "Iteration 22334, loss = 0.01022420\n",
      "Iteration 22335, loss = 0.01022375\n",
      "Iteration 22336, loss = 0.01022330\n",
      "Iteration 22337, loss = 0.01022286\n",
      "Iteration 22338, loss = 0.01022241\n",
      "Iteration 22339, loss = 0.01022196\n",
      "Iteration 22340, loss = 0.01022152\n",
      "Iteration 22341, loss = 0.01022107\n",
      "Iteration 22342, loss = 0.01022062\n",
      "Iteration 22343, loss = 0.01022017\n",
      "Iteration 22344, loss = 0.01021973\n",
      "Iteration 22345, loss = 0.01021928\n",
      "Iteration 22346, loss = 0.01021883\n",
      "Iteration 22347, loss = 0.01021839\n",
      "Iteration 22348, loss = 0.01021794\n",
      "Iteration 22349, loss = 0.01021749\n",
      "Iteration 22350, loss = 0.01021705\n",
      "Iteration 22351, loss = 0.01021660\n",
      "Iteration 22352, loss = 0.01021615\n",
      "Iteration 22353, loss = 0.01021571\n",
      "Iteration 22354, loss = 0.01021526\n",
      "Iteration 22355, loss = 0.01021481\n",
      "Iteration 22356, loss = 0.01021437\n",
      "Iteration 22357, loss = 0.01021392\n",
      "Iteration 22358, loss = 0.01021348\n",
      "Iteration 22359, loss = 0.01021303\n",
      "Iteration 22360, loss = 0.01021258\n",
      "Iteration 22361, loss = 0.01021214\n",
      "Iteration 22362, loss = 0.01021169\n",
      "Iteration 22363, loss = 0.01021125\n",
      "Iteration 22364, loss = 0.01021080\n",
      "Iteration 22365, loss = 0.01021035\n",
      "Iteration 22366, loss = 0.01020991\n",
      "Iteration 22367, loss = 0.01020946\n",
      "Iteration 22368, loss = 0.01020902\n",
      "Iteration 22369, loss = 0.01020857\n",
      "Iteration 22370, loss = 0.01020813\n",
      "Iteration 22371, loss = 0.01020768\n",
      "Iteration 22372, loss = 0.01020724\n",
      "Iteration 22373, loss = 0.01020679\n",
      "Iteration 22374, loss = 0.01020635\n",
      "Iteration 22375, loss = 0.01020590\n",
      "Iteration 22376, loss = 0.01020545\n",
      "Iteration 22377, loss = 0.01020501\n",
      "Iteration 22378, loss = 0.01020456\n",
      "Iteration 22379, loss = 0.01020412\n",
      "Iteration 22380, loss = 0.01020367\n",
      "Iteration 22381, loss = 0.01020323\n",
      "Iteration 22382, loss = 0.01020279\n",
      "Iteration 22383, loss = 0.01020234\n",
      "Iteration 22384, loss = 0.01020190\n",
      "Iteration 22385, loss = 0.01020145\n",
      "Iteration 22386, loss = 0.01020101\n",
      "Iteration 22387, loss = 0.01020056\n",
      "Iteration 22388, loss = 0.01020012\n",
      "Iteration 22389, loss = 0.01019967\n",
      "Iteration 22390, loss = 0.01019923\n",
      "Iteration 22391, loss = 0.01019878\n",
      "Iteration 22392, loss = 0.01019834\n",
      "Iteration 22393, loss = 0.01019790\n",
      "Iteration 22394, loss = 0.01019745\n",
      "Iteration 22395, loss = 0.01019701\n",
      "Iteration 22396, loss = 0.01019656\n",
      "Iteration 22397, loss = 0.01019612\n",
      "Iteration 22398, loss = 0.01019568\n",
      "Iteration 22399, loss = 0.01019523\n",
      "Iteration 22400, loss = 0.01019479\n",
      "Iteration 22401, loss = 0.01019434\n",
      "Iteration 22402, loss = 0.01019390\n",
      "Iteration 22403, loss = 0.01019346\n",
      "Iteration 22404, loss = 0.01019301\n",
      "Iteration 22405, loss = 0.01019257\n",
      "Iteration 22406, loss = 0.01019213\n",
      "Iteration 22407, loss = 0.01019168\n",
      "Iteration 22408, loss = 0.01019124\n",
      "Iteration 22409, loss = 0.01019080\n",
      "Iteration 22410, loss = 0.01019035\n",
      "Iteration 22411, loss = 0.01018991\n",
      "Iteration 22412, loss = 0.01018947\n",
      "Iteration 22413, loss = 0.01018902\n",
      "Iteration 22414, loss = 0.01018858\n",
      "Iteration 22415, loss = 0.01018814\n",
      "Iteration 22416, loss = 0.01018770\n",
      "Iteration 22417, loss = 0.01018725\n",
      "Iteration 22418, loss = 0.01018681\n",
      "Iteration 22419, loss = 0.01018637\n",
      "Iteration 22420, loss = 0.01018592\n",
      "Iteration 22421, loss = 0.01018548\n",
      "Iteration 22422, loss = 0.01018504\n",
      "Iteration 22423, loss = 0.01018460\n",
      "Iteration 22424, loss = 0.01018415\n",
      "Iteration 22425, loss = 0.01018371\n",
      "Iteration 22426, loss = 0.01018327\n",
      "Iteration 22427, loss = 0.01018283\n",
      "Iteration 22428, loss = 0.01018239\n",
      "Iteration 22429, loss = 0.01018194\n",
      "Iteration 22430, loss = 0.01018150\n",
      "Iteration 22431, loss = 0.01018106\n",
      "Iteration 22432, loss = 0.01018062\n",
      "Iteration 22433, loss = 0.01018017\n",
      "Iteration 22434, loss = 0.01017973\n",
      "Iteration 22435, loss = 0.01017929\n",
      "Iteration 22436, loss = 0.01017885\n",
      "Iteration 22437, loss = 0.01017841\n",
      "Iteration 22438, loss = 0.01017797\n",
      "Iteration 22439, loss = 0.01017752\n",
      "Iteration 22440, loss = 0.01017708\n",
      "Iteration 22441, loss = 0.01017664\n",
      "Iteration 22442, loss = 0.01017620\n",
      "Iteration 22443, loss = 0.01017576\n",
      "Iteration 22444, loss = 0.01017532\n",
      "Iteration 22445, loss = 0.01017488\n",
      "Iteration 22446, loss = 0.01017443\n",
      "Iteration 22447, loss = 0.01017399\n",
      "Iteration 22448, loss = 0.01017355\n",
      "Iteration 22449, loss = 0.01017311\n",
      "Iteration 22450, loss = 0.01017267\n",
      "Iteration 22451, loss = 0.01017223\n",
      "Iteration 22452, loss = 0.01017179\n",
      "Iteration 22453, loss = 0.01017135\n",
      "Iteration 22454, loss = 0.01017091\n",
      "Iteration 22455, loss = 0.01017047\n",
      "Iteration 22456, loss = 0.01017003\n",
      "Iteration 22457, loss = 0.01016958\n",
      "Iteration 22458, loss = 0.01016914\n",
      "Iteration 22459, loss = 0.01016870\n",
      "Iteration 22460, loss = 0.01016826\n",
      "Iteration 22461, loss = 0.01016782\n",
      "Iteration 22462, loss = 0.01016738\n",
      "Iteration 22463, loss = 0.01016694\n",
      "Iteration 22464, loss = 0.01016650\n",
      "Iteration 22465, loss = 0.01016606\n",
      "Iteration 22466, loss = 0.01016562\n",
      "Iteration 22467, loss = 0.01016518\n",
      "Iteration 22468, loss = 0.01016474\n",
      "Iteration 22469, loss = 0.01016430\n",
      "Iteration 22470, loss = 0.01016386\n",
      "Iteration 22471, loss = 0.01016342\n",
      "Iteration 22472, loss = 0.01016298\n",
      "Iteration 22473, loss = 0.01016254\n",
      "Iteration 22474, loss = 0.01016210\n",
      "Iteration 22475, loss = 0.01016166\n",
      "Iteration 22476, loss = 0.01016122\n",
      "Iteration 22477, loss = 0.01016078\n",
      "Iteration 22478, loss = 0.01016034\n",
      "Iteration 22479, loss = 0.01015991\n",
      "Iteration 22480, loss = 0.01015947\n",
      "Iteration 22481, loss = 0.01015903\n",
      "Iteration 22482, loss = 0.01015859\n",
      "Iteration 22483, loss = 0.01015815\n",
      "Iteration 22484, loss = 0.01015771\n",
      "Iteration 22485, loss = 0.01015727\n",
      "Iteration 22486, loss = 0.01015683\n",
      "Iteration 22487, loss = 0.01015639\n",
      "Iteration 22488, loss = 0.01015595\n",
      "Iteration 22489, loss = 0.01015551\n",
      "Iteration 22490, loss = 0.01015508\n",
      "Iteration 22491, loss = 0.01015464\n",
      "Iteration 22492, loss = 0.01015420\n",
      "Iteration 22493, loss = 0.01015376\n",
      "Iteration 22494, loss = 0.01015332\n",
      "Iteration 22495, loss = 0.01015288\n",
      "Iteration 22496, loss = 0.01015244\n",
      "Iteration 22497, loss = 0.01015201\n",
      "Iteration 22498, loss = 0.01015157\n",
      "Iteration 22499, loss = 0.01015113\n",
      "Iteration 22500, loss = 0.01015069\n",
      "Iteration 22501, loss = 0.01015025\n",
      "Iteration 22502, loss = 0.01014981\n",
      "Iteration 22503, loss = 0.01014938\n",
      "Iteration 22504, loss = 0.01014894\n",
      "Iteration 22505, loss = 0.01014850\n",
      "Iteration 22506, loss = 0.01014806\n",
      "Iteration 22507, loss = 0.01014762\n",
      "Iteration 22508, loss = 0.01014719\n",
      "Iteration 22509, loss = 0.01014675\n",
      "Iteration 22510, loss = 0.01014631\n",
      "Iteration 22511, loss = 0.01014587\n",
      "Iteration 22512, loss = 0.01014544\n",
      "Iteration 22513, loss = 0.01014500\n",
      "Iteration 22514, loss = 0.01014456\n",
      "Iteration 22515, loss = 0.01014412\n",
      "Iteration 22516, loss = 0.01014369\n",
      "Iteration 22517, loss = 0.01014325\n",
      "Iteration 22518, loss = 0.01014281\n",
      "Iteration 22519, loss = 0.01014237\n",
      "Iteration 22520, loss = 0.01014194\n",
      "Iteration 22521, loss = 0.01014150\n",
      "Iteration 22522, loss = 0.01014106\n",
      "Iteration 22523, loss = 0.01014063\n",
      "Iteration 22524, loss = 0.01014019\n",
      "Iteration 22525, loss = 0.01013975\n",
      "Iteration 22526, loss = 0.01013932\n",
      "Iteration 22527, loss = 0.01013888\n",
      "Iteration 22528, loss = 0.01013844\n",
      "Iteration 22529, loss = 0.01013800\n",
      "Iteration 22530, loss = 0.01013757\n",
      "Iteration 22531, loss = 0.01013713\n",
      "Iteration 22532, loss = 0.01013670\n",
      "Iteration 22533, loss = 0.01013626\n",
      "Iteration 22534, loss = 0.01013582\n",
      "Iteration 22535, loss = 0.01013539\n",
      "Iteration 22536, loss = 0.01013495\n",
      "Iteration 22537, loss = 0.01013451\n",
      "Iteration 22538, loss = 0.01013408\n",
      "Iteration 22539, loss = 0.01013364\n",
      "Iteration 22540, loss = 0.01013321\n",
      "Iteration 22541, loss = 0.01013277\n",
      "Iteration 22542, loss = 0.01013233\n",
      "Iteration 22543, loss = 0.01013190\n",
      "Iteration 22544, loss = 0.01013146\n",
      "Iteration 22545, loss = 0.01013103\n",
      "Iteration 22546, loss = 0.01013059\n",
      "Iteration 22547, loss = 0.01013015\n",
      "Iteration 22548, loss = 0.01012972\n",
      "Iteration 22549, loss = 0.01012928\n",
      "Iteration 22550, loss = 0.01012885\n",
      "Iteration 22551, loss = 0.01012841\n",
      "Iteration 22552, loss = 0.01012798\n",
      "Iteration 22553, loss = 0.01012754\n",
      "Iteration 22554, loss = 0.01012711\n",
      "Iteration 22555, loss = 0.01012667\n",
      "Iteration 22556, loss = 0.01012624\n",
      "Iteration 22557, loss = 0.01012580\n",
      "Iteration 22558, loss = 0.01012537\n",
      "Iteration 22559, loss = 0.01012493\n",
      "Iteration 22560, loss = 0.01012450\n",
      "Iteration 22561, loss = 0.01012406\n",
      "Iteration 22562, loss = 0.01012363\n",
      "Iteration 22563, loss = 0.01012319\n",
      "Iteration 22564, loss = 0.01012276\n",
      "Iteration 22565, loss = 0.01012232\n",
      "Iteration 22566, loss = 0.01012189\n",
      "Iteration 22567, loss = 0.01012145\n",
      "Iteration 22568, loss = 0.01012102\n",
      "Iteration 22569, loss = 0.01012058\n",
      "Iteration 22570, loss = 0.01012015\n",
      "Iteration 22571, loss = 0.01011971\n",
      "Iteration 22572, loss = 0.01011928\n",
      "Iteration 22573, loss = 0.01011885\n",
      "Iteration 22574, loss = 0.01011841\n",
      "Iteration 22575, loss = 0.01011798\n",
      "Iteration 22576, loss = 0.01011754\n",
      "Iteration 22577, loss = 0.01011711\n",
      "Iteration 22578, loss = 0.01011668\n",
      "Iteration 22579, loss = 0.01011624\n",
      "Iteration 22580, loss = 0.01011581\n",
      "Iteration 22581, loss = 0.01011537\n",
      "Iteration 22582, loss = 0.01011494\n",
      "Iteration 22583, loss = 0.01011451\n",
      "Iteration 22584, loss = 0.01011407\n",
      "Iteration 22585, loss = 0.01011364\n",
      "Iteration 22586, loss = 0.01011321\n",
      "Iteration 22587, loss = 0.01011277\n",
      "Iteration 22588, loss = 0.01011234\n",
      "Iteration 22589, loss = 0.01011191\n",
      "Iteration 22590, loss = 0.01011147\n",
      "Iteration 22591, loss = 0.01011104\n",
      "Iteration 22592, loss = 0.01011061\n",
      "Iteration 22593, loss = 0.01011017\n",
      "Iteration 22594, loss = 0.01010974\n",
      "Iteration 22595, loss = 0.01010931\n",
      "Iteration 22596, loss = 0.01010887\n",
      "Iteration 22597, loss = 0.01010844\n",
      "Iteration 22598, loss = 0.01010801\n",
      "Iteration 22599, loss = 0.01010757\n",
      "Iteration 22600, loss = 0.01010714\n",
      "Iteration 22601, loss = 0.01010671\n",
      "Iteration 22602, loss = 0.01010628\n",
      "Iteration 22603, loss = 0.01010584\n",
      "Iteration 22604, loss = 0.01010541\n",
      "Iteration 22605, loss = 0.01010498\n",
      "Iteration 22606, loss = 0.01010455\n",
      "Iteration 22607, loss = 0.01010411\n",
      "Iteration 22608, loss = 0.01010368\n",
      "Iteration 22609, loss = 0.01010325\n",
      "Iteration 22610, loss = 0.01010282\n",
      "Iteration 22611, loss = 0.01010238\n",
      "Iteration 22612, loss = 0.01010195\n",
      "Iteration 22613, loss = 0.01010152\n",
      "Iteration 22614, loss = 0.01010109\n",
      "Iteration 22615, loss = 0.01010066\n",
      "Iteration 22616, loss = 0.01010022\n",
      "Iteration 22617, loss = 0.01009979\n",
      "Iteration 22618, loss = 0.01009936\n",
      "Iteration 22619, loss = 0.01009893\n",
      "Iteration 22620, loss = 0.01009850\n",
      "Iteration 22621, loss = 0.01009806\n",
      "Iteration 22622, loss = 0.01009763\n",
      "Iteration 22623, loss = 0.01009720\n",
      "Iteration 22624, loss = 0.01009677\n",
      "Iteration 22625, loss = 0.01009634\n",
      "Iteration 22626, loss = 0.01009591\n",
      "Iteration 22627, loss = 0.01009548\n",
      "Iteration 22628, loss = 0.01009504\n",
      "Iteration 22629, loss = 0.01009461\n",
      "Iteration 22630, loss = 0.01009418\n",
      "Iteration 22631, loss = 0.01009375\n",
      "Iteration 22632, loss = 0.01009332\n",
      "Iteration 22633, loss = 0.01009289\n",
      "Iteration 22634, loss = 0.01009246\n",
      "Iteration 22635, loss = 0.01009203\n",
      "Iteration 22636, loss = 0.01009160\n",
      "Iteration 22637, loss = 0.01009117\n",
      "Iteration 22638, loss = 0.01009073\n",
      "Iteration 22639, loss = 0.01009030\n",
      "Iteration 22640, loss = 0.01008987\n",
      "Iteration 22641, loss = 0.01008944\n",
      "Iteration 22642, loss = 0.01008901\n",
      "Iteration 22643, loss = 0.01008858\n",
      "Iteration 22644, loss = 0.01008815\n",
      "Iteration 22645, loss = 0.01008772\n",
      "Iteration 22646, loss = 0.01008729\n",
      "Iteration 22647, loss = 0.01008686\n",
      "Iteration 22648, loss = 0.01008643\n",
      "Iteration 22649, loss = 0.01008600\n",
      "Iteration 22650, loss = 0.01008557\n",
      "Iteration 22651, loss = 0.01008514\n",
      "Iteration 22652, loss = 0.01008471\n",
      "Iteration 22653, loss = 0.01008428\n",
      "Iteration 22654, loss = 0.01008385\n",
      "Iteration 22655, loss = 0.01008342\n",
      "Iteration 22656, loss = 0.01008299\n",
      "Iteration 22657, loss = 0.01008256\n",
      "Iteration 22658, loss = 0.01008213\n",
      "Iteration 22659, loss = 0.01008170\n",
      "Iteration 22660, loss = 0.01008127\n",
      "Iteration 22661, loss = 0.01008084\n",
      "Iteration 22662, loss = 0.01008041\n",
      "Iteration 22663, loss = 0.01007998\n",
      "Iteration 22664, loss = 0.01007955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22665, loss = 0.01007912\n",
      "Iteration 22666, loss = 0.01007870\n",
      "Iteration 22667, loss = 0.01007827\n",
      "Iteration 22668, loss = 0.01007784\n",
      "Iteration 22669, loss = 0.01007741\n",
      "Iteration 22670, loss = 0.01007698\n",
      "Iteration 22671, loss = 0.01007655\n",
      "Iteration 22672, loss = 0.01007612\n",
      "Iteration 22673, loss = 0.01007569\n",
      "Iteration 22674, loss = 0.01007526\n",
      "Iteration 22675, loss = 0.01007483\n",
      "Iteration 22676, loss = 0.01007441\n",
      "Iteration 22677, loss = 0.01007398\n",
      "Iteration 22678, loss = 0.01007355\n",
      "Iteration 22679, loss = 0.01007312\n",
      "Iteration 22680, loss = 0.01007269\n",
      "Iteration 22681, loss = 0.01007226\n",
      "Iteration 22682, loss = 0.01007183\n",
      "Iteration 22683, loss = 0.01007141\n",
      "Iteration 22684, loss = 0.01007098\n",
      "Iteration 22685, loss = 0.01007055\n",
      "Iteration 22686, loss = 0.01007012\n",
      "Iteration 22687, loss = 0.01006969\n",
      "Iteration 22688, loss = 0.01006927\n",
      "Iteration 22689, loss = 0.01006884\n",
      "Iteration 22690, loss = 0.01006841\n",
      "Iteration 22691, loss = 0.01006798\n",
      "Iteration 22692, loss = 0.01006755\n",
      "Iteration 22693, loss = 0.01006713\n",
      "Iteration 22694, loss = 0.01006670\n",
      "Iteration 22695, loss = 0.01006627\n",
      "Iteration 22696, loss = 0.01006584\n",
      "Iteration 22697, loss = 0.01006541\n",
      "Iteration 22698, loss = 0.01006499\n",
      "Iteration 22699, loss = 0.01006456\n",
      "Iteration 22700, loss = 0.01006413\n",
      "Iteration 22701, loss = 0.01006370\n",
      "Iteration 22702, loss = 0.01006328\n",
      "Iteration 22703, loss = 0.01006285\n",
      "Iteration 22704, loss = 0.01006242\n",
      "Iteration 22705, loss = 0.01006200\n",
      "Iteration 22706, loss = 0.01006157\n",
      "Iteration 22707, loss = 0.01006114\n",
      "Iteration 22708, loss = 0.01006071\n",
      "Iteration 22709, loss = 0.01006029\n",
      "Iteration 22710, loss = 0.01005986\n",
      "Iteration 22711, loss = 0.01005943\n",
      "Iteration 22712, loss = 0.01005901\n",
      "Iteration 22713, loss = 0.01005858\n",
      "Iteration 22714, loss = 0.01005815\n",
      "Iteration 22715, loss = 0.01005773\n",
      "Iteration 22716, loss = 0.01005730\n",
      "Iteration 22717, loss = 0.01005687\n",
      "Iteration 22718, loss = 0.01005645\n",
      "Iteration 22719, loss = 0.01005602\n",
      "Iteration 22720, loss = 0.01005559\n",
      "Iteration 22721, loss = 0.01005517\n",
      "Iteration 22722, loss = 0.01005474\n",
      "Iteration 22723, loss = 0.01005432\n",
      "Iteration 22724, loss = 0.01005389\n",
      "Iteration 22725, loss = 0.01005346\n",
      "Iteration 22726, loss = 0.01005304\n",
      "Iteration 22727, loss = 0.01005261\n",
      "Iteration 22728, loss = 0.01005219\n",
      "Iteration 22729, loss = 0.01005176\n",
      "Iteration 22730, loss = 0.01005133\n",
      "Iteration 22731, loss = 0.01005091\n",
      "Iteration 22732, loss = 0.01005048\n",
      "Iteration 22733, loss = 0.01005006\n",
      "Iteration 22734, loss = 0.01004963\n",
      "Iteration 22735, loss = 0.01004920\n",
      "Iteration 22736, loss = 0.01004878\n",
      "Iteration 22737, loss = 0.01004835\n",
      "Iteration 22738, loss = 0.01004793\n",
      "Iteration 22739, loss = 0.01004750\n",
      "Iteration 22740, loss = 0.01004708\n",
      "Iteration 22741, loss = 0.01004665\n",
      "Iteration 22742, loss = 0.01004623\n",
      "Iteration 22743, loss = 0.01004580\n",
      "Iteration 22744, loss = 0.01004538\n",
      "Iteration 22745, loss = 0.01004495\n",
      "Iteration 22746, loss = 0.01004453\n",
      "Iteration 22747, loss = 0.01004410\n",
      "Iteration 22748, loss = 0.01004368\n",
      "Iteration 22749, loss = 0.01004325\n",
      "Iteration 22750, loss = 0.01004283\n",
      "Iteration 22751, loss = 0.01004240\n",
      "Iteration 22752, loss = 0.01004198\n",
      "Iteration 22753, loss = 0.01004155\n",
      "Iteration 22754, loss = 0.01004113\n",
      "Iteration 22755, loss = 0.01004070\n",
      "Iteration 22756, loss = 0.01004028\n",
      "Iteration 22757, loss = 0.01003985\n",
      "Iteration 22758, loss = 0.01003943\n",
      "Iteration 22759, loss = 0.01003901\n",
      "Iteration 22760, loss = 0.01003858\n",
      "Iteration 22761, loss = 0.01003816\n",
      "Iteration 22762, loss = 0.01003773\n",
      "Iteration 22763, loss = 0.01003731\n",
      "Iteration 22764, loss = 0.01003689\n",
      "Iteration 22765, loss = 0.01003646\n",
      "Iteration 22766, loss = 0.01003604\n",
      "Iteration 22767, loss = 0.01003561\n",
      "Iteration 22768, loss = 0.01003519\n",
      "Iteration 22769, loss = 0.01003477\n",
      "Iteration 22770, loss = 0.01003434\n",
      "Iteration 22771, loss = 0.01003392\n",
      "Iteration 22772, loss = 0.01003349\n",
      "Iteration 22773, loss = 0.01003307\n",
      "Iteration 22774, loss = 0.01003265\n",
      "Iteration 22775, loss = 0.01003222\n",
      "Iteration 22776, loss = 0.01003180\n",
      "Iteration 22777, loss = 0.01003138\n",
      "Iteration 22778, loss = 0.01003095\n",
      "Iteration 22779, loss = 0.01003053\n",
      "Iteration 22780, loss = 0.01003011\n",
      "Iteration 22781, loss = 0.01002968\n",
      "Iteration 22782, loss = 0.01002926\n",
      "Iteration 22783, loss = 0.01002884\n",
      "Iteration 22784, loss = 0.01002841\n",
      "Iteration 22785, loss = 0.01002799\n",
      "Iteration 22786, loss = 0.01002757\n",
      "Iteration 22787, loss = 0.01002715\n",
      "Iteration 22788, loss = 0.01002672\n",
      "Iteration 22789, loss = 0.01002630\n",
      "Iteration 22790, loss = 0.01002588\n",
      "Iteration 22791, loss = 0.01002546\n",
      "Iteration 22792, loss = 0.01002503\n",
      "Iteration 22793, loss = 0.01002461\n",
      "Iteration 22794, loss = 0.01002419\n",
      "Iteration 22795, loss = 0.01002376\n",
      "Iteration 22796, loss = 0.01002334\n",
      "Iteration 22797, loss = 0.01002292\n",
      "Iteration 22798, loss = 0.01002250\n",
      "Iteration 22799, loss = 0.01002208\n",
      "Iteration 22800, loss = 0.01002165\n",
      "Iteration 22801, loss = 0.01002123\n",
      "Iteration 22802, loss = 0.01002081\n",
      "Iteration 22803, loss = 0.01002039\n",
      "Iteration 22804, loss = 0.01001997\n",
      "Iteration 22805, loss = 0.01001954\n",
      "Iteration 22806, loss = 0.01001912\n",
      "Iteration 22807, loss = 0.01001870\n",
      "Iteration 22808, loss = 0.01001828\n",
      "Iteration 22809, loss = 0.01001786\n",
      "Iteration 22810, loss = 0.01001743\n",
      "Iteration 22811, loss = 0.01001701\n",
      "Iteration 22812, loss = 0.01001659\n",
      "Iteration 22813, loss = 0.01001617\n",
      "Iteration 22814, loss = 0.01001575\n",
      "Iteration 22815, loss = 0.01001533\n",
      "Iteration 22816, loss = 0.01001491\n",
      "Iteration 22817, loss = 0.01001448\n",
      "Iteration 22818, loss = 0.01001406\n",
      "Iteration 22819, loss = 0.01001364\n",
      "Iteration 22820, loss = 0.01001322\n",
      "Iteration 22821, loss = 0.01001280\n",
      "Iteration 22822, loss = 0.01001238\n",
      "Iteration 22823, loss = 0.01001196\n",
      "Iteration 22824, loss = 0.01001154\n",
      "Iteration 22825, loss = 0.01001112\n",
      "Iteration 22826, loss = 0.01001069\n",
      "Iteration 22827, loss = 0.01001027\n",
      "Iteration 22828, loss = 0.01000985\n",
      "Iteration 22829, loss = 0.01000943\n",
      "Iteration 22830, loss = 0.01000901\n",
      "Iteration 22831, loss = 0.01000859\n",
      "Iteration 22832, loss = 0.01000817\n",
      "Iteration 22833, loss = 0.01000775\n",
      "Iteration 22834, loss = 0.01000733\n",
      "Iteration 22835, loss = 0.01000691\n",
      "Iteration 22836, loss = 0.01000649\n",
      "Iteration 22837, loss = 0.01000607\n",
      "Iteration 22838, loss = 0.01000565\n",
      "Iteration 22839, loss = 0.01000523\n",
      "Iteration 22840, loss = 0.01000481\n",
      "Iteration 22841, loss = 0.01000439\n",
      "Iteration 22842, loss = 0.01000397\n",
      "Iteration 22843, loss = 0.01000355\n",
      "Iteration 22844, loss = 0.01000313\n",
      "Iteration 22845, loss = 0.01000271\n",
      "Iteration 22846, loss = 0.01000229\n",
      "Iteration 22847, loss = 0.01000187\n",
      "Iteration 22848, loss = 0.01000145\n",
      "Iteration 22849, loss = 0.01000103\n",
      "Iteration 22850, loss = 0.01000061\n",
      "Iteration 22851, loss = 0.01000019\n",
      "Iteration 22852, loss = 0.00999977\n",
      "Iteration 22853, loss = 0.00999935\n",
      "Iteration 22854, loss = 0.00999893\n",
      "Iteration 22855, loss = 0.00999851\n",
      "Iteration 22856, loss = 0.00999809\n",
      "Iteration 22857, loss = 0.00999767\n",
      "Iteration 22858, loss = 0.00999726\n",
      "Iteration 22859, loss = 0.00999684\n",
      "Iteration 22860, loss = 0.00999642\n",
      "Iteration 22861, loss = 0.00999600\n",
      "Iteration 22862, loss = 0.00999558\n",
      "Iteration 22863, loss = 0.00999516\n",
      "Iteration 22864, loss = 0.00999474\n",
      "Iteration 22865, loss = 0.00999432\n",
      "Iteration 22866, loss = 0.00999390\n",
      "Iteration 22867, loss = 0.00999349\n",
      "Iteration 22868, loss = 0.00999307\n",
      "Iteration 22869, loss = 0.00999265\n",
      "Iteration 22870, loss = 0.00999223\n",
      "Iteration 22871, loss = 0.00999181\n",
      "Iteration 22872, loss = 0.00999139\n",
      "Iteration 22873, loss = 0.00999097\n",
      "Iteration 22874, loss = 0.00999056\n",
      "Iteration 22875, loss = 0.00999014\n",
      "Iteration 22876, loss = 0.00998972\n",
      "Iteration 22877, loss = 0.00998930\n",
      "Iteration 22878, loss = 0.00998888\n",
      "Iteration 22879, loss = 0.00998846\n",
      "Iteration 22880, loss = 0.00998805\n",
      "Iteration 22881, loss = 0.00998763\n",
      "Iteration 22882, loss = 0.00998721\n",
      "Iteration 22883, loss = 0.00998679\n",
      "Iteration 22884, loss = 0.00998637\n",
      "Iteration 22885, loss = 0.00998596\n",
      "Iteration 22886, loss = 0.00998554\n",
      "Iteration 22887, loss = 0.00998512\n",
      "Iteration 22888, loss = 0.00998470\n",
      "Iteration 22889, loss = 0.00998429\n",
      "Iteration 22890, loss = 0.00998387\n",
      "Iteration 22891, loss = 0.00998345\n",
      "Iteration 22892, loss = 0.00998303\n",
      "Iteration 22893, loss = 0.00998262\n",
      "Iteration 22894, loss = 0.00998220\n",
      "Iteration 22895, loss = 0.00998178\n",
      "Iteration 22896, loss = 0.00998136\n",
      "Iteration 22897, loss = 0.00998095\n",
      "Iteration 22898, loss = 0.00998053\n",
      "Iteration 22899, loss = 0.00998011\n",
      "Iteration 22900, loss = 0.00997970\n",
      "Iteration 22901, loss = 0.00997928\n",
      "Iteration 22902, loss = 0.00997886\n",
      "Iteration 22903, loss = 0.00997845\n",
      "Iteration 22904, loss = 0.00997803\n",
      "Iteration 22905, loss = 0.00997761\n",
      "Iteration 22906, loss = 0.00997720\n",
      "Iteration 22907, loss = 0.00997678\n",
      "Iteration 22908, loss = 0.00997636\n",
      "Iteration 22909, loss = 0.00997595\n",
      "Iteration 22910, loss = 0.00997553\n",
      "Iteration 22911, loss = 0.00997511\n",
      "Iteration 22912, loss = 0.00997470\n",
      "Iteration 22913, loss = 0.00997428\n",
      "Iteration 22914, loss = 0.00997386\n",
      "Iteration 22915, loss = 0.00997345\n",
      "Iteration 22916, loss = 0.00997303\n",
      "Iteration 22917, loss = 0.00997261\n",
      "Iteration 22918, loss = 0.00997220\n",
      "Iteration 22919, loss = 0.00997178\n",
      "Iteration 22920, loss = 0.00997137\n",
      "Iteration 22921, loss = 0.00997095\n",
      "Iteration 22922, loss = 0.00997053\n",
      "Iteration 22923, loss = 0.00997012\n",
      "Iteration 22924, loss = 0.00996970\n",
      "Iteration 22925, loss = 0.00996929\n",
      "Iteration 22926, loss = 0.00996887\n",
      "Iteration 22927, loss = 0.00996846\n",
      "Iteration 22928, loss = 0.00996804\n",
      "Iteration 22929, loss = 0.00996762\n",
      "Iteration 22930, loss = 0.00996721\n",
      "Iteration 22931, loss = 0.00996679\n",
      "Iteration 22932, loss = 0.00996638\n",
      "Iteration 22933, loss = 0.00996596\n",
      "Iteration 22934, loss = 0.00996555\n",
      "Iteration 22935, loss = 0.00996513\n",
      "Iteration 22936, loss = 0.00996472\n",
      "Iteration 22937, loss = 0.00996430\n",
      "Iteration 22938, loss = 0.00996389\n",
      "Iteration 22939, loss = 0.00996347\n",
      "Iteration 22940, loss = 0.00996306\n",
      "Iteration 22941, loss = 0.00996264\n",
      "Iteration 22942, loss = 0.00996223\n",
      "Iteration 22943, loss = 0.00996181\n",
      "Iteration 22944, loss = 0.00996140\n",
      "Iteration 22945, loss = 0.00996098\n",
      "Iteration 22946, loss = 0.00996057\n",
      "Iteration 22947, loss = 0.00996015\n",
      "Iteration 22948, loss = 0.00995974\n",
      "Iteration 22949, loss = 0.00995932\n",
      "Iteration 22950, loss = 0.00995891\n",
      "Iteration 22951, loss = 0.00995850\n",
      "Iteration 22952, loss = 0.00995808\n",
      "Iteration 22953, loss = 0.00995767\n",
      "Iteration 22954, loss = 0.00995725\n",
      "Iteration 22955, loss = 0.00995684\n",
      "Iteration 22956, loss = 0.00995642\n",
      "Iteration 22957, loss = 0.00995601\n",
      "Iteration 22958, loss = 0.00995560\n",
      "Iteration 22959, loss = 0.00995518\n",
      "Iteration 22960, loss = 0.00995477\n",
      "Iteration 22961, loss = 0.00995435\n",
      "Iteration 22962, loss = 0.00995394\n",
      "Iteration 22963, loss = 0.00995353\n",
      "Iteration 22964, loss = 0.00995311\n",
      "Iteration 22965, loss = 0.00995270\n",
      "Iteration 22966, loss = 0.00995229\n",
      "Iteration 22967, loss = 0.00995187\n",
      "Iteration 22968, loss = 0.00995146\n",
      "Iteration 22969, loss = 0.00995105\n",
      "Iteration 22970, loss = 0.00995063\n",
      "Iteration 22971, loss = 0.00995022\n",
      "Iteration 22972, loss = 0.00994980\n",
      "Iteration 22973, loss = 0.00994939\n",
      "Iteration 22974, loss = 0.00994898\n",
      "Iteration 22975, loss = 0.00994857\n",
      "Iteration 22976, loss = 0.00994815\n",
      "Iteration 22977, loss = 0.00994774\n",
      "Iteration 22978, loss = 0.00994733\n",
      "Iteration 22979, loss = 0.00994691\n",
      "Iteration 22980, loss = 0.00994650\n",
      "Iteration 22981, loss = 0.00994609\n",
      "Iteration 22982, loss = 0.00994567\n",
      "Iteration 22983, loss = 0.00994526\n",
      "Iteration 22984, loss = 0.00994485\n",
      "Iteration 22985, loss = 0.00994444\n",
      "Iteration 22986, loss = 0.00994402\n",
      "Iteration 22987, loss = 0.00994361\n",
      "Iteration 22988, loss = 0.00994320\n",
      "Iteration 22989, loss = 0.00994279\n",
      "Iteration 22990, loss = 0.00994237\n",
      "Iteration 22991, loss = 0.00994196\n",
      "Iteration 22992, loss = 0.00994155\n",
      "Iteration 22993, loss = 0.00994114\n",
      "Iteration 22994, loss = 0.00994072\n",
      "Iteration 22995, loss = 0.00994031\n",
      "Iteration 22996, loss = 0.00993990\n",
      "Iteration 22997, loss = 0.00993949\n",
      "Iteration 22998, loss = 0.00993908\n",
      "Iteration 22999, loss = 0.00993866\n",
      "Iteration 23000, loss = 0.00993825\n",
      "Iteration 23001, loss = 0.00993784\n",
      "Iteration 23002, loss = 0.00993743\n",
      "Iteration 23003, loss = 0.00993702\n",
      "Iteration 23004, loss = 0.00993660\n",
      "Iteration 23005, loss = 0.00993619\n",
      "Iteration 23006, loss = 0.00993578\n",
      "Iteration 23007, loss = 0.00993537\n",
      "Iteration 23008, loss = 0.00993496\n",
      "Iteration 23009, loss = 0.00993455\n",
      "Iteration 23010, loss = 0.00993413\n",
      "Iteration 23011, loss = 0.00993372\n",
      "Iteration 23012, loss = 0.00993331\n",
      "Iteration 23013, loss = 0.00993290\n",
      "Iteration 23014, loss = 0.00993249\n",
      "Iteration 23015, loss = 0.00993208\n",
      "Iteration 23016, loss = 0.00993167\n",
      "Iteration 23017, loss = 0.00993126\n",
      "Iteration 23018, loss = 0.00993085\n",
      "Iteration 23019, loss = 0.00993043\n",
      "Iteration 23020, loss = 0.00993002\n",
      "Iteration 23021, loss = 0.00992961\n",
      "Iteration 23022, loss = 0.00992920\n",
      "Iteration 23023, loss = 0.00992879\n",
      "Iteration 23024, loss = 0.00992838\n",
      "Iteration 23025, loss = 0.00992797\n",
      "Iteration 23026, loss = 0.00992756\n",
      "Iteration 23027, loss = 0.00992715\n",
      "Iteration 23028, loss = 0.00992674\n",
      "Iteration 23029, loss = 0.00992633\n",
      "Iteration 23030, loss = 0.00992592\n",
      "Iteration 23031, loss = 0.00992551\n",
      "Iteration 23032, loss = 0.00992510\n",
      "Iteration 23033, loss = 0.00992469\n",
      "Iteration 23034, loss = 0.00992428\n",
      "Iteration 23035, loss = 0.00992387\n",
      "Iteration 23036, loss = 0.00992346\n",
      "Iteration 23037, loss = 0.00992305\n",
      "Iteration 23038, loss = 0.00992264\n",
      "Iteration 23039, loss = 0.00992223\n",
      "Iteration 23040, loss = 0.00992182\n",
      "Iteration 23041, loss = 0.00992141\n",
      "Iteration 23042, loss = 0.00992100\n",
      "Iteration 23043, loss = 0.00992059\n",
      "Iteration 23044, loss = 0.00992018\n",
      "Iteration 23045, loss = 0.00991977\n",
      "Iteration 23046, loss = 0.00991936\n",
      "Iteration 23047, loss = 0.00991895\n",
      "Iteration 23048, loss = 0.00991854\n",
      "Iteration 23049, loss = 0.00991813\n",
      "Iteration 23050, loss = 0.00991772\n",
      "Iteration 23051, loss = 0.00991731\n",
      "Iteration 23052, loss = 0.00991690\n",
      "Iteration 23053, loss = 0.00991649\n",
      "Iteration 23054, loss = 0.00991608\n",
      "Iteration 23055, loss = 0.00991567\n",
      "Iteration 23056, loss = 0.00991526\n",
      "Iteration 23057, loss = 0.00991485\n",
      "Iteration 23058, loss = 0.00991444\n",
      "Iteration 23059, loss = 0.00991404\n",
      "Iteration 23060, loss = 0.00991363\n",
      "Iteration 23061, loss = 0.00991322\n",
      "Iteration 23062, loss = 0.00991281\n",
      "Iteration 23063, loss = 0.00991240\n",
      "Iteration 23064, loss = 0.00991199\n",
      "Iteration 23065, loss = 0.00991158\n",
      "Iteration 23066, loss = 0.00991117\n",
      "Iteration 23067, loss = 0.00991077\n",
      "Iteration 23068, loss = 0.00991036\n",
      "Iteration 23069, loss = 0.00990995\n",
      "Iteration 23070, loss = 0.00990954\n",
      "Iteration 23071, loss = 0.00990913\n",
      "Iteration 23072, loss = 0.00990872\n",
      "Iteration 23073, loss = 0.00990832\n",
      "Iteration 23074, loss = 0.00990791\n",
      "Iteration 23075, loss = 0.00990750\n",
      "Iteration 23076, loss = 0.00990709\n",
      "Iteration 23077, loss = 0.00990668\n",
      "Iteration 23078, loss = 0.00990627\n",
      "Iteration 23079, loss = 0.00990587\n",
      "Iteration 23080, loss = 0.00990546\n",
      "Iteration 23081, loss = 0.00990505\n",
      "Iteration 23082, loss = 0.00990464\n",
      "Iteration 23083, loss = 0.00990424\n",
      "Iteration 23084, loss = 0.00990383\n",
      "Iteration 23085, loss = 0.00990342\n",
      "Iteration 23086, loss = 0.00990301\n",
      "Iteration 23087, loss = 0.00990260\n",
      "Iteration 23088, loss = 0.00990220\n",
      "Iteration 23089, loss = 0.00990179\n",
      "Iteration 23090, loss = 0.00990138\n",
      "Iteration 23091, loss = 0.00990097\n",
      "Iteration 23092, loss = 0.00990057\n",
      "Iteration 23093, loss = 0.00990016\n",
      "Iteration 23094, loss = 0.00989975\n",
      "Iteration 23095, loss = 0.00989935\n",
      "Iteration 23096, loss = 0.00989894\n",
      "Iteration 23097, loss = 0.00989853\n",
      "Iteration 23098, loss = 0.00989812\n",
      "Iteration 23099, loss = 0.00989772\n",
      "Iteration 23100, loss = 0.00989731\n",
      "Iteration 23101, loss = 0.00989690\n",
      "Iteration 23102, loss = 0.00989650\n",
      "Iteration 23103, loss = 0.00989609\n",
      "Iteration 23104, loss = 0.00989568\n",
      "Iteration 23105, loss = 0.00989528\n",
      "Iteration 23106, loss = 0.00989487\n",
      "Iteration 23107, loss = 0.00989446\n",
      "Iteration 23108, loss = 0.00989406\n",
      "Iteration 23109, loss = 0.00989365\n",
      "Iteration 23110, loss = 0.00989324\n",
      "Iteration 23111, loss = 0.00989284\n",
      "Iteration 23112, loss = 0.00989243\n",
      "Iteration 23113, loss = 0.00989202\n",
      "Iteration 23114, loss = 0.00989162\n",
      "Iteration 23115, loss = 0.00989121\n",
      "Iteration 23116, loss = 0.00989081\n",
      "Iteration 23117, loss = 0.00989040\n",
      "Iteration 23118, loss = 0.00988999\n",
      "Iteration 23119, loss = 0.00988959\n",
      "Iteration 23120, loss = 0.00988918\n",
      "Iteration 23121, loss = 0.00988878\n",
      "Iteration 23122, loss = 0.00988837\n",
      "Iteration 23123, loss = 0.00988796\n",
      "Iteration 23124, loss = 0.00988756\n",
      "Iteration 23125, loss = 0.00988715\n",
      "Iteration 23126, loss = 0.00988675\n",
      "Iteration 23127, loss = 0.00988634\n",
      "Iteration 23128, loss = 0.00988594\n",
      "Iteration 23129, loss = 0.00988553\n",
      "Iteration 23130, loss = 0.00988513\n",
      "Iteration 23131, loss = 0.00988472\n",
      "Iteration 23132, loss = 0.00988431\n",
      "Iteration 23133, loss = 0.00988391\n",
      "Iteration 23134, loss = 0.00988350\n",
      "Iteration 23135, loss = 0.00988310\n",
      "Iteration 23136, loss = 0.00988269\n",
      "Iteration 23137, loss = 0.00988229\n",
      "Iteration 23138, loss = 0.00988188\n",
      "Iteration 23139, loss = 0.00988148\n",
      "Iteration 23140, loss = 0.00988107\n",
      "Iteration 23141, loss = 0.00988067\n",
      "Iteration 23142, loss = 0.00988026\n",
      "Iteration 23143, loss = 0.00987986\n",
      "Iteration 23144, loss = 0.00987945\n",
      "Iteration 23145, loss = 0.00987905\n",
      "Iteration 23146, loss = 0.00987864\n",
      "Iteration 23147, loss = 0.00987824\n",
      "Iteration 23148, loss = 0.00987784\n",
      "Iteration 23149, loss = 0.00987743\n",
      "Iteration 23150, loss = 0.00987703\n",
      "Iteration 23151, loss = 0.00987662\n",
      "Iteration 23152, loss = 0.00987622\n",
      "Iteration 23153, loss = 0.00987581\n",
      "Iteration 23154, loss = 0.00987541\n",
      "Iteration 23155, loss = 0.00987500\n",
      "Iteration 23156, loss = 0.00987460\n",
      "Iteration 23157, loss = 0.00987420\n",
      "Iteration 23158, loss = 0.00987379\n",
      "Iteration 23159, loss = 0.00987339\n",
      "Iteration 23160, loss = 0.00987298\n",
      "Iteration 23161, loss = 0.00987258\n",
      "Iteration 23162, loss = 0.00987218\n",
      "Iteration 23163, loss = 0.00987177\n",
      "Iteration 23164, loss = 0.00987137\n",
      "Iteration 23165, loss = 0.00987097\n",
      "Iteration 23166, loss = 0.00987056\n",
      "Iteration 23167, loss = 0.00987016\n",
      "Iteration 23168, loss = 0.00986975\n",
      "Iteration 23169, loss = 0.00986935\n",
      "Iteration 23170, loss = 0.00986895\n",
      "Iteration 23171, loss = 0.00986854\n",
      "Iteration 23172, loss = 0.00986814\n",
      "Iteration 23173, loss = 0.00986774\n",
      "Iteration 23174, loss = 0.00986733\n",
      "Iteration 23175, loss = 0.00986693\n",
      "Iteration 23176, loss = 0.00986653\n",
      "Iteration 23177, loss = 0.00986612\n",
      "Iteration 23178, loss = 0.00986572\n",
      "Iteration 23179, loss = 0.00986532\n",
      "Iteration 23180, loss = 0.00986491\n",
      "Iteration 23181, loss = 0.00986451\n",
      "Iteration 23182, loss = 0.00986411\n",
      "Iteration 23183, loss = 0.00986371\n",
      "Iteration 23184, loss = 0.00986330\n",
      "Iteration 23185, loss = 0.00986290\n",
      "Iteration 23186, loss = 0.00986250\n",
      "Iteration 23187, loss = 0.00986210\n",
      "Iteration 23188, loss = 0.00986169\n",
      "Iteration 23189, loss = 0.00986129\n",
      "Iteration 23190, loss = 0.00986089\n",
      "Iteration 23191, loss = 0.00986048\n",
      "Iteration 23192, loss = 0.00986008\n",
      "Iteration 23193, loss = 0.00985968\n",
      "Iteration 23194, loss = 0.00985928\n",
      "Iteration 23195, loss = 0.00985888\n",
      "Iteration 23196, loss = 0.00985847\n",
      "Iteration 23197, loss = 0.00985807\n",
      "Iteration 23198, loss = 0.00985767\n",
      "Iteration 23199, loss = 0.00985727\n",
      "Iteration 23200, loss = 0.00985686\n",
      "Iteration 23201, loss = 0.00985646\n",
      "Iteration 23202, loss = 0.00985606\n",
      "Iteration 23203, loss = 0.00985566\n",
      "Iteration 23204, loss = 0.00985526\n",
      "Iteration 23205, loss = 0.00985486\n",
      "Iteration 23206, loss = 0.00985445\n",
      "Iteration 23207, loss = 0.00985405\n",
      "Iteration 23208, loss = 0.00985365\n",
      "Iteration 23209, loss = 0.00985325\n",
      "Iteration 23210, loss = 0.00985285\n",
      "Iteration 23211, loss = 0.00985245\n",
      "Iteration 23212, loss = 0.00985204\n",
      "Iteration 23213, loss = 0.00985164\n",
      "Iteration 23214, loss = 0.00985124\n",
      "Iteration 23215, loss = 0.00985084\n",
      "Iteration 23216, loss = 0.00985044\n",
      "Iteration 23217, loss = 0.00985004\n",
      "Iteration 23218, loss = 0.00984964\n",
      "Iteration 23219, loss = 0.00984924\n",
      "Iteration 23220, loss = 0.00984883\n",
      "Iteration 23221, loss = 0.00984843\n",
      "Iteration 23222, loss = 0.00984803\n",
      "Iteration 23223, loss = 0.00984763\n",
      "Iteration 23224, loss = 0.00984723\n",
      "Iteration 23225, loss = 0.00984683\n",
      "Iteration 23226, loss = 0.00984643\n",
      "Iteration 23227, loss = 0.00984603\n",
      "Iteration 23228, loss = 0.00984563\n",
      "Iteration 23229, loss = 0.00984523\n",
      "Iteration 23230, loss = 0.00984483\n",
      "Iteration 23231, loss = 0.00984443\n",
      "Iteration 23232, loss = 0.00984403\n",
      "Iteration 23233, loss = 0.00984363\n",
      "Iteration 23234, loss = 0.00984322\n",
      "Iteration 23235, loss = 0.00984282\n",
      "Iteration 23236, loss = 0.00984242\n",
      "Iteration 23237, loss = 0.00984202\n",
      "Iteration 23238, loss = 0.00984162\n",
      "Iteration 23239, loss = 0.00984122\n",
      "Iteration 23240, loss = 0.00984082\n",
      "Iteration 23241, loss = 0.00984042\n",
      "Iteration 23242, loss = 0.00984002\n",
      "Iteration 23243, loss = 0.00983962\n",
      "Iteration 23244, loss = 0.00983922\n",
      "Iteration 23245, loss = 0.00983882\n",
      "Iteration 23246, loss = 0.00983842\n",
      "Iteration 23247, loss = 0.00983802\n",
      "Iteration 23248, loss = 0.00983762\n",
      "Iteration 23249, loss = 0.00983722\n",
      "Iteration 23250, loss = 0.00983683\n",
      "Iteration 23251, loss = 0.00983643\n",
      "Iteration 23252, loss = 0.00983603\n",
      "Iteration 23253, loss = 0.00983563\n",
      "Iteration 23254, loss = 0.00983523\n",
      "Iteration 23255, loss = 0.00983483\n",
      "Iteration 23256, loss = 0.00983443\n",
      "Iteration 23257, loss = 0.00983403\n",
      "Iteration 23258, loss = 0.00983363\n",
      "Iteration 23259, loss = 0.00983323\n",
      "Iteration 23260, loss = 0.00983283\n",
      "Iteration 23261, loss = 0.00983243\n",
      "Iteration 23262, loss = 0.00983203\n",
      "Iteration 23263, loss = 0.00983163\n",
      "Iteration 23264, loss = 0.00983124\n",
      "Iteration 23265, loss = 0.00983084\n",
      "Iteration 23266, loss = 0.00983044\n",
      "Iteration 23267, loss = 0.00983004\n",
      "Iteration 23268, loss = 0.00982964\n",
      "Iteration 23269, loss = 0.00982924\n",
      "Iteration 23270, loss = 0.00982884\n",
      "Iteration 23271, loss = 0.00982844\n",
      "Iteration 23272, loss = 0.00982805\n",
      "Iteration 23273, loss = 0.00982765\n",
      "Iteration 23274, loss = 0.00982725\n",
      "Iteration 23275, loss = 0.00982685\n",
      "Iteration 23276, loss = 0.00982645\n",
      "Iteration 23277, loss = 0.00982605\n",
      "Iteration 23278, loss = 0.00982566\n",
      "Iteration 23279, loss = 0.00982526\n",
      "Iteration 23280, loss = 0.00982486\n",
      "Iteration 23281, loss = 0.00982446\n",
      "Iteration 23282, loss = 0.00982406\n",
      "Iteration 23283, loss = 0.00982367\n",
      "Iteration 23284, loss = 0.00982327\n",
      "Iteration 23285, loss = 0.00982287\n",
      "Iteration 23286, loss = 0.00982247\n",
      "Iteration 23287, loss = 0.00982207\n",
      "Iteration 23288, loss = 0.00982168\n",
      "Iteration 23289, loss = 0.00982128\n",
      "Iteration 23290, loss = 0.00982088\n",
      "Iteration 23291, loss = 0.00982048\n",
      "Iteration 23292, loss = 0.00982009\n",
      "Iteration 23293, loss = 0.00981969\n",
      "Iteration 23294, loss = 0.00981929\n",
      "Iteration 23295, loss = 0.00981889\n",
      "Iteration 23296, loss = 0.00981850\n",
      "Iteration 23297, loss = 0.00981810\n",
      "Iteration 23298, loss = 0.00981770\n",
      "Iteration 23299, loss = 0.00981730\n",
      "Iteration 23300, loss = 0.00981691\n",
      "Iteration 23301, loss = 0.00981651\n",
      "Iteration 23302, loss = 0.00981611\n",
      "Iteration 23303, loss = 0.00981572\n",
      "Iteration 23304, loss = 0.00981532\n",
      "Iteration 23305, loss = 0.00981492\n",
      "Iteration 23306, loss = 0.00981452\n",
      "Iteration 23307, loss = 0.00981413\n",
      "Iteration 23308, loss = 0.00981373\n",
      "Iteration 23309, loss = 0.00981333\n",
      "Iteration 23310, loss = 0.00981294\n",
      "Iteration 23311, loss = 0.00981254\n",
      "Iteration 23312, loss = 0.00981214\n",
      "Iteration 23313, loss = 0.00981175\n",
      "Iteration 23314, loss = 0.00981135\n",
      "Iteration 23315, loss = 0.00981095\n",
      "Iteration 23316, loss = 0.00981056\n",
      "Iteration 23317, loss = 0.00981016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23318, loss = 0.00980976\n",
      "Iteration 23319, loss = 0.00980937\n",
      "Iteration 23320, loss = 0.00980897\n",
      "Iteration 23321, loss = 0.00980858\n",
      "Iteration 23322, loss = 0.00980818\n",
      "Iteration 23323, loss = 0.00980778\n",
      "Iteration 23324, loss = 0.00980739\n",
      "Iteration 23325, loss = 0.00980699\n",
      "Iteration 23326, loss = 0.00980660\n",
      "Iteration 23327, loss = 0.00980620\n",
      "Iteration 23328, loss = 0.00980580\n",
      "Iteration 23329, loss = 0.00980541\n",
      "Iteration 23330, loss = 0.00980501\n",
      "Iteration 23331, loss = 0.00980462\n",
      "Iteration 23332, loss = 0.00980422\n",
      "Iteration 23333, loss = 0.00980383\n",
      "Iteration 23334, loss = 0.00980343\n",
      "Iteration 23335, loss = 0.00980303\n",
      "Iteration 23336, loss = 0.00980264\n",
      "Iteration 23337, loss = 0.00980224\n",
      "Iteration 23338, loss = 0.00980185\n",
      "Iteration 23339, loss = 0.00980145\n",
      "Iteration 23340, loss = 0.00980106\n",
      "Iteration 23341, loss = 0.00980066\n",
      "Iteration 23342, loss = 0.00980027\n",
      "Iteration 23343, loss = 0.00979987\n",
      "Iteration 23344, loss = 0.00979948\n",
      "Iteration 23345, loss = 0.00979908\n",
      "Iteration 23346, loss = 0.00979869\n",
      "Iteration 23347, loss = 0.00979829\n",
      "Iteration 23348, loss = 0.00979790\n",
      "Iteration 23349, loss = 0.00979750\n",
      "Iteration 23350, loss = 0.00979711\n",
      "Iteration 23351, loss = 0.00979671\n",
      "Iteration 23352, loss = 0.00979632\n",
      "Iteration 23353, loss = 0.00979592\n",
      "Iteration 23354, loss = 0.00979553\n",
      "Iteration 23355, loss = 0.00979513\n",
      "Iteration 23356, loss = 0.00979474\n",
      "Iteration 23357, loss = 0.00979434\n",
      "Iteration 23358, loss = 0.00979395\n",
      "Iteration 23359, loss = 0.00979356\n",
      "Iteration 23360, loss = 0.00979316\n",
      "Iteration 23361, loss = 0.00979277\n",
      "Iteration 23362, loss = 0.00979237\n",
      "Iteration 23363, loss = 0.00979198\n",
      "Iteration 23364, loss = 0.00979158\n",
      "Iteration 23365, loss = 0.00979119\n",
      "Iteration 23366, loss = 0.00979080\n",
      "Iteration 23367, loss = 0.00979040\n",
      "Iteration 23368, loss = 0.00979001\n",
      "Iteration 23369, loss = 0.00978961\n",
      "Iteration 23370, loss = 0.00978922\n",
      "Iteration 23371, loss = 0.00978883\n",
      "Iteration 23372, loss = 0.00978843\n",
      "Iteration 23373, loss = 0.00978804\n",
      "Iteration 23374, loss = 0.00978765\n",
      "Iteration 23375, loss = 0.00978725\n",
      "Iteration 23376, loss = 0.00978686\n",
      "Iteration 23377, loss = 0.00978646\n",
      "Iteration 23378, loss = 0.00978607\n",
      "Iteration 23379, loss = 0.00978568\n",
      "Iteration 23380, loss = 0.00978528\n",
      "Iteration 23381, loss = 0.00978489\n",
      "Iteration 23382, loss = 0.00978450\n",
      "Iteration 23383, loss = 0.00978410\n",
      "Iteration 23384, loss = 0.00978371\n",
      "Iteration 23385, loss = 0.00978332\n",
      "Iteration 23386, loss = 0.00978293\n",
      "Iteration 23387, loss = 0.00978253\n",
      "Iteration 23388, loss = 0.00978214\n",
      "Iteration 23389, loss = 0.00978175\n",
      "Iteration 23390, loss = 0.00978135\n",
      "Iteration 23391, loss = 0.00978096\n",
      "Iteration 23392, loss = 0.00978057\n",
      "Iteration 23393, loss = 0.00978017\n",
      "Iteration 23394, loss = 0.00977978\n",
      "Iteration 23395, loss = 0.00977939\n",
      "Iteration 23396, loss = 0.00977900\n",
      "Iteration 23397, loss = 0.00977860\n",
      "Iteration 23398, loss = 0.00977821\n",
      "Iteration 23399, loss = 0.00977782\n",
      "Iteration 23400, loss = 0.00977743\n",
      "Iteration 23401, loss = 0.00977703\n",
      "Iteration 23402, loss = 0.00977664\n",
      "Iteration 23403, loss = 0.00977625\n",
      "Iteration 23404, loss = 0.00977586\n",
      "Iteration 23405, loss = 0.00977547\n",
      "Iteration 23406, loss = 0.00977507\n",
      "Iteration 23407, loss = 0.00977468\n",
      "Iteration 23408, loss = 0.00977429\n",
      "Iteration 23409, loss = 0.00977390\n",
      "Iteration 23410, loss = 0.00977350\n",
      "Iteration 23411, loss = 0.00977311\n",
      "Iteration 23412, loss = 0.00977272\n",
      "Iteration 23413, loss = 0.00977233\n",
      "Iteration 23414, loss = 0.00977194\n",
      "Iteration 23415, loss = 0.00977155\n",
      "Iteration 23416, loss = 0.00977115\n",
      "Iteration 23417, loss = 0.00977076\n",
      "Iteration 23418, loss = 0.00977037\n",
      "Iteration 23419, loss = 0.00976998\n",
      "Iteration 23420, loss = 0.00976959\n",
      "Iteration 23421, loss = 0.00976920\n",
      "Iteration 23422, loss = 0.00976880\n",
      "Iteration 23423, loss = 0.00976841\n",
      "Iteration 23424, loss = 0.00976802\n",
      "Iteration 23425, loss = 0.00976763\n",
      "Iteration 23426, loss = 0.00976724\n",
      "Iteration 23427, loss = 0.00976685\n",
      "Iteration 23428, loss = 0.00976646\n",
      "Iteration 23429, loss = 0.00976607\n",
      "Iteration 23430, loss = 0.00976567\n",
      "Iteration 23431, loss = 0.00976528\n",
      "Iteration 23432, loss = 0.00976489\n",
      "Iteration 23433, loss = 0.00976450\n",
      "Iteration 23434, loss = 0.00976411\n",
      "Iteration 23435, loss = 0.00976372\n",
      "Iteration 23436, loss = 0.00976333\n",
      "Iteration 23437, loss = 0.00976294\n",
      "Iteration 23438, loss = 0.00976255\n",
      "Iteration 23439, loss = 0.00976216\n",
      "Iteration 23440, loss = 0.00976177\n",
      "Iteration 23441, loss = 0.00976138\n",
      "Iteration 23442, loss = 0.00976099\n",
      "Iteration 23443, loss = 0.00976060\n",
      "Iteration 23444, loss = 0.00976021\n",
      "Iteration 23445, loss = 0.00975981\n",
      "Iteration 23446, loss = 0.00975942\n",
      "Iteration 23447, loss = 0.00975903\n",
      "Iteration 23448, loss = 0.00975864\n",
      "Iteration 23449, loss = 0.00975825\n",
      "Iteration 23450, loss = 0.00975786\n",
      "Iteration 23451, loss = 0.00975747\n",
      "Iteration 23452, loss = 0.00975708\n",
      "Iteration 23453, loss = 0.00975669\n",
      "Iteration 23454, loss = 0.00975630\n",
      "Iteration 23455, loss = 0.00975591\n",
      "Iteration 23456, loss = 0.00975552\n",
      "Iteration 23457, loss = 0.00975513\n",
      "Iteration 23458, loss = 0.00975474\n",
      "Iteration 23459, loss = 0.00975435\n",
      "Iteration 23460, loss = 0.00975397\n",
      "Iteration 23461, loss = 0.00975358\n",
      "Iteration 23462, loss = 0.00975319\n",
      "Iteration 23463, loss = 0.00975280\n",
      "Iteration 23464, loss = 0.00975241\n",
      "Iteration 23465, loss = 0.00975202\n",
      "Iteration 23466, loss = 0.00975163\n",
      "Iteration 23467, loss = 0.00975124\n",
      "Iteration 23468, loss = 0.00975085\n",
      "Iteration 23469, loss = 0.00975046\n",
      "Iteration 23470, loss = 0.00975007\n",
      "Iteration 23471, loss = 0.00974968\n",
      "Iteration 23472, loss = 0.00974929\n",
      "Iteration 23473, loss = 0.00974890\n",
      "Iteration 23474, loss = 0.00974852\n",
      "Iteration 23475, loss = 0.00974813\n",
      "Iteration 23476, loss = 0.00974774\n",
      "Iteration 23477, loss = 0.00974735\n",
      "Iteration 23478, loss = 0.00974696\n",
      "Iteration 23479, loss = 0.00974657\n",
      "Iteration 23480, loss = 0.00974618\n",
      "Iteration 23481, loss = 0.00974579\n",
      "Iteration 23482, loss = 0.00974540\n",
      "Iteration 23483, loss = 0.00974502\n",
      "Iteration 23484, loss = 0.00974463\n",
      "Iteration 23485, loss = 0.00974424\n",
      "Iteration 23486, loss = 0.00974385\n",
      "Iteration 23487, loss = 0.00974346\n",
      "Iteration 23488, loss = 0.00974307\n",
      "Iteration 23489, loss = 0.00974269\n",
      "Iteration 23490, loss = 0.00974230\n",
      "Iteration 23491, loss = 0.00974191\n",
      "Iteration 23492, loss = 0.00974152\n",
      "Iteration 23493, loss = 0.00974113\n",
      "Iteration 23494, loss = 0.00974075\n",
      "Iteration 23495, loss = 0.00974036\n",
      "Iteration 23496, loss = 0.00973997\n",
      "Iteration 23497, loss = 0.00973958\n",
      "Iteration 23498, loss = 0.00973919\n",
      "Iteration 23499, loss = 0.00973881\n",
      "Iteration 23500, loss = 0.00973842\n",
      "Iteration 23501, loss = 0.00973803\n",
      "Iteration 23502, loss = 0.00973764\n",
      "Iteration 23503, loss = 0.00973725\n",
      "Iteration 23504, loss = 0.00973687\n",
      "Iteration 23505, loss = 0.00973648\n",
      "Iteration 23506, loss = 0.00973609\n",
      "Iteration 23507, loss = 0.00973570\n",
      "Iteration 23508, loss = 0.00973532\n",
      "Iteration 23509, loss = 0.00973493\n",
      "Iteration 23510, loss = 0.00973454\n",
      "Iteration 23511, loss = 0.00973416\n",
      "Iteration 23512, loss = 0.00973377\n",
      "Iteration 23513, loss = 0.00973338\n",
      "Iteration 23514, loss = 0.00973299\n",
      "Iteration 23515, loss = 0.00973261\n",
      "Iteration 23516, loss = 0.00973222\n",
      "Iteration 23517, loss = 0.00973183\n",
      "Iteration 23518, loss = 0.00973145\n",
      "Iteration 23519, loss = 0.00973106\n",
      "Iteration 23520, loss = 0.00973067\n",
      "Iteration 23521, loss = 0.00973029\n",
      "Iteration 23522, loss = 0.00972990\n",
      "Iteration 23523, loss = 0.00972951\n",
      "Iteration 23524, loss = 0.00972912\n",
      "Iteration 23525, loss = 0.00972874\n",
      "Iteration 23526, loss = 0.00972835\n",
      "Iteration 23527, loss = 0.00972797\n",
      "Iteration 23528, loss = 0.00972758\n",
      "Iteration 23529, loss = 0.00972719\n",
      "Iteration 23530, loss = 0.00972681\n",
      "Iteration 23531, loss = 0.00972642\n",
      "Iteration 23532, loss = 0.00972603\n",
      "Iteration 23533, loss = 0.00972565\n",
      "Iteration 23534, loss = 0.00972526\n",
      "Iteration 23535, loss = 0.00972487\n",
      "Iteration 23536, loss = 0.00972449\n",
      "Iteration 23537, loss = 0.00972410\n",
      "Iteration 23538, loss = 0.00972372\n",
      "Iteration 23539, loss = 0.00972333\n",
      "Iteration 23540, loss = 0.00972294\n",
      "Iteration 23541, loss = 0.00972256\n",
      "Iteration 23542, loss = 0.00972217\n",
      "Iteration 23543, loss = 0.00972179\n",
      "Iteration 23544, loss = 0.00972140\n",
      "Iteration 23545, loss = 0.00972102\n",
      "Iteration 23546, loss = 0.00972063\n",
      "Iteration 23547, loss = 0.00972024\n",
      "Iteration 23548, loss = 0.00971986\n",
      "Iteration 23549, loss = 0.00971947\n",
      "Iteration 23550, loss = 0.00971909\n",
      "Iteration 23551, loss = 0.00971870\n",
      "Iteration 23552, loss = 0.00971832\n",
      "Iteration 23553, loss = 0.00971793\n",
      "Iteration 23554, loss = 0.00971755\n",
      "Iteration 23555, loss = 0.00971716\n",
      "Iteration 23556, loss = 0.00971678\n",
      "Iteration 23557, loss = 0.00971639\n",
      "Iteration 23558, loss = 0.00971601\n",
      "Iteration 23559, loss = 0.00971562\n",
      "Iteration 23560, loss = 0.00971524\n",
      "Iteration 23561, loss = 0.00971485\n",
      "Iteration 23562, loss = 0.00971447\n",
      "Iteration 23563, loss = 0.00971408\n",
      "Iteration 23564, loss = 0.00971370\n",
      "Iteration 23565, loss = 0.00971331\n",
      "Iteration 23566, loss = 0.00971293\n",
      "Iteration 23567, loss = 0.00971254\n",
      "Iteration 23568, loss = 0.00971216\n",
      "Iteration 23569, loss = 0.00971177\n",
      "Iteration 23570, loss = 0.00971139\n",
      "Iteration 23571, loss = 0.00971100\n",
      "Iteration 23572, loss = 0.00971062\n",
      "Iteration 23573, loss = 0.00971023\n",
      "Iteration 23574, loss = 0.00970985\n",
      "Iteration 23575, loss = 0.00970947\n",
      "Iteration 23576, loss = 0.00970908\n",
      "Iteration 23577, loss = 0.00970870\n",
      "Iteration 23578, loss = 0.00970831\n",
      "Iteration 23579, loss = 0.00970793\n",
      "Iteration 23580, loss = 0.00970754\n",
      "Iteration 23581, loss = 0.00970716\n",
      "Iteration 23582, loss = 0.00970678\n",
      "Iteration 23583, loss = 0.00970639\n",
      "Iteration 23584, loss = 0.00970601\n",
      "Iteration 23585, loss = 0.00970562\n",
      "Iteration 23586, loss = 0.00970524\n",
      "Iteration 23587, loss = 0.00970486\n",
      "Iteration 23588, loss = 0.00970447\n",
      "Iteration 23589, loss = 0.00970409\n",
      "Iteration 23590, loss = 0.00970371\n",
      "Iteration 23591, loss = 0.00970332\n",
      "Iteration 23592, loss = 0.00970294\n",
      "Iteration 23593, loss = 0.00970256\n",
      "Iteration 23594, loss = 0.00970217\n",
      "Iteration 23595, loss = 0.00970179\n",
      "Iteration 23596, loss = 0.00970141\n",
      "Iteration 23597, loss = 0.00970102\n",
      "Iteration 23598, loss = 0.00970064\n",
      "Iteration 23599, loss = 0.00970026\n",
      "Iteration 23600, loss = 0.00969987\n",
      "Iteration 23601, loss = 0.00969949\n",
      "Iteration 23602, loss = 0.00969911\n",
      "Iteration 23603, loss = 0.00969872\n",
      "Iteration 23604, loss = 0.00969834\n",
      "Iteration 23605, loss = 0.00969796\n",
      "Iteration 23606, loss = 0.00969757\n",
      "Iteration 23607, loss = 0.00969719\n",
      "Iteration 23608, loss = 0.00969681\n",
      "Iteration 23609, loss = 0.00969643\n",
      "Iteration 23610, loss = 0.00969604\n",
      "Iteration 23611, loss = 0.00969566\n",
      "Iteration 23612, loss = 0.00969528\n",
      "Iteration 23613, loss = 0.00969489\n",
      "Iteration 23614, loss = 0.00969451\n",
      "Iteration 23615, loss = 0.00969413\n",
      "Iteration 23616, loss = 0.00969375\n",
      "Iteration 23617, loss = 0.00969336\n",
      "Iteration 23618, loss = 0.00969298\n",
      "Iteration 23619, loss = 0.00969260\n",
      "Iteration 23620, loss = 0.00969222\n",
      "Iteration 23621, loss = 0.00969184\n",
      "Iteration 23622, loss = 0.00969145\n",
      "Iteration 23623, loss = 0.00969107\n",
      "Iteration 23624, loss = 0.00969069\n",
      "Iteration 23625, loss = 0.00969031\n",
      "Iteration 23626, loss = 0.00968993\n",
      "Iteration 23627, loss = 0.00968954\n",
      "Iteration 23628, loss = 0.00968916\n",
      "Iteration 23629, loss = 0.00968878\n",
      "Iteration 23630, loss = 0.00968840\n",
      "Iteration 23631, loss = 0.00968802\n",
      "Iteration 23632, loss = 0.00968763\n",
      "Iteration 23633, loss = 0.00968725\n",
      "Iteration 23634, loss = 0.00968687\n",
      "Iteration 23635, loss = 0.00968649\n",
      "Iteration 23636, loss = 0.00968611\n",
      "Iteration 23637, loss = 0.00968573\n",
      "Iteration 23638, loss = 0.00968534\n",
      "Iteration 23639, loss = 0.00968496\n",
      "Iteration 23640, loss = 0.00968458\n",
      "Iteration 23641, loss = 0.00968420\n",
      "Iteration 23642, loss = 0.00968382\n",
      "Iteration 23643, loss = 0.00968344\n",
      "Iteration 23644, loss = 0.00968306\n",
      "Iteration 23645, loss = 0.00968268\n",
      "Iteration 23646, loss = 0.00968229\n",
      "Iteration 23647, loss = 0.00968191\n",
      "Iteration 23648, loss = 0.00968153\n",
      "Iteration 23649, loss = 0.00968115\n",
      "Iteration 23650, loss = 0.00968077\n",
      "Iteration 23651, loss = 0.00968039\n",
      "Iteration 23652, loss = 0.00968001\n",
      "Iteration 23653, loss = 0.00967963\n",
      "Iteration 23654, loss = 0.00967925\n",
      "Iteration 23655, loss = 0.00967887\n",
      "Iteration 23656, loss = 0.00967849\n",
      "Iteration 23657, loss = 0.00967811\n",
      "Iteration 23658, loss = 0.00967772\n",
      "Iteration 23659, loss = 0.00967734\n",
      "Iteration 23660, loss = 0.00967696\n",
      "Iteration 23661, loss = 0.00967658\n",
      "Iteration 23662, loss = 0.00967620\n",
      "Iteration 23663, loss = 0.00967582\n",
      "Iteration 23664, loss = 0.00967544\n",
      "Iteration 23665, loss = 0.00967506\n",
      "Iteration 23666, loss = 0.00967468\n",
      "Iteration 23667, loss = 0.00967430\n",
      "Iteration 23668, loss = 0.00967392\n",
      "Iteration 23669, loss = 0.00967354\n",
      "Iteration 23670, loss = 0.00967316\n",
      "Iteration 23671, loss = 0.00967278\n",
      "Iteration 23672, loss = 0.00967240\n",
      "Iteration 23673, loss = 0.00967202\n",
      "Iteration 23674, loss = 0.00967164\n",
      "Iteration 23675, loss = 0.00967126\n",
      "Iteration 23676, loss = 0.00967088\n",
      "Iteration 23677, loss = 0.00967050\n",
      "Iteration 23678, loss = 0.00967012\n",
      "Iteration 23679, loss = 0.00966974\n",
      "Iteration 23680, loss = 0.00966936\n",
      "Iteration 23681, loss = 0.00966898\n",
      "Iteration 23682, loss = 0.00966860\n",
      "Iteration 23683, loss = 0.00966823\n",
      "Iteration 23684, loss = 0.00966785\n",
      "Iteration 23685, loss = 0.00966747\n",
      "Iteration 23686, loss = 0.00966709\n",
      "Iteration 23687, loss = 0.00966671\n",
      "Iteration 23688, loss = 0.00966633\n",
      "Iteration 23689, loss = 0.00966595\n",
      "Iteration 23690, loss = 0.00966557\n",
      "Iteration 23691, loss = 0.00966519\n",
      "Iteration 23692, loss = 0.00966481\n",
      "Iteration 23693, loss = 0.00966443\n",
      "Iteration 23694, loss = 0.00966405\n",
      "Iteration 23695, loss = 0.00966368\n",
      "Iteration 23696, loss = 0.00966330\n",
      "Iteration 23697, loss = 0.00966292\n",
      "Iteration 23698, loss = 0.00966254\n",
      "Iteration 23699, loss = 0.00966216\n",
      "Iteration 23700, loss = 0.00966178\n",
      "Iteration 23701, loss = 0.00966140\n",
      "Iteration 23702, loss = 0.00966102\n",
      "Iteration 23703, loss = 0.00966065\n",
      "Iteration 23704, loss = 0.00966027\n",
      "Iteration 23705, loss = 0.00965989\n",
      "Iteration 23706, loss = 0.00965951\n",
      "Iteration 23707, loss = 0.00965913\n",
      "Iteration 23708, loss = 0.00965875\n",
      "Iteration 23709, loss = 0.00965838\n",
      "Iteration 23710, loss = 0.00965800\n",
      "Iteration 23711, loss = 0.00965762\n",
      "Iteration 23712, loss = 0.00965724\n",
      "Iteration 23713, loss = 0.00965686\n",
      "Iteration 23714, loss = 0.00965649\n",
      "Iteration 23715, loss = 0.00965611\n",
      "Iteration 23716, loss = 0.00965573\n",
      "Iteration 23717, loss = 0.00965535\n",
      "Iteration 23718, loss = 0.00965497\n",
      "Iteration 23719, loss = 0.00965460\n",
      "Iteration 23720, loss = 0.00965422\n",
      "Iteration 23721, loss = 0.00965384\n",
      "Iteration 23722, loss = 0.00965346\n",
      "Iteration 23723, loss = 0.00965308\n",
      "Iteration 23724, loss = 0.00965271\n",
      "Iteration 23725, loss = 0.00965233\n",
      "Iteration 23726, loss = 0.00965195\n",
      "Iteration 23727, loss = 0.00965157\n",
      "Iteration 23728, loss = 0.00965120\n",
      "Iteration 23729, loss = 0.00965082\n",
      "Iteration 23730, loss = 0.00965044\n",
      "Iteration 23731, loss = 0.00965007\n",
      "Iteration 23732, loss = 0.00964969\n",
      "Iteration 23733, loss = 0.00964931\n",
      "Iteration 23734, loss = 0.00964893\n",
      "Iteration 23735, loss = 0.00964856\n",
      "Iteration 23736, loss = 0.00964818\n",
      "Iteration 23737, loss = 0.00964780\n",
      "Iteration 23738, loss = 0.00964743\n",
      "Iteration 23739, loss = 0.00964705\n",
      "Iteration 23740, loss = 0.00964667\n",
      "Iteration 23741, loss = 0.00964629\n",
      "Iteration 23742, loss = 0.00964592\n",
      "Iteration 23743, loss = 0.00964554\n",
      "Iteration 23744, loss = 0.00964516\n",
      "Iteration 23745, loss = 0.00964479\n",
      "Iteration 23746, loss = 0.00964441\n",
      "Iteration 23747, loss = 0.00964403\n",
      "Iteration 23748, loss = 0.00964366\n",
      "Iteration 23749, loss = 0.00964328\n",
      "Iteration 23750, loss = 0.00964291\n",
      "Iteration 23751, loss = 0.00964253\n",
      "Iteration 23752, loss = 0.00964215\n",
      "Iteration 23753, loss = 0.00964178\n",
      "Iteration 23754, loss = 0.00964140\n",
      "Iteration 23755, loss = 0.00964102\n",
      "Iteration 23756, loss = 0.00964065\n",
      "Iteration 23757, loss = 0.00964027\n",
      "Iteration 23758, loss = 0.00963990\n",
      "Iteration 23759, loss = 0.00963952\n",
      "Iteration 23760, loss = 0.00963914\n",
      "Iteration 23761, loss = 0.00963877\n",
      "Iteration 23762, loss = 0.00963839\n",
      "Iteration 23763, loss = 0.00963802\n",
      "Iteration 23764, loss = 0.00963764\n",
      "Iteration 23765, loss = 0.00963726\n",
      "Iteration 23766, loss = 0.00963689\n",
      "Iteration 23767, loss = 0.00963651\n",
      "Iteration 23768, loss = 0.00963614\n",
      "Iteration 23769, loss = 0.00963576\n",
      "Iteration 23770, loss = 0.00963539\n",
      "Iteration 23771, loss = 0.00963501\n",
      "Iteration 23772, loss = 0.00963463\n",
      "Iteration 23773, loss = 0.00963426\n",
      "Iteration 23774, loss = 0.00963388\n",
      "Iteration 23775, loss = 0.00963351\n",
      "Iteration 23776, loss = 0.00963313\n",
      "Iteration 23777, loss = 0.00963276\n",
      "Iteration 23778, loss = 0.00963238\n",
      "Iteration 23779, loss = 0.00963201\n",
      "Iteration 23780, loss = 0.00963163\n",
      "Iteration 23781, loss = 0.00963126\n",
      "Iteration 23782, loss = 0.00963088\n",
      "Iteration 23783, loss = 0.00963051\n",
      "Iteration 23784, loss = 0.00963013\n",
      "Iteration 23785, loss = 0.00962976\n",
      "Iteration 23786, loss = 0.00962938\n",
      "Iteration 23787, loss = 0.00962901\n",
      "Iteration 23788, loss = 0.00962863\n",
      "Iteration 23789, loss = 0.00962826\n",
      "Iteration 23790, loss = 0.00962788\n",
      "Iteration 23791, loss = 0.00962751\n",
      "Iteration 23792, loss = 0.00962713\n",
      "Iteration 23793, loss = 0.00962676\n",
      "Iteration 23794, loss = 0.00962639\n",
      "Iteration 23795, loss = 0.00962601\n",
      "Iteration 23796, loss = 0.00962564\n",
      "Iteration 23797, loss = 0.00962526\n",
      "Iteration 23798, loss = 0.00962489\n",
      "Iteration 23799, loss = 0.00962451\n",
      "Iteration 23800, loss = 0.00962414\n",
      "Iteration 23801, loss = 0.00962377\n",
      "Iteration 23802, loss = 0.00962339\n",
      "Iteration 23803, loss = 0.00962302\n",
      "Iteration 23804, loss = 0.00962264\n",
      "Iteration 23805, loss = 0.00962227\n",
      "Iteration 23806, loss = 0.00962190\n",
      "Iteration 23807, loss = 0.00962152\n",
      "Iteration 23808, loss = 0.00962115\n",
      "Iteration 23809, loss = 0.00962077\n",
      "Iteration 23810, loss = 0.00962040\n",
      "Iteration 23811, loss = 0.00962003\n",
      "Iteration 23812, loss = 0.00961965\n",
      "Iteration 23813, loss = 0.00961928\n",
      "Iteration 23814, loss = 0.00961890\n",
      "Iteration 23815, loss = 0.00961853\n",
      "Iteration 23816, loss = 0.00961816\n",
      "Iteration 23817, loss = 0.00961778\n",
      "Iteration 23818, loss = 0.00961741\n",
      "Iteration 23819, loss = 0.00961704\n",
      "Iteration 23820, loss = 0.00961666\n",
      "Iteration 23821, loss = 0.00961629\n",
      "Iteration 23822, loss = 0.00961592\n",
      "Iteration 23823, loss = 0.00961554\n",
      "Iteration 23824, loss = 0.00961517\n",
      "Iteration 23825, loss = 0.00961480\n",
      "Iteration 23826, loss = 0.00961442\n",
      "Iteration 23827, loss = 0.00961405\n",
      "Iteration 23828, loss = 0.00961368\n",
      "Iteration 23829, loss = 0.00961331\n",
      "Iteration 23830, loss = 0.00961293\n",
      "Iteration 23831, loss = 0.00961256\n",
      "Iteration 23832, loss = 0.00961219\n",
      "Iteration 23833, loss = 0.00961181\n",
      "Iteration 23834, loss = 0.00961144\n",
      "Iteration 23835, loss = 0.00961107\n",
      "Iteration 23836, loss = 0.00961070\n",
      "Iteration 23837, loss = 0.00961032\n",
      "Iteration 23838, loss = 0.00960995\n",
      "Iteration 23839, loss = 0.00960958\n",
      "Iteration 23840, loss = 0.00960921\n",
      "Iteration 23841, loss = 0.00960883\n",
      "Iteration 23842, loss = 0.00960846\n",
      "Iteration 23843, loss = 0.00960809\n",
      "Iteration 23844, loss = 0.00960772\n",
      "Iteration 23845, loss = 0.00960734\n",
      "Iteration 23846, loss = 0.00960697\n",
      "Iteration 23847, loss = 0.00960660\n",
      "Iteration 23848, loss = 0.00960623\n",
      "Iteration 23849, loss = 0.00960586\n",
      "Iteration 23850, loss = 0.00960548\n",
      "Iteration 23851, loss = 0.00960511\n",
      "Iteration 23852, loss = 0.00960474\n",
      "Iteration 23853, loss = 0.00960437\n",
      "Iteration 23854, loss = 0.00960400\n",
      "Iteration 23855, loss = 0.00960362\n",
      "Iteration 23856, loss = 0.00960325\n",
      "Iteration 23857, loss = 0.00960288\n",
      "Iteration 23858, loss = 0.00960251\n",
      "Iteration 23859, loss = 0.00960214\n",
      "Iteration 23860, loss = 0.00960177\n",
      "Iteration 23861, loss = 0.00960139\n",
      "Iteration 23862, loss = 0.00960102\n",
      "Iteration 23863, loss = 0.00960065\n",
      "Iteration 23864, loss = 0.00960028\n",
      "Iteration 23865, loss = 0.00959991\n",
      "Iteration 23866, loss = 0.00959954\n",
      "Iteration 23867, loss = 0.00959917\n",
      "Iteration 23868, loss = 0.00959879\n",
      "Iteration 23869, loss = 0.00959842\n",
      "Iteration 23870, loss = 0.00959805\n",
      "Iteration 23871, loss = 0.00959768\n",
      "Iteration 23872, loss = 0.00959731\n",
      "Iteration 23873, loss = 0.00959694\n",
      "Iteration 23874, loss = 0.00959657\n",
      "Iteration 23875, loss = 0.00959620\n",
      "Iteration 23876, loss = 0.00959583\n",
      "Iteration 23877, loss = 0.00959546\n",
      "Iteration 23878, loss = 0.00959508\n",
      "Iteration 23879, loss = 0.00959471\n",
      "Iteration 23880, loss = 0.00959434\n",
      "Iteration 23881, loss = 0.00959397\n",
      "Iteration 23882, loss = 0.00959360\n",
      "Iteration 23883, loss = 0.00959323\n",
      "Iteration 23884, loss = 0.00959286\n",
      "Iteration 23885, loss = 0.00959249\n",
      "Iteration 23886, loss = 0.00959212\n",
      "Iteration 23887, loss = 0.00959175\n",
      "Iteration 23888, loss = 0.00959138\n",
      "Iteration 23889, loss = 0.00959101\n",
      "Iteration 23890, loss = 0.00959064\n",
      "Iteration 23891, loss = 0.00959027\n",
      "Iteration 23892, loss = 0.00958990\n",
      "Iteration 23893, loss = 0.00958953\n",
      "Iteration 23894, loss = 0.00958916\n",
      "Iteration 23895, loss = 0.00958879\n",
      "Iteration 23896, loss = 0.00958842\n",
      "Iteration 23897, loss = 0.00958805\n",
      "Iteration 23898, loss = 0.00958768\n",
      "Iteration 23899, loss = 0.00958731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23900, loss = 0.00958694\n",
      "Iteration 23901, loss = 0.00958657\n",
      "Iteration 23902, loss = 0.00958620\n",
      "Iteration 23903, loss = 0.00958583\n",
      "Iteration 23904, loss = 0.00958546\n",
      "Iteration 23905, loss = 0.00958509\n",
      "Iteration 23906, loss = 0.00958472\n",
      "Iteration 23907, loss = 0.00958435\n",
      "Iteration 23908, loss = 0.00958398\n",
      "Iteration 23909, loss = 0.00958361\n",
      "Iteration 23910, loss = 0.00958324\n",
      "Iteration 23911, loss = 0.00958287\n",
      "Iteration 23912, loss = 0.00958250\n",
      "Iteration 23913, loss = 0.00958213\n",
      "Iteration 23914, loss = 0.00958176\n",
      "Iteration 23915, loss = 0.00958140\n",
      "Iteration 23916, loss = 0.00958103\n",
      "Iteration 23917, loss = 0.00958066\n",
      "Iteration 23918, loss = 0.00958029\n",
      "Iteration 23919, loss = 0.00957992\n",
      "Iteration 23920, loss = 0.00957955\n",
      "Iteration 23921, loss = 0.00957918\n",
      "Iteration 23922, loss = 0.00957881\n",
      "Iteration 23923, loss = 0.00957844\n",
      "Iteration 23924, loss = 0.00957807\n",
      "Iteration 23925, loss = 0.00957771\n",
      "Iteration 23926, loss = 0.00957734\n",
      "Iteration 23927, loss = 0.00957697\n",
      "Iteration 23928, loss = 0.00957660\n",
      "Iteration 23929, loss = 0.00957623\n",
      "Iteration 23930, loss = 0.00957586\n",
      "Iteration 23931, loss = 0.00957549\n",
      "Iteration 23932, loss = 0.00957513\n",
      "Iteration 23933, loss = 0.00957476\n",
      "Iteration 23934, loss = 0.00957439\n",
      "Iteration 23935, loss = 0.00957402\n",
      "Iteration 23936, loss = 0.00957365\n",
      "Iteration 23937, loss = 0.00957328\n",
      "Iteration 23938, loss = 0.00957292\n",
      "Iteration 23939, loss = 0.00957255\n",
      "Iteration 23940, loss = 0.00957218\n",
      "Iteration 23941, loss = 0.00957181\n",
      "Iteration 23942, loss = 0.00957144\n",
      "Iteration 23943, loss = 0.00957107\n",
      "Iteration 23944, loss = 0.00957071\n",
      "Iteration 23945, loss = 0.00957034\n",
      "Iteration 23946, loss = 0.00956997\n",
      "Iteration 23947, loss = 0.00956960\n",
      "Iteration 23948, loss = 0.00956924\n",
      "Iteration 23949, loss = 0.00956887\n",
      "Iteration 23950, loss = 0.00956850\n",
      "Iteration 23951, loss = 0.00956813\n",
      "Iteration 23952, loss = 0.00956776\n",
      "Iteration 23953, loss = 0.00956740\n",
      "Iteration 23954, loss = 0.00956703\n",
      "Iteration 23955, loss = 0.00956666\n",
      "Iteration 23956, loss = 0.00956629\n",
      "Iteration 23957, loss = 0.00956593\n",
      "Iteration 23958, loss = 0.00956556\n",
      "Iteration 23959, loss = 0.00956519\n",
      "Iteration 23960, loss = 0.00956483\n",
      "Iteration 23961, loss = 0.00956446\n",
      "Iteration 23962, loss = 0.00956409\n",
      "Iteration 23963, loss = 0.00956372\n",
      "Iteration 23964, loss = 0.00956336\n",
      "Iteration 23965, loss = 0.00956299\n",
      "Iteration 23966, loss = 0.00956262\n",
      "Iteration 23967, loss = 0.00956226\n",
      "Iteration 23968, loss = 0.00956189\n",
      "Iteration 23969, loss = 0.00956152\n",
      "Iteration 23970, loss = 0.00956116\n",
      "Iteration 23971, loss = 0.00956079\n",
      "Iteration 23972, loss = 0.00956042\n",
      "Iteration 23973, loss = 0.00956005\n",
      "Iteration 23974, loss = 0.00955969\n",
      "Iteration 23975, loss = 0.00955932\n",
      "Iteration 23976, loss = 0.00955896\n",
      "Iteration 23977, loss = 0.00955859\n",
      "Iteration 23978, loss = 0.00955822\n",
      "Iteration 23979, loss = 0.00955786\n",
      "Iteration 23980, loss = 0.00955749\n",
      "Iteration 23981, loss = 0.00955712\n",
      "Iteration 23982, loss = 0.00955676\n",
      "Iteration 23983, loss = 0.00955639\n",
      "Iteration 23984, loss = 0.00955602\n",
      "Iteration 23985, loss = 0.00955566\n",
      "Iteration 23986, loss = 0.00955529\n",
      "Iteration 23987, loss = 0.00955493\n",
      "Iteration 23988, loss = 0.00955456\n",
      "Iteration 23989, loss = 0.00955419\n",
      "Iteration 23990, loss = 0.00955383\n",
      "Iteration 23991, loss = 0.00955346\n",
      "Iteration 23992, loss = 0.00955310\n",
      "Iteration 23993, loss = 0.00955273\n",
      "Iteration 23994, loss = 0.00955236\n",
      "Iteration 23995, loss = 0.00955200\n",
      "Iteration 23996, loss = 0.00955163\n",
      "Iteration 23997, loss = 0.00955127\n",
      "Iteration 23998, loss = 0.00955090\n",
      "Iteration 23999, loss = 0.00955054\n",
      "Iteration 24000, loss = 0.00955017\n",
      "Iteration 24001, loss = 0.00954980\n",
      "Iteration 24002, loss = 0.00954944\n",
      "Iteration 24003, loss = 0.00954907\n",
      "Iteration 24004, loss = 0.00954871\n",
      "Iteration 24005, loss = 0.00954834\n",
      "Iteration 24006, loss = 0.00954798\n",
      "Iteration 24007, loss = 0.00954761\n",
      "Iteration 24008, loss = 0.00954725\n",
      "Iteration 24009, loss = 0.00954688\n",
      "Iteration 24010, loss = 0.00954652\n",
      "Iteration 24011, loss = 0.00954615\n",
      "Iteration 24012, loss = 0.00954579\n",
      "Iteration 24013, loss = 0.00954542\n",
      "Iteration 24014, loss = 0.00954506\n",
      "Iteration 24015, loss = 0.00954469\n",
      "Iteration 24016, loss = 0.00954433\n",
      "Iteration 24017, loss = 0.00954396\n",
      "Iteration 24018, loss = 0.00954360\n",
      "Iteration 24019, loss = 0.00954323\n",
      "Iteration 24020, loss = 0.00954287\n",
      "Iteration 24021, loss = 0.00954250\n",
      "Iteration 24022, loss = 0.00954214\n",
      "Iteration 24023, loss = 0.00954177\n",
      "Iteration 24024, loss = 0.00954141\n",
      "Iteration 24025, loss = 0.00954104\n",
      "Iteration 24026, loss = 0.00954068\n",
      "Iteration 24027, loss = 0.00954032\n",
      "Iteration 24028, loss = 0.00953995\n",
      "Iteration 24029, loss = 0.00953959\n",
      "Iteration 24030, loss = 0.00953922\n",
      "Iteration 24031, loss = 0.00953886\n",
      "Iteration 24032, loss = 0.00953849\n",
      "Iteration 24033, loss = 0.00953813\n",
      "Iteration 24034, loss = 0.00953777\n",
      "Iteration 24035, loss = 0.00953740\n",
      "Iteration 24036, loss = 0.00953704\n",
      "Iteration 24037, loss = 0.00953667\n",
      "Iteration 24038, loss = 0.00953631\n",
      "Iteration 24039, loss = 0.00953595\n",
      "Iteration 24040, loss = 0.00953558\n",
      "Iteration 24041, loss = 0.00953522\n",
      "Iteration 24042, loss = 0.00953485\n",
      "Iteration 24043, loss = 0.00953449\n",
      "Iteration 24044, loss = 0.00953413\n",
      "Iteration 24045, loss = 0.00953376\n",
      "Iteration 24046, loss = 0.00953340\n",
      "Iteration 24047, loss = 0.00953304\n",
      "Iteration 24048, loss = 0.00953267\n",
      "Iteration 24049, loss = 0.00953231\n",
      "Iteration 24050, loss = 0.00953195\n",
      "Iteration 24051, loss = 0.00953158\n",
      "Iteration 24052, loss = 0.00953122\n",
      "Iteration 24053, loss = 0.00953086\n",
      "Iteration 24054, loss = 0.00953049\n",
      "Iteration 24055, loss = 0.00953013\n",
      "Iteration 24056, loss = 0.00952977\n",
      "Iteration 24057, loss = 0.00952940\n",
      "Iteration 24058, loss = 0.00952904\n",
      "Iteration 24059, loss = 0.00952868\n",
      "Iteration 24060, loss = 0.00952831\n",
      "Iteration 24061, loss = 0.00952795\n",
      "Iteration 24062, loss = 0.00952759\n",
      "Iteration 24063, loss = 0.00952723\n",
      "Iteration 24064, loss = 0.00952686\n",
      "Iteration 24065, loss = 0.00952650\n",
      "Iteration 24066, loss = 0.00952614\n",
      "Iteration 24067, loss = 0.00952577\n",
      "Iteration 24068, loss = 0.00952541\n",
      "Iteration 24069, loss = 0.00952505\n",
      "Iteration 24070, loss = 0.00952469\n",
      "Iteration 24071, loss = 0.00952432\n",
      "Iteration 24072, loss = 0.00952396\n",
      "Iteration 24073, loss = 0.00952360\n",
      "Iteration 24074, loss = 0.00952324\n",
      "Iteration 24075, loss = 0.00952287\n",
      "Iteration 24076, loss = 0.00952251\n",
      "Iteration 24077, loss = 0.00952215\n",
      "Iteration 24078, loss = 0.00952179\n",
      "Iteration 24079, loss = 0.00952142\n",
      "Iteration 24080, loss = 0.00952106\n",
      "Iteration 24081, loss = 0.00952070\n",
      "Iteration 24082, loss = 0.00952034\n",
      "Iteration 24083, loss = 0.00951998\n",
      "Iteration 24084, loss = 0.00951961\n",
      "Iteration 24085, loss = 0.00951925\n",
      "Iteration 24086, loss = 0.00951889\n",
      "Iteration 24087, loss = 0.00951853\n",
      "Iteration 24088, loss = 0.00951817\n",
      "Iteration 24089, loss = 0.00951781\n",
      "Iteration 24090, loss = 0.00951744\n",
      "Iteration 24091, loss = 0.00951708\n",
      "Iteration 24092, loss = 0.00951672\n",
      "Iteration 24093, loss = 0.00951636\n",
      "Iteration 24094, loss = 0.00951600\n",
      "Iteration 24095, loss = 0.00951564\n",
      "Iteration 24096, loss = 0.00951527\n",
      "Iteration 24097, loss = 0.00951491\n",
      "Iteration 24098, loss = 0.00951455\n",
      "Iteration 24099, loss = 0.00951419\n",
      "Iteration 24100, loss = 0.00951383\n",
      "Iteration 24101, loss = 0.00951347\n",
      "Iteration 24102, loss = 0.00951311\n",
      "Iteration 24103, loss = 0.00951274\n",
      "Iteration 24104, loss = 0.00951238\n",
      "Iteration 24105, loss = 0.00951202\n",
      "Iteration 24106, loss = 0.00951166\n",
      "Iteration 24107, loss = 0.00951130\n",
      "Iteration 24108, loss = 0.00951094\n",
      "Iteration 24109, loss = 0.00951058\n",
      "Iteration 24110, loss = 0.00951022\n",
      "Iteration 24111, loss = 0.00950986\n",
      "Iteration 24112, loss = 0.00950950\n",
      "Iteration 24113, loss = 0.00950913\n",
      "Iteration 24114, loss = 0.00950877\n",
      "Iteration 24115, loss = 0.00950841\n",
      "Iteration 24116, loss = 0.00950805\n",
      "Iteration 24117, loss = 0.00950769\n",
      "Iteration 24118, loss = 0.00950733\n",
      "Iteration 24119, loss = 0.00950697\n",
      "Iteration 24120, loss = 0.00950661\n",
      "Iteration 24121, loss = 0.00950625\n",
      "Iteration 24122, loss = 0.00950589\n",
      "Iteration 24123, loss = 0.00950553\n",
      "Iteration 24124, loss = 0.00950517\n",
      "Iteration 24125, loss = 0.00950481\n",
      "Iteration 24126, loss = 0.00950445\n",
      "Iteration 24127, loss = 0.00950409\n",
      "Iteration 24128, loss = 0.00950373\n",
      "Iteration 24129, loss = 0.00950337\n",
      "Iteration 24130, loss = 0.00950301\n",
      "Iteration 24131, loss = 0.00950265\n",
      "Iteration 24132, loss = 0.00950229\n",
      "Iteration 24133, loss = 0.00950193\n",
      "Iteration 24134, loss = 0.00950157\n",
      "Iteration 24135, loss = 0.00950121\n",
      "Iteration 24136, loss = 0.00950085\n",
      "Iteration 24137, loss = 0.00950049\n",
      "Iteration 24138, loss = 0.00950013\n",
      "Iteration 24139, loss = 0.00949977\n",
      "Iteration 24140, loss = 0.00949941\n",
      "Iteration 24141, loss = 0.00949905\n",
      "Iteration 24142, loss = 0.00949869\n",
      "Iteration 24143, loss = 0.00949833\n",
      "Iteration 24144, loss = 0.00949797\n",
      "Iteration 24145, loss = 0.00949761\n",
      "Iteration 24146, loss = 0.00949725\n",
      "Iteration 24147, loss = 0.00949689\n",
      "Iteration 24148, loss = 0.00949653\n",
      "Iteration 24149, loss = 0.00949617\n",
      "Iteration 24150, loss = 0.00949582\n",
      "Iteration 24151, loss = 0.00949546\n",
      "Iteration 24152, loss = 0.00949510\n",
      "Iteration 24153, loss = 0.00949474\n",
      "Iteration 24154, loss = 0.00949438\n",
      "Iteration 24155, loss = 0.00949402\n",
      "Iteration 24156, loss = 0.00949366\n",
      "Iteration 24157, loss = 0.00949330\n",
      "Iteration 24158, loss = 0.00949294\n",
      "Iteration 24159, loss = 0.00949258\n",
      "Iteration 24160, loss = 0.00949223\n",
      "Iteration 24161, loss = 0.00949187\n",
      "Iteration 24162, loss = 0.00949151\n",
      "Iteration 24163, loss = 0.00949115\n",
      "Iteration 24164, loss = 0.00949079\n",
      "Iteration 24165, loss = 0.00949043\n",
      "Iteration 24166, loss = 0.00949007\n",
      "Iteration 24167, loss = 0.00948971\n",
      "Iteration 24168, loss = 0.00948936\n",
      "Iteration 24169, loss = 0.00948900\n",
      "Iteration 24170, loss = 0.00948864\n",
      "Iteration 24171, loss = 0.00948828\n",
      "Iteration 24172, loss = 0.00948792\n",
      "Iteration 24173, loss = 0.00948756\n",
      "Iteration 24174, loss = 0.00948721\n",
      "Iteration 24175, loss = 0.00948685\n",
      "Iteration 24176, loss = 0.00948649\n",
      "Iteration 24177, loss = 0.00948613\n",
      "Iteration 24178, loss = 0.00948577\n",
      "Iteration 24179, loss = 0.00948542\n",
      "Iteration 24180, loss = 0.00948506\n",
      "Iteration 24181, loss = 0.00948470\n",
      "Iteration 24182, loss = 0.00948434\n",
      "Iteration 24183, loss = 0.00948398\n",
      "Iteration 24184, loss = 0.00948363\n",
      "Iteration 24185, loss = 0.00948327\n",
      "Iteration 24186, loss = 0.00948291\n",
      "Iteration 24187, loss = 0.00948255\n",
      "Iteration 24188, loss = 0.00948220\n",
      "Iteration 24189, loss = 0.00948184\n",
      "Iteration 24190, loss = 0.00948148\n",
      "Iteration 24191, loss = 0.00948112\n",
      "Iteration 24192, loss = 0.00948076\n",
      "Iteration 24193, loss = 0.00948041\n",
      "Iteration 24194, loss = 0.00948005\n",
      "Iteration 24195, loss = 0.00947969\n",
      "Iteration 24196, loss = 0.00947934\n",
      "Iteration 24197, loss = 0.00947898\n",
      "Iteration 24198, loss = 0.00947862\n",
      "Iteration 24199, loss = 0.00947826\n",
      "Iteration 24200, loss = 0.00947791\n",
      "Iteration 24201, loss = 0.00947755\n",
      "Iteration 24202, loss = 0.00947719\n",
      "Iteration 24203, loss = 0.00947684\n",
      "Iteration 24204, loss = 0.00947648\n",
      "Iteration 24205, loss = 0.00947612\n",
      "Iteration 24206, loss = 0.00947576\n",
      "Iteration 24207, loss = 0.00947541\n",
      "Iteration 24208, loss = 0.00947505\n",
      "Iteration 24209, loss = 0.00947469\n",
      "Iteration 24210, loss = 0.00947434\n",
      "Iteration 24211, loss = 0.00947398\n",
      "Iteration 24212, loss = 0.00947362\n",
      "Iteration 24213, loss = 0.00947327\n",
      "Iteration 24214, loss = 0.00947291\n",
      "Iteration 24215, loss = 0.00947255\n",
      "Iteration 24216, loss = 0.00947220\n",
      "Iteration 24217, loss = 0.00947184\n",
      "Iteration 24218, loss = 0.00947148\n",
      "Iteration 24219, loss = 0.00947113\n",
      "Iteration 24220, loss = 0.00947077\n",
      "Iteration 24221, loss = 0.00947042\n",
      "Iteration 24222, loss = 0.00947006\n",
      "Iteration 24223, loss = 0.00946970\n",
      "Iteration 24224, loss = 0.00946935\n",
      "Iteration 24225, loss = 0.00946899\n",
      "Iteration 24226, loss = 0.00946863\n",
      "Iteration 24227, loss = 0.00946828\n",
      "Iteration 24228, loss = 0.00946792\n",
      "Iteration 24229, loss = 0.00946757\n",
      "Iteration 24230, loss = 0.00946721\n",
      "Iteration 24231, loss = 0.00946685\n",
      "Iteration 24232, loss = 0.00946650\n",
      "Iteration 24233, loss = 0.00946614\n",
      "Iteration 24234, loss = 0.00946579\n",
      "Iteration 24235, loss = 0.00946543\n",
      "Iteration 24236, loss = 0.00946508\n",
      "Iteration 24237, loss = 0.00946472\n",
      "Iteration 24238, loss = 0.00946436\n",
      "Iteration 24239, loss = 0.00946401\n",
      "Iteration 24240, loss = 0.00946365\n",
      "Iteration 24241, loss = 0.00946330\n",
      "Iteration 24242, loss = 0.00946294\n",
      "Iteration 24243, loss = 0.00946259\n",
      "Iteration 24244, loss = 0.00946223\n",
      "Iteration 24245, loss = 0.00946188\n",
      "Iteration 24246, loss = 0.00946152\n",
      "Iteration 24247, loss = 0.00946117\n",
      "Iteration 24248, loss = 0.00946081\n",
      "Iteration 24249, loss = 0.00946046\n",
      "Iteration 24250, loss = 0.00946010\n",
      "Iteration 24251, loss = 0.00945975\n",
      "Iteration 24252, loss = 0.00945939\n",
      "Iteration 24253, loss = 0.00945904\n",
      "Iteration 24254, loss = 0.00945868\n",
      "Iteration 24255, loss = 0.00945833\n",
      "Iteration 24256, loss = 0.00945797\n",
      "Iteration 24257, loss = 0.00945762\n",
      "Iteration 24258, loss = 0.00945726\n",
      "Iteration 24259, loss = 0.00945691\n",
      "Iteration 24260, loss = 0.00945655\n",
      "Iteration 24261, loss = 0.00945620\n",
      "Iteration 24262, loss = 0.00945584\n",
      "Iteration 24263, loss = 0.00945549\n",
      "Iteration 24264, loss = 0.00945513\n",
      "Iteration 24265, loss = 0.00945478\n",
      "Iteration 24266, loss = 0.00945442\n",
      "Iteration 24267, loss = 0.00945407\n",
      "Iteration 24268, loss = 0.00945372\n",
      "Iteration 24269, loss = 0.00945336\n",
      "Iteration 24270, loss = 0.00945301\n",
      "Iteration 24271, loss = 0.00945265\n",
      "Iteration 24272, loss = 0.00945230\n",
      "Iteration 24273, loss = 0.00945194\n",
      "Iteration 24274, loss = 0.00945159\n",
      "Iteration 24275, loss = 0.00945124\n",
      "Iteration 24276, loss = 0.00945088\n",
      "Iteration 24277, loss = 0.00945053\n",
      "Iteration 24278, loss = 0.00945017\n",
      "Iteration 24279, loss = 0.00944982\n",
      "Iteration 24280, loss = 0.00944947\n",
      "Iteration 24281, loss = 0.00944911\n",
      "Iteration 24282, loss = 0.00944876\n",
      "Iteration 24283, loss = 0.00944841\n",
      "Iteration 24284, loss = 0.00944805\n",
      "Iteration 24285, loss = 0.00944770\n",
      "Iteration 24286, loss = 0.00944734\n",
      "Iteration 24287, loss = 0.00944699\n",
      "Iteration 24288, loss = 0.00944664\n",
      "Iteration 24289, loss = 0.00944628\n",
      "Iteration 24290, loss = 0.00944593\n",
      "Iteration 24291, loss = 0.00944558\n",
      "Iteration 24292, loss = 0.00944522\n",
      "Iteration 24293, loss = 0.00944487\n",
      "Iteration 24294, loss = 0.00944452\n",
      "Iteration 24295, loss = 0.00944416\n",
      "Iteration 24296, loss = 0.00944381\n",
      "Iteration 24297, loss = 0.00944346\n",
      "Iteration 24298, loss = 0.00944310\n",
      "Iteration 24299, loss = 0.00944275\n",
      "Iteration 24300, loss = 0.00944240\n",
      "Iteration 24301, loss = 0.00944204\n",
      "Iteration 24302, loss = 0.00944169\n",
      "Iteration 24303, loss = 0.00944134\n",
      "Iteration 24304, loss = 0.00944099\n",
      "Iteration 24305, loss = 0.00944063\n",
      "Iteration 24306, loss = 0.00944028\n",
      "Iteration 24307, loss = 0.00943993\n",
      "Iteration 24308, loss = 0.00943957\n",
      "Iteration 24309, loss = 0.00943922\n",
      "Iteration 24310, loss = 0.00943887\n",
      "Iteration 24311, loss = 0.00943852\n",
      "Iteration 24312, loss = 0.00943816\n",
      "Iteration 24313, loss = 0.00943781\n",
      "Iteration 24314, loss = 0.00943746\n",
      "Iteration 24315, loss = 0.00943711\n",
      "Iteration 24316, loss = 0.00943675\n",
      "Iteration 24317, loss = 0.00943640\n",
      "Iteration 24318, loss = 0.00943605\n",
      "Iteration 24319, loss = 0.00943570\n",
      "Iteration 24320, loss = 0.00943534\n",
      "Iteration 24321, loss = 0.00943499\n",
      "Iteration 24322, loss = 0.00943464\n",
      "Iteration 24323, loss = 0.00943429\n",
      "Iteration 24324, loss = 0.00943394\n",
      "Iteration 24325, loss = 0.00943358\n",
      "Iteration 24326, loss = 0.00943323\n",
      "Iteration 24327, loss = 0.00943288\n",
      "Iteration 24328, loss = 0.00943253\n",
      "Iteration 24329, loss = 0.00943218\n",
      "Iteration 24330, loss = 0.00943182\n",
      "Iteration 24331, loss = 0.00943147\n",
      "Iteration 24332, loss = 0.00943112\n",
      "Iteration 24333, loss = 0.00943077\n",
      "Iteration 24334, loss = 0.00943042\n",
      "Iteration 24335, loss = 0.00943007\n",
      "Iteration 24336, loss = 0.00942971\n",
      "Iteration 24337, loss = 0.00942936\n",
      "Iteration 24338, loss = 0.00942901\n",
      "Iteration 24339, loss = 0.00942866\n",
      "Iteration 24340, loss = 0.00942831\n",
      "Iteration 24341, loss = 0.00942796\n",
      "Iteration 24342, loss = 0.00942761\n",
      "Iteration 24343, loss = 0.00942725\n",
      "Iteration 24344, loss = 0.00942690\n",
      "Iteration 24345, loss = 0.00942655\n",
      "Iteration 24346, loss = 0.00942620\n",
      "Iteration 24347, loss = 0.00942585\n",
      "Iteration 24348, loss = 0.00942550\n",
      "Iteration 24349, loss = 0.00942515\n",
      "Iteration 24350, loss = 0.00942480\n",
      "Iteration 24351, loss = 0.00942444\n",
      "Iteration 24352, loss = 0.00942409\n",
      "Iteration 24353, loss = 0.00942374\n",
      "Iteration 24354, loss = 0.00942339\n",
      "Iteration 24355, loss = 0.00942304\n",
      "Iteration 24356, loss = 0.00942269\n",
      "Iteration 24357, loss = 0.00942234\n",
      "Iteration 24358, loss = 0.00942199\n",
      "Iteration 24359, loss = 0.00942164\n",
      "Iteration 24360, loss = 0.00942129\n",
      "Iteration 24361, loss = 0.00942094\n",
      "Iteration 24362, loss = 0.00942059\n",
      "Iteration 24363, loss = 0.00942024\n",
      "Iteration 24364, loss = 0.00941989\n",
      "Iteration 24365, loss = 0.00941953\n",
      "Iteration 24366, loss = 0.00941918\n",
      "Iteration 24367, loss = 0.00941883\n",
      "Iteration 24368, loss = 0.00941848\n",
      "Iteration 24369, loss = 0.00941813\n",
      "Iteration 24370, loss = 0.00941778\n",
      "Iteration 24371, loss = 0.00941743\n",
      "Iteration 24372, loss = 0.00941708\n",
      "Iteration 24373, loss = 0.00941673\n",
      "Iteration 24374, loss = 0.00941638\n",
      "Iteration 24375, loss = 0.00941603\n",
      "Iteration 24376, loss = 0.00941568\n",
      "Iteration 24377, loss = 0.00941533\n",
      "Iteration 24378, loss = 0.00941498\n",
      "Iteration 24379, loss = 0.00941463\n",
      "Iteration 24380, loss = 0.00941428\n",
      "Iteration 24381, loss = 0.00941393\n",
      "Iteration 24382, loss = 0.00941358\n",
      "Iteration 24383, loss = 0.00941323\n",
      "Iteration 24384, loss = 0.00941288\n",
      "Iteration 24385, loss = 0.00941253\n",
      "Iteration 24386, loss = 0.00941218\n",
      "Iteration 24387, loss = 0.00941184\n",
      "Iteration 24388, loss = 0.00941149\n",
      "Iteration 24389, loss = 0.00941114\n",
      "Iteration 24390, loss = 0.00941079\n",
      "Iteration 24391, loss = 0.00941044\n",
      "Iteration 24392, loss = 0.00941009\n",
      "Iteration 24393, loss = 0.00940974\n",
      "Iteration 24394, loss = 0.00940939\n",
      "Iteration 24395, loss = 0.00940904\n",
      "Iteration 24396, loss = 0.00940869\n",
      "Iteration 24397, loss = 0.00940834\n",
      "Iteration 24398, loss = 0.00940799\n",
      "Iteration 24399, loss = 0.00940764\n",
      "Iteration 24400, loss = 0.00940729\n",
      "Iteration 24401, loss = 0.00940695\n",
      "Iteration 24402, loss = 0.00940660\n",
      "Iteration 24403, loss = 0.00940625\n",
      "Iteration 24404, loss = 0.00940590\n",
      "Iteration 24405, loss = 0.00940555\n",
      "Iteration 24406, loss = 0.00940520\n",
      "Iteration 24407, loss = 0.00940485\n",
      "Iteration 24408, loss = 0.00940450\n",
      "Iteration 24409, loss = 0.00940416\n",
      "Iteration 24410, loss = 0.00940381\n",
      "Iteration 24411, loss = 0.00940346\n",
      "Iteration 24412, loss = 0.00940311\n",
      "Iteration 24413, loss = 0.00940276\n",
      "Iteration 24414, loss = 0.00940241\n",
      "Iteration 24415, loss = 0.00940206\n",
      "Iteration 24416, loss = 0.00940172\n",
      "Iteration 24417, loss = 0.00940137\n",
      "Iteration 24418, loss = 0.00940102\n",
      "Iteration 24419, loss = 0.00940067\n",
      "Iteration 24420, loss = 0.00940032\n",
      "Iteration 24421, loss = 0.00939997\n",
      "Iteration 24422, loss = 0.00939963\n",
      "Iteration 24423, loss = 0.00939928\n",
      "Iteration 24424, loss = 0.00939893\n",
      "Iteration 24425, loss = 0.00939858\n",
      "Iteration 24426, loss = 0.00939823\n",
      "Iteration 24427, loss = 0.00939789\n",
      "Iteration 24428, loss = 0.00939754\n",
      "Iteration 24429, loss = 0.00939719\n",
      "Iteration 24430, loss = 0.00939684\n",
      "Iteration 24431, loss = 0.00939649\n",
      "Iteration 24432, loss = 0.00939615\n",
      "Iteration 24433, loss = 0.00939580\n",
      "Iteration 24434, loss = 0.00939545\n",
      "Iteration 24435, loss = 0.00939510\n",
      "Iteration 24436, loss = 0.00939476\n",
      "Iteration 24437, loss = 0.00939441\n",
      "Iteration 24438, loss = 0.00939406\n",
      "Iteration 24439, loss = 0.00939371\n",
      "Iteration 24440, loss = 0.00939337\n",
      "Iteration 24441, loss = 0.00939302\n",
      "Iteration 24442, loss = 0.00939267\n",
      "Iteration 24443, loss = 0.00939232\n",
      "Iteration 24444, loss = 0.00939198\n",
      "Iteration 24445, loss = 0.00939163\n",
      "Iteration 24446, loss = 0.00939128\n",
      "Iteration 24447, loss = 0.00939094\n",
      "Iteration 24448, loss = 0.00939059\n",
      "Iteration 24449, loss = 0.00939024\n",
      "Iteration 24450, loss = 0.00938989\n",
      "Iteration 24451, loss = 0.00938955\n",
      "Iteration 24452, loss = 0.00938920\n",
      "Iteration 24453, loss = 0.00938885\n",
      "Iteration 24454, loss = 0.00938851\n",
      "Iteration 24455, loss = 0.00938816\n",
      "Iteration 24456, loss = 0.00938781\n",
      "Iteration 24457, loss = 0.00938747\n",
      "Iteration 24458, loss = 0.00938712\n",
      "Iteration 24459, loss = 0.00938677\n",
      "Iteration 24460, loss = 0.00938643\n",
      "Iteration 24461, loss = 0.00938608\n",
      "Iteration 24462, loss = 0.00938573\n",
      "Iteration 24463, loss = 0.00938539\n",
      "Iteration 24464, loss = 0.00938504\n",
      "Iteration 24465, loss = 0.00938469\n",
      "Iteration 24466, loss = 0.00938435\n",
      "Iteration 24467, loss = 0.00938400\n",
      "Iteration 24468, loss = 0.00938365\n",
      "Iteration 24469, loss = 0.00938331\n",
      "Iteration 24470, loss = 0.00938296\n",
      "Iteration 24471, loss = 0.00938262\n",
      "Iteration 24472, loss = 0.00938227\n",
      "Iteration 24473, loss = 0.00938192\n",
      "Iteration 24474, loss = 0.00938158\n",
      "Iteration 24475, loss = 0.00938123\n",
      "Iteration 24476, loss = 0.00938088\n",
      "Iteration 24477, loss = 0.00938054\n",
      "Iteration 24478, loss = 0.00938019\n",
      "Iteration 24479, loss = 0.00937985\n",
      "Iteration 24480, loss = 0.00937950\n",
      "Iteration 24481, loss = 0.00937916\n",
      "Iteration 24482, loss = 0.00937881\n",
      "Iteration 24483, loss = 0.00937846\n",
      "Iteration 24484, loss = 0.00937812\n",
      "Iteration 24485, loss = 0.00937777\n",
      "Iteration 24486, loss = 0.00937743\n",
      "Iteration 24487, loss = 0.00937708\n",
      "Iteration 24488, loss = 0.00937674\n",
      "Iteration 24489, loss = 0.00937639\n",
      "Iteration 24490, loss = 0.00937604\n",
      "Iteration 24491, loss = 0.00937570\n",
      "Iteration 24492, loss = 0.00937535\n",
      "Iteration 24493, loss = 0.00937501\n",
      "Iteration 24494, loss = 0.00937466\n",
      "Iteration 24495, loss = 0.00937432\n",
      "Iteration 24496, loss = 0.00937397\n",
      "Iteration 24497, loss = 0.00937363\n",
      "Iteration 24498, loss = 0.00937328\n",
      "Iteration 24499, loss = 0.00937294\n",
      "Iteration 24500, loss = 0.00937259\n",
      "Iteration 24501, loss = 0.00937225\n",
      "Iteration 24502, loss = 0.00937190\n",
      "Iteration 24503, loss = 0.00937156\n",
      "Iteration 24504, loss = 0.00937121\n",
      "Iteration 24505, loss = 0.00937087\n",
      "Iteration 24506, loss = 0.00937052\n",
      "Iteration 24507, loss = 0.00937018\n",
      "Iteration 24508, loss = 0.00936983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24509, loss = 0.00936949\n",
      "Iteration 24510, loss = 0.00936914\n",
      "Iteration 24511, loss = 0.00936880\n",
      "Iteration 24512, loss = 0.00936845\n",
      "Iteration 24513, loss = 0.00936811\n",
      "Iteration 24514, loss = 0.00936777\n",
      "Iteration 24515, loss = 0.00936742\n",
      "Iteration 24516, loss = 0.00936708\n",
      "Iteration 24517, loss = 0.00936673\n",
      "Iteration 24518, loss = 0.00936639\n",
      "Iteration 24519, loss = 0.00936604\n",
      "Iteration 24520, loss = 0.00936570\n",
      "Iteration 24521, loss = 0.00936535\n",
      "Iteration 24522, loss = 0.00936501\n",
      "Iteration 24523, loss = 0.00936467\n",
      "Iteration 24524, loss = 0.00936432\n",
      "Iteration 24525, loss = 0.00936398\n",
      "Iteration 24526, loss = 0.00936363\n",
      "Iteration 24527, loss = 0.00936329\n",
      "Iteration 24528, loss = 0.00936295\n",
      "Iteration 24529, loss = 0.00936260\n",
      "Iteration 24530, loss = 0.00936226\n",
      "Iteration 24531, loss = 0.00936191\n",
      "Iteration 24532, loss = 0.00936157\n",
      "Iteration 24533, loss = 0.00936123\n",
      "Iteration 24534, loss = 0.00936088\n",
      "Iteration 24535, loss = 0.00936054\n",
      "Iteration 24536, loss = 0.00936020\n",
      "Iteration 24537, loss = 0.00935985\n",
      "Iteration 24538, loss = 0.00935951\n",
      "Iteration 24539, loss = 0.00935916\n",
      "Iteration 24540, loss = 0.00935882\n",
      "Iteration 24541, loss = 0.00935848\n",
      "Iteration 24542, loss = 0.00935813\n",
      "Iteration 24543, loss = 0.00935779\n",
      "Iteration 24544, loss = 0.00935745\n",
      "Iteration 24545, loss = 0.00935710\n",
      "Iteration 24546, loss = 0.00935676\n",
      "Iteration 24547, loss = 0.00935642\n",
      "Iteration 24548, loss = 0.00935607\n",
      "Iteration 24549, loss = 0.00935573\n",
      "Iteration 24550, loss = 0.00935539\n",
      "Iteration 24551, loss = 0.00935505\n",
      "Iteration 24552, loss = 0.00935470\n",
      "Iteration 24553, loss = 0.00935436\n",
      "Iteration 24554, loss = 0.00935402\n",
      "Iteration 24555, loss = 0.00935367\n",
      "Iteration 24556, loss = 0.00935333\n",
      "Iteration 24557, loss = 0.00935299\n",
      "Iteration 24558, loss = 0.00935265\n",
      "Iteration 24559, loss = 0.00935230\n",
      "Iteration 24560, loss = 0.00935196\n",
      "Iteration 24561, loss = 0.00935162\n",
      "Iteration 24562, loss = 0.00935127\n",
      "Iteration 24563, loss = 0.00935093\n",
      "Iteration 24564, loss = 0.00935059\n",
      "Iteration 24565, loss = 0.00935025\n",
      "Iteration 24566, loss = 0.00934990\n",
      "Iteration 24567, loss = 0.00934956\n",
      "Iteration 24568, loss = 0.00934922\n",
      "Iteration 24569, loss = 0.00934888\n",
      "Iteration 24570, loss = 0.00934853\n",
      "Iteration 24571, loss = 0.00934819\n",
      "Iteration 24572, loss = 0.00934785\n",
      "Iteration 24573, loss = 0.00934751\n",
      "Iteration 24574, loss = 0.00934717\n",
      "Iteration 24575, loss = 0.00934682\n",
      "Iteration 24576, loss = 0.00934648\n",
      "Iteration 24577, loss = 0.00934614\n",
      "Iteration 24578, loss = 0.00934580\n",
      "Iteration 24579, loss = 0.00934546\n",
      "Iteration 24580, loss = 0.00934511\n",
      "Iteration 24581, loss = 0.00934477\n",
      "Iteration 24582, loss = 0.00934443\n",
      "Iteration 24583, loss = 0.00934409\n",
      "Iteration 24584, loss = 0.00934375\n",
      "Iteration 24585, loss = 0.00934340\n",
      "Iteration 24586, loss = 0.00934306\n",
      "Iteration 24587, loss = 0.00934272\n",
      "Iteration 24588, loss = 0.00934238\n",
      "Iteration 24589, loss = 0.00934204\n",
      "Iteration 24590, loss = 0.00934170\n",
      "Iteration 24591, loss = 0.00934135\n",
      "Iteration 24592, loss = 0.00934101\n",
      "Iteration 24593, loss = 0.00934067\n",
      "Iteration 24594, loss = 0.00934033\n",
      "Iteration 24595, loss = 0.00933999\n",
      "Iteration 24596, loss = 0.00933965\n",
      "Iteration 24597, loss = 0.00933931\n",
      "Iteration 24598, loss = 0.00933897\n",
      "Iteration 24599, loss = 0.00933862\n",
      "Iteration 24600, loss = 0.00933828\n",
      "Iteration 24601, loss = 0.00933794\n",
      "Iteration 24602, loss = 0.00933760\n",
      "Iteration 24603, loss = 0.00933726\n",
      "Iteration 24604, loss = 0.00933692\n",
      "Iteration 24605, loss = 0.00933658\n",
      "Iteration 24606, loss = 0.00933624\n",
      "Iteration 24607, loss = 0.00933590\n",
      "Iteration 24608, loss = 0.00933555\n",
      "Iteration 24609, loss = 0.00933521\n",
      "Iteration 24610, loss = 0.00933487\n",
      "Iteration 24611, loss = 0.00933453\n",
      "Iteration 24612, loss = 0.00933419\n",
      "Iteration 24613, loss = 0.00933385\n",
      "Iteration 24614, loss = 0.00933351\n",
      "Iteration 24615, loss = 0.00933317\n",
      "Iteration 24616, loss = 0.00933283\n",
      "Iteration 24617, loss = 0.00933249\n",
      "Iteration 24618, loss = 0.00933215\n",
      "Iteration 24619, loss = 0.00933181\n",
      "Iteration 24620, loss = 0.00933147\n",
      "Iteration 24621, loss = 0.00933113\n",
      "Iteration 24622, loss = 0.00933079\n",
      "Iteration 24623, loss = 0.00933045\n",
      "Iteration 24624, loss = 0.00933011\n",
      "Iteration 24625, loss = 0.00932977\n",
      "Iteration 24626, loss = 0.00932943\n",
      "Iteration 24627, loss = 0.00932909\n",
      "Iteration 24628, loss = 0.00932875\n",
      "Iteration 24629, loss = 0.00932841\n",
      "Iteration 24630, loss = 0.00932807\n",
      "Iteration 24631, loss = 0.00932773\n",
      "Iteration 24632, loss = 0.00932739\n",
      "Iteration 24633, loss = 0.00932705\n",
      "Iteration 24634, loss = 0.00932671\n",
      "Iteration 24635, loss = 0.00932637\n",
      "Iteration 24636, loss = 0.00932603\n",
      "Iteration 24637, loss = 0.00932569\n",
      "Iteration 24638, loss = 0.00932535\n",
      "Iteration 24639, loss = 0.00932501\n",
      "Iteration 24640, loss = 0.00932467\n",
      "Iteration 24641, loss = 0.00932433\n",
      "Iteration 24642, loss = 0.00932399\n",
      "Iteration 24643, loss = 0.00932365\n",
      "Iteration 24644, loss = 0.00932331\n",
      "Iteration 24645, loss = 0.00932297\n",
      "Iteration 24646, loss = 0.00932263\n",
      "Iteration 24647, loss = 0.00932229\n",
      "Iteration 24648, loss = 0.00932195\n",
      "Iteration 24649, loss = 0.00932161\n",
      "Iteration 24650, loss = 0.00932127\n",
      "Iteration 24651, loss = 0.00932093\n",
      "Iteration 24652, loss = 0.00932060\n",
      "Iteration 24653, loss = 0.00932026\n",
      "Iteration 24654, loss = 0.00931992\n",
      "Iteration 24655, loss = 0.00931958\n",
      "Iteration 24656, loss = 0.00931924\n",
      "Iteration 24657, loss = 0.00931890\n",
      "Iteration 24658, loss = 0.00931856\n",
      "Iteration 24659, loss = 0.00931822\n",
      "Iteration 24660, loss = 0.00931788\n",
      "Iteration 24661, loss = 0.00931754\n",
      "Iteration 24662, loss = 0.00931721\n",
      "Iteration 24663, loss = 0.00931687\n",
      "Iteration 24664, loss = 0.00931653\n",
      "Iteration 24665, loss = 0.00931619\n",
      "Iteration 24666, loss = 0.00931585\n",
      "Iteration 24667, loss = 0.00931551\n",
      "Iteration 24668, loss = 0.00931517\n",
      "Iteration 24669, loss = 0.00931484\n",
      "Iteration 24670, loss = 0.00931450\n",
      "Iteration 24671, loss = 0.00931416\n",
      "Iteration 24672, loss = 0.00931382\n",
      "Iteration 24673, loss = 0.00931348\n",
      "Iteration 24674, loss = 0.00931314\n",
      "Iteration 24675, loss = 0.00931281\n",
      "Iteration 24676, loss = 0.00931247\n",
      "Iteration 24677, loss = 0.00931213\n",
      "Iteration 24678, loss = 0.00931179\n",
      "Iteration 24679, loss = 0.00931145\n",
      "Iteration 24680, loss = 0.00931111\n",
      "Iteration 24681, loss = 0.00931078\n",
      "Iteration 24682, loss = 0.00931044\n",
      "Iteration 24683, loss = 0.00931010\n",
      "Iteration 24684, loss = 0.00930976\n",
      "Iteration 24685, loss = 0.00930942\n",
      "Iteration 24686, loss = 0.00930909\n",
      "Iteration 24687, loss = 0.00930875\n",
      "Iteration 24688, loss = 0.00930841\n",
      "Iteration 24689, loss = 0.00930807\n",
      "Iteration 24690, loss = 0.00930774\n",
      "Iteration 24691, loss = 0.00930740\n",
      "Iteration 24692, loss = 0.00930706\n",
      "Iteration 24693, loss = 0.00930672\n",
      "Iteration 24694, loss = 0.00930639\n",
      "Iteration 24695, loss = 0.00930605\n",
      "Iteration 24696, loss = 0.00930571\n",
      "Iteration 24697, loss = 0.00930537\n",
      "Iteration 24698, loss = 0.00930504\n",
      "Iteration 24699, loss = 0.00930470\n",
      "Iteration 24700, loss = 0.00930436\n",
      "Iteration 24701, loss = 0.00930402\n",
      "Iteration 24702, loss = 0.00930369\n",
      "Iteration 24703, loss = 0.00930335\n",
      "Iteration 24704, loss = 0.00930301\n",
      "Iteration 24705, loss = 0.00930268\n",
      "Iteration 24706, loss = 0.00930234\n",
      "Iteration 24707, loss = 0.00930200\n",
      "Iteration 24708, loss = 0.00930166\n",
      "Iteration 24709, loss = 0.00930133\n",
      "Iteration 24710, loss = 0.00930099\n",
      "Iteration 24711, loss = 0.00930065\n",
      "Iteration 24712, loss = 0.00930032\n",
      "Iteration 24713, loss = 0.00929998\n",
      "Iteration 24714, loss = 0.00929964\n",
      "Iteration 24715, loss = 0.00929931\n",
      "Iteration 24716, loss = 0.00929897\n",
      "Iteration 24717, loss = 0.00929863\n",
      "Iteration 24718, loss = 0.00929830\n",
      "Iteration 24719, loss = 0.00929796\n",
      "Iteration 24720, loss = 0.00929762\n",
      "Iteration 24721, loss = 0.00929729\n",
      "Iteration 24722, loss = 0.00929695\n",
      "Iteration 24723, loss = 0.00929661\n",
      "Iteration 24724, loss = 0.00929628\n",
      "Iteration 24725, loss = 0.00929594\n",
      "Iteration 24726, loss = 0.00929560\n",
      "Iteration 24727, loss = 0.00929527\n",
      "Iteration 24728, loss = 0.00929493\n",
      "Iteration 24729, loss = 0.00929460\n",
      "Iteration 24730, loss = 0.00929426\n",
      "Iteration 24731, loss = 0.00929392\n",
      "Iteration 24732, loss = 0.00929359\n",
      "Iteration 24733, loss = 0.00929325\n",
      "Iteration 24734, loss = 0.00929291\n",
      "Iteration 24735, loss = 0.00929258\n",
      "Iteration 24736, loss = 0.00929224\n",
      "Iteration 24737, loss = 0.00929191\n",
      "Iteration 24738, loss = 0.00929157\n",
      "Iteration 24739, loss = 0.00929124\n",
      "Iteration 24740, loss = 0.00929090\n",
      "Iteration 24741, loss = 0.00929056\n",
      "Iteration 24742, loss = 0.00929023\n",
      "Iteration 24743, loss = 0.00928989\n",
      "Iteration 24744, loss = 0.00928956\n",
      "Iteration 24745, loss = 0.00928922\n",
      "Iteration 24746, loss = 0.00928889\n",
      "Iteration 24747, loss = 0.00928855\n",
      "Iteration 24748, loss = 0.00928821\n",
      "Iteration 24749, loss = 0.00928788\n",
      "Iteration 24750, loss = 0.00928754\n",
      "Iteration 24751, loss = 0.00928721\n",
      "Iteration 24752, loss = 0.00928687\n",
      "Iteration 24753, loss = 0.00928654\n",
      "Iteration 24754, loss = 0.00928620\n",
      "Iteration 24755, loss = 0.00928587\n",
      "Iteration 24756, loss = 0.00928553\n",
      "Iteration 24757, loss = 0.00928520\n",
      "Iteration 24758, loss = 0.00928486\n",
      "Iteration 24759, loss = 0.00928453\n",
      "Iteration 24760, loss = 0.00928419\n",
      "Iteration 24761, loss = 0.00928386\n",
      "Iteration 24762, loss = 0.00928352\n",
      "Iteration 24763, loss = 0.00928319\n",
      "Iteration 24764, loss = 0.00928285\n",
      "Iteration 24765, loss = 0.00928252\n",
      "Iteration 24766, loss = 0.00928218\n",
      "Iteration 24767, loss = 0.00928185\n",
      "Iteration 24768, loss = 0.00928151\n",
      "Iteration 24769, loss = 0.00928118\n",
      "Iteration 24770, loss = 0.00928084\n",
      "Iteration 24771, loss = 0.00928051\n",
      "Iteration 24772, loss = 0.00928017\n",
      "Iteration 24773, loss = 0.00927984\n",
      "Iteration 24774, loss = 0.00927950\n",
      "Iteration 24775, loss = 0.00927917\n",
      "Iteration 24776, loss = 0.00927884\n",
      "Iteration 24777, loss = 0.00927850\n",
      "Iteration 24778, loss = 0.00927817\n",
      "Iteration 24779, loss = 0.00927783\n",
      "Iteration 24780, loss = 0.00927750\n",
      "Iteration 24781, loss = 0.00927716\n",
      "Iteration 24782, loss = 0.00927683\n",
      "Iteration 24783, loss = 0.00927650\n",
      "Iteration 24784, loss = 0.00927616\n",
      "Iteration 24785, loss = 0.00927583\n",
      "Iteration 24786, loss = 0.00927549\n",
      "Iteration 24787, loss = 0.00927516\n",
      "Iteration 24788, loss = 0.00927483\n",
      "Iteration 24789, loss = 0.00927449\n",
      "Iteration 24790, loss = 0.00927416\n",
      "Iteration 24791, loss = 0.00927382\n",
      "Iteration 24792, loss = 0.00927349\n",
      "Iteration 24793, loss = 0.00927316\n",
      "Iteration 24794, loss = 0.00927282\n",
      "Iteration 24795, loss = 0.00927249\n",
      "Iteration 24796, loss = 0.00927215\n",
      "Iteration 24797, loss = 0.00927182\n",
      "Iteration 24798, loss = 0.00927149\n",
      "Iteration 24799, loss = 0.00927115\n",
      "Iteration 24800, loss = 0.00927082\n",
      "Iteration 24801, loss = 0.00927049\n",
      "Iteration 24802, loss = 0.00927015\n",
      "Iteration 24803, loss = 0.00926982\n",
      "Iteration 24804, loss = 0.00926949\n",
      "Iteration 24805, loss = 0.00926915\n",
      "Iteration 24806, loss = 0.00926882\n",
      "Iteration 24807, loss = 0.00926849\n",
      "Iteration 24808, loss = 0.00926815\n",
      "Iteration 24809, loss = 0.00926782\n",
      "Iteration 24810, loss = 0.00926749\n",
      "Iteration 24811, loss = 0.00926715\n",
      "Iteration 24812, loss = 0.00926682\n",
      "Iteration 24813, loss = 0.00926649\n",
      "Iteration 24814, loss = 0.00926616\n",
      "Iteration 24815, loss = 0.00926582\n",
      "Iteration 24816, loss = 0.00926549\n",
      "Iteration 24817, loss = 0.00926516\n",
      "Iteration 24818, loss = 0.00926482\n",
      "Iteration 24819, loss = 0.00926449\n",
      "Iteration 24820, loss = 0.00926416\n",
      "Iteration 24821, loss = 0.00926382\n",
      "Iteration 24822, loss = 0.00926349\n",
      "Iteration 24823, loss = 0.00926316\n",
      "Iteration 24824, loss = 0.00926283\n",
      "Iteration 24825, loss = 0.00926249\n",
      "Iteration 24826, loss = 0.00926216\n",
      "Iteration 24827, loss = 0.00926183\n",
      "Iteration 24828, loss = 0.00926150\n",
      "Iteration 24829, loss = 0.00926116\n",
      "Iteration 24830, loss = 0.00926083\n",
      "Iteration 24831, loss = 0.00926050\n",
      "Iteration 24832, loss = 0.00926017\n",
      "Iteration 24833, loss = 0.00925983\n",
      "Iteration 24834, loss = 0.00925950\n",
      "Iteration 24835, loss = 0.00925917\n",
      "Iteration 24836, loss = 0.00925884\n",
      "Iteration 24837, loss = 0.00925851\n",
      "Iteration 24838, loss = 0.00925817\n",
      "Iteration 24839, loss = 0.00925784\n",
      "Iteration 24840, loss = 0.00925751\n",
      "Iteration 24841, loss = 0.00925718\n",
      "Iteration 24842, loss = 0.00925685\n",
      "Iteration 24843, loss = 0.00925651\n",
      "Iteration 24844, loss = 0.00925618\n",
      "Iteration 24845, loss = 0.00925585\n",
      "Iteration 24846, loss = 0.00925552\n",
      "Iteration 24847, loss = 0.00925519\n",
      "Iteration 24848, loss = 0.00925485\n",
      "Iteration 24849, loss = 0.00925452\n",
      "Iteration 24850, loss = 0.00925419\n",
      "Iteration 24851, loss = 0.00925386\n",
      "Iteration 24852, loss = 0.00925353\n",
      "Iteration 24853, loss = 0.00925320\n",
      "Iteration 24854, loss = 0.00925287\n",
      "Iteration 24855, loss = 0.00925253\n",
      "Iteration 24856, loss = 0.00925220\n",
      "Iteration 24857, loss = 0.00925187\n",
      "Iteration 24858, loss = 0.00925154\n",
      "Iteration 24859, loss = 0.00925121\n",
      "Iteration 24860, loss = 0.00925088\n",
      "Iteration 24861, loss = 0.00925055\n",
      "Iteration 24862, loss = 0.00925021\n",
      "Iteration 24863, loss = 0.00924988\n",
      "Iteration 24864, loss = 0.00924955\n",
      "Iteration 24865, loss = 0.00924922\n",
      "Iteration 24866, loss = 0.00924889\n",
      "Iteration 24867, loss = 0.00924856\n",
      "Iteration 24868, loss = 0.00924823\n",
      "Iteration 24869, loss = 0.00924790\n",
      "Iteration 24870, loss = 0.00924757\n",
      "Iteration 24871, loss = 0.00924724\n",
      "Iteration 24872, loss = 0.00924690\n",
      "Iteration 24873, loss = 0.00924657\n",
      "Iteration 24874, loss = 0.00924624\n",
      "Iteration 24875, loss = 0.00924591\n",
      "Iteration 24876, loss = 0.00924558\n",
      "Iteration 24877, loss = 0.00924525\n",
      "Iteration 24878, loss = 0.00924492\n",
      "Iteration 24879, loss = 0.00924459\n",
      "Iteration 24880, loss = 0.00924426\n",
      "Iteration 24881, loss = 0.00924393\n",
      "Iteration 24882, loss = 0.00924360\n",
      "Iteration 24883, loss = 0.00924327\n",
      "Iteration 24884, loss = 0.00924294\n",
      "Iteration 24885, loss = 0.00924261\n",
      "Iteration 24886, loss = 0.00924228\n",
      "Iteration 24887, loss = 0.00924195\n",
      "Iteration 24888, loss = 0.00924162\n",
      "Iteration 24889, loss = 0.00924129\n",
      "Iteration 24890, loss = 0.00924096\n",
      "Iteration 24891, loss = 0.00924063\n",
      "Iteration 24892, loss = 0.00924030\n",
      "Iteration 24893, loss = 0.00923996\n",
      "Iteration 24894, loss = 0.00923963\n",
      "Iteration 24895, loss = 0.00923930\n",
      "Iteration 24896, loss = 0.00923897\n",
      "Iteration 24897, loss = 0.00923865\n",
      "Iteration 24898, loss = 0.00923832\n",
      "Iteration 24899, loss = 0.00923799\n",
      "Iteration 24900, loss = 0.00923766\n",
      "Iteration 24901, loss = 0.00923733\n",
      "Iteration 24902, loss = 0.00923700\n",
      "Iteration 24903, loss = 0.00923667\n",
      "Iteration 24904, loss = 0.00923634\n",
      "Iteration 24905, loss = 0.00923601\n",
      "Iteration 24906, loss = 0.00923568\n",
      "Iteration 24907, loss = 0.00923535\n",
      "Iteration 24908, loss = 0.00923502\n",
      "Iteration 24909, loss = 0.00923469\n",
      "Iteration 24910, loss = 0.00923436\n",
      "Iteration 24911, loss = 0.00923403\n",
      "Iteration 24912, loss = 0.00923370\n",
      "Iteration 24913, loss = 0.00923337\n",
      "Iteration 24914, loss = 0.00923304\n",
      "Iteration 24915, loss = 0.00923271\n",
      "Iteration 24916, loss = 0.00923238\n",
      "Iteration 24917, loss = 0.00923205\n",
      "Iteration 24918, loss = 0.00923173\n",
      "Iteration 24919, loss = 0.00923140\n",
      "Iteration 24920, loss = 0.00923107\n",
      "Iteration 24921, loss = 0.00923074\n",
      "Iteration 24922, loss = 0.00923041\n",
      "Iteration 24923, loss = 0.00923008\n",
      "Iteration 24924, loss = 0.00922975\n",
      "Iteration 24925, loss = 0.00922942\n",
      "Iteration 24926, loss = 0.00922909\n",
      "Iteration 24927, loss = 0.00922876\n",
      "Iteration 24928, loss = 0.00922844\n",
      "Iteration 24929, loss = 0.00922811\n",
      "Iteration 24930, loss = 0.00922778\n",
      "Iteration 24931, loss = 0.00922745\n",
      "Iteration 24932, loss = 0.00922712\n",
      "Iteration 24933, loss = 0.00922679\n",
      "Iteration 24934, loss = 0.00922646\n",
      "Iteration 24935, loss = 0.00922614\n",
      "Iteration 24936, loss = 0.00922581\n",
      "Iteration 24937, loss = 0.00922548\n",
      "Iteration 24938, loss = 0.00922515\n",
      "Iteration 24939, loss = 0.00922482\n",
      "Iteration 24940, loss = 0.00922449\n",
      "Iteration 24941, loss = 0.00922416\n",
      "Iteration 24942, loss = 0.00922384\n",
      "Iteration 24943, loss = 0.00922351\n",
      "Iteration 24944, loss = 0.00922318\n",
      "Iteration 24945, loss = 0.00922285\n",
      "Iteration 24946, loss = 0.00922252\n",
      "Iteration 24947, loss = 0.00922220\n",
      "Iteration 24948, loss = 0.00922187\n",
      "Iteration 24949, loss = 0.00922154\n",
      "Iteration 24950, loss = 0.00922121\n",
      "Iteration 24951, loss = 0.00922088\n",
      "Iteration 24952, loss = 0.00922056\n",
      "Iteration 24953, loss = 0.00922023\n",
      "Iteration 24954, loss = 0.00921990\n",
      "Iteration 24955, loss = 0.00921957\n",
      "Iteration 24956, loss = 0.00921924\n",
      "Iteration 24957, loss = 0.00921892\n",
      "Iteration 24958, loss = 0.00921859\n",
      "Iteration 24959, loss = 0.00921826\n",
      "Iteration 24960, loss = 0.00921793\n",
      "Iteration 24961, loss = 0.00921761\n",
      "Iteration 24962, loss = 0.00921728\n",
      "Iteration 24963, loss = 0.00921695\n",
      "Iteration 24964, loss = 0.00921662\n",
      "Iteration 24965, loss = 0.00921630\n",
      "Iteration 24966, loss = 0.00921597\n",
      "Iteration 24967, loss = 0.00921564\n",
      "Iteration 24968, loss = 0.00921531\n",
      "Iteration 24969, loss = 0.00921499\n",
      "Iteration 24970, loss = 0.00921466\n",
      "Iteration 24971, loss = 0.00921433\n",
      "Iteration 24972, loss = 0.00921401\n",
      "Iteration 24973, loss = 0.00921368\n",
      "Iteration 24974, loss = 0.00921335\n",
      "Iteration 24975, loss = 0.00921302\n",
      "Iteration 24976, loss = 0.00921270\n",
      "Iteration 24977, loss = 0.00921237\n",
      "Iteration 24978, loss = 0.00921204\n",
      "Iteration 24979, loss = 0.00921172\n",
      "Iteration 24980, loss = 0.00921139\n",
      "Iteration 24981, loss = 0.00921106\n",
      "Iteration 24982, loss = 0.00921074\n",
      "Iteration 24983, loss = 0.00921041\n",
      "Iteration 24984, loss = 0.00921008\n",
      "Iteration 24985, loss = 0.00920976\n",
      "Iteration 24986, loss = 0.00920943\n",
      "Iteration 24987, loss = 0.00920910\n",
      "Iteration 24988, loss = 0.00920878\n",
      "Iteration 24989, loss = 0.00920845\n",
      "Iteration 24990, loss = 0.00920812\n",
      "Iteration 24991, loss = 0.00920780\n",
      "Iteration 24992, loss = 0.00920747\n",
      "Iteration 24993, loss = 0.00920714\n",
      "Iteration 24994, loss = 0.00920682\n",
      "Iteration 24995, loss = 0.00920649\n",
      "Iteration 24996, loss = 0.00920616\n",
      "Iteration 24997, loss = 0.00920584\n",
      "Iteration 24998, loss = 0.00920551\n",
      "Iteration 24999, loss = 0.00920519\n",
      "Iteration 25000, loss = 0.00920486\n",
      "Iteration 25001, loss = 0.00920453\n",
      "Iteration 25002, loss = 0.00920421\n",
      "Iteration 25003, loss = 0.00920388\n",
      "Iteration 25004, loss = 0.00920356\n",
      "Iteration 25005, loss = 0.00920323\n",
      "Iteration 25006, loss = 0.00920290\n",
      "Iteration 25007, loss = 0.00920258\n",
      "Iteration 25008, loss = 0.00920225\n",
      "Iteration 25009, loss = 0.00920193\n",
      "Iteration 25010, loss = 0.00920160\n",
      "Iteration 25011, loss = 0.00920127\n",
      "Iteration 25012, loss = 0.00920095\n",
      "Iteration 25013, loss = 0.00920062\n",
      "Iteration 25014, loss = 0.00920030\n",
      "Iteration 25015, loss = 0.00919997\n",
      "Iteration 25016, loss = 0.00919965\n",
      "Iteration 25017, loss = 0.00919932\n",
      "Iteration 25018, loss = 0.00919900\n",
      "Iteration 25019, loss = 0.00919867\n",
      "Iteration 25020, loss = 0.00919834\n",
      "Iteration 25021, loss = 0.00919802\n",
      "Iteration 25022, loss = 0.00919769\n",
      "Iteration 25023, loss = 0.00919737\n",
      "Iteration 25024, loss = 0.00919704\n",
      "Iteration 25025, loss = 0.00919672\n",
      "Iteration 25026, loss = 0.00919639\n",
      "Iteration 25027, loss = 0.00919607\n",
      "Iteration 25028, loss = 0.00919574\n",
      "Iteration 25029, loss = 0.00919542\n",
      "Iteration 25030, loss = 0.00919509\n",
      "Iteration 25031, loss = 0.00919477\n",
      "Iteration 25032, loss = 0.00919444\n",
      "Iteration 25033, loss = 0.00919412\n",
      "Iteration 25034, loss = 0.00919379\n",
      "Iteration 25035, loss = 0.00919347\n",
      "Iteration 25036, loss = 0.00919314\n",
      "Iteration 25037, loss = 0.00919282\n",
      "Iteration 25038, loss = 0.00919249\n",
      "Iteration 25039, loss = 0.00919217\n",
      "Iteration 25040, loss = 0.00919184\n",
      "Iteration 25041, loss = 0.00919152\n",
      "Iteration 25042, loss = 0.00919119\n",
      "Iteration 25043, loss = 0.00919087\n",
      "Iteration 25044, loss = 0.00919055\n",
      "Iteration 25045, loss = 0.00919022\n",
      "Iteration 25046, loss = 0.00918990\n",
      "Iteration 25047, loss = 0.00918957\n",
      "Iteration 25048, loss = 0.00918925\n",
      "Iteration 25049, loss = 0.00918892\n",
      "Iteration 25050, loss = 0.00918860\n",
      "Iteration 25051, loss = 0.00918827\n",
      "Iteration 25052, loss = 0.00918795\n",
      "Iteration 25053, loss = 0.00918763\n",
      "Iteration 25054, loss = 0.00918730\n",
      "Iteration 25055, loss = 0.00918698\n",
      "Iteration 25056, loss = 0.00918665\n",
      "Iteration 25057, loss = 0.00918633\n",
      "Iteration 25058, loss = 0.00918601\n",
      "Iteration 25059, loss = 0.00918568\n",
      "Iteration 25060, loss = 0.00918536\n",
      "Iteration 25061, loss = 0.00918503\n",
      "Iteration 25062, loss = 0.00918471\n",
      "Iteration 25063, loss = 0.00918439\n",
      "Iteration 25064, loss = 0.00918406\n",
      "Iteration 25065, loss = 0.00918374\n",
      "Iteration 25066, loss = 0.00918341\n",
      "Iteration 25067, loss = 0.00918309\n",
      "Iteration 25068, loss = 0.00918277\n",
      "Iteration 25069, loss = 0.00918244\n",
      "Iteration 25070, loss = 0.00918212\n",
      "Iteration 25071, loss = 0.00918180\n",
      "Iteration 25072, loss = 0.00918147\n",
      "Iteration 25073, loss = 0.00918115\n",
      "Iteration 25074, loss = 0.00918083\n",
      "Iteration 25075, loss = 0.00918050\n",
      "Iteration 25076, loss = 0.00918018\n",
      "Iteration 25077, loss = 0.00917986\n",
      "Iteration 25078, loss = 0.00917953\n",
      "Iteration 25079, loss = 0.00917921\n",
      "Iteration 25080, loss = 0.00917889\n",
      "Iteration 25081, loss = 0.00917856\n",
      "Iteration 25082, loss = 0.00917824\n",
      "Iteration 25083, loss = 0.00917792\n",
      "Iteration 25084, loss = 0.00917759\n",
      "Iteration 25085, loss = 0.00917727\n",
      "Iteration 25086, loss = 0.00917695\n",
      "Iteration 25087, loss = 0.00917662\n",
      "Iteration 25088, loss = 0.00917630\n",
      "Iteration 25089, loss = 0.00917598\n",
      "Iteration 25090, loss = 0.00917566\n",
      "Iteration 25091, loss = 0.00917533\n",
      "Iteration 25092, loss = 0.00917501\n",
      "Iteration 25093, loss = 0.00917469\n",
      "Iteration 25094, loss = 0.00917436\n",
      "Iteration 25095, loss = 0.00917404\n",
      "Iteration 25096, loss = 0.00917372\n",
      "Iteration 25097, loss = 0.00917340\n",
      "Iteration 25098, loss = 0.00917307\n",
      "Iteration 25099, loss = 0.00917275\n",
      "Iteration 25100, loss = 0.00917243\n",
      "Iteration 25101, loss = 0.00917211\n",
      "Iteration 25102, loss = 0.00917178\n",
      "Iteration 25103, loss = 0.00917146\n",
      "Iteration 25104, loss = 0.00917114\n",
      "Iteration 25105, loss = 0.00917082\n",
      "Iteration 25106, loss = 0.00917049\n",
      "Iteration 25107, loss = 0.00917017\n",
      "Iteration 25108, loss = 0.00916985\n",
      "Iteration 25109, loss = 0.00916953\n",
      "Iteration 25110, loss = 0.00916920\n",
      "Iteration 25111, loss = 0.00916888\n",
      "Iteration 25112, loss = 0.00916856\n",
      "Iteration 25113, loss = 0.00916824\n",
      "Iteration 25114, loss = 0.00916792\n",
      "Iteration 25115, loss = 0.00916759\n",
      "Iteration 25116, loss = 0.00916727\n",
      "Iteration 25117, loss = 0.00916695\n",
      "Iteration 25118, loss = 0.00916663\n",
      "Iteration 25119, loss = 0.00916631\n",
      "Iteration 25120, loss = 0.00916599\n",
      "Iteration 25121, loss = 0.00916566\n",
      "Iteration 25122, loss = 0.00916534\n",
      "Iteration 25123, loss = 0.00916502\n",
      "Iteration 25124, loss = 0.00916470\n",
      "Iteration 25125, loss = 0.00916438\n",
      "Iteration 25126, loss = 0.00916405\n",
      "Iteration 25127, loss = 0.00916373\n",
      "Iteration 25128, loss = 0.00916341\n",
      "Iteration 25129, loss = 0.00916309\n",
      "Iteration 25130, loss = 0.00916277\n",
      "Iteration 25131, loss = 0.00916245\n",
      "Iteration 25132, loss = 0.00916213\n",
      "Iteration 25133, loss = 0.00916180\n",
      "Iteration 25134, loss = 0.00916148\n",
      "Iteration 25135, loss = 0.00916116\n",
      "Iteration 25136, loss = 0.00916084\n",
      "Iteration 25137, loss = 0.00916052\n",
      "Iteration 25138, loss = 0.00916020\n",
      "Iteration 25139, loss = 0.00915988\n",
      "Iteration 25140, loss = 0.00915956\n",
      "Iteration 25141, loss = 0.00915924\n",
      "Iteration 25142, loss = 0.00915891\n",
      "Iteration 25143, loss = 0.00915859\n",
      "Iteration 25144, loss = 0.00915827\n",
      "Iteration 25145, loss = 0.00915795\n",
      "Iteration 25146, loss = 0.00915763\n",
      "Iteration 25147, loss = 0.00915731\n",
      "Iteration 25148, loss = 0.00915699\n",
      "Iteration 25149, loss = 0.00915667\n",
      "Iteration 25150, loss = 0.00915635\n",
      "Iteration 25151, loss = 0.00915603\n",
      "Iteration 25152, loss = 0.00915571\n",
      "Iteration 25153, loss = 0.00915539\n",
      "Iteration 25154, loss = 0.00915506\n",
      "Iteration 25155, loss = 0.00915474\n",
      "Iteration 25156, loss = 0.00915442\n",
      "Iteration 25157, loss = 0.00915410\n",
      "Iteration 25158, loss = 0.00915378\n",
      "Iteration 25159, loss = 0.00915346\n",
      "Iteration 25160, loss = 0.00915314\n",
      "Iteration 25161, loss = 0.00915282\n",
      "Iteration 25162, loss = 0.00915250\n",
      "Iteration 25163, loss = 0.00915218\n",
      "Iteration 25164, loss = 0.00915186\n",
      "Iteration 25165, loss = 0.00915154\n",
      "Iteration 25166, loss = 0.00915122\n",
      "Iteration 25167, loss = 0.00915090\n",
      "Iteration 25168, loss = 0.00915058\n",
      "Iteration 25169, loss = 0.00915026\n",
      "Iteration 25170, loss = 0.00914994\n",
      "Iteration 25171, loss = 0.00914962\n",
      "Iteration 25172, loss = 0.00914930\n",
      "Iteration 25173, loss = 0.00914898\n",
      "Iteration 25174, loss = 0.00914866\n",
      "Iteration 25175, loss = 0.00914834\n",
      "Iteration 25176, loss = 0.00914802\n",
      "Iteration 25177, loss = 0.00914770\n",
      "Iteration 25178, loss = 0.00914738\n",
      "Iteration 25179, loss = 0.00914706\n",
      "Iteration 25180, loss = 0.00914674\n",
      "Iteration 25181, loss = 0.00914642\n",
      "Iteration 25182, loss = 0.00914610\n",
      "Iteration 25183, loss = 0.00914578\n",
      "Iteration 25184, loss = 0.00914546\n",
      "Iteration 25185, loss = 0.00914514\n",
      "Iteration 25186, loss = 0.00914482\n",
      "Iteration 25187, loss = 0.00914450\n",
      "Iteration 25188, loss = 0.00914418\n",
      "Iteration 25189, loss = 0.00914387\n",
      "Iteration 25190, loss = 0.00914355\n",
      "Iteration 25191, loss = 0.00914323\n",
      "Iteration 25192, loss = 0.00914291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25193, loss = 0.00914259\n",
      "Iteration 25194, loss = 0.00914227\n",
      "Iteration 25195, loss = 0.00914195\n",
      "Iteration 25196, loss = 0.00914163\n",
      "Iteration 25197, loss = 0.00914131\n",
      "Iteration 25198, loss = 0.00914099\n",
      "Iteration 25199, loss = 0.00914067\n",
      "Iteration 25200, loss = 0.00914035\n",
      "Iteration 25201, loss = 0.00914004\n",
      "Iteration 25202, loss = 0.00913972\n",
      "Iteration 25203, loss = 0.00913940\n",
      "Iteration 25204, loss = 0.00913908\n",
      "Iteration 25205, loss = 0.00913876\n",
      "Iteration 25206, loss = 0.00913844\n",
      "Iteration 25207, loss = 0.00913812\n",
      "Iteration 25208, loss = 0.00913780\n",
      "Iteration 25209, loss = 0.00913749\n",
      "Iteration 25210, loss = 0.00913717\n",
      "Iteration 25211, loss = 0.00913685\n",
      "Iteration 25212, loss = 0.00913653\n",
      "Iteration 25213, loss = 0.00913621\n",
      "Iteration 25214, loss = 0.00913589\n",
      "Iteration 25215, loss = 0.00913557\n",
      "Iteration 25216, loss = 0.00913526\n",
      "Iteration 25217, loss = 0.00913494\n",
      "Iteration 25218, loss = 0.00913462\n",
      "Iteration 25219, loss = 0.00913430\n",
      "Iteration 25220, loss = 0.00913398\n",
      "Iteration 25221, loss = 0.00913366\n",
      "Iteration 25222, loss = 0.00913335\n",
      "Iteration 25223, loss = 0.00913303\n",
      "Iteration 25224, loss = 0.00913271\n",
      "Iteration 25225, loss = 0.00913239\n",
      "Iteration 25226, loss = 0.00913207\n",
      "Iteration 25227, loss = 0.00913176\n",
      "Iteration 25228, loss = 0.00913144\n",
      "Iteration 25229, loss = 0.00913112\n",
      "Iteration 25230, loss = 0.00913080\n",
      "Iteration 25231, loss = 0.00913048\n",
      "Iteration 25232, loss = 0.00913017\n",
      "Iteration 25233, loss = 0.00912985\n",
      "Iteration 25234, loss = 0.00912953\n",
      "Iteration 25235, loss = 0.00912921\n",
      "Iteration 25236, loss = 0.00912890\n",
      "Iteration 25237, loss = 0.00912858\n",
      "Iteration 25238, loss = 0.00912826\n",
      "Iteration 25239, loss = 0.00912794\n",
      "Iteration 25240, loss = 0.00912762\n",
      "Iteration 25241, loss = 0.00912731\n",
      "Iteration 25242, loss = 0.00912699\n",
      "Iteration 25243, loss = 0.00912667\n",
      "Iteration 25244, loss = 0.00912635\n",
      "Iteration 25245, loss = 0.00912604\n",
      "Iteration 25246, loss = 0.00912572\n",
      "Iteration 25247, loss = 0.00912540\n",
      "Iteration 25248, loss = 0.00912509\n",
      "Iteration 25249, loss = 0.00912477\n",
      "Iteration 25250, loss = 0.00912445\n",
      "Iteration 25251, loss = 0.00912413\n",
      "Iteration 25252, loss = 0.00912382\n",
      "Iteration 25253, loss = 0.00912350\n",
      "Iteration 25254, loss = 0.00912318\n",
      "Iteration 25255, loss = 0.00912287\n",
      "Iteration 25256, loss = 0.00912255\n",
      "Iteration 25257, loss = 0.00912223\n",
      "Iteration 25258, loss = 0.00912191\n",
      "Iteration 25259, loss = 0.00912160\n",
      "Iteration 25260, loss = 0.00912128\n",
      "Iteration 25261, loss = 0.00912096\n",
      "Iteration 25262, loss = 0.00912065\n",
      "Iteration 25263, loss = 0.00912033\n",
      "Iteration 25264, loss = 0.00912001\n",
      "Iteration 25265, loss = 0.00911970\n",
      "Iteration 25266, loss = 0.00911938\n",
      "Iteration 25267, loss = 0.00911906\n",
      "Iteration 25268, loss = 0.00911875\n",
      "Iteration 25269, loss = 0.00911843\n",
      "Iteration 25270, loss = 0.00911811\n",
      "Iteration 25271, loss = 0.00911780\n",
      "Iteration 25272, loss = 0.00911748\n",
      "Iteration 25273, loss = 0.00911716\n",
      "Iteration 25274, loss = 0.00911685\n",
      "Iteration 25275, loss = 0.00911653\n",
      "Iteration 25276, loss = 0.00911622\n",
      "Iteration 25277, loss = 0.00911590\n",
      "Iteration 25278, loss = 0.00911558\n",
      "Iteration 25279, loss = 0.00911527\n",
      "Iteration 25280, loss = 0.00911495\n",
      "Iteration 25281, loss = 0.00911463\n",
      "Iteration 25282, loss = 0.00911432\n",
      "Iteration 25283, loss = 0.00911400\n",
      "Iteration 25284, loss = 0.00911369\n",
      "Iteration 25285, loss = 0.00911337\n",
      "Iteration 25286, loss = 0.00911305\n",
      "Iteration 25287, loss = 0.00911274\n",
      "Iteration 25288, loss = 0.00911242\n",
      "Iteration 25289, loss = 0.00911211\n",
      "Iteration 25290, loss = 0.00911179\n",
      "Iteration 25291, loss = 0.00911147\n",
      "Iteration 25292, loss = 0.00911116\n",
      "Iteration 25293, loss = 0.00911084\n",
      "Iteration 25294, loss = 0.00911053\n",
      "Iteration 25295, loss = 0.00911021\n",
      "Iteration 25296, loss = 0.00910990\n",
      "Iteration 25297, loss = 0.00910958\n",
      "Iteration 25298, loss = 0.00910927\n",
      "Iteration 25299, loss = 0.00910895\n",
      "Iteration 25300, loss = 0.00910863\n",
      "Iteration 25301, loss = 0.00910832\n",
      "Iteration 25302, loss = 0.00910800\n",
      "Iteration 25303, loss = 0.00910769\n",
      "Iteration 25304, loss = 0.00910737\n",
      "Iteration 25305, loss = 0.00910706\n",
      "Iteration 25306, loss = 0.00910674\n",
      "Iteration 25307, loss = 0.00910643\n",
      "Iteration 25308, loss = 0.00910611\n",
      "Iteration 25309, loss = 0.00910580\n",
      "Iteration 25310, loss = 0.00910548\n",
      "Iteration 25311, loss = 0.00910517\n",
      "Iteration 25312, loss = 0.00910485\n",
      "Iteration 25313, loss = 0.00910454\n",
      "Iteration 25314, loss = 0.00910422\n",
      "Iteration 25315, loss = 0.00910391\n",
      "Iteration 25316, loss = 0.00910359\n",
      "Iteration 25317, loss = 0.00910328\n",
      "Iteration 25318, loss = 0.00910296\n",
      "Iteration 25319, loss = 0.00910265\n",
      "Iteration 25320, loss = 0.00910233\n",
      "Iteration 25321, loss = 0.00910202\n",
      "Iteration 25322, loss = 0.00910170\n",
      "Iteration 25323, loss = 0.00910139\n",
      "Iteration 25324, loss = 0.00910107\n",
      "Iteration 25325, loss = 0.00910076\n",
      "Iteration 25326, loss = 0.00910044\n",
      "Iteration 25327, loss = 0.00910013\n",
      "Iteration 25328, loss = 0.00909981\n",
      "Iteration 25329, loss = 0.00909950\n",
      "Iteration 25330, loss = 0.00909919\n",
      "Iteration 25331, loss = 0.00909887\n",
      "Iteration 25332, loss = 0.00909856\n",
      "Iteration 25333, loss = 0.00909824\n",
      "Iteration 25334, loss = 0.00909793\n",
      "Iteration 25335, loss = 0.00909761\n",
      "Iteration 25336, loss = 0.00909730\n",
      "Iteration 25337, loss = 0.00909699\n",
      "Iteration 25338, loss = 0.00909667\n",
      "Iteration 25339, loss = 0.00909636\n",
      "Iteration 25340, loss = 0.00909604\n",
      "Iteration 25341, loss = 0.00909573\n",
      "Iteration 25342, loss = 0.00909542\n",
      "Iteration 25343, loss = 0.00909510\n",
      "Iteration 25344, loss = 0.00909479\n",
      "Iteration 25345, loss = 0.00909447\n",
      "Iteration 25346, loss = 0.00909416\n",
      "Iteration 25347, loss = 0.00909385\n",
      "Iteration 25348, loss = 0.00909353\n",
      "Iteration 25349, loss = 0.00909322\n",
      "Iteration 25350, loss = 0.00909290\n",
      "Iteration 25351, loss = 0.00909259\n",
      "Iteration 25352, loss = 0.00909228\n",
      "Iteration 25353, loss = 0.00909196\n",
      "Iteration 25354, loss = 0.00909165\n",
      "Iteration 25355, loss = 0.00909134\n",
      "Iteration 25356, loss = 0.00909102\n",
      "Iteration 25357, loss = 0.00909071\n",
      "Iteration 25358, loss = 0.00909040\n",
      "Iteration 25359, loss = 0.00909008\n",
      "Iteration 25360, loss = 0.00908977\n",
      "Iteration 25361, loss = 0.00908946\n",
      "Iteration 25362, loss = 0.00908914\n",
      "Iteration 25363, loss = 0.00908883\n",
      "Iteration 25364, loss = 0.00908852\n",
      "Iteration 25365, loss = 0.00908820\n",
      "Iteration 25366, loss = 0.00908789\n",
      "Iteration 25367, loss = 0.00908758\n",
      "Iteration 25368, loss = 0.00908726\n",
      "Iteration 25369, loss = 0.00908695\n",
      "Iteration 25370, loss = 0.00908664\n",
      "Iteration 25371, loss = 0.00908632\n",
      "Iteration 25372, loss = 0.00908601\n",
      "Iteration 25373, loss = 0.00908570\n",
      "Iteration 25374, loss = 0.00908538\n",
      "Iteration 25375, loss = 0.00908507\n",
      "Iteration 25376, loss = 0.00908476\n",
      "Iteration 25377, loss = 0.00908445\n",
      "Iteration 25378, loss = 0.00908413\n",
      "Iteration 25379, loss = 0.00908382\n",
      "Iteration 25380, loss = 0.00908351\n",
      "Iteration 25381, loss = 0.00908320\n",
      "Iteration 25382, loss = 0.00908288\n",
      "Iteration 25383, loss = 0.00908257\n",
      "Iteration 25384, loss = 0.00908226\n",
      "Iteration 25385, loss = 0.00908195\n",
      "Iteration 25386, loss = 0.00908163\n",
      "Iteration 25387, loss = 0.00908132\n",
      "Iteration 25388, loss = 0.00908101\n",
      "Iteration 25389, loss = 0.00908070\n",
      "Iteration 25390, loss = 0.00908038\n",
      "Iteration 25391, loss = 0.00908007\n",
      "Iteration 25392, loss = 0.00907976\n",
      "Iteration 25393, loss = 0.00907945\n",
      "Iteration 25394, loss = 0.00907913\n",
      "Iteration 25395, loss = 0.00907882\n",
      "Iteration 25396, loss = 0.00907851\n",
      "Iteration 25397, loss = 0.00907820\n",
      "Iteration 25398, loss = 0.00907789\n",
      "Iteration 25399, loss = 0.00907757\n",
      "Iteration 25400, loss = 0.00907726\n",
      "Iteration 25401, loss = 0.00907695\n",
      "Iteration 25402, loss = 0.00907664\n",
      "Iteration 25403, loss = 0.00907633\n",
      "Iteration 25404, loss = 0.00907601\n",
      "Iteration 25405, loss = 0.00907570\n",
      "Iteration 25406, loss = 0.00907539\n",
      "Iteration 25407, loss = 0.00907508\n",
      "Iteration 25408, loss = 0.00907477\n",
      "Iteration 25409, loss = 0.00907445\n",
      "Iteration 25410, loss = 0.00907414\n",
      "Iteration 25411, loss = 0.00907383\n",
      "Iteration 25412, loss = 0.00907352\n",
      "Iteration 25413, loss = 0.00907321\n",
      "Iteration 25414, loss = 0.00907290\n",
      "Iteration 25415, loss = 0.00907258\n",
      "Iteration 25416, loss = 0.00907227\n",
      "Iteration 25417, loss = 0.00907196\n",
      "Iteration 25418, loss = 0.00907165\n",
      "Iteration 25419, loss = 0.00907134\n",
      "Iteration 25420, loss = 0.00907103\n",
      "Iteration 25421, loss = 0.00907072\n",
      "Iteration 25422, loss = 0.00907040\n",
      "Iteration 25423, loss = 0.00907009\n",
      "Iteration 25424, loss = 0.00906978\n",
      "Iteration 25425, loss = 0.00906947\n",
      "Iteration 25426, loss = 0.00906916\n",
      "Iteration 25427, loss = 0.00906885\n",
      "Iteration 25428, loss = 0.00906854\n",
      "Iteration 25429, loss = 0.00906823\n",
      "Iteration 25430, loss = 0.00906792\n",
      "Iteration 25431, loss = 0.00906760\n",
      "Iteration 25432, loss = 0.00906729\n",
      "Iteration 25433, loss = 0.00906698\n",
      "Iteration 25434, loss = 0.00906667\n",
      "Iteration 25435, loss = 0.00906636\n",
      "Iteration 25436, loss = 0.00906605\n",
      "Iteration 25437, loss = 0.00906574\n",
      "Iteration 25438, loss = 0.00906543\n",
      "Iteration 25439, loss = 0.00906512\n",
      "Iteration 25440, loss = 0.00906481\n",
      "Iteration 25441, loss = 0.00906450\n",
      "Iteration 25442, loss = 0.00906419\n",
      "Iteration 25443, loss = 0.00906388\n",
      "Iteration 25444, loss = 0.00906357\n",
      "Iteration 25445, loss = 0.00906325\n",
      "Iteration 25446, loss = 0.00906294\n",
      "Iteration 25447, loss = 0.00906263\n",
      "Iteration 25448, loss = 0.00906232\n",
      "Iteration 25449, loss = 0.00906201\n",
      "Iteration 25450, loss = 0.00906170\n",
      "Iteration 25451, loss = 0.00906139\n",
      "Iteration 25452, loss = 0.00906108\n",
      "Iteration 25453, loss = 0.00906077\n",
      "Iteration 25454, loss = 0.00906046\n",
      "Iteration 25455, loss = 0.00906015\n",
      "Iteration 25456, loss = 0.00905984\n",
      "Iteration 25457, loss = 0.00905953\n",
      "Iteration 25458, loss = 0.00905922\n",
      "Iteration 25459, loss = 0.00905891\n",
      "Iteration 25460, loss = 0.00905860\n",
      "Iteration 25461, loss = 0.00905829\n",
      "Iteration 25462, loss = 0.00905798\n",
      "Iteration 25463, loss = 0.00905767\n",
      "Iteration 25464, loss = 0.00905736\n",
      "Iteration 25465, loss = 0.00905705\n",
      "Iteration 25466, loss = 0.00905674\n",
      "Iteration 25467, loss = 0.00905643\n",
      "Iteration 25468, loss = 0.00905612\n",
      "Iteration 25469, loss = 0.00905581\n",
      "Iteration 25470, loss = 0.00905550\n",
      "Iteration 25471, loss = 0.00905519\n",
      "Iteration 25472, loss = 0.00905488\n",
      "Iteration 25473, loss = 0.00905457\n",
      "Iteration 25474, loss = 0.00905427\n",
      "Iteration 25475, loss = 0.00905396\n",
      "Iteration 25476, loss = 0.00905365\n",
      "Iteration 25477, loss = 0.00905334\n",
      "Iteration 25478, loss = 0.00905303\n",
      "Iteration 25479, loss = 0.00905272\n",
      "Iteration 25480, loss = 0.00905241\n",
      "Iteration 25481, loss = 0.00905210\n",
      "Iteration 25482, loss = 0.00905179\n",
      "Iteration 25483, loss = 0.00905148\n",
      "Iteration 25484, loss = 0.00905117\n",
      "Iteration 25485, loss = 0.00905086\n",
      "Iteration 25486, loss = 0.00905055\n",
      "Iteration 25487, loss = 0.00905024\n",
      "Iteration 25488, loss = 0.00904994\n",
      "Iteration 25489, loss = 0.00904963\n",
      "Iteration 25490, loss = 0.00904932\n",
      "Iteration 25491, loss = 0.00904901\n",
      "Iteration 25492, loss = 0.00904870\n",
      "Iteration 25493, loss = 0.00904839\n",
      "Iteration 25494, loss = 0.00904808\n",
      "Iteration 25495, loss = 0.00904777\n",
      "Iteration 25496, loss = 0.00904746\n",
      "Iteration 25497, loss = 0.00904716\n",
      "Iteration 25498, loss = 0.00904685\n",
      "Iteration 25499, loss = 0.00904654\n",
      "Iteration 25500, loss = 0.00904623\n",
      "Iteration 25501, loss = 0.00904592\n",
      "Iteration 25502, loss = 0.00904561\n",
      "Iteration 25503, loss = 0.00904530\n",
      "Iteration 25504, loss = 0.00904500\n",
      "Iteration 25505, loss = 0.00904469\n",
      "Iteration 25506, loss = 0.00904438\n",
      "Iteration 25507, loss = 0.00904407\n",
      "Iteration 25508, loss = 0.00904376\n",
      "Iteration 25509, loss = 0.00904345\n",
      "Iteration 25510, loss = 0.00904315\n",
      "Iteration 25511, loss = 0.00904284\n",
      "Iteration 25512, loss = 0.00904253\n",
      "Iteration 25513, loss = 0.00904222\n",
      "Iteration 25514, loss = 0.00904191\n",
      "Iteration 25515, loss = 0.00904160\n",
      "Iteration 25516, loss = 0.00904130\n",
      "Iteration 25517, loss = 0.00904099\n",
      "Iteration 25518, loss = 0.00904068\n",
      "Iteration 25519, loss = 0.00904037\n",
      "Iteration 25520, loss = 0.00904006\n",
      "Iteration 25521, loss = 0.00903976\n",
      "Iteration 25522, loss = 0.00903945\n",
      "Iteration 25523, loss = 0.00903914\n",
      "Iteration 25524, loss = 0.00903883\n",
      "Iteration 25525, loss = 0.00903852\n",
      "Iteration 25526, loss = 0.00903822\n",
      "Iteration 25527, loss = 0.00903791\n",
      "Iteration 25528, loss = 0.00903760\n",
      "Iteration 25529, loss = 0.00903729\n",
      "Iteration 25530, loss = 0.00903699\n",
      "Iteration 25531, loss = 0.00903668\n",
      "Iteration 25532, loss = 0.00903637\n",
      "Iteration 25533, loss = 0.00903606\n",
      "Iteration 25534, loss = 0.00903576\n",
      "Iteration 25535, loss = 0.00903545\n",
      "Iteration 25536, loss = 0.00903514\n",
      "Iteration 25537, loss = 0.00903483\n",
      "Iteration 25538, loss = 0.00903453\n",
      "Iteration 25539, loss = 0.00903422\n",
      "Iteration 25540, loss = 0.00903391\n",
      "Iteration 25541, loss = 0.00903360\n",
      "Iteration 25542, loss = 0.00903330\n",
      "Iteration 25543, loss = 0.00903299\n",
      "Iteration 25544, loss = 0.00903268\n",
      "Iteration 25545, loss = 0.00903238\n",
      "Iteration 25546, loss = 0.00903207\n",
      "Iteration 25547, loss = 0.00903176\n",
      "Iteration 25548, loss = 0.00903145\n",
      "Iteration 25549, loss = 0.00903115\n",
      "Iteration 25550, loss = 0.00903084\n",
      "Iteration 25551, loss = 0.00903053\n",
      "Iteration 25552, loss = 0.00903023\n",
      "Iteration 25553, loss = 0.00902992\n",
      "Iteration 25554, loss = 0.00902961\n",
      "Iteration 25555, loss = 0.00902931\n",
      "Iteration 25556, loss = 0.00902900\n",
      "Iteration 25557, loss = 0.00902869\n",
      "Iteration 25558, loss = 0.00902839\n",
      "Iteration 25559, loss = 0.00902808\n",
      "Iteration 25560, loss = 0.00902777\n",
      "Iteration 25561, loss = 0.00902747\n",
      "Iteration 25562, loss = 0.00902716\n",
      "Iteration 25563, loss = 0.00902685\n",
      "Iteration 25564, loss = 0.00902655\n",
      "Iteration 25565, loss = 0.00902624\n",
      "Iteration 25566, loss = 0.00902593\n",
      "Iteration 25567, loss = 0.00902563\n",
      "Iteration 25568, loss = 0.00902532\n",
      "Iteration 25569, loss = 0.00902502\n",
      "Iteration 25570, loss = 0.00902471\n",
      "Iteration 25571, loss = 0.00902440\n",
      "Iteration 25572, loss = 0.00902410\n",
      "Iteration 25573, loss = 0.00902379\n",
      "Iteration 25574, loss = 0.00902348\n",
      "Iteration 25575, loss = 0.00902318\n",
      "Iteration 25576, loss = 0.00902287\n",
      "Iteration 25577, loss = 0.00902257\n",
      "Iteration 25578, loss = 0.00902226\n",
      "Iteration 25579, loss = 0.00902195\n",
      "Iteration 25580, loss = 0.00902165\n",
      "Iteration 25581, loss = 0.00902134\n",
      "Iteration 25582, loss = 0.00902104\n",
      "Iteration 25583, loss = 0.00902073\n",
      "Iteration 25584, loss = 0.00902042\n",
      "Iteration 25585, loss = 0.00902012\n",
      "Iteration 25586, loss = 0.00901981\n",
      "Iteration 25587, loss = 0.00901951\n",
      "Iteration 25588, loss = 0.00901920\n",
      "Iteration 25589, loss = 0.00901890\n",
      "Iteration 25590, loss = 0.00901859\n",
      "Iteration 25591, loss = 0.00901828\n",
      "Iteration 25592, loss = 0.00901798\n",
      "Iteration 25593, loss = 0.00901767\n",
      "Iteration 25594, loss = 0.00901737\n",
      "Iteration 25595, loss = 0.00901706\n",
      "Iteration 25596, loss = 0.00901676\n",
      "Iteration 25597, loss = 0.00901645\n",
      "Iteration 25598, loss = 0.00901615\n",
      "Iteration 25599, loss = 0.00901584\n",
      "Iteration 25600, loss = 0.00901554\n",
      "Iteration 25601, loss = 0.00901523\n",
      "Iteration 25602, loss = 0.00901493\n",
      "Iteration 25603, loss = 0.00901462\n",
      "Iteration 25604, loss = 0.00901431\n",
      "Iteration 25605, loss = 0.00901401\n",
      "Iteration 25606, loss = 0.00901370\n",
      "Iteration 25607, loss = 0.00901340\n",
      "Iteration 25608, loss = 0.00901309\n",
      "Iteration 25609, loss = 0.00901279\n",
      "Iteration 25610, loss = 0.00901248\n",
      "Iteration 25611, loss = 0.00901218\n",
      "Iteration 25612, loss = 0.00901187\n",
      "Iteration 25613, loss = 0.00901157\n",
      "Iteration 25614, loss = 0.00901127\n",
      "Iteration 25615, loss = 0.00901096\n",
      "Iteration 25616, loss = 0.00901066\n",
      "Iteration 25617, loss = 0.00901035\n",
      "Iteration 25618, loss = 0.00901005\n",
      "Iteration 25619, loss = 0.00900974\n",
      "Iteration 25620, loss = 0.00900944\n",
      "Iteration 25621, loss = 0.00900913\n",
      "Iteration 25622, loss = 0.00900883\n",
      "Iteration 25623, loss = 0.00900852\n",
      "Iteration 25624, loss = 0.00900822\n",
      "Iteration 25625, loss = 0.00900791\n",
      "Iteration 25626, loss = 0.00900761\n",
      "Iteration 25627, loss = 0.00900731\n",
      "Iteration 25628, loss = 0.00900700\n",
      "Iteration 25629, loss = 0.00900670\n",
      "Iteration 25630, loss = 0.00900639\n",
      "Iteration 25631, loss = 0.00900609\n",
      "Iteration 25632, loss = 0.00900578\n",
      "Iteration 25633, loss = 0.00900548\n",
      "Iteration 25634, loss = 0.00900518\n",
      "Iteration 25635, loss = 0.00900487\n",
      "Iteration 25636, loss = 0.00900457\n",
      "Iteration 25637, loss = 0.00900426\n",
      "Iteration 25638, loss = 0.00900396\n",
      "Iteration 25639, loss = 0.00900366\n",
      "Iteration 25640, loss = 0.00900335\n",
      "Iteration 25641, loss = 0.00900305\n",
      "Iteration 25642, loss = 0.00900274\n",
      "Iteration 25643, loss = 0.00900244\n",
      "Iteration 25644, loss = 0.00900214\n",
      "Iteration 25645, loss = 0.00900183\n",
      "Iteration 25646, loss = 0.00900153\n",
      "Iteration 25647, loss = 0.00900122\n",
      "Iteration 25648, loss = 0.00900092\n",
      "Iteration 25649, loss = 0.00900062\n",
      "Iteration 25650, loss = 0.00900031\n",
      "Iteration 25651, loss = 0.00900001\n",
      "Iteration 25652, loss = 0.00899971\n",
      "Iteration 25653, loss = 0.00899940\n",
      "Iteration 25654, loss = 0.00899910\n",
      "Iteration 25655, loss = 0.00899880\n",
      "Iteration 25656, loss = 0.00899849\n",
      "Iteration 25657, loss = 0.00899819\n",
      "Iteration 25658, loss = 0.00899789\n",
      "Iteration 25659, loss = 0.00899758\n",
      "Iteration 25660, loss = 0.00899728\n",
      "Iteration 25661, loss = 0.00899698\n",
      "Iteration 25662, loss = 0.00899667\n",
      "Iteration 25663, loss = 0.00899637\n",
      "Iteration 25664, loss = 0.00899607\n",
      "Iteration 25665, loss = 0.00899576\n",
      "Iteration 25666, loss = 0.00899546\n",
      "Iteration 25667, loss = 0.00899516\n",
      "Iteration 25668, loss = 0.00899485\n",
      "Iteration 25669, loss = 0.00899455\n",
      "Iteration 25670, loss = 0.00899425\n",
      "Iteration 25671, loss = 0.00899394\n",
      "Iteration 25672, loss = 0.00899364\n",
      "Iteration 25673, loss = 0.00899334\n",
      "Iteration 25674, loss = 0.00899304\n",
      "Iteration 25675, loss = 0.00899273\n",
      "Iteration 25676, loss = 0.00899243\n",
      "Iteration 25677, loss = 0.00899213\n",
      "Iteration 25678, loss = 0.00899182\n",
      "Iteration 25679, loss = 0.00899152\n",
      "Iteration 25680, loss = 0.00899122\n",
      "Iteration 25681, loss = 0.00899092\n",
      "Iteration 25682, loss = 0.00899061\n",
      "Iteration 25683, loss = 0.00899031\n",
      "Iteration 25684, loss = 0.00899001\n",
      "Iteration 25685, loss = 0.00898971\n",
      "Iteration 25686, loss = 0.00898940\n",
      "Iteration 25687, loss = 0.00898910\n",
      "Iteration 25688, loss = 0.00898880\n",
      "Iteration 25689, loss = 0.00898850\n",
      "Iteration 25690, loss = 0.00898819\n",
      "Iteration 25691, loss = 0.00898789\n",
      "Iteration 25692, loss = 0.00898759\n",
      "Iteration 25693, loss = 0.00898729\n",
      "Iteration 25694, loss = 0.00898699\n",
      "Iteration 25695, loss = 0.00898668\n",
      "Iteration 25696, loss = 0.00898638\n",
      "Iteration 25697, loss = 0.00898608\n",
      "Iteration 25698, loss = 0.00898578\n",
      "Iteration 25699, loss = 0.00898548\n",
      "Iteration 25700, loss = 0.00898517\n",
      "Iteration 25701, loss = 0.00898487\n",
      "Iteration 25702, loss = 0.00898457\n",
      "Iteration 25703, loss = 0.00898427\n",
      "Iteration 25704, loss = 0.00898397\n",
      "Iteration 25705, loss = 0.00898366\n",
      "Iteration 25706, loss = 0.00898336\n",
      "Iteration 25707, loss = 0.00898306\n",
      "Iteration 25708, loss = 0.00898276\n",
      "Iteration 25709, loss = 0.00898246\n",
      "Iteration 25710, loss = 0.00898216\n",
      "Iteration 25711, loss = 0.00898185\n",
      "Iteration 25712, loss = 0.00898155\n",
      "Iteration 25713, loss = 0.00898125\n",
      "Iteration 25714, loss = 0.00898095\n",
      "Iteration 25715, loss = 0.00898065\n",
      "Iteration 25716, loss = 0.00898035\n",
      "Iteration 25717, loss = 0.00898004\n",
      "Iteration 25718, loss = 0.00897974\n",
      "Iteration 25719, loss = 0.00897944\n",
      "Iteration 25720, loss = 0.00897914\n",
      "Iteration 25721, loss = 0.00897884\n",
      "Iteration 25722, loss = 0.00897854\n",
      "Iteration 25723, loss = 0.00897824\n",
      "Iteration 25724, loss = 0.00897794\n",
      "Iteration 25725, loss = 0.00897763\n",
      "Iteration 25726, loss = 0.00897733\n",
      "Iteration 25727, loss = 0.00897703\n",
      "Iteration 25728, loss = 0.00897673\n",
      "Iteration 25729, loss = 0.00897643\n",
      "Iteration 25730, loss = 0.00897613\n",
      "Iteration 25731, loss = 0.00897583\n",
      "Iteration 25732, loss = 0.00897553\n",
      "Iteration 25733, loss = 0.00897523\n",
      "Iteration 25734, loss = 0.00897492\n",
      "Iteration 25735, loss = 0.00897462\n",
      "Iteration 25736, loss = 0.00897432\n",
      "Iteration 25737, loss = 0.00897402\n",
      "Iteration 25738, loss = 0.00897372\n",
      "Iteration 25739, loss = 0.00897342\n",
      "Iteration 25740, loss = 0.00897312\n",
      "Iteration 25741, loss = 0.00897282\n",
      "Iteration 25742, loss = 0.00897252\n",
      "Iteration 25743, loss = 0.00897222\n",
      "Iteration 25744, loss = 0.00897192\n",
      "Iteration 25745, loss = 0.00897162\n",
      "Iteration 25746, loss = 0.00897132\n",
      "Iteration 25747, loss = 0.00897102\n",
      "Iteration 25748, loss = 0.00897072\n",
      "Iteration 25749, loss = 0.00897042\n",
      "Iteration 25750, loss = 0.00897012\n",
      "Iteration 25751, loss = 0.00896981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25752, loss = 0.00896951\n",
      "Iteration 25753, loss = 0.00896921\n",
      "Iteration 25754, loss = 0.00896891\n",
      "Iteration 25755, loss = 0.00896861\n",
      "Iteration 25756, loss = 0.00896831\n",
      "Iteration 25757, loss = 0.00896801\n",
      "Iteration 25758, loss = 0.00896771\n",
      "Iteration 25759, loss = 0.00896741\n",
      "Iteration 25760, loss = 0.00896711\n",
      "Iteration 25761, loss = 0.00896681\n",
      "Iteration 25762, loss = 0.00896651\n",
      "Iteration 25763, loss = 0.00896621\n",
      "Iteration 25764, loss = 0.00896591\n",
      "Iteration 25765, loss = 0.00896561\n",
      "Iteration 25766, loss = 0.00896531\n",
      "Iteration 25767, loss = 0.00896501\n",
      "Iteration 25768, loss = 0.00896471\n",
      "Iteration 25769, loss = 0.00896441\n",
      "Iteration 25770, loss = 0.00896411\n",
      "Iteration 25771, loss = 0.00896381\n",
      "Iteration 25772, loss = 0.00896352\n",
      "Iteration 25773, loss = 0.00896322\n",
      "Iteration 25774, loss = 0.00896292\n",
      "Iteration 25775, loss = 0.00896262\n",
      "Iteration 25776, loss = 0.00896232\n",
      "Iteration 25777, loss = 0.00896202\n",
      "Iteration 25778, loss = 0.00896172\n",
      "Iteration 25779, loss = 0.00896142\n",
      "Iteration 25780, loss = 0.00896112\n",
      "Iteration 25781, loss = 0.00896082\n",
      "Iteration 25782, loss = 0.00896052\n",
      "Iteration 25783, loss = 0.00896022\n",
      "Iteration 25784, loss = 0.00895992\n",
      "Iteration 25785, loss = 0.00895962\n",
      "Iteration 25786, loss = 0.00895932\n",
      "Iteration 25787, loss = 0.00895902\n",
      "Iteration 25788, loss = 0.00895873\n",
      "Iteration 25789, loss = 0.00895843\n",
      "Iteration 25790, loss = 0.00895813\n",
      "Iteration 25791, loss = 0.00895783\n",
      "Iteration 25792, loss = 0.00895753\n",
      "Iteration 25793, loss = 0.00895723\n",
      "Iteration 25794, loss = 0.00895693\n",
      "Iteration 25795, loss = 0.00895663\n",
      "Iteration 25796, loss = 0.00895633\n",
      "Iteration 25797, loss = 0.00895603\n",
      "Iteration 25798, loss = 0.00895574\n",
      "Iteration 25799, loss = 0.00895544\n",
      "Iteration 25800, loss = 0.00895514\n",
      "Iteration 25801, loss = 0.00895484\n",
      "Iteration 25802, loss = 0.00895454\n",
      "Iteration 25803, loss = 0.00895424\n",
      "Iteration 25804, loss = 0.00895394\n",
      "Iteration 25805, loss = 0.00895364\n",
      "Iteration 25806, loss = 0.00895335\n",
      "Iteration 25807, loss = 0.00895305\n",
      "Iteration 25808, loss = 0.00895275\n",
      "Iteration 25809, loss = 0.00895245\n",
      "Iteration 25810, loss = 0.00895215\n",
      "Iteration 25811, loss = 0.00895185\n",
      "Iteration 25812, loss = 0.00895156\n",
      "Iteration 25813, loss = 0.00895126\n",
      "Iteration 25814, loss = 0.00895096\n",
      "Iteration 25815, loss = 0.00895066\n",
      "Iteration 25816, loss = 0.00895036\n",
      "Iteration 25817, loss = 0.00895006\n",
      "Iteration 25818, loss = 0.00894977\n",
      "Iteration 25819, loss = 0.00894947\n",
      "Iteration 25820, loss = 0.00894917\n",
      "Iteration 25821, loss = 0.00894887\n",
      "Iteration 25822, loss = 0.00894857\n",
      "Iteration 25823, loss = 0.00894828\n",
      "Iteration 25824, loss = 0.00894798\n",
      "Iteration 25825, loss = 0.00894768\n",
      "Iteration 25826, loss = 0.00894738\n",
      "Iteration 25827, loss = 0.00894708\n",
      "Iteration 25828, loss = 0.00894679\n",
      "Iteration 25829, loss = 0.00894649\n",
      "Iteration 25830, loss = 0.00894619\n",
      "Iteration 25831, loss = 0.00894589\n",
      "Iteration 25832, loss = 0.00894559\n",
      "Iteration 25833, loss = 0.00894530\n",
      "Iteration 25834, loss = 0.00894500\n",
      "Iteration 25835, loss = 0.00894470\n",
      "Iteration 25836, loss = 0.00894440\n",
      "Iteration 25837, loss = 0.00894411\n",
      "Iteration 25838, loss = 0.00894381\n",
      "Iteration 25839, loss = 0.00894351\n",
      "Iteration 25840, loss = 0.00894321\n",
      "Iteration 25841, loss = 0.00894292\n",
      "Iteration 25842, loss = 0.00894262\n",
      "Iteration 25843, loss = 0.00894232\n",
      "Iteration 25844, loss = 0.00894202\n",
      "Iteration 25845, loss = 0.00894173\n",
      "Iteration 25846, loss = 0.00894143\n",
      "Iteration 25847, loss = 0.00894113\n",
      "Iteration 25848, loss = 0.00894084\n",
      "Iteration 25849, loss = 0.00894054\n",
      "Iteration 25850, loss = 0.00894024\n",
      "Iteration 25851, loss = 0.00893994\n",
      "Iteration 25852, loss = 0.00893965\n",
      "Iteration 25853, loss = 0.00893935\n",
      "Iteration 25854, loss = 0.00893905\n",
      "Iteration 25855, loss = 0.00893876\n",
      "Iteration 25856, loss = 0.00893846\n",
      "Iteration 25857, loss = 0.00893816\n",
      "Iteration 25858, loss = 0.00893787\n",
      "Iteration 25859, loss = 0.00893757\n",
      "Iteration 25860, loss = 0.00893727\n",
      "Iteration 25861, loss = 0.00893697\n",
      "Iteration 25862, loss = 0.00893668\n",
      "Iteration 25863, loss = 0.00893638\n",
      "Iteration 25864, loss = 0.00893608\n",
      "Iteration 25865, loss = 0.00893579\n",
      "Iteration 25866, loss = 0.00893549\n",
      "Iteration 25867, loss = 0.00893519\n",
      "Iteration 25868, loss = 0.00893490\n",
      "Iteration 25869, loss = 0.00893460\n",
      "Iteration 25870, loss = 0.00893431\n",
      "Iteration 25871, loss = 0.00893401\n",
      "Iteration 25872, loss = 0.00893371\n",
      "Iteration 25873, loss = 0.00893342\n",
      "Iteration 25874, loss = 0.00893312\n",
      "Iteration 25875, loss = 0.00893282\n",
      "Iteration 25876, loss = 0.00893253\n",
      "Iteration 25877, loss = 0.00893223\n",
      "Iteration 25878, loss = 0.00893193\n",
      "Iteration 25879, loss = 0.00893164\n",
      "Iteration 25880, loss = 0.00893134\n",
      "Iteration 25881, loss = 0.00893105\n",
      "Iteration 25882, loss = 0.00893075\n",
      "Iteration 25883, loss = 0.00893045\n",
      "Iteration 25884, loss = 0.00893016\n",
      "Iteration 25885, loss = 0.00892986\n",
      "Iteration 25886, loss = 0.00892957\n",
      "Iteration 25887, loss = 0.00892927\n",
      "Iteration 25888, loss = 0.00892897\n",
      "Iteration 25889, loss = 0.00892868\n",
      "Iteration 25890, loss = 0.00892838\n",
      "Iteration 25891, loss = 0.00892809\n",
      "Iteration 25892, loss = 0.00892779\n",
      "Iteration 25893, loss = 0.00892749\n",
      "Iteration 25894, loss = 0.00892720\n",
      "Iteration 25895, loss = 0.00892690\n",
      "Iteration 25896, loss = 0.00892661\n",
      "Iteration 25897, loss = 0.00892631\n",
      "Iteration 25898, loss = 0.00892602\n",
      "Iteration 25899, loss = 0.00892572\n",
      "Iteration 25900, loss = 0.00892542\n",
      "Iteration 25901, loss = 0.00892513\n",
      "Iteration 25902, loss = 0.00892483\n",
      "Iteration 25903, loss = 0.00892454\n",
      "Iteration 25904, loss = 0.00892424\n",
      "Iteration 25905, loss = 0.00892395\n",
      "Iteration 25906, loss = 0.00892365\n",
      "Iteration 25907, loss = 0.00892336\n",
      "Iteration 25908, loss = 0.00892306\n",
      "Iteration 25909, loss = 0.00892277\n",
      "Iteration 25910, loss = 0.00892247\n",
      "Iteration 25911, loss = 0.00892218\n",
      "Iteration 25912, loss = 0.00892188\n",
      "Iteration 25913, loss = 0.00892159\n",
      "Iteration 25914, loss = 0.00892129\n",
      "Iteration 25915, loss = 0.00892100\n",
      "Iteration 25916, loss = 0.00892070\n",
      "Iteration 25917, loss = 0.00892040\n",
      "Iteration 25918, loss = 0.00892011\n",
      "Iteration 25919, loss = 0.00891981\n",
      "Iteration 25920, loss = 0.00891952\n",
      "Iteration 25921, loss = 0.00891923\n",
      "Iteration 25922, loss = 0.00891893\n",
      "Iteration 25923, loss = 0.00891864\n",
      "Iteration 25924, loss = 0.00891834\n",
      "Iteration 25925, loss = 0.00891805\n",
      "Iteration 25926, loss = 0.00891775\n",
      "Iteration 25927, loss = 0.00891746\n",
      "Iteration 25928, loss = 0.00891716\n",
      "Iteration 25929, loss = 0.00891687\n",
      "Iteration 25930, loss = 0.00891657\n",
      "Iteration 25931, loss = 0.00891628\n",
      "Iteration 25932, loss = 0.00891598\n",
      "Iteration 25933, loss = 0.00891569\n",
      "Iteration 25934, loss = 0.00891539\n",
      "Iteration 25935, loss = 0.00891510\n",
      "Iteration 25936, loss = 0.00891481\n",
      "Iteration 25937, loss = 0.00891451\n",
      "Iteration 25938, loss = 0.00891422\n",
      "Iteration 25939, loss = 0.00891392\n",
      "Iteration 25940, loss = 0.00891363\n",
      "Iteration 25941, loss = 0.00891333\n",
      "Iteration 25942, loss = 0.00891304\n",
      "Iteration 25943, loss = 0.00891275\n",
      "Iteration 25944, loss = 0.00891245\n",
      "Iteration 25945, loss = 0.00891216\n",
      "Iteration 25946, loss = 0.00891186\n",
      "Iteration 25947, loss = 0.00891157\n",
      "Iteration 25948, loss = 0.00891128\n",
      "Iteration 25949, loss = 0.00891098\n",
      "Iteration 25950, loss = 0.00891069\n",
      "Iteration 25951, loss = 0.00891039\n",
      "Iteration 25952, loss = 0.00891010\n",
      "Iteration 25953, loss = 0.00890981\n",
      "Iteration 25954, loss = 0.00890951\n",
      "Iteration 25955, loss = 0.00890922\n",
      "Iteration 25956, loss = 0.00890892\n",
      "Iteration 25957, loss = 0.00890863\n",
      "Iteration 25958, loss = 0.00890834\n",
      "Iteration 25959, loss = 0.00890804\n",
      "Iteration 25960, loss = 0.00890775\n",
      "Iteration 25961, loss = 0.00890746\n",
      "Iteration 25962, loss = 0.00890716\n",
      "Iteration 25963, loss = 0.00890687\n",
      "Iteration 25964, loss = 0.00890657\n",
      "Iteration 25965, loss = 0.00890628\n",
      "Iteration 25966, loss = 0.00890599\n",
      "Iteration 25967, loss = 0.00890569\n",
      "Iteration 25968, loss = 0.00890540\n",
      "Iteration 25969, loss = 0.00890511\n",
      "Iteration 25970, loss = 0.00890481\n",
      "Iteration 25971, loss = 0.00890452\n",
      "Iteration 25972, loss = 0.00890423\n",
      "Iteration 25973, loss = 0.00890393\n",
      "Iteration 25974, loss = 0.00890364\n",
      "Iteration 25975, loss = 0.00890335\n",
      "Iteration 25976, loss = 0.00890305\n",
      "Iteration 25977, loss = 0.00890276\n",
      "Iteration 25978, loss = 0.00890247\n",
      "Iteration 25979, loss = 0.00890218\n",
      "Iteration 25980, loss = 0.00890188\n",
      "Iteration 25981, loss = 0.00890159\n",
      "Iteration 25982, loss = 0.00890130\n",
      "Iteration 25983, loss = 0.00890100\n",
      "Iteration 25984, loss = 0.00890071\n",
      "Iteration 25985, loss = 0.00890042\n",
      "Iteration 25986, loss = 0.00890012\n",
      "Iteration 25987, loss = 0.00889983\n",
      "Iteration 25988, loss = 0.00889954\n",
      "Iteration 25989, loss = 0.00889925\n",
      "Iteration 25990, loss = 0.00889895\n",
      "Iteration 25991, loss = 0.00889866\n",
      "Iteration 25992, loss = 0.00889837\n",
      "Iteration 25993, loss = 0.00889808\n",
      "Iteration 25994, loss = 0.00889778\n",
      "Iteration 25995, loss = 0.00889749\n",
      "Iteration 25996, loss = 0.00889720\n",
      "Iteration 25997, loss = 0.00889691\n",
      "Iteration 25998, loss = 0.00889661\n",
      "Iteration 25999, loss = 0.00889632\n",
      "Iteration 26000, loss = 0.00889603\n",
      "Iteration 26001, loss = 0.00889574\n",
      "Iteration 26002, loss = 0.00889544\n",
      "Iteration 26003, loss = 0.00889515\n",
      "Iteration 26004, loss = 0.00889486\n",
      "Iteration 26005, loss = 0.00889457\n",
      "Iteration 26006, loss = 0.00889427\n",
      "Iteration 26007, loss = 0.00889398\n",
      "Iteration 26008, loss = 0.00889369\n",
      "Iteration 26009, loss = 0.00889340\n",
      "Iteration 26010, loss = 0.00889311\n",
      "Iteration 26011, loss = 0.00889281\n",
      "Iteration 26012, loss = 0.00889252\n",
      "Iteration 26013, loss = 0.00889223\n",
      "Iteration 26014, loss = 0.00889194\n",
      "Iteration 26015, loss = 0.00889165\n",
      "Iteration 26016, loss = 0.00889135\n",
      "Iteration 26017, loss = 0.00889106\n",
      "Iteration 26018, loss = 0.00889077\n",
      "Iteration 26019, loss = 0.00889048\n",
      "Iteration 26020, loss = 0.00889019\n",
      "Iteration 26021, loss = 0.00888990\n",
      "Iteration 26022, loss = 0.00888960\n",
      "Iteration 26023, loss = 0.00888931\n",
      "Iteration 26024, loss = 0.00888902\n",
      "Iteration 26025, loss = 0.00888873\n",
      "Iteration 26026, loss = 0.00888844\n",
      "Iteration 26027, loss = 0.00888815\n",
      "Iteration 26028, loss = 0.00888785\n",
      "Iteration 26029, loss = 0.00888756\n",
      "Iteration 26030, loss = 0.00888727\n",
      "Iteration 26031, loss = 0.00888698\n",
      "Iteration 26032, loss = 0.00888669\n",
      "Iteration 26033, loss = 0.00888640\n",
      "Iteration 26034, loss = 0.00888611\n",
      "Iteration 26035, loss = 0.00888581\n",
      "Iteration 26036, loss = 0.00888552\n",
      "Iteration 26037, loss = 0.00888523\n",
      "Iteration 26038, loss = 0.00888494\n",
      "Iteration 26039, loss = 0.00888465\n",
      "Iteration 26040, loss = 0.00888436\n",
      "Iteration 26041, loss = 0.00888407\n",
      "Iteration 26042, loss = 0.00888378\n",
      "Iteration 26043, loss = 0.00888348\n",
      "Iteration 26044, loss = 0.00888319\n",
      "Iteration 26045, loss = 0.00888290\n",
      "Iteration 26046, loss = 0.00888261\n",
      "Iteration 26047, loss = 0.00888232\n",
      "Iteration 26048, loss = 0.00888203\n",
      "Iteration 26049, loss = 0.00888174\n",
      "Iteration 26050, loss = 0.00888145\n",
      "Iteration 26051, loss = 0.00888116\n",
      "Iteration 26052, loss = 0.00888087\n",
      "Iteration 26053, loss = 0.00888058\n",
      "Iteration 26054, loss = 0.00888028\n",
      "Iteration 26055, loss = 0.00887999\n",
      "Iteration 26056, loss = 0.00887970\n",
      "Iteration 26057, loss = 0.00887941\n",
      "Iteration 26058, loss = 0.00887912\n",
      "Iteration 26059, loss = 0.00887883\n",
      "Iteration 26060, loss = 0.00887854\n",
      "Iteration 26061, loss = 0.00887825\n",
      "Iteration 26062, loss = 0.00887796\n",
      "Iteration 26063, loss = 0.00887767\n",
      "Iteration 26064, loss = 0.00887738\n",
      "Iteration 26065, loss = 0.00887709\n",
      "Iteration 26066, loss = 0.00887680\n",
      "Iteration 26067, loss = 0.00887651\n",
      "Iteration 26068, loss = 0.00887622\n",
      "Iteration 26069, loss = 0.00887593\n",
      "Iteration 26070, loss = 0.00887564\n",
      "Iteration 26071, loss = 0.00887535\n",
      "Iteration 26072, loss = 0.00887506\n",
      "Iteration 26073, loss = 0.00887477\n",
      "Iteration 26074, loss = 0.00887448\n",
      "Iteration 26075, loss = 0.00887419\n",
      "Iteration 26076, loss = 0.00887390\n",
      "Iteration 26077, loss = 0.00887361\n",
      "Iteration 26078, loss = 0.00887332\n",
      "Iteration 26079, loss = 0.00887303\n",
      "Iteration 26080, loss = 0.00887274\n",
      "Iteration 26081, loss = 0.00887245\n",
      "Iteration 26082, loss = 0.00887216\n",
      "Iteration 26083, loss = 0.00887187\n",
      "Iteration 26084, loss = 0.00887158\n",
      "Iteration 26085, loss = 0.00887129\n",
      "Iteration 26086, loss = 0.00887100\n",
      "Iteration 26087, loss = 0.00887071\n",
      "Iteration 26088, loss = 0.00887042\n",
      "Iteration 26089, loss = 0.00887013\n",
      "Iteration 26090, loss = 0.00886984\n",
      "Iteration 26091, loss = 0.00886955\n",
      "Iteration 26092, loss = 0.00886926\n",
      "Iteration 26093, loss = 0.00886897\n",
      "Iteration 26094, loss = 0.00886868\n",
      "Iteration 26095, loss = 0.00886839\n",
      "Iteration 26096, loss = 0.00886810\n",
      "Iteration 26097, loss = 0.00886781\n",
      "Iteration 26098, loss = 0.00886752\n",
      "Iteration 26099, loss = 0.00886723\n",
      "Iteration 26100, loss = 0.00886694\n",
      "Iteration 26101, loss = 0.00886666\n",
      "Iteration 26102, loss = 0.00886637\n",
      "Iteration 26103, loss = 0.00886608\n",
      "Iteration 26104, loss = 0.00886579\n",
      "Iteration 26105, loss = 0.00886550\n",
      "Iteration 26106, loss = 0.00886521\n",
      "Iteration 26107, loss = 0.00886492\n",
      "Iteration 26108, loss = 0.00886463\n",
      "Iteration 26109, loss = 0.00886434\n",
      "Iteration 26110, loss = 0.00886405\n",
      "Iteration 26111, loss = 0.00886376\n",
      "Iteration 26112, loss = 0.00886348\n",
      "Iteration 26113, loss = 0.00886319\n",
      "Iteration 26114, loss = 0.00886290\n",
      "Iteration 26115, loss = 0.00886261\n",
      "Iteration 26116, loss = 0.00886232\n",
      "Iteration 26117, loss = 0.00886203\n",
      "Iteration 26118, loss = 0.00886174\n",
      "Iteration 26119, loss = 0.00886145\n",
      "Iteration 26120, loss = 0.00886117\n",
      "Iteration 26121, loss = 0.00886088\n",
      "Iteration 26122, loss = 0.00886059\n",
      "Iteration 26123, loss = 0.00886030\n",
      "Iteration 26124, loss = 0.00886001\n",
      "Iteration 26125, loss = 0.00885972\n",
      "Iteration 26126, loss = 0.00885943\n",
      "Iteration 26127, loss = 0.00885915\n",
      "Iteration 26128, loss = 0.00885886\n",
      "Iteration 26129, loss = 0.00885857\n",
      "Iteration 26130, loss = 0.00885828\n",
      "Iteration 26131, loss = 0.00885799\n",
      "Iteration 26132, loss = 0.00885770\n",
      "Iteration 26133, loss = 0.00885742\n",
      "Iteration 26134, loss = 0.00885713\n",
      "Iteration 26135, loss = 0.00885684\n",
      "Iteration 26136, loss = 0.00885655\n",
      "Iteration 26137, loss = 0.00885626\n",
      "Iteration 26138, loss = 0.00885597\n",
      "Iteration 26139, loss = 0.00885569\n",
      "Iteration 26140, loss = 0.00885540\n",
      "Iteration 26141, loss = 0.00885511\n",
      "Iteration 26142, loss = 0.00885482\n",
      "Iteration 26143, loss = 0.00885453\n",
      "Iteration 26144, loss = 0.00885425\n",
      "Iteration 26145, loss = 0.00885396\n",
      "Iteration 26146, loss = 0.00885367\n",
      "Iteration 26147, loss = 0.00885338\n",
      "Iteration 26148, loss = 0.00885309\n",
      "Iteration 26149, loss = 0.00885281\n",
      "Iteration 26150, loss = 0.00885252\n",
      "Iteration 26151, loss = 0.00885223\n",
      "Iteration 26152, loss = 0.00885194\n",
      "Iteration 26153, loss = 0.00885166\n",
      "Iteration 26154, loss = 0.00885137\n",
      "Iteration 26155, loss = 0.00885108\n",
      "Iteration 26156, loss = 0.00885079\n",
      "Iteration 26157, loss = 0.00885051\n",
      "Iteration 26158, loss = 0.00885022\n",
      "Iteration 26159, loss = 0.00884993\n",
      "Iteration 26160, loss = 0.00884964\n",
      "Iteration 26161, loss = 0.00884936\n",
      "Iteration 26162, loss = 0.00884907\n",
      "Iteration 26163, loss = 0.00884878\n",
      "Iteration 26164, loss = 0.00884849\n",
      "Iteration 26165, loss = 0.00884821\n",
      "Iteration 26166, loss = 0.00884792\n",
      "Iteration 26167, loss = 0.00884763\n",
      "Iteration 26168, loss = 0.00884734\n",
      "Iteration 26169, loss = 0.00884706\n",
      "Iteration 26170, loss = 0.00884677\n",
      "Iteration 26171, loss = 0.00884648\n",
      "Iteration 26172, loss = 0.00884620\n",
      "Iteration 26173, loss = 0.00884591\n",
      "Iteration 26174, loss = 0.00884562\n",
      "Iteration 26175, loss = 0.00884533\n",
      "Iteration 26176, loss = 0.00884505\n",
      "Iteration 26177, loss = 0.00884476\n",
      "Iteration 26178, loss = 0.00884447\n",
      "Iteration 26179, loss = 0.00884419\n",
      "Iteration 26180, loss = 0.00884390\n",
      "Iteration 26181, loss = 0.00884361\n",
      "Iteration 26182, loss = 0.00884333\n",
      "Iteration 26183, loss = 0.00884304\n",
      "Iteration 26184, loss = 0.00884275\n",
      "Iteration 26185, loss = 0.00884247\n",
      "Iteration 26186, loss = 0.00884218\n",
      "Iteration 26187, loss = 0.00884189\n",
      "Iteration 26188, loss = 0.00884161\n",
      "Iteration 26189, loss = 0.00884132\n",
      "Iteration 26190, loss = 0.00884103\n",
      "Iteration 26191, loss = 0.00884075\n",
      "Iteration 26192, loss = 0.00884046\n",
      "Iteration 26193, loss = 0.00884017\n",
      "Iteration 26194, loss = 0.00883989\n",
      "Iteration 26195, loss = 0.00883960\n",
      "Iteration 26196, loss = 0.00883931\n",
      "Iteration 26197, loss = 0.00883903\n",
      "Iteration 26198, loss = 0.00883874\n",
      "Iteration 26199, loss = 0.00883846\n",
      "Iteration 26200, loss = 0.00883817\n",
      "Iteration 26201, loss = 0.00883788\n",
      "Iteration 26202, loss = 0.00883760\n",
      "Iteration 26203, loss = 0.00883731\n",
      "Iteration 26204, loss = 0.00883702\n",
      "Iteration 26205, loss = 0.00883674\n",
      "Iteration 26206, loss = 0.00883645\n",
      "Iteration 26207, loss = 0.00883617\n",
      "Iteration 26208, loss = 0.00883588\n",
      "Iteration 26209, loss = 0.00883559\n",
      "Iteration 26210, loss = 0.00883531\n",
      "Iteration 26211, loss = 0.00883502\n",
      "Iteration 26212, loss = 0.00883474\n",
      "Iteration 26213, loss = 0.00883445\n",
      "Iteration 26214, loss = 0.00883417\n",
      "Iteration 26215, loss = 0.00883388\n",
      "Iteration 26216, loss = 0.00883359\n",
      "Iteration 26217, loss = 0.00883331\n",
      "Iteration 26218, loss = 0.00883302\n",
      "Iteration 26219, loss = 0.00883274\n",
      "Iteration 26220, loss = 0.00883245\n",
      "Iteration 26221, loss = 0.00883217\n",
      "Iteration 26222, loss = 0.00883188\n",
      "Iteration 26223, loss = 0.00883159\n",
      "Iteration 26224, loss = 0.00883131\n",
      "Iteration 26225, loss = 0.00883102\n",
      "Iteration 26226, loss = 0.00883074\n",
      "Iteration 26227, loss = 0.00883045\n",
      "Iteration 26228, loss = 0.00883017\n",
      "Iteration 26229, loss = 0.00882988\n",
      "Iteration 26230, loss = 0.00882960\n",
      "Iteration 26231, loss = 0.00882931\n",
      "Iteration 26232, loss = 0.00882903\n",
      "Iteration 26233, loss = 0.00882874\n",
      "Iteration 26234, loss = 0.00882845\n",
      "Iteration 26235, loss = 0.00882817\n",
      "Iteration 26236, loss = 0.00882788\n",
      "Iteration 26237, loss = 0.00882760\n",
      "Iteration 26238, loss = 0.00882731\n",
      "Iteration 26239, loss = 0.00882703\n",
      "Iteration 26240, loss = 0.00882674\n",
      "Iteration 26241, loss = 0.00882646\n",
      "Iteration 26242, loss = 0.00882617\n",
      "Iteration 26243, loss = 0.00882589\n",
      "Iteration 26244, loss = 0.00882560\n",
      "Iteration 26245, loss = 0.00882532\n",
      "Iteration 26246, loss = 0.00882503\n",
      "Iteration 26247, loss = 0.00882475\n",
      "Iteration 26248, loss = 0.00882447\n",
      "Iteration 26249, loss = 0.00882418\n",
      "Iteration 26250, loss = 0.00882390\n",
      "Iteration 26251, loss = 0.00882361\n",
      "Iteration 26252, loss = 0.00882333\n",
      "Iteration 26253, loss = 0.00882304\n",
      "Iteration 26254, loss = 0.00882276\n",
      "Iteration 26255, loss = 0.00882247\n",
      "Iteration 26256, loss = 0.00882219\n",
      "Iteration 26257, loss = 0.00882190\n",
      "Iteration 26258, loss = 0.00882162\n",
      "Iteration 26259, loss = 0.00882133\n",
      "Iteration 26260, loss = 0.00882105\n",
      "Iteration 26261, loss = 0.00882077\n",
      "Iteration 26262, loss = 0.00882048\n",
      "Iteration 26263, loss = 0.00882020\n",
      "Iteration 26264, loss = 0.00881991\n",
      "Iteration 26265, loss = 0.00881963\n",
      "Iteration 26266, loss = 0.00881934\n",
      "Iteration 26267, loss = 0.00881906\n",
      "Iteration 26268, loss = 0.00881878\n",
      "Iteration 26269, loss = 0.00881849\n",
      "Iteration 26270, loss = 0.00881821\n",
      "Iteration 26271, loss = 0.00881792\n",
      "Iteration 26272, loss = 0.00881764\n",
      "Iteration 26273, loss = 0.00881735\n",
      "Iteration 26274, loss = 0.00881707\n",
      "Iteration 26275, loss = 0.00881679\n",
      "Iteration 26276, loss = 0.00881650\n",
      "Iteration 26277, loss = 0.00881622\n",
      "Iteration 26278, loss = 0.00881594\n",
      "Iteration 26279, loss = 0.00881565\n",
      "Iteration 26280, loss = 0.00881537\n",
      "Iteration 26281, loss = 0.00881508\n",
      "Iteration 26282, loss = 0.00881480\n",
      "Iteration 26283, loss = 0.00881452\n",
      "Iteration 26284, loss = 0.00881423\n",
      "Iteration 26285, loss = 0.00881395\n",
      "Iteration 26286, loss = 0.00881367\n",
      "Iteration 26287, loss = 0.00881338\n",
      "Iteration 26288, loss = 0.00881310\n",
      "Iteration 26289, loss = 0.00881281\n",
      "Iteration 26290, loss = 0.00881253\n",
      "Iteration 26291, loss = 0.00881225\n",
      "Iteration 26292, loss = 0.00881196\n",
      "Iteration 26293, loss = 0.00881168\n",
      "Iteration 26294, loss = 0.00881140\n",
      "Iteration 26295, loss = 0.00881111\n",
      "Iteration 26296, loss = 0.00881083\n",
      "Iteration 26297, loss = 0.00881055\n",
      "Iteration 26298, loss = 0.00881026\n",
      "Iteration 26299, loss = 0.00880998\n",
      "Iteration 26300, loss = 0.00880970\n",
      "Iteration 26301, loss = 0.00880941\n",
      "Iteration 26302, loss = 0.00880913\n",
      "Iteration 26303, loss = 0.00880885\n",
      "Iteration 26304, loss = 0.00880856\n",
      "Iteration 26305, loss = 0.00880828\n",
      "Iteration 26306, loss = 0.00880800\n",
      "Iteration 26307, loss = 0.00880772\n",
      "Iteration 26308, loss = 0.00880743\n",
      "Iteration 26309, loss = 0.00880715\n",
      "Iteration 26310, loss = 0.00880687\n",
      "Iteration 26311, loss = 0.00880658\n",
      "Iteration 26312, loss = 0.00880630\n",
      "Iteration 26313, loss = 0.00880602\n",
      "Iteration 26314, loss = 0.00880574\n",
      "Iteration 26315, loss = 0.00880545\n",
      "Iteration 26316, loss = 0.00880517\n",
      "Iteration 26317, loss = 0.00880489\n",
      "Iteration 26318, loss = 0.00880460\n",
      "Iteration 26319, loss = 0.00880432\n",
      "Iteration 26320, loss = 0.00880404\n",
      "Iteration 26321, loss = 0.00880376\n",
      "Iteration 26322, loss = 0.00880347\n",
      "Iteration 26323, loss = 0.00880319\n",
      "Iteration 26324, loss = 0.00880291\n",
      "Iteration 26325, loss = 0.00880263\n",
      "Iteration 26326, loss = 0.00880234\n",
      "Iteration 26327, loss = 0.00880206\n",
      "Iteration 26328, loss = 0.00880178\n",
      "Iteration 26329, loss = 0.00880150\n",
      "Iteration 26330, loss = 0.00880121\n",
      "Iteration 26331, loss = 0.00880093\n",
      "Iteration 26332, loss = 0.00880065\n",
      "Iteration 26333, loss = 0.00880037\n",
      "Iteration 26334, loss = 0.00880008\n",
      "Iteration 26335, loss = 0.00879980\n",
      "Iteration 26336, loss = 0.00879952\n",
      "Iteration 26337, loss = 0.00879924\n",
      "Iteration 26338, loss = 0.00879896\n",
      "Iteration 26339, loss = 0.00879867\n",
      "Iteration 26340, loss = 0.00879839\n",
      "Iteration 26341, loss = 0.00879811\n",
      "Iteration 26342, loss = 0.00879783\n",
      "Iteration 26343, loss = 0.00879755\n",
      "Iteration 26344, loss = 0.00879726\n",
      "Iteration 26345, loss = 0.00879698\n",
      "Iteration 26346, loss = 0.00879670\n",
      "Iteration 26347, loss = 0.00879642\n",
      "Iteration 26348, loss = 0.00879614\n",
      "Iteration 26349, loss = 0.00879586\n",
      "Iteration 26350, loss = 0.00879557\n",
      "Iteration 26351, loss = 0.00879529\n",
      "Iteration 26352, loss = 0.00879501\n",
      "Iteration 26353, loss = 0.00879473\n",
      "Iteration 26354, loss = 0.00879445\n",
      "Iteration 26355, loss = 0.00879417\n",
      "Iteration 26356, loss = 0.00879388\n",
      "Iteration 26357, loss = 0.00879360\n",
      "Iteration 26358, loss = 0.00879332\n",
      "Iteration 26359, loss = 0.00879304\n",
      "Iteration 26360, loss = 0.00879276\n",
      "Iteration 26361, loss = 0.00879248\n",
      "Iteration 26362, loss = 0.00879219\n",
      "Iteration 26363, loss = 0.00879191\n",
      "Iteration 26364, loss = 0.00879163\n",
      "Iteration 26365, loss = 0.00879135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26366, loss = 0.00879107\n",
      "Iteration 26367, loss = 0.00879079\n",
      "Iteration 26368, loss = 0.00879051\n",
      "Iteration 26369, loss = 0.00879023\n",
      "Iteration 26370, loss = 0.00878994\n",
      "Iteration 26371, loss = 0.00878966\n",
      "Iteration 26372, loss = 0.00878938\n",
      "Iteration 26373, loss = 0.00878910\n",
      "Iteration 26374, loss = 0.00878882\n",
      "Iteration 26375, loss = 0.00878854\n",
      "Iteration 26376, loss = 0.00878826\n",
      "Iteration 26377, loss = 0.00878798\n",
      "Iteration 26378, loss = 0.00878770\n",
      "Iteration 26379, loss = 0.00878742\n",
      "Iteration 26380, loss = 0.00878713\n",
      "Iteration 26381, loss = 0.00878685\n",
      "Iteration 26382, loss = 0.00878657\n",
      "Iteration 26383, loss = 0.00878629\n",
      "Iteration 26384, loss = 0.00878601\n",
      "Iteration 26385, loss = 0.00878573\n",
      "Iteration 26386, loss = 0.00878545\n",
      "Iteration 26387, loss = 0.00878517\n",
      "Iteration 26388, loss = 0.00878489\n",
      "Iteration 26389, loss = 0.00878461\n",
      "Iteration 26390, loss = 0.00878433\n",
      "Iteration 26391, loss = 0.00878405\n",
      "Iteration 26392, loss = 0.00878377\n",
      "Iteration 26393, loss = 0.00878349\n",
      "Iteration 26394, loss = 0.00878321\n",
      "Iteration 26395, loss = 0.00878293\n",
      "Iteration 26396, loss = 0.00878265\n",
      "Iteration 26397, loss = 0.00878236\n",
      "Iteration 26398, loss = 0.00878208\n",
      "Iteration 26399, loss = 0.00878180\n",
      "Iteration 26400, loss = 0.00878152\n",
      "Iteration 26401, loss = 0.00878124\n",
      "Iteration 26402, loss = 0.00878096\n",
      "Iteration 26403, loss = 0.00878068\n",
      "Iteration 26404, loss = 0.00878040\n",
      "Iteration 26405, loss = 0.00878012\n",
      "Iteration 26406, loss = 0.00877984\n",
      "Iteration 26407, loss = 0.00877956\n",
      "Iteration 26408, loss = 0.00877928\n",
      "Iteration 26409, loss = 0.00877900\n",
      "Iteration 26410, loss = 0.00877872\n",
      "Iteration 26411, loss = 0.00877844\n",
      "Iteration 26412, loss = 0.00877816\n",
      "Iteration 26413, loss = 0.00877788\n",
      "Iteration 26414, loss = 0.00877760\n",
      "Iteration 26415, loss = 0.00877732\n",
      "Iteration 26416, loss = 0.00877704\n",
      "Iteration 26417, loss = 0.00877676\n",
      "Iteration 26418, loss = 0.00877648\n",
      "Iteration 26419, loss = 0.00877620\n",
      "Iteration 26420, loss = 0.00877592\n",
      "Iteration 26421, loss = 0.00877565\n",
      "Iteration 26422, loss = 0.00877537\n",
      "Iteration 26423, loss = 0.00877509\n",
      "Iteration 26424, loss = 0.00877481\n",
      "Iteration 26425, loss = 0.00877453\n",
      "Iteration 26426, loss = 0.00877425\n",
      "Iteration 26427, loss = 0.00877397\n",
      "Iteration 26428, loss = 0.00877369\n",
      "Iteration 26429, loss = 0.00877341\n",
      "Iteration 26430, loss = 0.00877313\n",
      "Iteration 26431, loss = 0.00877285\n",
      "Iteration 26432, loss = 0.00877257\n",
      "Iteration 26433, loss = 0.00877229\n",
      "Iteration 26434, loss = 0.00877201\n",
      "Iteration 26435, loss = 0.00877173\n",
      "Iteration 26436, loss = 0.00877145\n",
      "Iteration 26437, loss = 0.00877117\n",
      "Iteration 26438, loss = 0.00877090\n",
      "Iteration 26439, loss = 0.00877062\n",
      "Iteration 26440, loss = 0.00877034\n",
      "Iteration 26441, loss = 0.00877006\n",
      "Iteration 26442, loss = 0.00876978\n",
      "Iteration 26443, loss = 0.00876950\n",
      "Iteration 26444, loss = 0.00876922\n",
      "Iteration 26445, loss = 0.00876894\n",
      "Iteration 26446, loss = 0.00876866\n",
      "Iteration 26447, loss = 0.00876838\n",
      "Iteration 26448, loss = 0.00876811\n",
      "Iteration 26449, loss = 0.00876783\n",
      "Iteration 26450, loss = 0.00876755\n",
      "Iteration 26451, loss = 0.00876727\n",
      "Iteration 26452, loss = 0.00876699\n",
      "Iteration 26453, loss = 0.00876671\n",
      "Iteration 26454, loss = 0.00876643\n",
      "Iteration 26455, loss = 0.00876615\n",
      "Iteration 26456, loss = 0.00876588\n",
      "Iteration 26457, loss = 0.00876560\n",
      "Iteration 26458, loss = 0.00876532\n",
      "Iteration 26459, loss = 0.00876504\n",
      "Iteration 26460, loss = 0.00876476\n",
      "Iteration 26461, loss = 0.00876448\n",
      "Iteration 26462, loss = 0.00876420\n",
      "Iteration 26463, loss = 0.00876393\n",
      "Iteration 26464, loss = 0.00876365\n",
      "Iteration 26465, loss = 0.00876337\n",
      "Iteration 26466, loss = 0.00876309\n",
      "Iteration 26467, loss = 0.00876281\n",
      "Iteration 26468, loss = 0.00876253\n",
      "Iteration 26469, loss = 0.00876226\n",
      "Iteration 26470, loss = 0.00876198\n",
      "Iteration 26471, loss = 0.00876170\n",
      "Iteration 26472, loss = 0.00876142\n",
      "Iteration 26473, loss = 0.00876114\n",
      "Iteration 26474, loss = 0.00876087\n",
      "Iteration 26475, loss = 0.00876059\n",
      "Iteration 26476, loss = 0.00876031\n",
      "Iteration 26477, loss = 0.00876003\n",
      "Iteration 26478, loss = 0.00875975\n",
      "Iteration 26479, loss = 0.00875948\n",
      "Iteration 26480, loss = 0.00875920\n",
      "Iteration 26481, loss = 0.00875892\n",
      "Iteration 26482, loss = 0.00875864\n",
      "Iteration 26483, loss = 0.00875836\n",
      "Iteration 26484, loss = 0.00875809\n",
      "Iteration 26485, loss = 0.00875781\n",
      "Iteration 26486, loss = 0.00875753\n",
      "Iteration 26487, loss = 0.00875725\n",
      "Iteration 26488, loss = 0.00875698\n",
      "Iteration 26489, loss = 0.00875670\n",
      "Iteration 26490, loss = 0.00875642\n",
      "Iteration 26491, loss = 0.00875614\n",
      "Iteration 26492, loss = 0.00875587\n",
      "Iteration 26493, loss = 0.00875559\n",
      "Iteration 26494, loss = 0.00875531\n",
      "Iteration 26495, loss = 0.00875503\n",
      "Iteration 26496, loss = 0.00875476\n",
      "Iteration 26497, loss = 0.00875448\n",
      "Iteration 26498, loss = 0.00875420\n",
      "Iteration 26499, loss = 0.00875392\n",
      "Iteration 26500, loss = 0.00875365\n",
      "Iteration 26501, loss = 0.00875337\n",
      "Iteration 26502, loss = 0.00875309\n",
      "Iteration 26503, loss = 0.00875281\n",
      "Iteration 26504, loss = 0.00875254\n",
      "Iteration 26505, loss = 0.00875226\n",
      "Iteration 26506, loss = 0.00875198\n",
      "Iteration 26507, loss = 0.00875171\n",
      "Iteration 26508, loss = 0.00875143\n",
      "Iteration 26509, loss = 0.00875115\n",
      "Iteration 26510, loss = 0.00875087\n",
      "Iteration 26511, loss = 0.00875060\n",
      "Iteration 26512, loss = 0.00875032\n",
      "Iteration 26513, loss = 0.00875004\n",
      "Iteration 26514, loss = 0.00874977\n",
      "Iteration 26515, loss = 0.00874949\n",
      "Iteration 26516, loss = 0.00874921\n",
      "Iteration 26517, loss = 0.00874894\n",
      "Iteration 26518, loss = 0.00874866\n",
      "Iteration 26519, loss = 0.00874838\n",
      "Iteration 26520, loss = 0.00874810\n",
      "Iteration 26521, loss = 0.00874783\n",
      "Iteration 26522, loss = 0.00874755\n",
      "Iteration 26523, loss = 0.00874727\n",
      "Iteration 26524, loss = 0.00874700\n",
      "Iteration 26525, loss = 0.00874672\n",
      "Iteration 26526, loss = 0.00874644\n",
      "Iteration 26527, loss = 0.00874617\n",
      "Iteration 26528, loss = 0.00874589\n",
      "Iteration 26529, loss = 0.00874562\n",
      "Iteration 26530, loss = 0.00874534\n",
      "Iteration 26531, loss = 0.00874506\n",
      "Iteration 26532, loss = 0.00874479\n",
      "Iteration 26533, loss = 0.00874451\n",
      "Iteration 26534, loss = 0.00874423\n",
      "Iteration 26535, loss = 0.00874396\n",
      "Iteration 26536, loss = 0.00874368\n",
      "Iteration 26537, loss = 0.00874340\n",
      "Iteration 26538, loss = 0.00874313\n",
      "Iteration 26539, loss = 0.00874285\n",
      "Iteration 26540, loss = 0.00874258\n",
      "Iteration 26541, loss = 0.00874230\n",
      "Iteration 26542, loss = 0.00874202\n",
      "Iteration 26543, loss = 0.00874175\n",
      "Iteration 26544, loss = 0.00874147\n",
      "Iteration 26545, loss = 0.00874120\n",
      "Iteration 26546, loss = 0.00874092\n",
      "Iteration 26547, loss = 0.00874064\n",
      "Iteration 26548, loss = 0.00874037\n",
      "Iteration 26549, loss = 0.00874009\n",
      "Iteration 26550, loss = 0.00873982\n",
      "Iteration 26551, loss = 0.00873954\n",
      "Iteration 26552, loss = 0.00873926\n",
      "Iteration 26553, loss = 0.00873899\n",
      "Iteration 26554, loss = 0.00873871\n",
      "Iteration 26555, loss = 0.00873844\n",
      "Iteration 26556, loss = 0.00873816\n",
      "Iteration 26557, loss = 0.00873789\n",
      "Iteration 26558, loss = 0.00873761\n",
      "Iteration 26559, loss = 0.00873733\n",
      "Iteration 26560, loss = 0.00873706\n",
      "Iteration 26561, loss = 0.00873678\n",
      "Iteration 26562, loss = 0.00873651\n",
      "Iteration 26563, loss = 0.00873623\n",
      "Iteration 26564, loss = 0.00873596\n",
      "Iteration 26565, loss = 0.00873568\n",
      "Iteration 26566, loss = 0.00873541\n",
      "Iteration 26567, loss = 0.00873513\n",
      "Iteration 26568, loss = 0.00873485\n",
      "Iteration 26569, loss = 0.00873458\n",
      "Iteration 26570, loss = 0.00873430\n",
      "Iteration 26571, loss = 0.00873403\n",
      "Iteration 26572, loss = 0.00873375\n",
      "Iteration 26573, loss = 0.00873348\n",
      "Iteration 26574, loss = 0.00873320\n",
      "Iteration 26575, loss = 0.00873293\n",
      "Iteration 26576, loss = 0.00873265\n",
      "Iteration 26577, loss = 0.00873238\n",
      "Iteration 26578, loss = 0.00873210\n",
      "Iteration 26579, loss = 0.00873183\n",
      "Iteration 26580, loss = 0.00873155\n",
      "Iteration 26581, loss = 0.00873128\n",
      "Iteration 26582, loss = 0.00873100\n",
      "Iteration 26583, loss = 0.00873073\n",
      "Iteration 26584, loss = 0.00873045\n",
      "Iteration 26585, loss = 0.00873018\n",
      "Iteration 26586, loss = 0.00872990\n",
      "Iteration 26587, loss = 0.00872963\n",
      "Iteration 26588, loss = 0.00872935\n",
      "Iteration 26589, loss = 0.00872908\n",
      "Iteration 26590, loss = 0.00872880\n",
      "Iteration 26591, loss = 0.00872853\n",
      "Iteration 26592, loss = 0.00872825\n",
      "Iteration 26593, loss = 0.00872798\n",
      "Iteration 26594, loss = 0.00872770\n",
      "Iteration 26595, loss = 0.00872743\n",
      "Iteration 26596, loss = 0.00872716\n",
      "Iteration 26597, loss = 0.00872688\n",
      "Iteration 26598, loss = 0.00872661\n",
      "Iteration 26599, loss = 0.00872633\n",
      "Iteration 26600, loss = 0.00872606\n",
      "Iteration 26601, loss = 0.00872578\n",
      "Iteration 26602, loss = 0.00872551\n",
      "Iteration 26603, loss = 0.00872523\n",
      "Iteration 26604, loss = 0.00872496\n",
      "Iteration 26605, loss = 0.00872469\n",
      "Iteration 26606, loss = 0.00872441\n",
      "Iteration 26607, loss = 0.00872414\n",
      "Iteration 26608, loss = 0.00872386\n",
      "Iteration 26609, loss = 0.00872359\n",
      "Iteration 26610, loss = 0.00872331\n",
      "Iteration 26611, loss = 0.00872304\n",
      "Iteration 26612, loss = 0.00872277\n",
      "Iteration 26613, loss = 0.00872249\n",
      "Iteration 26614, loss = 0.00872222\n",
      "Iteration 26615, loss = 0.00872194\n",
      "Iteration 26616, loss = 0.00872167\n",
      "Iteration 26617, loss = 0.00872140\n",
      "Iteration 26618, loss = 0.00872112\n",
      "Iteration 26619, loss = 0.00872085\n",
      "Iteration 26620, loss = 0.00872057\n",
      "Iteration 26621, loss = 0.00872030\n",
      "Iteration 26622, loss = 0.00872003\n",
      "Iteration 26623, loss = 0.00871975\n",
      "Iteration 26624, loss = 0.00871948\n",
      "Iteration 26625, loss = 0.00871921\n",
      "Iteration 26626, loss = 0.00871893\n",
      "Iteration 26627, loss = 0.00871866\n",
      "Iteration 26628, loss = 0.00871838\n",
      "Iteration 26629, loss = 0.00871811\n",
      "Iteration 26630, loss = 0.00871784\n",
      "Iteration 26631, loss = 0.00871756\n",
      "Iteration 26632, loss = 0.00871729\n",
      "Iteration 26633, loss = 0.00871702\n",
      "Iteration 26634, loss = 0.00871674\n",
      "Iteration 26635, loss = 0.00871647\n",
      "Iteration 26636, loss = 0.00871620\n",
      "Iteration 26637, loss = 0.00871592\n",
      "Iteration 26638, loss = 0.00871565\n",
      "Iteration 26639, loss = 0.00871538\n",
      "Iteration 26640, loss = 0.00871510\n",
      "Iteration 26641, loss = 0.00871483\n",
      "Iteration 26642, loss = 0.00871456\n",
      "Iteration 26643, loss = 0.00871428\n",
      "Iteration 26644, loss = 0.00871401\n",
      "Iteration 26645, loss = 0.00871374\n",
      "Iteration 26646, loss = 0.00871346\n",
      "Iteration 26647, loss = 0.00871319\n",
      "Iteration 26648, loss = 0.00871292\n",
      "Iteration 26649, loss = 0.00871264\n",
      "Iteration 26650, loss = 0.00871237\n",
      "Iteration 26651, loss = 0.00871210\n",
      "Iteration 26652, loss = 0.00871183\n",
      "Iteration 26653, loss = 0.00871155\n",
      "Iteration 26654, loss = 0.00871128\n",
      "Iteration 26655, loss = 0.00871101\n",
      "Iteration 26656, loss = 0.00871073\n",
      "Iteration 26657, loss = 0.00871046\n",
      "Iteration 26658, loss = 0.00871019\n",
      "Iteration 26659, loss = 0.00870992\n",
      "Iteration 26660, loss = 0.00870964\n",
      "Iteration 26661, loss = 0.00870937\n",
      "Iteration 26662, loss = 0.00870910\n",
      "Iteration 26663, loss = 0.00870882\n",
      "Iteration 26664, loss = 0.00870855\n",
      "Iteration 26665, loss = 0.00870828\n",
      "Iteration 26666, loss = 0.00870801\n",
      "Iteration 26667, loss = 0.00870773\n",
      "Iteration 26668, loss = 0.00870746\n",
      "Iteration 26669, loss = 0.00870719\n",
      "Iteration 26670, loss = 0.00870692\n",
      "Iteration 26671, loss = 0.00870664\n",
      "Iteration 26672, loss = 0.00870637\n",
      "Iteration 26673, loss = 0.00870610\n",
      "Iteration 26674, loss = 0.00870583\n",
      "Iteration 26675, loss = 0.00870555\n",
      "Iteration 26676, loss = 0.00870528\n",
      "Iteration 26677, loss = 0.00870501\n",
      "Iteration 26678, loss = 0.00870474\n",
      "Iteration 26679, loss = 0.00870447\n",
      "Iteration 26680, loss = 0.00870419\n",
      "Iteration 26681, loss = 0.00870392\n",
      "Iteration 26682, loss = 0.00870365\n",
      "Iteration 26683, loss = 0.00870338\n",
      "Iteration 26684, loss = 0.00870311\n",
      "Iteration 26685, loss = 0.00870283\n",
      "Iteration 26686, loss = 0.00870256\n",
      "Iteration 26687, loss = 0.00870229\n",
      "Iteration 26688, loss = 0.00870202\n",
      "Iteration 26689, loss = 0.00870175\n",
      "Iteration 26690, loss = 0.00870147\n",
      "Iteration 26691, loss = 0.00870120\n",
      "Iteration 26692, loss = 0.00870093\n",
      "Iteration 26693, loss = 0.00870066\n",
      "Iteration 26694, loss = 0.00870039\n",
      "Iteration 26695, loss = 0.00870011\n",
      "Iteration 26696, loss = 0.00869984\n",
      "Iteration 26697, loss = 0.00869957\n",
      "Iteration 26698, loss = 0.00869930\n",
      "Iteration 26699, loss = 0.00869903\n",
      "Iteration 26700, loss = 0.00869876\n",
      "Iteration 26701, loss = 0.00869848\n",
      "Iteration 26702, loss = 0.00869821\n",
      "Iteration 26703, loss = 0.00869794\n",
      "Iteration 26704, loss = 0.00869767\n",
      "Iteration 26705, loss = 0.00869740\n",
      "Iteration 26706, loss = 0.00869713\n",
      "Iteration 26707, loss = 0.00869686\n",
      "Iteration 26708, loss = 0.00869658\n",
      "Iteration 26709, loss = 0.00869631\n",
      "Iteration 26710, loss = 0.00869604\n",
      "Iteration 26711, loss = 0.00869577\n",
      "Iteration 26712, loss = 0.00869550\n",
      "Iteration 26713, loss = 0.00869523\n",
      "Iteration 26714, loss = 0.00869496\n",
      "Iteration 26715, loss = 0.00869468\n",
      "Iteration 26716, loss = 0.00869441\n",
      "Iteration 26717, loss = 0.00869414\n",
      "Iteration 26718, loss = 0.00869387\n",
      "Iteration 26719, loss = 0.00869360\n",
      "Iteration 26720, loss = 0.00869333\n",
      "Iteration 26721, loss = 0.00869306\n",
      "Iteration 26722, loss = 0.00869279\n",
      "Iteration 26723, loss = 0.00869252\n",
      "Iteration 26724, loss = 0.00869225\n",
      "Iteration 26725, loss = 0.00869197\n",
      "Iteration 26726, loss = 0.00869170\n",
      "Iteration 26727, loss = 0.00869143\n",
      "Iteration 26728, loss = 0.00869116\n",
      "Iteration 26729, loss = 0.00869089\n",
      "Iteration 26730, loss = 0.00869062\n",
      "Iteration 26731, loss = 0.00869035\n",
      "Iteration 26732, loss = 0.00869008\n",
      "Iteration 26733, loss = 0.00868981\n",
      "Iteration 26734, loss = 0.00868954\n",
      "Iteration 26735, loss = 0.00868927\n",
      "Iteration 26736, loss = 0.00868900\n",
      "Iteration 26737, loss = 0.00868873\n",
      "Iteration 26738, loss = 0.00868846\n",
      "Iteration 26739, loss = 0.00868818\n",
      "Iteration 26740, loss = 0.00868791\n",
      "Iteration 26741, loss = 0.00868764\n",
      "Iteration 26742, loss = 0.00868737\n",
      "Iteration 26743, loss = 0.00868710\n",
      "Iteration 26744, loss = 0.00868683\n",
      "Iteration 26745, loss = 0.00868656\n",
      "Iteration 26746, loss = 0.00868629\n",
      "Iteration 26747, loss = 0.00868602\n",
      "Iteration 26748, loss = 0.00868575\n",
      "Iteration 26749, loss = 0.00868548\n",
      "Iteration 26750, loss = 0.00868521\n",
      "Iteration 26751, loss = 0.00868494\n",
      "Iteration 26752, loss = 0.00868467\n",
      "Iteration 26753, loss = 0.00868440\n",
      "Iteration 26754, loss = 0.00868413\n",
      "Iteration 26755, loss = 0.00868386\n",
      "Iteration 26756, loss = 0.00868359\n",
      "Iteration 26757, loss = 0.00868332\n",
      "Iteration 26758, loss = 0.00868305\n",
      "Iteration 26759, loss = 0.00868278\n",
      "Iteration 26760, loss = 0.00868251\n",
      "Iteration 26761, loss = 0.00868224\n",
      "Iteration 26762, loss = 0.00868197\n",
      "Iteration 26763, loss = 0.00868170\n",
      "Iteration 26764, loss = 0.00868143\n",
      "Iteration 26765, loss = 0.00868116\n",
      "Iteration 26766, loss = 0.00868089\n",
      "Iteration 26767, loss = 0.00868062\n",
      "Iteration 26768, loss = 0.00868035\n",
      "Iteration 26769, loss = 0.00868008\n",
      "Iteration 26770, loss = 0.00867981\n",
      "Iteration 26771, loss = 0.00867954\n",
      "Iteration 26772, loss = 0.00867927\n",
      "Iteration 26773, loss = 0.00867900\n",
      "Iteration 26774, loss = 0.00867873\n",
      "Iteration 26775, loss = 0.00867846\n",
      "Iteration 26776, loss = 0.00867820\n",
      "Iteration 26777, loss = 0.00867793\n",
      "Iteration 26778, loss = 0.00867766\n",
      "Iteration 26779, loss = 0.00867739\n",
      "Iteration 26780, loss = 0.00867712\n",
      "Iteration 26781, loss = 0.00867685\n",
      "Iteration 26782, loss = 0.00867658\n",
      "Iteration 26783, loss = 0.00867631\n",
      "Iteration 26784, loss = 0.00867604\n",
      "Iteration 26785, loss = 0.00867577\n",
      "Iteration 26786, loss = 0.00867550\n",
      "Iteration 26787, loss = 0.00867523\n",
      "Iteration 26788, loss = 0.00867496\n",
      "Iteration 26789, loss = 0.00867469\n",
      "Iteration 26790, loss = 0.00867443\n",
      "Iteration 26791, loss = 0.00867416\n",
      "Iteration 26792, loss = 0.00867389\n",
      "Iteration 26793, loss = 0.00867362\n",
      "Iteration 26794, loss = 0.00867335\n",
      "Iteration 26795, loss = 0.00867308\n",
      "Iteration 26796, loss = 0.00867281\n",
      "Iteration 26797, loss = 0.00867254\n",
      "Iteration 26798, loss = 0.00867227\n",
      "Iteration 26799, loss = 0.00867200\n",
      "Iteration 26800, loss = 0.00867174\n",
      "Iteration 26801, loss = 0.00867147\n",
      "Iteration 26802, loss = 0.00867120\n",
      "Iteration 26803, loss = 0.00867093\n",
      "Iteration 26804, loss = 0.00867066\n",
      "Iteration 26805, loss = 0.00867039\n",
      "Iteration 26806, loss = 0.00867012\n",
      "Iteration 26807, loss = 0.00866986\n",
      "Iteration 26808, loss = 0.00866959\n",
      "Iteration 26809, loss = 0.00866932\n",
      "Iteration 26810, loss = 0.00866905\n",
      "Iteration 26811, loss = 0.00866878\n",
      "Iteration 26812, loss = 0.00866851\n",
      "Iteration 26813, loss = 0.00866824\n",
      "Iteration 26814, loss = 0.00866798\n",
      "Iteration 26815, loss = 0.00866771\n",
      "Iteration 26816, loss = 0.00866744\n",
      "Iteration 26817, loss = 0.00866717\n",
      "Iteration 26818, loss = 0.00866690\n",
      "Iteration 26819, loss = 0.00866663\n",
      "Iteration 26820, loss = 0.00866637\n",
      "Iteration 26821, loss = 0.00866610\n",
      "Iteration 26822, loss = 0.00866583\n",
      "Iteration 26823, loss = 0.00866556\n",
      "Iteration 26824, loss = 0.00866529\n",
      "Iteration 26825, loss = 0.00866502\n",
      "Iteration 26826, loss = 0.00866476\n",
      "Iteration 26827, loss = 0.00866449\n",
      "Iteration 26828, loss = 0.00866422\n",
      "Iteration 26829, loss = 0.00866395\n",
      "Iteration 26830, loss = 0.00866368\n",
      "Iteration 26831, loss = 0.00866342\n",
      "Iteration 26832, loss = 0.00866315\n",
      "Iteration 26833, loss = 0.00866288\n",
      "Iteration 26834, loss = 0.00866261\n",
      "Iteration 26835, loss = 0.00866235\n",
      "Iteration 26836, loss = 0.00866208\n",
      "Iteration 26837, loss = 0.00866181\n",
      "Iteration 26838, loss = 0.00866154\n",
      "Iteration 26839, loss = 0.00866127\n",
      "Iteration 26840, loss = 0.00866101\n",
      "Iteration 26841, loss = 0.00866074\n",
      "Iteration 26842, loss = 0.00866047\n",
      "Iteration 26843, loss = 0.00866020\n",
      "Iteration 26844, loss = 0.00865994\n",
      "Iteration 26845, loss = 0.00865967\n",
      "Iteration 26846, loss = 0.00865940\n",
      "Iteration 26847, loss = 0.00865913\n",
      "Iteration 26848, loss = 0.00865887\n",
      "Iteration 26849, loss = 0.00865860\n",
      "Iteration 26850, loss = 0.00865833\n",
      "Iteration 26851, loss = 0.00865806\n",
      "Iteration 26852, loss = 0.00865780\n",
      "Iteration 26853, loss = 0.00865753\n",
      "Iteration 26854, loss = 0.00865726\n",
      "Iteration 26855, loss = 0.00865699\n",
      "Iteration 26856, loss = 0.00865673\n",
      "Iteration 26857, loss = 0.00865646\n",
      "Iteration 26858, loss = 0.00865619\n",
      "Iteration 26859, loss = 0.00865593\n",
      "Iteration 26860, loss = 0.00865566\n",
      "Iteration 26861, loss = 0.00865539\n",
      "Iteration 26862, loss = 0.00865512\n",
      "Iteration 26863, loss = 0.00865486\n",
      "Iteration 26864, loss = 0.00865459\n",
      "Iteration 26865, loss = 0.00865432\n",
      "Iteration 26866, loss = 0.00865406\n",
      "Iteration 26867, loss = 0.00865379\n",
      "Iteration 26868, loss = 0.00865352\n",
      "Iteration 26869, loss = 0.00865325\n",
      "Iteration 26870, loss = 0.00865299\n",
      "Iteration 26871, loss = 0.00865272\n",
      "Iteration 26872, loss = 0.00865245\n",
      "Iteration 26873, loss = 0.00865219\n",
      "Iteration 26874, loss = 0.00865192\n",
      "Iteration 26875, loss = 0.00865165\n",
      "Iteration 26876, loss = 0.00865139\n",
      "Iteration 26877, loss = 0.00865112\n",
      "Iteration 26878, loss = 0.00865085\n",
      "Iteration 26879, loss = 0.00865059\n",
      "Iteration 26880, loss = 0.00865032\n",
      "Iteration 26881, loss = 0.00865005\n",
      "Iteration 26882, loss = 0.00864979\n",
      "Iteration 26883, loss = 0.00864952\n",
      "Iteration 26884, loss = 0.00864925\n",
      "Iteration 26885, loss = 0.00864899\n",
      "Iteration 26886, loss = 0.00864872\n",
      "Iteration 26887, loss = 0.00864846\n",
      "Iteration 26888, loss = 0.00864819\n",
      "Iteration 26889, loss = 0.00864792\n",
      "Iteration 26890, loss = 0.00864766\n",
      "Iteration 26891, loss = 0.00864739\n",
      "Iteration 26892, loss = 0.00864712\n",
      "Iteration 26893, loss = 0.00864686\n",
      "Iteration 26894, loss = 0.00864659\n",
      "Iteration 26895, loss = 0.00864633\n",
      "Iteration 26896, loss = 0.00864606\n",
      "Iteration 26897, loss = 0.00864579\n",
      "Iteration 26898, loss = 0.00864553\n",
      "Iteration 26899, loss = 0.00864526\n",
      "Iteration 26900, loss = 0.00864499\n",
      "Iteration 26901, loss = 0.00864473\n",
      "Iteration 26902, loss = 0.00864446\n",
      "Iteration 26903, loss = 0.00864420\n",
      "Iteration 26904, loss = 0.00864393\n",
      "Iteration 26905, loss = 0.00864366\n",
      "Iteration 26906, loss = 0.00864340\n",
      "Iteration 26907, loss = 0.00864313\n",
      "Iteration 26908, loss = 0.00864287\n",
      "Iteration 26909, loss = 0.00864260\n",
      "Iteration 26910, loss = 0.00864234\n",
      "Iteration 26911, loss = 0.00864207\n",
      "Iteration 26912, loss = 0.00864180\n",
      "Iteration 26913, loss = 0.00864154\n",
      "Iteration 26914, loss = 0.00864127\n",
      "Iteration 26915, loss = 0.00864101\n",
      "Iteration 26916, loss = 0.00864074\n",
      "Iteration 26917, loss = 0.00864048\n",
      "Iteration 26918, loss = 0.00864021\n",
      "Iteration 26919, loss = 0.00863994\n",
      "Iteration 26920, loss = 0.00863968\n",
      "Iteration 26921, loss = 0.00863941\n",
      "Iteration 26922, loss = 0.00863915\n",
      "Iteration 26923, loss = 0.00863888\n",
      "Iteration 26924, loss = 0.00863862\n",
      "Iteration 26925, loss = 0.00863835\n",
      "Iteration 26926, loss = 0.00863809\n",
      "Iteration 26927, loss = 0.00863782\n",
      "Iteration 26928, loss = 0.00863756\n",
      "Iteration 26929, loss = 0.00863729\n",
      "Iteration 26930, loss = 0.00863703\n",
      "Iteration 26931, loss = 0.00863676\n",
      "Iteration 26932, loss = 0.00863650\n",
      "Iteration 26933, loss = 0.00863623\n",
      "Iteration 26934, loss = 0.00863596\n",
      "Iteration 26935, loss = 0.00863570\n",
      "Iteration 26936, loss = 0.00863543\n",
      "Iteration 26937, loss = 0.00863517\n",
      "Iteration 26938, loss = 0.00863490\n",
      "Iteration 26939, loss = 0.00863464\n",
      "Iteration 26940, loss = 0.00863437\n",
      "Iteration 26941, loss = 0.00863411\n",
      "Iteration 26942, loss = 0.00863384\n",
      "Iteration 26943, loss = 0.00863358\n",
      "Iteration 26944, loss = 0.00863332\n",
      "Iteration 26945, loss = 0.00863305\n",
      "Iteration 26946, loss = 0.00863279\n",
      "Iteration 26947, loss = 0.00863252\n",
      "Iteration 26948, loss = 0.00863226\n",
      "Iteration 26949, loss = 0.00863199\n",
      "Iteration 26950, loss = 0.00863173\n",
      "Iteration 26951, loss = 0.00863146\n",
      "Iteration 26952, loss = 0.00863120\n",
      "Iteration 26953, loss = 0.00863093\n",
      "Iteration 26954, loss = 0.00863067\n",
      "Iteration 26955, loss = 0.00863040\n",
      "Iteration 26956, loss = 0.00863014\n",
      "Iteration 26957, loss = 0.00862987\n",
      "Iteration 26958, loss = 0.00862961\n",
      "Iteration 26959, loss = 0.00862935\n",
      "Iteration 26960, loss = 0.00862908\n",
      "Iteration 26961, loss = 0.00862882\n",
      "Iteration 26962, loss = 0.00862855\n",
      "Iteration 26963, loss = 0.00862829\n",
      "Iteration 26964, loss = 0.00862802\n",
      "Iteration 26965, loss = 0.00862776\n",
      "Iteration 26966, loss = 0.00862750\n",
      "Iteration 26967, loss = 0.00862723\n",
      "Iteration 26968, loss = 0.00862697\n",
      "Iteration 26969, loss = 0.00862670\n",
      "Iteration 26970, loss = 0.00862644\n",
      "Iteration 26971, loss = 0.00862617\n",
      "Iteration 26972, loss = 0.00862591\n",
      "Iteration 26973, loss = 0.00862565\n",
      "Iteration 26974, loss = 0.00862538\n",
      "Iteration 26975, loss = 0.00862512\n",
      "Iteration 26976, loss = 0.00862485\n",
      "Iteration 26977, loss = 0.00862459\n",
      "Iteration 26978, loss = 0.00862433\n",
      "Iteration 26979, loss = 0.00862406\n",
      "Iteration 26980, loss = 0.00862380\n",
      "Iteration 26981, loss = 0.00862353\n",
      "Iteration 26982, loss = 0.00862327\n",
      "Iteration 26983, loss = 0.00862301\n",
      "Iteration 26984, loss = 0.00862274\n",
      "Iteration 26985, loss = 0.00862248\n",
      "Iteration 26986, loss = 0.00862222\n",
      "Iteration 26987, loss = 0.00862195\n",
      "Iteration 26988, loss = 0.00862169\n",
      "Iteration 26989, loss = 0.00862143\n",
      "Iteration 26990, loss = 0.00862116\n",
      "Iteration 26991, loss = 0.00862090\n",
      "Iteration 26992, loss = 0.00862063\n",
      "Iteration 26993, loss = 0.00862037\n",
      "Iteration 26994, loss = 0.00862011\n",
      "Iteration 26995, loss = 0.00861984\n",
      "Iteration 26996, loss = 0.00861958\n",
      "Iteration 26997, loss = 0.00861932\n",
      "Iteration 26998, loss = 0.00861905\n",
      "Iteration 26999, loss = 0.00861879\n",
      "Iteration 27000, loss = 0.00861853\n",
      "Iteration 27001, loss = 0.00861826\n",
      "Iteration 27002, loss = 0.00861800\n",
      "Iteration 27003, loss = 0.00861774\n",
      "Iteration 27004, loss = 0.00861747\n",
      "Iteration 27005, loss = 0.00861721\n",
      "Iteration 27006, loss = 0.00861695\n",
      "Iteration 27007, loss = 0.00861668\n",
      "Iteration 27008, loss = 0.00861642\n",
      "Iteration 27009, loss = 0.00861616\n",
      "Iteration 27010, loss = 0.00861590\n",
      "Iteration 27011, loss = 0.00861563\n",
      "Iteration 27012, loss = 0.00861537\n",
      "Iteration 27013, loss = 0.00861511\n",
      "Iteration 27014, loss = 0.00861484\n",
      "Iteration 27015, loss = 0.00861458\n",
      "Iteration 27016, loss = 0.00861432\n",
      "Iteration 27017, loss = 0.00861405\n",
      "Iteration 27018, loss = 0.00861379\n",
      "Iteration 27019, loss = 0.00861353\n",
      "Iteration 27020, loss = 0.00861327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27021, loss = 0.00861300\n",
      "Iteration 27022, loss = 0.00861274\n",
      "Iteration 27023, loss = 0.00861248\n",
      "Iteration 27024, loss = 0.00861222\n",
      "Iteration 27025, loss = 0.00861195\n",
      "Iteration 27026, loss = 0.00861169\n",
      "Iteration 27027, loss = 0.00861143\n",
      "Iteration 27028, loss = 0.00861116\n",
      "Iteration 27029, loss = 0.00861090\n",
      "Iteration 27030, loss = 0.00861064\n",
      "Iteration 27031, loss = 0.00861038\n",
      "Iteration 27032, loss = 0.00861011\n",
      "Iteration 27033, loss = 0.00860985\n",
      "Iteration 27034, loss = 0.00860959\n",
      "Iteration 27035, loss = 0.00860933\n",
      "Iteration 27036, loss = 0.00860907\n",
      "Iteration 27037, loss = 0.00860880\n",
      "Iteration 27038, loss = 0.00860854\n",
      "Iteration 27039, loss = 0.00860828\n",
      "Iteration 27040, loss = 0.00860802\n",
      "Iteration 27041, loss = 0.00860775\n",
      "Iteration 27042, loss = 0.00860749\n",
      "Iteration 27043, loss = 0.00860723\n",
      "Iteration 27044, loss = 0.00860697\n",
      "Iteration 27045, loss = 0.00860671\n",
      "Iteration 27046, loss = 0.00860644\n",
      "Iteration 27047, loss = 0.00860618\n",
      "Iteration 27048, loss = 0.00860592\n",
      "Iteration 27049, loss = 0.00860566\n",
      "Iteration 27050, loss = 0.00860539\n",
      "Iteration 27051, loss = 0.00860513\n",
      "Iteration 27052, loss = 0.00860487\n",
      "Iteration 27053, loss = 0.00860461\n",
      "Iteration 27054, loss = 0.00860435\n",
      "Iteration 27055, loss = 0.00860409\n",
      "Iteration 27056, loss = 0.00860382\n",
      "Iteration 27057, loss = 0.00860356\n",
      "Iteration 27058, loss = 0.00860330\n",
      "Iteration 27059, loss = 0.00860304\n",
      "Iteration 27060, loss = 0.00860278\n",
      "Iteration 27061, loss = 0.00860251\n",
      "Iteration 27062, loss = 0.00860225\n",
      "Iteration 27063, loss = 0.00860199\n",
      "Iteration 27064, loss = 0.00860173\n",
      "Iteration 27065, loss = 0.00860147\n",
      "Iteration 27066, loss = 0.00860121\n",
      "Iteration 27067, loss = 0.00860095\n",
      "Iteration 27068, loss = 0.00860068\n",
      "Iteration 27069, loss = 0.00860042\n",
      "Iteration 27070, loss = 0.00860016\n",
      "Iteration 27071, loss = 0.00859990\n",
      "Iteration 27072, loss = 0.00859964\n",
      "Iteration 27073, loss = 0.00859938\n",
      "Iteration 27074, loss = 0.00859912\n",
      "Iteration 27075, loss = 0.00859885\n",
      "Iteration 27076, loss = 0.00859859\n",
      "Iteration 27077, loss = 0.00859833\n",
      "Iteration 27078, loss = 0.00859807\n",
      "Iteration 27079, loss = 0.00859781\n",
      "Iteration 27080, loss = 0.00859755\n",
      "Iteration 27081, loss = 0.00859729\n",
      "Iteration 27082, loss = 0.00859703\n",
      "Iteration 27083, loss = 0.00859676\n",
      "Iteration 27084, loss = 0.00859650\n",
      "Iteration 27085, loss = 0.00859624\n",
      "Iteration 27086, loss = 0.00859598\n",
      "Iteration 27087, loss = 0.00859572\n",
      "Iteration 27088, loss = 0.00859546\n",
      "Iteration 27089, loss = 0.00859520\n",
      "Iteration 27090, loss = 0.00859494\n",
      "Iteration 27091, loss = 0.00859468\n",
      "Iteration 27092, loss = 0.00859442\n",
      "Iteration 27093, loss = 0.00859415\n",
      "Iteration 27094, loss = 0.00859389\n",
      "Iteration 27095, loss = 0.00859363\n",
      "Iteration 27096, loss = 0.00859337\n",
      "Iteration 27097, loss = 0.00859311\n",
      "Iteration 27098, loss = 0.00859285\n",
      "Iteration 27099, loss = 0.00859259\n",
      "Iteration 27100, loss = 0.00859233\n",
      "Iteration 27101, loss = 0.00859207\n",
      "Iteration 27102, loss = 0.00859181\n",
      "Iteration 27103, loss = 0.00859155\n",
      "Iteration 27104, loss = 0.00859129\n",
      "Iteration 27105, loss = 0.00859103\n",
      "Iteration 27106, loss = 0.00859077\n",
      "Iteration 27107, loss = 0.00859051\n",
      "Iteration 27108, loss = 0.00859025\n",
      "Iteration 27109, loss = 0.00858999\n",
      "Iteration 27110, loss = 0.00858972\n",
      "Iteration 27111, loss = 0.00858946\n",
      "Iteration 27112, loss = 0.00858920\n",
      "Iteration 27113, loss = 0.00858894\n",
      "Iteration 27114, loss = 0.00858868\n",
      "Iteration 27115, loss = 0.00858842\n",
      "Iteration 27116, loss = 0.00858816\n",
      "Iteration 27117, loss = 0.00858790\n",
      "Iteration 27118, loss = 0.00858764\n",
      "Iteration 27119, loss = 0.00858738\n",
      "Iteration 27120, loss = 0.00858712\n",
      "Iteration 27121, loss = 0.00858686\n",
      "Iteration 27122, loss = 0.00858660\n",
      "Iteration 27123, loss = 0.00858634\n",
      "Iteration 27124, loss = 0.00858608\n",
      "Iteration 27125, loss = 0.00858582\n",
      "Iteration 27126, loss = 0.00858556\n",
      "Iteration 27127, loss = 0.00858530\n",
      "Iteration 27128, loss = 0.00858504\n",
      "Iteration 27129, loss = 0.00858478\n",
      "Iteration 27130, loss = 0.00858452\n",
      "Iteration 27131, loss = 0.00858426\n",
      "Iteration 27132, loss = 0.00858400\n",
      "Iteration 27133, loss = 0.00858374\n",
      "Iteration 27134, loss = 0.00858348\n",
      "Iteration 27135, loss = 0.00858322\n",
      "Iteration 27136, loss = 0.00858296\n",
      "Iteration 27137, loss = 0.00858270\n",
      "Iteration 27138, loss = 0.00858244\n",
      "Iteration 27139, loss = 0.00858219\n",
      "Iteration 27140, loss = 0.00858193\n",
      "Iteration 27141, loss = 0.00858167\n",
      "Iteration 27142, loss = 0.00858141\n",
      "Iteration 27143, loss = 0.00858115\n",
      "Iteration 27144, loss = 0.00858089\n",
      "Iteration 27145, loss = 0.00858063\n",
      "Iteration 27146, loss = 0.00858037\n",
      "Iteration 27147, loss = 0.00858011\n",
      "Iteration 27148, loss = 0.00857985\n",
      "Iteration 27149, loss = 0.00857959\n",
      "Iteration 27150, loss = 0.00857933\n",
      "Iteration 27151, loss = 0.00857907\n",
      "Iteration 27152, loss = 0.00857881\n",
      "Iteration 27153, loss = 0.00857855\n",
      "Iteration 27154, loss = 0.00857829\n",
      "Iteration 27155, loss = 0.00857804\n",
      "Iteration 27156, loss = 0.00857778\n",
      "Iteration 27157, loss = 0.00857752\n",
      "Iteration 27158, loss = 0.00857726\n",
      "Iteration 27159, loss = 0.00857700\n",
      "Iteration 27160, loss = 0.00857674\n",
      "Iteration 27161, loss = 0.00857648\n",
      "Iteration 27162, loss = 0.00857622\n",
      "Iteration 27163, loss = 0.00857596\n",
      "Iteration 27164, loss = 0.00857570\n",
      "Iteration 27165, loss = 0.00857544\n",
      "Iteration 27166, loss = 0.00857519\n",
      "Iteration 27167, loss = 0.00857493\n",
      "Iteration 27168, loss = 0.00857467\n",
      "Iteration 27169, loss = 0.00857441\n",
      "Iteration 27170, loss = 0.00857415\n",
      "Iteration 27171, loss = 0.00857389\n",
      "Iteration 27172, loss = 0.00857363\n",
      "Iteration 27173, loss = 0.00857337\n",
      "Iteration 27174, loss = 0.00857312\n",
      "Iteration 27175, loss = 0.00857286\n",
      "Iteration 27176, loss = 0.00857260\n",
      "Iteration 27177, loss = 0.00857234\n",
      "Iteration 27178, loss = 0.00857208\n",
      "Iteration 27179, loss = 0.00857182\n",
      "Iteration 27180, loss = 0.00857156\n",
      "Iteration 27181, loss = 0.00857131\n",
      "Iteration 27182, loss = 0.00857105\n",
      "Iteration 27183, loss = 0.00857079\n",
      "Iteration 27184, loss = 0.00857053\n",
      "Iteration 27185, loss = 0.00857027\n",
      "Iteration 27186, loss = 0.00857001\n",
      "Iteration 27187, loss = 0.00856976\n",
      "Iteration 27188, loss = 0.00856950\n",
      "Iteration 27189, loss = 0.00856924\n",
      "Iteration 27190, loss = 0.00856898\n",
      "Iteration 27191, loss = 0.00856872\n",
      "Iteration 27192, loss = 0.00856846\n",
      "Iteration 27193, loss = 0.00856821\n",
      "Iteration 27194, loss = 0.00856795\n",
      "Iteration 27195, loss = 0.00856769\n",
      "Iteration 27196, loss = 0.00856743\n",
      "Iteration 27197, loss = 0.00856717\n",
      "Iteration 27198, loss = 0.00856691\n",
      "Iteration 27199, loss = 0.00856666\n",
      "Iteration 27200, loss = 0.00856640\n",
      "Iteration 27201, loss = 0.00856614\n",
      "Iteration 27202, loss = 0.00856588\n",
      "Iteration 27203, loss = 0.00856563\n",
      "Iteration 27204, loss = 0.00856537\n",
      "Iteration 27205, loss = 0.00856511\n",
      "Iteration 27206, loss = 0.00856485\n",
      "Iteration 27207, loss = 0.00856459\n",
      "Iteration 27208, loss = 0.00856434\n",
      "Iteration 27209, loss = 0.00856408\n",
      "Iteration 27210, loss = 0.00856382\n",
      "Iteration 27211, loss = 0.00856356\n",
      "Iteration 27212, loss = 0.00856330\n",
      "Iteration 27213, loss = 0.00856305\n",
      "Iteration 27214, loss = 0.00856279\n",
      "Iteration 27215, loss = 0.00856253\n",
      "Iteration 27216, loss = 0.00856227\n",
      "Iteration 27217, loss = 0.00856202\n",
      "Iteration 27218, loss = 0.00856176\n",
      "Iteration 27219, loss = 0.00856150\n",
      "Iteration 27220, loss = 0.00856124\n",
      "Iteration 27221, loss = 0.00856099\n",
      "Iteration 27222, loss = 0.00856073\n",
      "Iteration 27223, loss = 0.00856047\n",
      "Iteration 27224, loss = 0.00856021\n",
      "Iteration 27225, loss = 0.00855996\n",
      "Iteration 27226, loss = 0.00855970\n",
      "Iteration 27227, loss = 0.00855944\n",
      "Iteration 27228, loss = 0.00855919\n",
      "Iteration 27229, loss = 0.00855893\n",
      "Iteration 27230, loss = 0.00855867\n",
      "Iteration 27231, loss = 0.00855841\n",
      "Iteration 27232, loss = 0.00855816\n",
      "Iteration 27233, loss = 0.00855790\n",
      "Iteration 27234, loss = 0.00855764\n",
      "Iteration 27235, loss = 0.00855739\n",
      "Iteration 27236, loss = 0.00855713\n",
      "Iteration 27237, loss = 0.00855687\n",
      "Iteration 27238, loss = 0.00855661\n",
      "Iteration 27239, loss = 0.00855636\n",
      "Iteration 27240, loss = 0.00855610\n",
      "Iteration 27241, loss = 0.00855584\n",
      "Iteration 27242, loss = 0.00855559\n",
      "Iteration 27243, loss = 0.00855533\n",
      "Iteration 27244, loss = 0.00855507\n",
      "Iteration 27245, loss = 0.00855482\n",
      "Iteration 27246, loss = 0.00855456\n",
      "Iteration 27247, loss = 0.00855430\n",
      "Iteration 27248, loss = 0.00855405\n",
      "Iteration 27249, loss = 0.00855379\n",
      "Iteration 27250, loss = 0.00855353\n",
      "Iteration 27251, loss = 0.00855328\n",
      "Iteration 27252, loss = 0.00855302\n",
      "Iteration 27253, loss = 0.00855276\n",
      "Iteration 27254, loss = 0.00855251\n",
      "Iteration 27255, loss = 0.00855225\n",
      "Iteration 27256, loss = 0.00855199\n",
      "Iteration 27257, loss = 0.00855174\n",
      "Iteration 27258, loss = 0.00855148\n",
      "Iteration 27259, loss = 0.00855122\n",
      "Iteration 27260, loss = 0.00855097\n",
      "Iteration 27261, loss = 0.00855071\n",
      "Iteration 27262, loss = 0.00855045\n",
      "Iteration 27263, loss = 0.00855020\n",
      "Iteration 27264, loss = 0.00854994\n",
      "Iteration 27265, loss = 0.00854968\n",
      "Iteration 27266, loss = 0.00854943\n",
      "Iteration 27267, loss = 0.00854917\n",
      "Iteration 27268, loss = 0.00854892\n",
      "Iteration 27269, loss = 0.00854866\n",
      "Iteration 27270, loss = 0.00854840\n",
      "Iteration 27271, loss = 0.00854815\n",
      "Iteration 27272, loss = 0.00854789\n",
      "Iteration 27273, loss = 0.00854764\n",
      "Iteration 27274, loss = 0.00854738\n",
      "Iteration 27275, loss = 0.00854712\n",
      "Iteration 27276, loss = 0.00854687\n",
      "Iteration 27277, loss = 0.00854661\n",
      "Iteration 27278, loss = 0.00854636\n",
      "Iteration 27279, loss = 0.00854610\n",
      "Iteration 27280, loss = 0.00854584\n",
      "Iteration 27281, loss = 0.00854559\n",
      "Iteration 27282, loss = 0.00854533\n",
      "Iteration 27283, loss = 0.00854508\n",
      "Iteration 27284, loss = 0.00854482\n",
      "Iteration 27285, loss = 0.00854456\n",
      "Iteration 27286, loss = 0.00854431\n",
      "Iteration 27287, loss = 0.00854405\n",
      "Iteration 27288, loss = 0.00854380\n",
      "Iteration 27289, loss = 0.00854354\n",
      "Iteration 27290, loss = 0.00854329\n",
      "Iteration 27291, loss = 0.00854303\n",
      "Iteration 27292, loss = 0.00854277\n",
      "Iteration 27293, loss = 0.00854252\n",
      "Iteration 27294, loss = 0.00854226\n",
      "Iteration 27295, loss = 0.00854201\n",
      "Iteration 27296, loss = 0.00854175\n",
      "Iteration 27297, loss = 0.00854150\n",
      "Iteration 27298, loss = 0.00854124\n",
      "Iteration 27299, loss = 0.00854099\n",
      "Iteration 27300, loss = 0.00854073\n",
      "Iteration 27301, loss = 0.00854048\n",
      "Iteration 27302, loss = 0.00854022\n",
      "Iteration 27303, loss = 0.00853996\n",
      "Iteration 27304, loss = 0.00853971\n",
      "Iteration 27305, loss = 0.00853945\n",
      "Iteration 27306, loss = 0.00853920\n",
      "Iteration 27307, loss = 0.00853894\n",
      "Iteration 27308, loss = 0.00853869\n",
      "Iteration 27309, loss = 0.00853843\n",
      "Iteration 27310, loss = 0.00853818\n",
      "Iteration 27311, loss = 0.00853792\n",
      "Iteration 27312, loss = 0.00853767\n",
      "Iteration 27313, loss = 0.00853741\n",
      "Iteration 27314, loss = 0.00853716\n",
      "Iteration 27315, loss = 0.00853690\n",
      "Iteration 27316, loss = 0.00853665\n",
      "Iteration 27317, loss = 0.00853639\n",
      "Iteration 27318, loss = 0.00853614\n",
      "Iteration 27319, loss = 0.00853588\n",
      "Iteration 27320, loss = 0.00853563\n",
      "Iteration 27321, loss = 0.00853537\n",
      "Iteration 27322, loss = 0.00853512\n",
      "Iteration 27323, loss = 0.00853486\n",
      "Iteration 27324, loss = 0.00853461\n",
      "Iteration 27325, loss = 0.00853435\n",
      "Iteration 27326, loss = 0.00853410\n",
      "Iteration 27327, loss = 0.00853384\n",
      "Iteration 27328, loss = 0.00853359\n",
      "Iteration 27329, loss = 0.00853334\n",
      "Iteration 27330, loss = 0.00853308\n",
      "Iteration 27331, loss = 0.00853283\n",
      "Iteration 27332, loss = 0.00853257\n",
      "Iteration 27333, loss = 0.00853232\n",
      "Iteration 27334, loss = 0.00853206\n",
      "Iteration 27335, loss = 0.00853181\n",
      "Iteration 27336, loss = 0.00853155\n",
      "Iteration 27337, loss = 0.00853130\n",
      "Iteration 27338, loss = 0.00853104\n",
      "Iteration 27339, loss = 0.00853079\n",
      "Iteration 27340, loss = 0.00853054\n",
      "Iteration 27341, loss = 0.00853028\n",
      "Iteration 27342, loss = 0.00853003\n",
      "Iteration 27343, loss = 0.00852977\n",
      "Iteration 27344, loss = 0.00852952\n",
      "Iteration 27345, loss = 0.00852926\n",
      "Iteration 27346, loss = 0.00852901\n",
      "Iteration 27347, loss = 0.00852876\n",
      "Iteration 27348, loss = 0.00852850\n",
      "Iteration 27349, loss = 0.00852825\n",
      "Iteration 27350, loss = 0.00852799\n",
      "Iteration 27351, loss = 0.00852774\n",
      "Iteration 27352, loss = 0.00852749\n",
      "Iteration 27353, loss = 0.00852723\n",
      "Iteration 27354, loss = 0.00852698\n",
      "Iteration 27355, loss = 0.00852672\n",
      "Iteration 27356, loss = 0.00852647\n",
      "Iteration 27357, loss = 0.00852622\n",
      "Iteration 27358, loss = 0.00852596\n",
      "Iteration 27359, loss = 0.00852571\n",
      "Iteration 27360, loss = 0.00852545\n",
      "Iteration 27361, loss = 0.00852520\n",
      "Iteration 27362, loss = 0.00852495\n",
      "Iteration 27363, loss = 0.00852469\n",
      "Iteration 27364, loss = 0.00852444\n",
      "Iteration 27365, loss = 0.00852419\n",
      "Iteration 27366, loss = 0.00852393\n",
      "Iteration 27367, loss = 0.00852368\n",
      "Iteration 27368, loss = 0.00852342\n",
      "Iteration 27369, loss = 0.00852317\n",
      "Iteration 27370, loss = 0.00852292\n",
      "Iteration 27371, loss = 0.00852266\n",
      "Iteration 27372, loss = 0.00852241\n",
      "Iteration 27373, loss = 0.00852216\n",
      "Iteration 27374, loss = 0.00852190\n",
      "Iteration 27375, loss = 0.00852165\n",
      "Iteration 27376, loss = 0.00852140\n",
      "Iteration 27377, loss = 0.00852114\n",
      "Iteration 27378, loss = 0.00852089\n",
      "Iteration 27379, loss = 0.00852064\n",
      "Iteration 27380, loss = 0.00852038\n",
      "Iteration 27381, loss = 0.00852013\n",
      "Iteration 27382, loss = 0.00851988\n",
      "Iteration 27383, loss = 0.00851962\n",
      "Iteration 27384, loss = 0.00851937\n",
      "Iteration 27385, loss = 0.00851912\n",
      "Iteration 27386, loss = 0.00851886\n",
      "Iteration 27387, loss = 0.00851861\n",
      "Iteration 27388, loss = 0.00851836\n",
      "Iteration 27389, loss = 0.00851810\n",
      "Iteration 27390, loss = 0.00851785\n",
      "Iteration 27391, loss = 0.00851760\n",
      "Iteration 27392, loss = 0.00851734\n",
      "Iteration 27393, loss = 0.00851709\n",
      "Iteration 27394, loss = 0.00851684\n",
      "Iteration 27395, loss = 0.00851659\n",
      "Iteration 27396, loss = 0.00851633\n",
      "Iteration 27397, loss = 0.00851608\n",
      "Iteration 27398, loss = 0.00851583\n",
      "Iteration 27399, loss = 0.00851557\n",
      "Iteration 27400, loss = 0.00851532\n",
      "Iteration 27401, loss = 0.00851507\n",
      "Iteration 27402, loss = 0.00851482\n",
      "Iteration 27403, loss = 0.00851456\n",
      "Iteration 27404, loss = 0.00851431\n",
      "Iteration 27405, loss = 0.00851406\n",
      "Iteration 27406, loss = 0.00851381\n",
      "Iteration 27407, loss = 0.00851355\n",
      "Iteration 27408, loss = 0.00851330\n",
      "Iteration 27409, loss = 0.00851305\n",
      "Iteration 27410, loss = 0.00851279\n",
      "Iteration 27411, loss = 0.00851254\n",
      "Iteration 27412, loss = 0.00851229\n",
      "Iteration 27413, loss = 0.00851204\n",
      "Iteration 27414, loss = 0.00851178\n",
      "Iteration 27415, loss = 0.00851153\n",
      "Iteration 27416, loss = 0.00851128\n",
      "Iteration 27417, loss = 0.00851103\n",
      "Iteration 27418, loss = 0.00851078\n",
      "Iteration 27419, loss = 0.00851052\n",
      "Iteration 27420, loss = 0.00851027\n",
      "Iteration 27421, loss = 0.00851002\n",
      "Iteration 27422, loss = 0.00850977\n",
      "Iteration 27423, loss = 0.00850951\n",
      "Iteration 27424, loss = 0.00850926\n",
      "Iteration 27425, loss = 0.00850901\n",
      "Iteration 27426, loss = 0.00850876\n",
      "Iteration 27427, loss = 0.00850851\n",
      "Iteration 27428, loss = 0.00850825\n",
      "Iteration 27429, loss = 0.00850800\n",
      "Iteration 27430, loss = 0.00850775\n",
      "Iteration 27431, loss = 0.00850750\n",
      "Iteration 27432, loss = 0.00850724\n",
      "Iteration 27433, loss = 0.00850699\n",
      "Iteration 27434, loss = 0.00850674\n",
      "Iteration 27435, loss = 0.00850649\n",
      "Iteration 27436, loss = 0.00850624\n",
      "Iteration 27437, loss = 0.00850599\n",
      "Iteration 27438, loss = 0.00850573\n",
      "Iteration 27439, loss = 0.00850548\n",
      "Iteration 27440, loss = 0.00850523\n",
      "Iteration 27441, loss = 0.00850498\n",
      "Iteration 27442, loss = 0.00850473\n",
      "Iteration 27443, loss = 0.00850447\n",
      "Iteration 27444, loss = 0.00850422\n",
      "Iteration 27445, loss = 0.00850397\n",
      "Iteration 27446, loss = 0.00850372\n",
      "Iteration 27447, loss = 0.00850347\n",
      "Iteration 27448, loss = 0.00850322\n",
      "Iteration 27449, loss = 0.00850296\n",
      "Iteration 27450, loss = 0.00850271\n",
      "Iteration 27451, loss = 0.00850246\n",
      "Iteration 27452, loss = 0.00850221\n",
      "Iteration 27453, loss = 0.00850196\n",
      "Iteration 27454, loss = 0.00850171\n",
      "Iteration 27455, loss = 0.00850146\n",
      "Iteration 27456, loss = 0.00850120\n",
      "Iteration 27457, loss = 0.00850095\n",
      "Iteration 27458, loss = 0.00850070\n",
      "Iteration 27459, loss = 0.00850045\n",
      "Iteration 27460, loss = 0.00850020\n",
      "Iteration 27461, loss = 0.00849995\n",
      "Iteration 27462, loss = 0.00849970\n",
      "Iteration 27463, loss = 0.00849945\n",
      "Iteration 27464, loss = 0.00849919\n",
      "Iteration 27465, loss = 0.00849894\n",
      "Iteration 27466, loss = 0.00849869\n",
      "Iteration 27467, loss = 0.00849844\n",
      "Iteration 27468, loss = 0.00849819\n",
      "Iteration 27469, loss = 0.00849794\n",
      "Iteration 27470, loss = 0.00849769\n",
      "Iteration 27471, loss = 0.00849744\n",
      "Iteration 27472, loss = 0.00849719\n",
      "Iteration 27473, loss = 0.00849693\n",
      "Iteration 27474, loss = 0.00849668\n",
      "Iteration 27475, loss = 0.00849643\n",
      "Iteration 27476, loss = 0.00849618\n",
      "Iteration 27477, loss = 0.00849593\n",
      "Iteration 27478, loss = 0.00849568\n",
      "Iteration 27479, loss = 0.00849543\n",
      "Iteration 27480, loss = 0.00849518\n",
      "Iteration 27481, loss = 0.00849493\n",
      "Iteration 27482, loss = 0.00849468\n",
      "Iteration 27483, loss = 0.00849443\n",
      "Iteration 27484, loss = 0.00849418\n",
      "Iteration 27485, loss = 0.00849392\n",
      "Iteration 27486, loss = 0.00849367\n",
      "Iteration 27487, loss = 0.00849342\n",
      "Iteration 27488, loss = 0.00849317\n",
      "Iteration 27489, loss = 0.00849292\n",
      "Iteration 27490, loss = 0.00849267\n",
      "Iteration 27491, loss = 0.00849242\n",
      "Iteration 27492, loss = 0.00849217\n",
      "Iteration 27493, loss = 0.00849192\n",
      "Iteration 27494, loss = 0.00849167\n",
      "Iteration 27495, loss = 0.00849142\n",
      "Iteration 27496, loss = 0.00849117\n",
      "Iteration 27497, loss = 0.00849092\n",
      "Iteration 27498, loss = 0.00849067\n",
      "Iteration 27499, loss = 0.00849042\n",
      "Iteration 27500, loss = 0.00849017\n",
      "Iteration 27501, loss = 0.00848992\n",
      "Iteration 27502, loss = 0.00848967\n",
      "Iteration 27503, loss = 0.00848942\n",
      "Iteration 27504, loss = 0.00848917\n",
      "Iteration 27505, loss = 0.00848892\n",
      "Iteration 27506, loss = 0.00848867\n",
      "Iteration 27507, loss = 0.00848842\n",
      "Iteration 27508, loss = 0.00848817\n",
      "Iteration 27509, loss = 0.00848792\n",
      "Iteration 27510, loss = 0.00848767\n",
      "Iteration 27511, loss = 0.00848742\n",
      "Iteration 27512, loss = 0.00848717\n",
      "Iteration 27513, loss = 0.00848692\n",
      "Iteration 27514, loss = 0.00848667\n",
      "Iteration 27515, loss = 0.00848642\n",
      "Iteration 27516, loss = 0.00848617\n",
      "Iteration 27517, loss = 0.00848592\n",
      "Iteration 27518, loss = 0.00848567\n",
      "Iteration 27519, loss = 0.00848542\n",
      "Iteration 27520, loss = 0.00848517\n",
      "Iteration 27521, loss = 0.00848492\n",
      "Iteration 27522, loss = 0.00848467\n",
      "Iteration 27523, loss = 0.00848442\n",
      "Iteration 27524, loss = 0.00848417\n",
      "Iteration 27525, loss = 0.00848392\n",
      "Iteration 27526, loss = 0.00848367\n",
      "Iteration 27527, loss = 0.00848342\n",
      "Iteration 27528, loss = 0.00848317\n",
      "Iteration 27529, loss = 0.00848292\n",
      "Iteration 27530, loss = 0.00848267\n",
      "Iteration 27531, loss = 0.00848242\n",
      "Iteration 27532, loss = 0.00848217\n",
      "Iteration 27533, loss = 0.00848192\n",
      "Iteration 27534, loss = 0.00848167\n",
      "Iteration 27535, loss = 0.00848142\n",
      "Iteration 27536, loss = 0.00848117\n",
      "Iteration 27537, loss = 0.00848092\n",
      "Iteration 27538, loss = 0.00848068\n",
      "Iteration 27539, loss = 0.00848043\n",
      "Iteration 27540, loss = 0.00848018\n",
      "Iteration 27541, loss = 0.00847993\n",
      "Iteration 27542, loss = 0.00847968\n",
      "Iteration 27543, loss = 0.00847943\n",
      "Iteration 27544, loss = 0.00847918\n",
      "Iteration 27545, loss = 0.00847893\n",
      "Iteration 27546, loss = 0.00847868\n",
      "Iteration 27547, loss = 0.00847843\n",
      "Iteration 27548, loss = 0.00847818\n",
      "Iteration 27549, loss = 0.00847793\n",
      "Iteration 27550, loss = 0.00847769\n",
      "Iteration 27551, loss = 0.00847744\n",
      "Iteration 27552, loss = 0.00847719\n",
      "Iteration 27553, loss = 0.00847694\n",
      "Iteration 27554, loss = 0.00847669\n",
      "Iteration 27555, loss = 0.00847644\n",
      "Iteration 27556, loss = 0.00847619\n",
      "Iteration 27557, loss = 0.00847594\n",
      "Iteration 27558, loss = 0.00847569\n",
      "Iteration 27559, loss = 0.00847545\n",
      "Iteration 27560, loss = 0.00847520\n",
      "Iteration 27561, loss = 0.00847495\n",
      "Iteration 27562, loss = 0.00847470\n",
      "Iteration 27563, loss = 0.00847445\n",
      "Iteration 27564, loss = 0.00847420\n",
      "Iteration 27565, loss = 0.00847395\n",
      "Iteration 27566, loss = 0.00847370\n",
      "Iteration 27567, loss = 0.00847346\n",
      "Iteration 27568, loss = 0.00847321\n",
      "Iteration 27569, loss = 0.00847296\n",
      "Iteration 27570, loss = 0.00847271\n",
      "Iteration 27571, loss = 0.00847246\n",
      "Iteration 27572, loss = 0.00847221\n",
      "Iteration 27573, loss = 0.00847196\n",
      "Iteration 27574, loss = 0.00847172\n",
      "Iteration 27575, loss = 0.00847147\n",
      "Iteration 27576, loss = 0.00847122\n",
      "Iteration 27577, loss = 0.00847097\n",
      "Iteration 27578, loss = 0.00847072\n",
      "Iteration 27579, loss = 0.00847047\n",
      "Iteration 27580, loss = 0.00847023\n",
      "Iteration 27581, loss = 0.00846998\n",
      "Iteration 27582, loss = 0.00846973\n",
      "Iteration 27583, loss = 0.00846948\n",
      "Iteration 27584, loss = 0.00846923\n",
      "Iteration 27585, loss = 0.00846899\n",
      "Iteration 27586, loss = 0.00846874\n",
      "Iteration 27587, loss = 0.00846849\n",
      "Iteration 27588, loss = 0.00846824\n",
      "Iteration 27589, loss = 0.00846799\n",
      "Iteration 27590, loss = 0.00846774\n",
      "Iteration 27591, loss = 0.00846750\n",
      "Iteration 27592, loss = 0.00846725\n",
      "Iteration 27593, loss = 0.00846700\n",
      "Iteration 27594, loss = 0.00846675\n",
      "Iteration 27595, loss = 0.00846651\n",
      "Iteration 27596, loss = 0.00846626\n",
      "Iteration 27597, loss = 0.00846601\n",
      "Iteration 27598, loss = 0.00846576\n",
      "Iteration 27599, loss = 0.00846551\n",
      "Iteration 27600, loss = 0.00846527\n",
      "Iteration 27601, loss = 0.00846502\n",
      "Iteration 27602, loss = 0.00846477\n",
      "Iteration 27603, loss = 0.00846452\n",
      "Iteration 27604, loss = 0.00846428\n",
      "Iteration 27605, loss = 0.00846403\n",
      "Iteration 27606, loss = 0.00846378\n",
      "Iteration 27607, loss = 0.00846353\n",
      "Iteration 27608, loss = 0.00846328\n",
      "Iteration 27609, loss = 0.00846304\n",
      "Iteration 27610, loss = 0.00846279\n",
      "Iteration 27611, loss = 0.00846254\n",
      "Iteration 27612, loss = 0.00846229\n",
      "Iteration 27613, loss = 0.00846205\n",
      "Iteration 27614, loss = 0.00846180\n",
      "Iteration 27615, loss = 0.00846155\n",
      "Iteration 27616, loss = 0.00846130\n",
      "Iteration 27617, loss = 0.00846106\n",
      "Iteration 27618, loss = 0.00846081\n",
      "Iteration 27619, loss = 0.00846056\n",
      "Iteration 27620, loss = 0.00846032\n",
      "Iteration 27621, loss = 0.00846007\n",
      "Iteration 27622, loss = 0.00845982\n",
      "Iteration 27623, loss = 0.00845957\n",
      "Iteration 27624, loss = 0.00845933\n",
      "Iteration 27625, loss = 0.00845908\n",
      "Iteration 27626, loss = 0.00845883\n",
      "Iteration 27627, loss = 0.00845859\n",
      "Iteration 27628, loss = 0.00845834\n",
      "Iteration 27629, loss = 0.00845809\n",
      "Iteration 27630, loss = 0.00845784\n",
      "Iteration 27631, loss = 0.00845760\n",
      "Iteration 27632, loss = 0.00845735\n",
      "Iteration 27633, loss = 0.00845710\n",
      "Iteration 27634, loss = 0.00845686\n",
      "Iteration 27635, loss = 0.00845661\n",
      "Iteration 27636, loss = 0.00845636\n",
      "Iteration 27637, loss = 0.00845612\n",
      "Iteration 27638, loss = 0.00845587\n",
      "Iteration 27639, loss = 0.00845562\n",
      "Iteration 27640, loss = 0.00845537\n",
      "Iteration 27641, loss = 0.00845513\n",
      "Iteration 27642, loss = 0.00845488\n",
      "Iteration 27643, loss = 0.00845463\n",
      "Iteration 27644, loss = 0.00845439\n",
      "Iteration 27645, loss = 0.00845414\n",
      "Iteration 27646, loss = 0.00845389\n",
      "Iteration 27647, loss = 0.00845365\n",
      "Iteration 27648, loss = 0.00845340\n",
      "Iteration 27649, loss = 0.00845315\n",
      "Iteration 27650, loss = 0.00845291\n",
      "Iteration 27651, loss = 0.00845266\n",
      "Iteration 27652, loss = 0.00845242\n",
      "Iteration 27653, loss = 0.00845217\n",
      "Iteration 27654, loss = 0.00845192\n",
      "Iteration 27655, loss = 0.00845168\n",
      "Iteration 27656, loss = 0.00845143\n",
      "Iteration 27657, loss = 0.00845118\n",
      "Iteration 27658, loss = 0.00845094\n",
      "Iteration 27659, loss = 0.00845069\n",
      "Iteration 27660, loss = 0.00845044\n",
      "Iteration 27661, loss = 0.00845020\n",
      "Iteration 27662, loss = 0.00844995\n",
      "Iteration 27663, loss = 0.00844971\n",
      "Iteration 27664, loss = 0.00844946\n",
      "Iteration 27665, loss = 0.00844921\n",
      "Iteration 27666, loss = 0.00844897\n",
      "Iteration 27667, loss = 0.00844872\n",
      "Iteration 27668, loss = 0.00844847\n",
      "Iteration 27669, loss = 0.00844823\n",
      "Iteration 27670, loss = 0.00844798\n",
      "Iteration 27671, loss = 0.00844774\n",
      "Iteration 27672, loss = 0.00844749\n",
      "Iteration 27673, loss = 0.00844724\n",
      "Iteration 27674, loss = 0.00844700\n",
      "Iteration 27675, loss = 0.00844675\n",
      "Iteration 27676, loss = 0.00844651\n",
      "Iteration 27677, loss = 0.00844626\n",
      "Iteration 27678, loss = 0.00844602\n",
      "Iteration 27679, loss = 0.00844577\n",
      "Iteration 27680, loss = 0.00844552\n",
      "Iteration 27681, loss = 0.00844528\n",
      "Iteration 27682, loss = 0.00844503\n",
      "Iteration 27683, loss = 0.00844479\n",
      "Iteration 27684, loss = 0.00844454\n",
      "Iteration 27685, loss = 0.00844429\n",
      "Iteration 27686, loss = 0.00844405\n",
      "Iteration 27687, loss = 0.00844380\n",
      "Iteration 27688, loss = 0.00844356\n",
      "Iteration 27689, loss = 0.00844331\n",
      "Iteration 27690, loss = 0.00844307\n",
      "Iteration 27691, loss = 0.00844282\n",
      "Iteration 27692, loss = 0.00844258\n",
      "Iteration 27693, loss = 0.00844233\n",
      "Iteration 27694, loss = 0.00844208\n",
      "Iteration 27695, loss = 0.00844184\n",
      "Iteration 27696, loss = 0.00844159\n",
      "Iteration 27697, loss = 0.00844135\n",
      "Iteration 27698, loss = 0.00844110\n",
      "Iteration 27699, loss = 0.00844086\n",
      "Iteration 27700, loss = 0.00844061\n",
      "Iteration 27701, loss = 0.00844037\n",
      "Iteration 27702, loss = 0.00844012\n",
      "Iteration 27703, loss = 0.00843988\n",
      "Iteration 27704, loss = 0.00843963\n",
      "Iteration 27705, loss = 0.00843939\n",
      "Iteration 27706, loss = 0.00843914\n",
      "Iteration 27707, loss = 0.00843890\n",
      "Iteration 27708, loss = 0.00843865\n",
      "Iteration 27709, loss = 0.00843841\n",
      "Iteration 27710, loss = 0.00843816\n",
      "Iteration 27711, loss = 0.00843792\n",
      "Iteration 27712, loss = 0.00843767\n",
      "Iteration 27713, loss = 0.00843743\n",
      "Iteration 27714, loss = 0.00843718\n",
      "Iteration 27715, loss = 0.00843694\n",
      "Iteration 27716, loss = 0.00843669\n",
      "Iteration 27717, loss = 0.00843645\n",
      "Iteration 27718, loss = 0.00843620\n",
      "Iteration 27719, loss = 0.00843596\n",
      "Iteration 27720, loss = 0.00843571\n",
      "Iteration 27721, loss = 0.00843547\n",
      "Iteration 27722, loss = 0.00843522\n",
      "Iteration 27723, loss = 0.00843498\n",
      "Iteration 27724, loss = 0.00843473\n",
      "Iteration 27725, loss = 0.00843449\n",
      "Iteration 27726, loss = 0.00843424\n",
      "Iteration 27727, loss = 0.00843400\n",
      "Iteration 27728, loss = 0.00843375\n",
      "Iteration 27729, loss = 0.00843351\n",
      "Iteration 27730, loss = 0.00843326\n",
      "Iteration 27731, loss = 0.00843302\n",
      "Iteration 27732, loss = 0.00843278\n",
      "Iteration 27733, loss = 0.00843253\n",
      "Iteration 27734, loss = 0.00843229\n",
      "Iteration 27735, loss = 0.00843204\n",
      "Iteration 27736, loss = 0.00843180\n",
      "Iteration 27737, loss = 0.00843155\n",
      "Iteration 27738, loss = 0.00843131\n",
      "Iteration 27739, loss = 0.00843106\n",
      "Iteration 27740, loss = 0.00843082\n",
      "Iteration 27741, loss = 0.00843058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27742, loss = 0.00843033\n",
      "Iteration 27743, loss = 0.00843009\n",
      "Iteration 27744, loss = 0.00842984\n",
      "Iteration 27745, loss = 0.00842960\n",
      "Iteration 27746, loss = 0.00842935\n",
      "Iteration 27747, loss = 0.00842911\n",
      "Iteration 27748, loss = 0.00842887\n",
      "Iteration 27749, loss = 0.00842862\n",
      "Iteration 27750, loss = 0.00842838\n",
      "Iteration 27751, loss = 0.00842813\n",
      "Iteration 27752, loss = 0.00842789\n",
      "Iteration 27753, loss = 0.00842765\n",
      "Iteration 27754, loss = 0.00842740\n",
      "Iteration 27755, loss = 0.00842716\n",
      "Iteration 27756, loss = 0.00842691\n",
      "Iteration 27757, loss = 0.00842667\n",
      "Iteration 27758, loss = 0.00842643\n",
      "Iteration 27759, loss = 0.00842618\n",
      "Iteration 27760, loss = 0.00842594\n",
      "Iteration 27761, loss = 0.00842569\n",
      "Iteration 27762, loss = 0.00842545\n",
      "Iteration 27763, loss = 0.00842521\n",
      "Iteration 27764, loss = 0.00842496\n",
      "Iteration 27765, loss = 0.00842472\n",
      "Iteration 27766, loss = 0.00842448\n",
      "Iteration 27767, loss = 0.00842423\n",
      "Iteration 27768, loss = 0.00842399\n",
      "Iteration 27769, loss = 0.00842374\n",
      "Iteration 27770, loss = 0.00842350\n",
      "Iteration 27771, loss = 0.00842326\n",
      "Iteration 27772, loss = 0.00842301\n",
      "Iteration 27773, loss = 0.00842277\n",
      "Iteration 27774, loss = 0.00842253\n",
      "Iteration 27775, loss = 0.00842228\n",
      "Iteration 27776, loss = 0.00842204\n",
      "Iteration 27777, loss = 0.00842180\n",
      "Iteration 27778, loss = 0.00842155\n",
      "Iteration 27779, loss = 0.00842131\n",
      "Iteration 27780, loss = 0.00842107\n",
      "Iteration 27781, loss = 0.00842082\n",
      "Iteration 27782, loss = 0.00842058\n",
      "Iteration 27783, loss = 0.00842034\n",
      "Iteration 27784, loss = 0.00842009\n",
      "Iteration 27785, loss = 0.00841985\n",
      "Iteration 27786, loss = 0.00841961\n",
      "Iteration 27787, loss = 0.00841936\n",
      "Iteration 27788, loss = 0.00841912\n",
      "Iteration 27789, loss = 0.00841888\n",
      "Iteration 27790, loss = 0.00841863\n",
      "Iteration 27791, loss = 0.00841839\n",
      "Iteration 27792, loss = 0.00841815\n",
      "Iteration 27793, loss = 0.00841790\n",
      "Iteration 27794, loss = 0.00841766\n",
      "Iteration 27795, loss = 0.00841742\n",
      "Iteration 27796, loss = 0.00841718\n",
      "Iteration 27797, loss = 0.00841693\n",
      "Iteration 27798, loss = 0.00841669\n",
      "Iteration 27799, loss = 0.00841645\n",
      "Iteration 27800, loss = 0.00841620\n",
      "Iteration 27801, loss = 0.00841596\n",
      "Iteration 27802, loss = 0.00841572\n",
      "Iteration 27803, loss = 0.00841548\n",
      "Iteration 27804, loss = 0.00841523\n",
      "Iteration 27805, loss = 0.00841499\n",
      "Iteration 27806, loss = 0.00841475\n",
      "Iteration 27807, loss = 0.00841450\n",
      "Iteration 27808, loss = 0.00841426\n",
      "Iteration 27809, loss = 0.00841402\n",
      "Iteration 27810, loss = 0.00841378\n",
      "Iteration 27811, loss = 0.00841353\n",
      "Iteration 27812, loss = 0.00841329\n",
      "Iteration 27813, loss = 0.00841305\n",
      "Iteration 27814, loss = 0.00841281\n",
      "Iteration 27815, loss = 0.00841256\n",
      "Iteration 27816, loss = 0.00841232\n",
      "Iteration 27817, loss = 0.00841208\n",
      "Iteration 27818, loss = 0.00841184\n",
      "Iteration 27819, loss = 0.00841159\n",
      "Iteration 27820, loss = 0.00841135\n",
      "Iteration 27821, loss = 0.00841111\n",
      "Iteration 27822, loss = 0.00841087\n",
      "Iteration 27823, loss = 0.00841062\n",
      "Iteration 27824, loss = 0.00841038\n",
      "Iteration 27825, loss = 0.00841014\n",
      "Iteration 27826, loss = 0.00840990\n",
      "Iteration 27827, loss = 0.00840965\n",
      "Iteration 27828, loss = 0.00840941\n",
      "Iteration 27829, loss = 0.00840917\n",
      "Iteration 27830, loss = 0.00840893\n",
      "Iteration 27831, loss = 0.00840869\n",
      "Iteration 27832, loss = 0.00840844\n",
      "Iteration 27833, loss = 0.00840820\n",
      "Iteration 27834, loss = 0.00840796\n",
      "Iteration 27835, loss = 0.00840772\n",
      "Iteration 27836, loss = 0.00840748\n",
      "Iteration 27837, loss = 0.00840723\n",
      "Iteration 27838, loss = 0.00840699\n",
      "Iteration 27839, loss = 0.00840675\n",
      "Iteration 27840, loss = 0.00840651\n",
      "Iteration 27841, loss = 0.00840627\n",
      "Iteration 27842, loss = 0.00840602\n",
      "Iteration 27843, loss = 0.00840578\n",
      "Iteration 27844, loss = 0.00840554\n",
      "Iteration 27845, loss = 0.00840530\n",
      "Iteration 27846, loss = 0.00840506\n",
      "Iteration 27847, loss = 0.00840482\n",
      "Iteration 27848, loss = 0.00840457\n",
      "Iteration 27849, loss = 0.00840433\n",
      "Iteration 27850, loss = 0.00840409\n",
      "Iteration 27851, loss = 0.00840385\n",
      "Iteration 27852, loss = 0.00840361\n",
      "Iteration 27853, loss = 0.00840337\n",
      "Iteration 27854, loss = 0.00840312\n",
      "Iteration 27855, loss = 0.00840288\n",
      "Iteration 27856, loss = 0.00840264\n",
      "Iteration 27857, loss = 0.00840240\n",
      "Iteration 27858, loss = 0.00840216\n",
      "Iteration 27859, loss = 0.00840192\n",
      "Iteration 27860, loss = 0.00840167\n",
      "Iteration 27861, loss = 0.00840143\n",
      "Iteration 27862, loss = 0.00840119\n",
      "Iteration 27863, loss = 0.00840095\n",
      "Iteration 27864, loss = 0.00840071\n",
      "Iteration 27865, loss = 0.00840047\n",
      "Iteration 27866, loss = 0.00840023\n",
      "Iteration 27867, loss = 0.00839999\n",
      "Iteration 27868, loss = 0.00839974\n",
      "Iteration 27869, loss = 0.00839950\n",
      "Iteration 27870, loss = 0.00839926\n",
      "Iteration 27871, loss = 0.00839902\n",
      "Iteration 27872, loss = 0.00839878\n",
      "Iteration 27873, loss = 0.00839854\n",
      "Iteration 27874, loss = 0.00839830\n",
      "Iteration 27875, loss = 0.00839806\n",
      "Iteration 27876, loss = 0.00839781\n",
      "Iteration 27877, loss = 0.00839757\n",
      "Iteration 27878, loss = 0.00839733\n",
      "Iteration 27879, loss = 0.00839709\n",
      "Iteration 27880, loss = 0.00839685\n",
      "Iteration 27881, loss = 0.00839661\n",
      "Iteration 27882, loss = 0.00839637\n",
      "Iteration 27883, loss = 0.00839613\n",
      "Iteration 27884, loss = 0.00839589\n",
      "Iteration 27885, loss = 0.00839565\n",
      "Iteration 27886, loss = 0.00839541\n",
      "Iteration 27887, loss = 0.00839516\n",
      "Iteration 27888, loss = 0.00839492\n",
      "Iteration 27889, loss = 0.00839468\n",
      "Iteration 27890, loss = 0.00839444\n",
      "Iteration 27891, loss = 0.00839420\n",
      "Iteration 27892, loss = 0.00839396\n",
      "Iteration 27893, loss = 0.00839372\n",
      "Iteration 27894, loss = 0.00839348\n",
      "Iteration 27895, loss = 0.00839324\n",
      "Iteration 27896, loss = 0.00839300\n",
      "Iteration 27897, loss = 0.00839276\n",
      "Iteration 27898, loss = 0.00839252\n",
      "Iteration 27899, loss = 0.00839228\n",
      "Iteration 27900, loss = 0.00839204\n",
      "Iteration 27901, loss = 0.00839180\n",
      "Iteration 27902, loss = 0.00839156\n",
      "Iteration 27903, loss = 0.00839132\n",
      "Iteration 27904, loss = 0.00839107\n",
      "Iteration 27905, loss = 0.00839083\n",
      "Iteration 27906, loss = 0.00839059\n",
      "Iteration 27907, loss = 0.00839035\n",
      "Iteration 27908, loss = 0.00839011\n",
      "Iteration 27909, loss = 0.00838987\n",
      "Iteration 27910, loss = 0.00838963\n",
      "Iteration 27911, loss = 0.00838939\n",
      "Iteration 27912, loss = 0.00838915\n",
      "Iteration 27913, loss = 0.00838891\n",
      "Iteration 27914, loss = 0.00838867\n",
      "Iteration 27915, loss = 0.00838843\n",
      "Iteration 27916, loss = 0.00838819\n",
      "Iteration 27917, loss = 0.00838795\n",
      "Iteration 27918, loss = 0.00838771\n",
      "Iteration 27919, loss = 0.00838747\n",
      "Iteration 27920, loss = 0.00838723\n",
      "Iteration 27921, loss = 0.00838699\n",
      "Iteration 27922, loss = 0.00838675\n",
      "Iteration 27923, loss = 0.00838651\n",
      "Iteration 27924, loss = 0.00838627\n",
      "Iteration 27925, loss = 0.00838603\n",
      "Iteration 27926, loss = 0.00838579\n",
      "Iteration 27927, loss = 0.00838555\n",
      "Iteration 27928, loss = 0.00838531\n",
      "Iteration 27929, loss = 0.00838507\n",
      "Iteration 27930, loss = 0.00838483\n",
      "Iteration 27931, loss = 0.00838459\n",
      "Iteration 27932, loss = 0.00838435\n",
      "Iteration 27933, loss = 0.00838411\n",
      "Iteration 27934, loss = 0.00838387\n",
      "Iteration 27935, loss = 0.00838363\n",
      "Iteration 27936, loss = 0.00838339\n",
      "Iteration 27937, loss = 0.00838316\n",
      "Iteration 27938, loss = 0.00838292\n",
      "Iteration 27939, loss = 0.00838268\n",
      "Iteration 27940, loss = 0.00838244\n",
      "Iteration 27941, loss = 0.00838220\n",
      "Iteration 27942, loss = 0.00838196\n",
      "Iteration 27943, loss = 0.00838172\n",
      "Iteration 27944, loss = 0.00838148\n",
      "Iteration 27945, loss = 0.00838124\n",
      "Iteration 27946, loss = 0.00838100\n",
      "Iteration 27947, loss = 0.00838076\n",
      "Iteration 27948, loss = 0.00838052\n",
      "Iteration 27949, loss = 0.00838028\n",
      "Iteration 27950, loss = 0.00838004\n",
      "Iteration 27951, loss = 0.00837980\n",
      "Iteration 27952, loss = 0.00837956\n",
      "Iteration 27953, loss = 0.00837932\n",
      "Iteration 27954, loss = 0.00837909\n",
      "Iteration 27955, loss = 0.00837885\n",
      "Iteration 27956, loss = 0.00837861\n",
      "Iteration 27957, loss = 0.00837837\n",
      "Iteration 27958, loss = 0.00837813\n",
      "Iteration 27959, loss = 0.00837789\n",
      "Iteration 27960, loss = 0.00837765\n",
      "Iteration 27961, loss = 0.00837741\n",
      "Iteration 27962, loss = 0.00837717\n",
      "Iteration 27963, loss = 0.00837693\n",
      "Iteration 27964, loss = 0.00837669\n",
      "Iteration 27965, loss = 0.00837646\n",
      "Iteration 27966, loss = 0.00837622\n",
      "Iteration 27967, loss = 0.00837598\n",
      "Iteration 27968, loss = 0.00837574\n",
      "Iteration 27969, loss = 0.00837550\n",
      "Iteration 27970, loss = 0.00837526\n",
      "Iteration 27971, loss = 0.00837502\n",
      "Iteration 27972, loss = 0.00837478\n",
      "Iteration 27973, loss = 0.00837455\n",
      "Iteration 27974, loss = 0.00837431\n",
      "Iteration 27975, loss = 0.00837407\n",
      "Iteration 27976, loss = 0.00837383\n",
      "Iteration 27977, loss = 0.00837359\n",
      "Iteration 27978, loss = 0.00837335\n",
      "Iteration 27979, loss = 0.00837311\n",
      "Iteration 27980, loss = 0.00837287\n",
      "Iteration 27981, loss = 0.00837264\n",
      "Iteration 27982, loss = 0.00837240\n",
      "Iteration 27983, loss = 0.00837216\n",
      "Iteration 27984, loss = 0.00837192\n",
      "Iteration 27985, loss = 0.00837168\n",
      "Iteration 27986, loss = 0.00837144\n",
      "Iteration 27987, loss = 0.00837121\n",
      "Iteration 27988, loss = 0.00837097\n",
      "Iteration 27989, loss = 0.00837073\n",
      "Iteration 27990, loss = 0.00837049\n",
      "Iteration 27991, loss = 0.00837025\n",
      "Iteration 27992, loss = 0.00837001\n",
      "Iteration 27993, loss = 0.00836978\n",
      "Iteration 27994, loss = 0.00836954\n",
      "Iteration 27995, loss = 0.00836930\n",
      "Iteration 27996, loss = 0.00836906\n",
      "Iteration 27997, loss = 0.00836882\n",
      "Iteration 27998, loss = 0.00836858\n",
      "Iteration 27999, loss = 0.00836835\n",
      "Iteration 28000, loss = 0.00836811\n",
      "Iteration 28001, loss = 0.00836787\n",
      "Iteration 28002, loss = 0.00836763\n",
      "Iteration 28003, loss = 0.00836739\n",
      "Iteration 28004, loss = 0.00836716\n",
      "Iteration 28005, loss = 0.00836692\n",
      "Iteration 28006, loss = 0.00836668\n",
      "Iteration 28007, loss = 0.00836644\n",
      "Iteration 28008, loss = 0.00836620\n",
      "Iteration 28009, loss = 0.00836597\n",
      "Iteration 28010, loss = 0.00836573\n",
      "Iteration 28011, loss = 0.00836549\n",
      "Iteration 28012, loss = 0.00836525\n",
      "Iteration 28013, loss = 0.00836501\n",
      "Iteration 28014, loss = 0.00836478\n",
      "Iteration 28015, loss = 0.00836454\n",
      "Iteration 28016, loss = 0.00836430\n",
      "Iteration 28017, loss = 0.00836406\n",
      "Iteration 28018, loss = 0.00836383\n",
      "Iteration 28019, loss = 0.00836359\n",
      "Iteration 28020, loss = 0.00836335\n",
      "Iteration 28021, loss = 0.00836311\n",
      "Iteration 28022, loss = 0.00836288\n",
      "Iteration 28023, loss = 0.00836264\n",
      "Iteration 28024, loss = 0.00836240\n",
      "Iteration 28025, loss = 0.00836216\n",
      "Iteration 28026, loss = 0.00836192\n",
      "Iteration 28027, loss = 0.00836169\n",
      "Iteration 28028, loss = 0.00836145\n",
      "Iteration 28029, loss = 0.00836121\n",
      "Iteration 28030, loss = 0.00836098\n",
      "Iteration 28031, loss = 0.00836074\n",
      "Iteration 28032, loss = 0.00836050\n",
      "Iteration 28033, loss = 0.00836026\n",
      "Iteration 28034, loss = 0.00836003\n",
      "Iteration 28035, loss = 0.00835979\n",
      "Iteration 28036, loss = 0.00835955\n",
      "Iteration 28037, loss = 0.00835931\n",
      "Iteration 28038, loss = 0.00835908\n",
      "Iteration 28039, loss = 0.00835884\n",
      "Iteration 28040, loss = 0.00835860\n",
      "Iteration 28041, loss = 0.00835837\n",
      "Iteration 28042, loss = 0.00835813\n",
      "Iteration 28043, loss = 0.00835789\n",
      "Iteration 28044, loss = 0.00835765\n",
      "Iteration 28045, loss = 0.00835742\n",
      "Iteration 28046, loss = 0.00835718\n",
      "Iteration 28047, loss = 0.00835694\n",
      "Iteration 28048, loss = 0.00835671\n",
      "Iteration 28049, loss = 0.00835647\n",
      "Iteration 28050, loss = 0.00835623\n",
      "Iteration 28051, loss = 0.00835599\n",
      "Iteration 28052, loss = 0.00835576\n",
      "Iteration 28053, loss = 0.00835552\n",
      "Iteration 28054, loss = 0.00835528\n",
      "Iteration 28055, loss = 0.00835505\n",
      "Iteration 28056, loss = 0.00835481\n",
      "Iteration 28057, loss = 0.00835457\n",
      "Iteration 28058, loss = 0.00835434\n",
      "Iteration 28059, loss = 0.00835410\n",
      "Iteration 28060, loss = 0.00835386\n",
      "Iteration 28061, loss = 0.00835363\n",
      "Iteration 28062, loss = 0.00835339\n",
      "Iteration 28063, loss = 0.00835315\n",
      "Iteration 28064, loss = 0.00835292\n",
      "Iteration 28065, loss = 0.00835268\n",
      "Iteration 28066, loss = 0.00835244\n",
      "Iteration 28067, loss = 0.00835221\n",
      "Iteration 28068, loss = 0.00835197\n",
      "Iteration 28069, loss = 0.00835173\n",
      "Iteration 28070, loss = 0.00835150\n",
      "Iteration 28071, loss = 0.00835126\n",
      "Iteration 28072, loss = 0.00835102\n",
      "Iteration 28073, loss = 0.00835079\n",
      "Iteration 28074, loss = 0.00835055\n",
      "Iteration 28075, loss = 0.00835032\n",
      "Iteration 28076, loss = 0.00835008\n",
      "Iteration 28077, loss = 0.00834984\n",
      "Iteration 28078, loss = 0.00834961\n",
      "Iteration 28079, loss = 0.00834937\n",
      "Iteration 28080, loss = 0.00834913\n",
      "Iteration 28081, loss = 0.00834890\n",
      "Iteration 28082, loss = 0.00834866\n",
      "Iteration 28083, loss = 0.00834843\n",
      "Iteration 28084, loss = 0.00834819\n",
      "Iteration 28085, loss = 0.00834795\n",
      "Iteration 28086, loss = 0.00834772\n",
      "Iteration 28087, loss = 0.00834748\n",
      "Iteration 28088, loss = 0.00834724\n",
      "Iteration 28089, loss = 0.00834701\n",
      "Iteration 28090, loss = 0.00834677\n",
      "Iteration 28091, loss = 0.00834654\n",
      "Iteration 28092, loss = 0.00834630\n",
      "Iteration 28093, loss = 0.00834606\n",
      "Iteration 28094, loss = 0.00834583\n",
      "Iteration 28095, loss = 0.00834559\n",
      "Iteration 28096, loss = 0.00834536\n",
      "Iteration 28097, loss = 0.00834512\n",
      "Iteration 28098, loss = 0.00834489\n",
      "Iteration 28099, loss = 0.00834465\n",
      "Iteration 28100, loss = 0.00834441\n",
      "Iteration 28101, loss = 0.00834418\n",
      "Iteration 28102, loss = 0.00834394\n",
      "Iteration 28103, loss = 0.00834371\n",
      "Iteration 28104, loss = 0.00834347\n",
      "Iteration 28105, loss = 0.00834324\n",
      "Iteration 28106, loss = 0.00834300\n",
      "Iteration 28107, loss = 0.00834276\n",
      "Iteration 28108, loss = 0.00834253\n",
      "Iteration 28109, loss = 0.00834229\n",
      "Iteration 28110, loss = 0.00834206\n",
      "Iteration 28111, loss = 0.00834182\n",
      "Iteration 28112, loss = 0.00834159\n",
      "Iteration 28113, loss = 0.00834135\n",
      "Iteration 28114, loss = 0.00834112\n",
      "Iteration 28115, loss = 0.00834088\n",
      "Iteration 28116, loss = 0.00834064\n",
      "Iteration 28117, loss = 0.00834041\n",
      "Iteration 28118, loss = 0.00834017\n",
      "Iteration 28119, loss = 0.00833994\n",
      "Iteration 28120, loss = 0.00833970\n",
      "Iteration 28121, loss = 0.00833947\n",
      "Iteration 28122, loss = 0.00833923\n",
      "Iteration 28123, loss = 0.00833900\n",
      "Iteration 28124, loss = 0.00833876\n",
      "Iteration 28125, loss = 0.00833853\n",
      "Iteration 28126, loss = 0.00833829\n",
      "Iteration 28127, loss = 0.00833806\n",
      "Iteration 28128, loss = 0.00833782\n",
      "Iteration 28129, loss = 0.00833759\n",
      "Iteration 28130, loss = 0.00833735\n",
      "Iteration 28131, loss = 0.00833712\n",
      "Iteration 28132, loss = 0.00833688\n",
      "Iteration 28133, loss = 0.00833665\n",
      "Iteration 28134, loss = 0.00833641\n",
      "Iteration 28135, loss = 0.00833618\n",
      "Iteration 28136, loss = 0.00833594\n",
      "Iteration 28137, loss = 0.00833571\n",
      "Iteration 28138, loss = 0.00833547\n",
      "Iteration 28139, loss = 0.00833524\n",
      "Iteration 28140, loss = 0.00833500\n",
      "Iteration 28141, loss = 0.00833477\n",
      "Iteration 28142, loss = 0.00833453\n",
      "Iteration 28143, loss = 0.00833430\n",
      "Iteration 28144, loss = 0.00833406\n",
      "Iteration 28145, loss = 0.00833383\n",
      "Iteration 28146, loss = 0.00833359\n",
      "Iteration 28147, loss = 0.00833336\n",
      "Iteration 28148, loss = 0.00833312\n",
      "Iteration 28149, loss = 0.00833289\n",
      "Iteration 28150, loss = 0.00833265\n",
      "Iteration 28151, loss = 0.00833242\n",
      "Iteration 28152, loss = 0.00833219\n",
      "Iteration 28153, loss = 0.00833195\n",
      "Iteration 28154, loss = 0.00833172\n",
      "Iteration 28155, loss = 0.00833148\n",
      "Iteration 28156, loss = 0.00833125\n",
      "Iteration 28157, loss = 0.00833101\n",
      "Iteration 28158, loss = 0.00833078\n",
      "Iteration 28159, loss = 0.00833054\n",
      "Iteration 28160, loss = 0.00833031\n",
      "Iteration 28161, loss = 0.00833007\n",
      "Iteration 28162, loss = 0.00832984\n",
      "Iteration 28163, loss = 0.00832961\n",
      "Iteration 28164, loss = 0.00832937\n",
      "Iteration 28165, loss = 0.00832914\n",
      "Iteration 28166, loss = 0.00832890\n",
      "Iteration 28167, loss = 0.00832867\n",
      "Iteration 28168, loss = 0.00832843\n",
      "Iteration 28169, loss = 0.00832820\n",
      "Iteration 28170, loss = 0.00832797\n",
      "Iteration 28171, loss = 0.00832773\n",
      "Iteration 28172, loss = 0.00832750\n",
      "Iteration 28173, loss = 0.00832726\n",
      "Iteration 28174, loss = 0.00832703\n",
      "Iteration 28175, loss = 0.00832680\n",
      "Iteration 28176, loss = 0.00832656\n",
      "Iteration 28177, loss = 0.00832633\n",
      "Iteration 28178, loss = 0.00832609\n",
      "Iteration 28179, loss = 0.00832586\n",
      "Iteration 28180, loss = 0.00832563\n",
      "Iteration 28181, loss = 0.00832539\n",
      "Iteration 28182, loss = 0.00832516\n",
      "Iteration 28183, loss = 0.00832492\n",
      "Iteration 28184, loss = 0.00832469\n",
      "Iteration 28185, loss = 0.00832446\n",
      "Iteration 28186, loss = 0.00832422\n",
      "Iteration 28187, loss = 0.00832399\n",
      "Iteration 28188, loss = 0.00832376\n",
      "Iteration 28189, loss = 0.00832352\n",
      "Iteration 28190, loss = 0.00832329\n",
      "Iteration 28191, loss = 0.00832305\n",
      "Iteration 28192, loss = 0.00832282\n",
      "Iteration 28193, loss = 0.00832259\n",
      "Iteration 28194, loss = 0.00832235\n",
      "Iteration 28195, loss = 0.00832212\n",
      "Iteration 28196, loss = 0.00832189\n",
      "Iteration 28197, loss = 0.00832165\n",
      "Iteration 28198, loss = 0.00832142\n",
      "Iteration 28199, loss = 0.00832119\n",
      "Iteration 28200, loss = 0.00832095\n",
      "Iteration 28201, loss = 0.00832072\n",
      "Iteration 28202, loss = 0.00832049\n",
      "Iteration 28203, loss = 0.00832025\n",
      "Iteration 28204, loss = 0.00832002\n",
      "Iteration 28205, loss = 0.00831978\n",
      "Iteration 28206, loss = 0.00831955\n",
      "Iteration 28207, loss = 0.00831932\n",
      "Iteration 28208, loss = 0.00831908\n",
      "Iteration 28209, loss = 0.00831885\n",
      "Iteration 28210, loss = 0.00831862\n",
      "Iteration 28211, loss = 0.00831839\n",
      "Iteration 28212, loss = 0.00831815\n",
      "Iteration 28213, loss = 0.00831792\n",
      "Iteration 28214, loss = 0.00831769\n",
      "Iteration 28215, loss = 0.00831745\n",
      "Iteration 28216, loss = 0.00831722\n",
      "Iteration 28217, loss = 0.00831699\n",
      "Iteration 28218, loss = 0.00831675\n",
      "Iteration 28219, loss = 0.00831652\n",
      "Iteration 28220, loss = 0.00831629\n",
      "Iteration 28221, loss = 0.00831605\n",
      "Iteration 28222, loss = 0.00831582\n",
      "Iteration 28223, loss = 0.00831559\n",
      "Iteration 28224, loss = 0.00831536\n",
      "Iteration 28225, loss = 0.00831512\n",
      "Iteration 28226, loss = 0.00831489\n",
      "Iteration 28227, loss = 0.00831466\n",
      "Iteration 28228, loss = 0.00831442\n",
      "Iteration 28229, loss = 0.00831419\n",
      "Iteration 28230, loss = 0.00831396\n",
      "Iteration 28231, loss = 0.00831373\n",
      "Iteration 28232, loss = 0.00831349\n",
      "Iteration 28233, loss = 0.00831326\n",
      "Iteration 28234, loss = 0.00831303\n",
      "Iteration 28235, loss = 0.00831279\n",
      "Iteration 28236, loss = 0.00831256\n",
      "Iteration 28237, loss = 0.00831233\n",
      "Iteration 28238, loss = 0.00831210\n",
      "Iteration 28239, loss = 0.00831186\n",
      "Iteration 28240, loss = 0.00831163\n",
      "Iteration 28241, loss = 0.00831140\n",
      "Iteration 28242, loss = 0.00831117\n",
      "Iteration 28243, loss = 0.00831093\n",
      "Iteration 28244, loss = 0.00831070\n",
      "Iteration 28245, loss = 0.00831047\n",
      "Iteration 28246, loss = 0.00831024\n",
      "Iteration 28247, loss = 0.00831000\n",
      "Iteration 28248, loss = 0.00830977\n",
      "Iteration 28249, loss = 0.00830954\n",
      "Iteration 28250, loss = 0.00830931\n",
      "Iteration 28251, loss = 0.00830907\n",
      "Iteration 28252, loss = 0.00830884\n",
      "Iteration 28253, loss = 0.00830861\n",
      "Iteration 28254, loss = 0.00830838\n",
      "Iteration 28255, loss = 0.00830815\n",
      "Iteration 28256, loss = 0.00830791\n",
      "Iteration 28257, loss = 0.00830768\n",
      "Iteration 28258, loss = 0.00830745\n",
      "Iteration 28259, loss = 0.00830722\n",
      "Iteration 28260, loss = 0.00830698\n",
      "Iteration 28261, loss = 0.00830675\n",
      "Iteration 28262, loss = 0.00830652\n",
      "Iteration 28263, loss = 0.00830629\n",
      "Iteration 28264, loss = 0.00830606\n",
      "Iteration 28265, loss = 0.00830582\n",
      "Iteration 28266, loss = 0.00830559\n",
      "Iteration 28267, loss = 0.00830536\n",
      "Iteration 28268, loss = 0.00830513\n",
      "Iteration 28269, loss = 0.00830490\n",
      "Iteration 28270, loss = 0.00830466\n",
      "Iteration 28271, loss = 0.00830443\n",
      "Iteration 28272, loss = 0.00830420\n",
      "Iteration 28273, loss = 0.00830397\n",
      "Iteration 28274, loss = 0.00830374\n",
      "Iteration 28275, loss = 0.00830351\n",
      "Iteration 28276, loss = 0.00830327\n",
      "Iteration 28277, loss = 0.00830304\n",
      "Iteration 28278, loss = 0.00830281\n",
      "Iteration 28279, loss = 0.00830258\n",
      "Iteration 28280, loss = 0.00830235\n",
      "Iteration 28281, loss = 0.00830212\n",
      "Iteration 28282, loss = 0.00830188\n",
      "Iteration 28283, loss = 0.00830165\n",
      "Iteration 28284, loss = 0.00830142\n",
      "Iteration 28285, loss = 0.00830119\n",
      "Iteration 28286, loss = 0.00830096\n",
      "Iteration 28287, loss = 0.00830073\n",
      "Iteration 28288, loss = 0.00830049\n",
      "Iteration 28289, loss = 0.00830026\n",
      "Iteration 28290, loss = 0.00830003\n",
      "Iteration 28291, loss = 0.00829980\n",
      "Iteration 28292, loss = 0.00829957\n",
      "Iteration 28293, loss = 0.00829934\n",
      "Iteration 28294, loss = 0.00829911\n",
      "Iteration 28295, loss = 0.00829888\n",
      "Iteration 28296, loss = 0.00829864\n",
      "Iteration 28297, loss = 0.00829841\n",
      "Iteration 28298, loss = 0.00829818\n",
      "Iteration 28299, loss = 0.00829795\n",
      "Iteration 28300, loss = 0.00829772\n",
      "Iteration 28301, loss = 0.00829749\n",
      "Iteration 28302, loss = 0.00829726\n",
      "Iteration 28303, loss = 0.00829703\n",
      "Iteration 28304, loss = 0.00829679\n",
      "Iteration 28305, loss = 0.00829656\n",
      "Iteration 28306, loss = 0.00829633\n",
      "Iteration 28307, loss = 0.00829610\n",
      "Iteration 28308, loss = 0.00829587\n",
      "Iteration 28309, loss = 0.00829564\n",
      "Iteration 28310, loss = 0.00829541\n",
      "Iteration 28311, loss = 0.00829518\n",
      "Iteration 28312, loss = 0.00829495\n",
      "Iteration 28313, loss = 0.00829472\n",
      "Iteration 28314, loss = 0.00829448\n",
      "Iteration 28315, loss = 0.00829425\n",
      "Iteration 28316, loss = 0.00829402\n",
      "Iteration 28317, loss = 0.00829379\n",
      "Iteration 28318, loss = 0.00829356\n",
      "Iteration 28319, loss = 0.00829333\n",
      "Iteration 28320, loss = 0.00829310\n",
      "Iteration 28321, loss = 0.00829287\n",
      "Iteration 28322, loss = 0.00829264\n",
      "Iteration 28323, loss = 0.00829241\n",
      "Iteration 28324, loss = 0.00829218\n",
      "Iteration 28325, loss = 0.00829195\n",
      "Iteration 28326, loss = 0.00829172\n",
      "Iteration 28327, loss = 0.00829148\n",
      "Iteration 28328, loss = 0.00829125\n",
      "Iteration 28329, loss = 0.00829102\n",
      "Iteration 28330, loss = 0.00829079\n",
      "Iteration 28331, loss = 0.00829056\n",
      "Iteration 28332, loss = 0.00829033\n",
      "Iteration 28333, loss = 0.00829010\n",
      "Iteration 28334, loss = 0.00828987\n",
      "Iteration 28335, loss = 0.00828964\n",
      "Iteration 28336, loss = 0.00828941\n",
      "Iteration 28337, loss = 0.00828918\n",
      "Iteration 28338, loss = 0.00828895\n",
      "Iteration 28339, loss = 0.00828872\n",
      "Iteration 28340, loss = 0.00828849\n",
      "Iteration 28341, loss = 0.00828826\n",
      "Iteration 28342, loss = 0.00828803\n",
      "Iteration 28343, loss = 0.00828780\n",
      "Iteration 28344, loss = 0.00828757\n",
      "Iteration 28345, loss = 0.00828734\n",
      "Iteration 28346, loss = 0.00828711\n",
      "Iteration 28347, loss = 0.00828688\n",
      "Iteration 28348, loss = 0.00828665\n",
      "Iteration 28349, loss = 0.00828642\n",
      "Iteration 28350, loss = 0.00828619\n",
      "Iteration 28351, loss = 0.00828596\n",
      "Iteration 28352, loss = 0.00828573\n",
      "Iteration 28353, loss = 0.00828550\n",
      "Iteration 28354, loss = 0.00828527\n",
      "Iteration 28355, loss = 0.00828504\n",
      "Iteration 28356, loss = 0.00828481\n",
      "Iteration 28357, loss = 0.00828458\n",
      "Iteration 28358, loss = 0.00828435\n",
      "Iteration 28359, loss = 0.00828412\n",
      "Iteration 28360, loss = 0.00828389\n",
      "Iteration 28361, loss = 0.00828366\n",
      "Iteration 28362, loss = 0.00828343\n",
      "Iteration 28363, loss = 0.00828320\n",
      "Iteration 28364, loss = 0.00828297\n",
      "Iteration 28365, loss = 0.00828274\n",
      "Iteration 28366, loss = 0.00828251\n",
      "Iteration 28367, loss = 0.00828228\n",
      "Iteration 28368, loss = 0.00828205\n",
      "Iteration 28369, loss = 0.00828182\n",
      "Iteration 28370, loss = 0.00828159\n",
      "Iteration 28371, loss = 0.00828136\n",
      "Iteration 28372, loss = 0.00828113\n",
      "Iteration 28373, loss = 0.00828090\n",
      "Iteration 28374, loss = 0.00828067\n",
      "Iteration 28375, loss = 0.00828044\n",
      "Iteration 28376, loss = 0.00828021\n",
      "Iteration 28377, loss = 0.00827998\n",
      "Iteration 28378, loss = 0.00827975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28379, loss = 0.00827952\n",
      "Iteration 28380, loss = 0.00827929\n",
      "Iteration 28381, loss = 0.00827907\n",
      "Iteration 28382, loss = 0.00827884\n",
      "Iteration 28383, loss = 0.00827861\n",
      "Iteration 28384, loss = 0.00827838\n",
      "Iteration 28385, loss = 0.00827815\n",
      "Iteration 28386, loss = 0.00827792\n",
      "Iteration 28387, loss = 0.00827769\n",
      "Iteration 28388, loss = 0.00827746\n",
      "Iteration 28389, loss = 0.00827723\n",
      "Iteration 28390, loss = 0.00827700\n",
      "Iteration 28391, loss = 0.00827677\n",
      "Iteration 28392, loss = 0.00827654\n",
      "Iteration 28393, loss = 0.00827631\n",
      "Iteration 28394, loss = 0.00827609\n",
      "Iteration 28395, loss = 0.00827586\n",
      "Iteration 28396, loss = 0.00827563\n",
      "Iteration 28397, loss = 0.00827540\n",
      "Iteration 28398, loss = 0.00827517\n",
      "Iteration 28399, loss = 0.00827494\n",
      "Iteration 28400, loss = 0.00827471\n",
      "Iteration 28401, loss = 0.00827448\n",
      "Iteration 28402, loss = 0.00827425\n",
      "Iteration 28403, loss = 0.00827402\n",
      "Iteration 28404, loss = 0.00827380\n",
      "Iteration 28405, loss = 0.00827357\n",
      "Iteration 28406, loss = 0.00827334\n",
      "Iteration 28407, loss = 0.00827311\n",
      "Iteration 28408, loss = 0.00827288\n",
      "Iteration 28409, loss = 0.00827265\n",
      "Iteration 28410, loss = 0.00827242\n",
      "Iteration 28411, loss = 0.00827219\n",
      "Iteration 28412, loss = 0.00827197\n",
      "Iteration 28413, loss = 0.00827174\n",
      "Iteration 28414, loss = 0.00827151\n",
      "Iteration 28415, loss = 0.00827128\n",
      "Iteration 28416, loss = 0.00827105\n",
      "Iteration 28417, loss = 0.00827082\n",
      "Iteration 28418, loss = 0.00827059\n",
      "Iteration 28419, loss = 0.00827036\n",
      "Iteration 28420, loss = 0.00827014\n",
      "Iteration 28421, loss = 0.00826991\n",
      "Iteration 28422, loss = 0.00826968\n",
      "Iteration 28423, loss = 0.00826945\n",
      "Iteration 28424, loss = 0.00826922\n",
      "Iteration 28425, loss = 0.00826899\n",
      "Iteration 28426, loss = 0.00826877\n",
      "Iteration 28427, loss = 0.00826854\n",
      "Iteration 28428, loss = 0.00826831\n",
      "Iteration 28429, loss = 0.00826808\n",
      "Iteration 28430, loss = 0.00826785\n",
      "Iteration 28431, loss = 0.00826762\n",
      "Iteration 28432, loss = 0.00826740\n",
      "Iteration 28433, loss = 0.00826717\n",
      "Iteration 28434, loss = 0.00826694\n",
      "Iteration 28435, loss = 0.00826671\n",
      "Iteration 28436, loss = 0.00826648\n",
      "Iteration 28437, loss = 0.00826625\n",
      "Iteration 28438, loss = 0.00826603\n",
      "Iteration 28439, loss = 0.00826580\n",
      "Iteration 28440, loss = 0.00826557\n",
      "Iteration 28441, loss = 0.00826534\n",
      "Iteration 28442, loss = 0.00826511\n",
      "Iteration 28443, loss = 0.00826489\n",
      "Iteration 28444, loss = 0.00826466\n",
      "Iteration 28445, loss = 0.00826443\n",
      "Iteration 28446, loss = 0.00826420\n",
      "Iteration 28447, loss = 0.00826397\n",
      "Iteration 28448, loss = 0.00826375\n",
      "Iteration 28449, loss = 0.00826352\n",
      "Iteration 28450, loss = 0.00826329\n",
      "Iteration 28451, loss = 0.00826306\n",
      "Iteration 28452, loss = 0.00826284\n",
      "Iteration 28453, loss = 0.00826261\n",
      "Iteration 28454, loss = 0.00826238\n",
      "Iteration 28455, loss = 0.00826215\n",
      "Iteration 28456, loss = 0.00826192\n",
      "Iteration 28457, loss = 0.00826170\n",
      "Iteration 28458, loss = 0.00826147\n",
      "Iteration 28459, loss = 0.00826124\n",
      "Iteration 28460, loss = 0.00826101\n",
      "Iteration 28461, loss = 0.00826079\n",
      "Iteration 28462, loss = 0.00826056\n",
      "Iteration 28463, loss = 0.00826033\n",
      "Iteration 28464, loss = 0.00826010\n",
      "Iteration 28465, loss = 0.00825988\n",
      "Iteration 28466, loss = 0.00825965\n",
      "Iteration 28467, loss = 0.00825942\n",
      "Iteration 28468, loss = 0.00825919\n",
      "Iteration 28469, loss = 0.00825897\n",
      "Iteration 28470, loss = 0.00825874\n",
      "Iteration 28471, loss = 0.00825851\n",
      "Iteration 28472, loss = 0.00825828\n",
      "Iteration 28473, loss = 0.00825806\n",
      "Iteration 28474, loss = 0.00825783\n",
      "Iteration 28475, loss = 0.00825760\n",
      "Iteration 28476, loss = 0.00825737\n",
      "Iteration 28477, loss = 0.00825715\n",
      "Iteration 28478, loss = 0.00825692\n",
      "Iteration 28479, loss = 0.00825669\n",
      "Iteration 28480, loss = 0.00825647\n",
      "Iteration 28481, loss = 0.00825624\n",
      "Iteration 28482, loss = 0.00825601\n",
      "Iteration 28483, loss = 0.00825578\n",
      "Iteration 28484, loss = 0.00825556\n",
      "Iteration 28485, loss = 0.00825533\n",
      "Iteration 28486, loss = 0.00825510\n",
      "Iteration 28487, loss = 0.00825488\n",
      "Iteration 28488, loss = 0.00825465\n",
      "Iteration 28489, loss = 0.00825442\n",
      "Iteration 28490, loss = 0.00825419\n",
      "Iteration 28491, loss = 0.00825397\n",
      "Iteration 28492, loss = 0.00825374\n",
      "Iteration 28493, loss = 0.00825351\n",
      "Iteration 28494, loss = 0.00825329\n",
      "Iteration 28495, loss = 0.00825306\n",
      "Iteration 28496, loss = 0.00825283\n",
      "Iteration 28497, loss = 0.00825261\n",
      "Iteration 28498, loss = 0.00825238\n",
      "Iteration 28499, loss = 0.00825215\n",
      "Iteration 28500, loss = 0.00825193\n",
      "Iteration 28501, loss = 0.00825170\n",
      "Iteration 28502, loss = 0.00825147\n",
      "Iteration 28503, loss = 0.00825125\n",
      "Iteration 28504, loss = 0.00825102\n",
      "Iteration 28505, loss = 0.00825079\n",
      "Iteration 28506, loss = 0.00825057\n",
      "Iteration 28507, loss = 0.00825034\n",
      "Iteration 28508, loss = 0.00825011\n",
      "Iteration 28509, loss = 0.00824989\n",
      "Iteration 28510, loss = 0.00824966\n",
      "Iteration 28511, loss = 0.00824943\n",
      "Iteration 28512, loss = 0.00824921\n",
      "Iteration 28513, loss = 0.00824898\n",
      "Iteration 28514, loss = 0.00824875\n",
      "Iteration 28515, loss = 0.00824853\n",
      "Iteration 28516, loss = 0.00824830\n",
      "Iteration 28517, loss = 0.00824807\n",
      "Iteration 28518, loss = 0.00824785\n",
      "Iteration 28519, loss = 0.00824762\n",
      "Iteration 28520, loss = 0.00824740\n",
      "Iteration 28521, loss = 0.00824717\n",
      "Iteration 28522, loss = 0.00824694\n",
      "Iteration 28523, loss = 0.00824672\n",
      "Iteration 28524, loss = 0.00824649\n",
      "Iteration 28525, loss = 0.00824626\n",
      "Iteration 28526, loss = 0.00824604\n",
      "Iteration 28527, loss = 0.00824581\n",
      "Iteration 28528, loss = 0.00824559\n",
      "Iteration 28529, loss = 0.00824536\n",
      "Iteration 28530, loss = 0.00824513\n",
      "Iteration 28531, loss = 0.00824491\n",
      "Iteration 28532, loss = 0.00824468\n",
      "Iteration 28533, loss = 0.00824446\n",
      "Iteration 28534, loss = 0.00824423\n",
      "Iteration 28535, loss = 0.00824400\n",
      "Iteration 28536, loss = 0.00824378\n",
      "Iteration 28537, loss = 0.00824355\n",
      "Iteration 28538, loss = 0.00824333\n",
      "Iteration 28539, loss = 0.00824310\n",
      "Iteration 28540, loss = 0.00824287\n",
      "Iteration 28541, loss = 0.00824265\n",
      "Iteration 28542, loss = 0.00824242\n",
      "Iteration 28543, loss = 0.00824220\n",
      "Iteration 28544, loss = 0.00824197\n",
      "Iteration 28545, loss = 0.00824174\n",
      "Iteration 28546, loss = 0.00824152\n",
      "Iteration 28547, loss = 0.00824129\n",
      "Iteration 28548, loss = 0.00824107\n",
      "Iteration 28549, loss = 0.00824084\n",
      "Iteration 28550, loss = 0.00824062\n",
      "Iteration 28551, loss = 0.00824039\n",
      "Iteration 28552, loss = 0.00824016\n",
      "Iteration 28553, loss = 0.00823994\n",
      "Iteration 28554, loss = 0.00823971\n",
      "Iteration 28555, loss = 0.00823949\n",
      "Iteration 28556, loss = 0.00823926\n",
      "Iteration 28557, loss = 0.00823904\n",
      "Iteration 28558, loss = 0.00823881\n",
      "Iteration 28559, loss = 0.00823859\n",
      "Iteration 28560, loss = 0.00823836\n",
      "Iteration 28561, loss = 0.00823814\n",
      "Iteration 28562, loss = 0.00823791\n",
      "Iteration 28563, loss = 0.00823768\n",
      "Iteration 28564, loss = 0.00823746\n",
      "Iteration 28565, loss = 0.00823723\n",
      "Iteration 28566, loss = 0.00823701\n",
      "Iteration 28567, loss = 0.00823678\n",
      "Iteration 28568, loss = 0.00823656\n",
      "Iteration 28569, loss = 0.00823633\n",
      "Iteration 28570, loss = 0.00823611\n",
      "Iteration 28571, loss = 0.00823588\n",
      "Iteration 28572, loss = 0.00823566\n",
      "Iteration 28573, loss = 0.00823543\n",
      "Iteration 28574, loss = 0.00823521\n",
      "Iteration 28575, loss = 0.00823498\n",
      "Iteration 28576, loss = 0.00823476\n",
      "Iteration 28577, loss = 0.00823453\n",
      "Iteration 28578, loss = 0.00823431\n",
      "Iteration 28579, loss = 0.00823408\n",
      "Iteration 28580, loss = 0.00823386\n",
      "Iteration 28581, loss = 0.00823363\n",
      "Iteration 28582, loss = 0.00823341\n",
      "Iteration 28583, loss = 0.00823318\n",
      "Iteration 28584, loss = 0.00823296\n",
      "Iteration 28585, loss = 0.00823273\n",
      "Iteration 28586, loss = 0.00823251\n",
      "Iteration 28587, loss = 0.00823228\n",
      "Iteration 28588, loss = 0.00823206\n",
      "Iteration 28589, loss = 0.00823183\n",
      "Iteration 28590, loss = 0.00823161\n",
      "Iteration 28591, loss = 0.00823138\n",
      "Iteration 28592, loss = 0.00823116\n",
      "Iteration 28593, loss = 0.00823093\n",
      "Iteration 28594, loss = 0.00823071\n",
      "Iteration 28595, loss = 0.00823048\n",
      "Iteration 28596, loss = 0.00823026\n",
      "Iteration 28597, loss = 0.00823003\n",
      "Iteration 28598, loss = 0.00822981\n",
      "Iteration 28599, loss = 0.00822959\n",
      "Iteration 28600, loss = 0.00822936\n",
      "Iteration 28601, loss = 0.00822914\n",
      "Iteration 28602, loss = 0.00822891\n",
      "Iteration 28603, loss = 0.00822869\n",
      "Iteration 28604, loss = 0.00822846\n",
      "Iteration 28605, loss = 0.00822824\n",
      "Iteration 28606, loss = 0.00822801\n",
      "Iteration 28607, loss = 0.00822779\n",
      "Iteration 28608, loss = 0.00822756\n",
      "Iteration 28609, loss = 0.00822734\n",
      "Iteration 28610, loss = 0.00822712\n",
      "Iteration 28611, loss = 0.00822689\n",
      "Iteration 28612, loss = 0.00822667\n",
      "Iteration 28613, loss = 0.00822644\n",
      "Iteration 28614, loss = 0.00822622\n",
      "Iteration 28615, loss = 0.00822599\n",
      "Iteration 28616, loss = 0.00822577\n",
      "Iteration 28617, loss = 0.00822555\n",
      "Iteration 28618, loss = 0.00822532\n",
      "Iteration 28619, loss = 0.00822510\n",
      "Iteration 28620, loss = 0.00822487\n",
      "Iteration 28621, loss = 0.00822465\n",
      "Iteration 28622, loss = 0.00822443\n",
      "Iteration 28623, loss = 0.00822420\n",
      "Iteration 28624, loss = 0.00822398\n",
      "Iteration 28625, loss = 0.00822375\n",
      "Iteration 28626, loss = 0.00822353\n",
      "Iteration 28627, loss = 0.00822331\n",
      "Iteration 28628, loss = 0.00822308\n",
      "Iteration 28629, loss = 0.00822286\n",
      "Iteration 28630, loss = 0.00822263\n",
      "Iteration 28631, loss = 0.00822241\n",
      "Iteration 28632, loss = 0.00822219\n",
      "Iteration 28633, loss = 0.00822196\n",
      "Iteration 28634, loss = 0.00822174\n",
      "Iteration 28635, loss = 0.00822151\n",
      "Iteration 28636, loss = 0.00822129\n",
      "Iteration 28637, loss = 0.00822107\n",
      "Iteration 28638, loss = 0.00822084\n",
      "Iteration 28639, loss = 0.00822062\n",
      "Iteration 28640, loss = 0.00822040\n",
      "Iteration 28641, loss = 0.00822017\n",
      "Iteration 28642, loss = 0.00821995\n",
      "Iteration 28643, loss = 0.00821972\n",
      "Iteration 28644, loss = 0.00821950\n",
      "Iteration 28645, loss = 0.00821928\n",
      "Iteration 28646, loss = 0.00821905\n",
      "Iteration 28647, loss = 0.00821883\n",
      "Iteration 28648, loss = 0.00821861\n",
      "Iteration 28649, loss = 0.00821838\n",
      "Iteration 28650, loss = 0.00821816\n",
      "Iteration 28651, loss = 0.00821794\n",
      "Iteration 28652, loss = 0.00821771\n",
      "Iteration 28653, loss = 0.00821749\n",
      "Iteration 28654, loss = 0.00821727\n",
      "Iteration 28655, loss = 0.00821704\n",
      "Iteration 28656, loss = 0.00821682\n",
      "Iteration 28657, loss = 0.00821660\n",
      "Iteration 28658, loss = 0.00821637\n",
      "Iteration 28659, loss = 0.00821615\n",
      "Iteration 28660, loss = 0.00821593\n",
      "Iteration 28661, loss = 0.00821570\n",
      "Iteration 28662, loss = 0.00821548\n",
      "Iteration 28663, loss = 0.00821526\n",
      "Iteration 28664, loss = 0.00821503\n",
      "Iteration 28665, loss = 0.00821481\n",
      "Iteration 28666, loss = 0.00821459\n",
      "Iteration 28667, loss = 0.00821436\n",
      "Iteration 28668, loss = 0.00821414\n",
      "Iteration 28669, loss = 0.00821392\n",
      "Iteration 28670, loss = 0.00821369\n",
      "Iteration 28671, loss = 0.00821347\n",
      "Iteration 28672, loss = 0.00821325\n",
      "Iteration 28673, loss = 0.00821302\n",
      "Iteration 28674, loss = 0.00821280\n",
      "Iteration 28675, loss = 0.00821258\n",
      "Iteration 28676, loss = 0.00821236\n",
      "Iteration 28677, loss = 0.00821213\n",
      "Iteration 28678, loss = 0.00821191\n",
      "Iteration 28679, loss = 0.00821169\n",
      "Iteration 28680, loss = 0.00821146\n",
      "Iteration 28681, loss = 0.00821124\n",
      "Iteration 28682, loss = 0.00821102\n",
      "Iteration 28683, loss = 0.00821080\n",
      "Iteration 28684, loss = 0.00821057\n",
      "Iteration 28685, loss = 0.00821035\n",
      "Iteration 28686, loss = 0.00821013\n",
      "Iteration 28687, loss = 0.00820991\n",
      "Iteration 28688, loss = 0.00820968\n",
      "Iteration 28689, loss = 0.00820946\n",
      "Iteration 28690, loss = 0.00820924\n",
      "Iteration 28691, loss = 0.00820901\n",
      "Iteration 28692, loss = 0.00820879\n",
      "Iteration 28693, loss = 0.00820857\n",
      "Iteration 28694, loss = 0.00820835\n",
      "Iteration 28695, loss = 0.00820812\n",
      "Iteration 28696, loss = 0.00820790\n",
      "Iteration 28697, loss = 0.00820768\n",
      "Iteration 28698, loss = 0.00820746\n",
      "Iteration 28699, loss = 0.00820723\n",
      "Iteration 28700, loss = 0.00820701\n",
      "Iteration 28701, loss = 0.00820679\n",
      "Iteration 28702, loss = 0.00820657\n",
      "Iteration 28703, loss = 0.00820634\n",
      "Iteration 28704, loss = 0.00820612\n",
      "Iteration 28705, loss = 0.00820590\n",
      "Iteration 28706, loss = 0.00820568\n",
      "Iteration 28707, loss = 0.00820546\n",
      "Iteration 28708, loss = 0.00820523\n",
      "Iteration 28709, loss = 0.00820501\n",
      "Iteration 28710, loss = 0.00820479\n",
      "Iteration 28711, loss = 0.00820457\n",
      "Iteration 28712, loss = 0.00820434\n",
      "Iteration 28713, loss = 0.00820412\n",
      "Iteration 28714, loss = 0.00820390\n",
      "Iteration 28715, loss = 0.00820368\n",
      "Iteration 28716, loss = 0.00820346\n",
      "Iteration 28717, loss = 0.00820323\n",
      "Iteration 28718, loss = 0.00820301\n",
      "Iteration 28719, loss = 0.00820279\n",
      "Iteration 28720, loss = 0.00820257\n",
      "Iteration 28721, loss = 0.00820235\n",
      "Iteration 28722, loss = 0.00820212\n",
      "Iteration 28723, loss = 0.00820190\n",
      "Iteration 28724, loss = 0.00820168\n",
      "Iteration 28725, loss = 0.00820146\n",
      "Iteration 28726, loss = 0.00820124\n",
      "Iteration 28727, loss = 0.00820101\n",
      "Iteration 28728, loss = 0.00820079\n",
      "Iteration 28729, loss = 0.00820057\n",
      "Iteration 28730, loss = 0.00820035\n",
      "Iteration 28731, loss = 0.00820013\n",
      "Iteration 28732, loss = 0.00819991\n",
      "Iteration 28733, loss = 0.00819968\n",
      "Iteration 28734, loss = 0.00819946\n",
      "Iteration 28735, loss = 0.00819924\n",
      "Iteration 28736, loss = 0.00819902\n",
      "Iteration 28737, loss = 0.00819880\n",
      "Iteration 28738, loss = 0.00819858\n",
      "Iteration 28739, loss = 0.00819835\n",
      "Iteration 28740, loss = 0.00819813\n",
      "Iteration 28741, loss = 0.00819791\n",
      "Iteration 28742, loss = 0.00819769\n",
      "Iteration 28743, loss = 0.00819747\n",
      "Iteration 28744, loss = 0.00819725\n",
      "Iteration 28745, loss = 0.00819702\n",
      "Iteration 28746, loss = 0.00819680\n",
      "Iteration 28747, loss = 0.00819658\n",
      "Iteration 28748, loss = 0.00819636\n",
      "Iteration 28749, loss = 0.00819614\n",
      "Iteration 28750, loss = 0.00819592\n",
      "Iteration 28751, loss = 0.00819570\n",
      "Iteration 28752, loss = 0.00819548\n",
      "Iteration 28753, loss = 0.00819525\n",
      "Iteration 28754, loss = 0.00819503\n",
      "Iteration 28755, loss = 0.00819481\n",
      "Iteration 28756, loss = 0.00819459\n",
      "Iteration 28757, loss = 0.00819437\n",
      "Iteration 28758, loss = 0.00819415\n",
      "Iteration 28759, loss = 0.00819393\n",
      "Iteration 28760, loss = 0.00819371\n",
      "Iteration 28761, loss = 0.00819348\n",
      "Iteration 28762, loss = 0.00819326\n",
      "Iteration 28763, loss = 0.00819304\n",
      "Iteration 28764, loss = 0.00819282\n",
      "Iteration 28765, loss = 0.00819260\n",
      "Iteration 28766, loss = 0.00819238\n",
      "Iteration 28767, loss = 0.00819216\n",
      "Iteration 28768, loss = 0.00819194\n",
      "Iteration 28769, loss = 0.00819172\n",
      "Iteration 28770, loss = 0.00819150\n",
      "Iteration 28771, loss = 0.00819127\n",
      "Iteration 28772, loss = 0.00819105\n",
      "Iteration 28773, loss = 0.00819083\n",
      "Iteration 28774, loss = 0.00819061\n",
      "Iteration 28775, loss = 0.00819039\n",
      "Iteration 28776, loss = 0.00819017\n",
      "Iteration 28777, loss = 0.00818995\n",
      "Iteration 28778, loss = 0.00818973\n",
      "Iteration 28779, loss = 0.00818951\n",
      "Iteration 28780, loss = 0.00818929\n",
      "Iteration 28781, loss = 0.00818907\n",
      "Iteration 28782, loss = 0.00818885\n",
      "Iteration 28783, loss = 0.00818862\n",
      "Iteration 28784, loss = 0.00818840\n",
      "Iteration 28785, loss = 0.00818818\n",
      "Iteration 28786, loss = 0.00818796\n",
      "Iteration 28787, loss = 0.00818774\n",
      "Iteration 28788, loss = 0.00818752\n",
      "Iteration 28789, loss = 0.00818730\n",
      "Iteration 28790, loss = 0.00818708\n",
      "Iteration 28791, loss = 0.00818686\n",
      "Iteration 28792, loss = 0.00818664\n",
      "Iteration 28793, loss = 0.00818642\n",
      "Iteration 28794, loss = 0.00818620\n",
      "Iteration 28795, loss = 0.00818598\n",
      "Iteration 28796, loss = 0.00818576\n",
      "Iteration 28797, loss = 0.00818554\n",
      "Iteration 28798, loss = 0.00818532\n",
      "Iteration 28799, loss = 0.00818510\n",
      "Iteration 28800, loss = 0.00818488\n",
      "Iteration 28801, loss = 0.00818466\n",
      "Iteration 28802, loss = 0.00818444\n",
      "Iteration 28803, loss = 0.00818422\n",
      "Iteration 28804, loss = 0.00818400\n",
      "Iteration 28805, loss = 0.00818378\n",
      "Iteration 28806, loss = 0.00818356\n",
      "Iteration 28807, loss = 0.00818334\n",
      "Iteration 28808, loss = 0.00818312\n",
      "Iteration 28809, loss = 0.00818290\n",
      "Iteration 28810, loss = 0.00818268\n",
      "Iteration 28811, loss = 0.00818246\n",
      "Iteration 28812, loss = 0.00818223\n",
      "Iteration 28813, loss = 0.00818201\n",
      "Iteration 28814, loss = 0.00818179\n",
      "Iteration 28815, loss = 0.00818157\n",
      "Iteration 28816, loss = 0.00818135\n",
      "Iteration 28817, loss = 0.00818114\n",
      "Iteration 28818, loss = 0.00818092\n",
      "Iteration 28819, loss = 0.00818070\n",
      "Iteration 28820, loss = 0.00818048\n",
      "Iteration 28821, loss = 0.00818026\n",
      "Iteration 28822, loss = 0.00818004\n",
      "Iteration 28823, loss = 0.00817982\n",
      "Iteration 28824, loss = 0.00817960\n",
      "Iteration 28825, loss = 0.00817938\n",
      "Iteration 28826, loss = 0.00817916\n",
      "Iteration 28827, loss = 0.00817894\n",
      "Iteration 28828, loss = 0.00817872\n",
      "Iteration 28829, loss = 0.00817850\n",
      "Iteration 28830, loss = 0.00817828\n",
      "Iteration 28831, loss = 0.00817806\n",
      "Iteration 28832, loss = 0.00817784\n",
      "Iteration 28833, loss = 0.00817762\n",
      "Iteration 28834, loss = 0.00817740\n",
      "Iteration 28835, loss = 0.00817718\n",
      "Iteration 28836, loss = 0.00817696\n",
      "Iteration 28837, loss = 0.00817674\n",
      "Iteration 28838, loss = 0.00817652\n",
      "Iteration 28839, loss = 0.00817630\n",
      "Iteration 28840, loss = 0.00817608\n",
      "Iteration 28841, loss = 0.00817586\n",
      "Iteration 28842, loss = 0.00817564\n",
      "Iteration 28843, loss = 0.00817542\n",
      "Iteration 28844, loss = 0.00817520\n",
      "Iteration 28845, loss = 0.00817499\n",
      "Iteration 28846, loss = 0.00817477\n",
      "Iteration 28847, loss = 0.00817455\n",
      "Iteration 28848, loss = 0.00817433\n",
      "Iteration 28849, loss = 0.00817411\n",
      "Iteration 28850, loss = 0.00817389\n",
      "Iteration 28851, loss = 0.00817367\n",
      "Iteration 28852, loss = 0.00817345\n",
      "Iteration 28853, loss = 0.00817323\n",
      "Iteration 28854, loss = 0.00817301\n",
      "Iteration 28855, loss = 0.00817279\n",
      "Iteration 28856, loss = 0.00817257\n",
      "Iteration 28857, loss = 0.00817235\n",
      "Iteration 28858, loss = 0.00817214\n",
      "Iteration 28859, loss = 0.00817192\n",
      "Iteration 28860, loss = 0.00817170\n",
      "Iteration 28861, loss = 0.00817148\n",
      "Iteration 28862, loss = 0.00817126\n",
      "Iteration 28863, loss = 0.00817104\n",
      "Iteration 28864, loss = 0.00817082\n",
      "Iteration 28865, loss = 0.00817060\n",
      "Iteration 28866, loss = 0.00817038\n",
      "Iteration 28867, loss = 0.00817016\n",
      "Iteration 28868, loss = 0.00816995\n",
      "Iteration 28869, loss = 0.00816973\n",
      "Iteration 28870, loss = 0.00816951\n",
      "Iteration 28871, loss = 0.00816929\n",
      "Iteration 28872, loss = 0.00816907\n",
      "Iteration 28873, loss = 0.00816885\n",
      "Iteration 28874, loss = 0.00816863\n",
      "Iteration 28875, loss = 0.00816841\n",
      "Iteration 28876, loss = 0.00816820\n",
      "Iteration 28877, loss = 0.00816798\n",
      "Iteration 28878, loss = 0.00816776\n",
      "Iteration 28879, loss = 0.00816754\n",
      "Iteration 28880, loss = 0.00816732\n",
      "Iteration 28881, loss = 0.00816710\n",
      "Iteration 28882, loss = 0.00816688\n",
      "Iteration 28883, loss = 0.00816667\n",
      "Iteration 28884, loss = 0.00816645\n",
      "Iteration 28885, loss = 0.00816623\n",
      "Iteration 28886, loss = 0.00816601\n",
      "Iteration 28887, loss = 0.00816579\n",
      "Iteration 28888, loss = 0.00816557\n",
      "Iteration 28889, loss = 0.00816535\n",
      "Iteration 28890, loss = 0.00816514\n",
      "Iteration 28891, loss = 0.00816492\n",
      "Iteration 28892, loss = 0.00816470\n",
      "Iteration 28893, loss = 0.00816448\n",
      "Iteration 28894, loss = 0.00816426\n",
      "Iteration 28895, loss = 0.00816404\n",
      "Iteration 28896, loss = 0.00816383\n",
      "Iteration 28897, loss = 0.00816361\n",
      "Iteration 28898, loss = 0.00816339\n",
      "Iteration 28899, loss = 0.00816317\n",
      "Iteration 28900, loss = 0.00816295\n",
      "Iteration 28901, loss = 0.00816273\n",
      "Iteration 28902, loss = 0.00816252\n",
      "Iteration 28903, loss = 0.00816230\n",
      "Iteration 28904, loss = 0.00816208\n",
      "Iteration 28905, loss = 0.00816186\n",
      "Iteration 28906, loss = 0.00816164\n",
      "Iteration 28907, loss = 0.00816143\n",
      "Iteration 28908, loss = 0.00816121\n",
      "Iteration 28909, loss = 0.00816099\n",
      "Iteration 28910, loss = 0.00816077\n",
      "Iteration 28911, loss = 0.00816055\n",
      "Iteration 28912, loss = 0.00816034\n",
      "Iteration 28913, loss = 0.00816012\n",
      "Iteration 28914, loss = 0.00815990\n",
      "Iteration 28915, loss = 0.00815968\n",
      "Iteration 28916, loss = 0.00815946\n",
      "Iteration 28917, loss = 0.00815925\n",
      "Iteration 28918, loss = 0.00815903\n",
      "Iteration 28919, loss = 0.00815881\n",
      "Iteration 28920, loss = 0.00815859\n",
      "Iteration 28921, loss = 0.00815838\n",
      "Iteration 28922, loss = 0.00815816\n",
      "Iteration 28923, loss = 0.00815794\n",
      "Iteration 28924, loss = 0.00815772\n",
      "Iteration 28925, loss = 0.00815750\n",
      "Iteration 28926, loss = 0.00815729\n",
      "Iteration 28927, loss = 0.00815707\n",
      "Iteration 28928, loss = 0.00815685\n",
      "Iteration 28929, loss = 0.00815663\n",
      "Iteration 28930, loss = 0.00815642\n",
      "Iteration 28931, loss = 0.00815620\n",
      "Iteration 28932, loss = 0.00815598\n",
      "Iteration 28933, loss = 0.00815576\n",
      "Iteration 28934, loss = 0.00815555\n",
      "Iteration 28935, loss = 0.00815533\n",
      "Iteration 28936, loss = 0.00815511\n",
      "Iteration 28937, loss = 0.00815489\n",
      "Iteration 28938, loss = 0.00815468\n",
      "Iteration 28939, loss = 0.00815446\n",
      "Iteration 28940, loss = 0.00815424\n",
      "Iteration 28941, loss = 0.00815402\n",
      "Iteration 28942, loss = 0.00815381\n",
      "Iteration 28943, loss = 0.00815359\n",
      "Iteration 28944, loss = 0.00815337\n",
      "Iteration 28945, loss = 0.00815316\n",
      "Iteration 28946, loss = 0.00815294\n",
      "Iteration 28947, loss = 0.00815272\n",
      "Iteration 28948, loss = 0.00815250\n",
      "Iteration 28949, loss = 0.00815229\n",
      "Iteration 28950, loss = 0.00815207\n",
      "Iteration 28951, loss = 0.00815185\n",
      "Iteration 28952, loss = 0.00815164\n",
      "Iteration 28953, loss = 0.00815142\n",
      "Iteration 28954, loss = 0.00815120\n",
      "Iteration 28955, loss = 0.00815098\n",
      "Iteration 28956, loss = 0.00815077\n",
      "Iteration 28957, loss = 0.00815055\n",
      "Iteration 28958, loss = 0.00815033\n",
      "Iteration 28959, loss = 0.00815012\n",
      "Iteration 28960, loss = 0.00814990\n",
      "Iteration 28961, loss = 0.00814968\n",
      "Iteration 28962, loss = 0.00814946\n",
      "Iteration 28963, loss = 0.00814925\n",
      "Iteration 28964, loss = 0.00814903\n",
      "Iteration 28965, loss = 0.00814881\n",
      "Iteration 28966, loss = 0.00814860\n",
      "Iteration 28967, loss = 0.00814838\n",
      "Iteration 28968, loss = 0.00814816\n",
      "Iteration 28969, loss = 0.00814795\n",
      "Iteration 28970, loss = 0.00814773\n",
      "Iteration 28971, loss = 0.00814751\n",
      "Iteration 28972, loss = 0.00814730\n",
      "Iteration 28973, loss = 0.00814708\n",
      "Iteration 28974, loss = 0.00814686\n",
      "Iteration 28975, loss = 0.00814665\n",
      "Iteration 28976, loss = 0.00814643\n",
      "Iteration 28977, loss = 0.00814621\n",
      "Iteration 28978, loss = 0.00814600\n",
      "Iteration 28979, loss = 0.00814578\n",
      "Iteration 28980, loss = 0.00814556\n",
      "Iteration 28981, loss = 0.00814535\n",
      "Iteration 28982, loss = 0.00814513\n",
      "Iteration 28983, loss = 0.00814491\n",
      "Iteration 28984, loss = 0.00814470\n",
      "Iteration 28985, loss = 0.00814448\n",
      "Iteration 28986, loss = 0.00814427\n",
      "Iteration 28987, loss = 0.00814405\n",
      "Iteration 28988, loss = 0.00814383\n",
      "Iteration 28989, loss = 0.00814362\n",
      "Iteration 28990, loss = 0.00814340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28991, loss = 0.00814318\n",
      "Iteration 28992, loss = 0.00814297\n",
      "Iteration 28993, loss = 0.00814275\n",
      "Iteration 28994, loss = 0.00814253\n",
      "Iteration 28995, loss = 0.00814232\n",
      "Iteration 28996, loss = 0.00814210\n",
      "Iteration 28997, loss = 0.00814189\n",
      "Iteration 28998, loss = 0.00814167\n",
      "Iteration 28999, loss = 0.00814145\n",
      "Iteration 29000, loss = 0.00814124\n",
      "Iteration 29001, loss = 0.00814102\n",
      "Iteration 29002, loss = 0.00814081\n",
      "Iteration 29003, loss = 0.00814059\n",
      "Iteration 29004, loss = 0.00814037\n",
      "Iteration 29005, loss = 0.00814016\n",
      "Iteration 29006, loss = 0.00813994\n",
      "Iteration 29007, loss = 0.00813972\n",
      "Iteration 29008, loss = 0.00813951\n",
      "Iteration 29009, loss = 0.00813929\n",
      "Iteration 29010, loss = 0.00813908\n",
      "Iteration 29011, loss = 0.00813886\n",
      "Iteration 29012, loss = 0.00813865\n",
      "Iteration 29013, loss = 0.00813843\n",
      "Iteration 29014, loss = 0.00813821\n",
      "Iteration 29015, loss = 0.00813800\n",
      "Iteration 29016, loss = 0.00813778\n",
      "Iteration 29017, loss = 0.00813757\n",
      "Iteration 29018, loss = 0.00813735\n",
      "Iteration 29019, loss = 0.00813713\n",
      "Iteration 29020, loss = 0.00813692\n",
      "Iteration 29021, loss = 0.00813670\n",
      "Iteration 29022, loss = 0.00813649\n",
      "Iteration 29023, loss = 0.00813627\n",
      "Iteration 29024, loss = 0.00813606\n",
      "Iteration 29025, loss = 0.00813584\n",
      "Iteration 29026, loss = 0.00813563\n",
      "Iteration 29027, loss = 0.00813541\n",
      "Iteration 29028, loss = 0.00813519\n",
      "Iteration 29029, loss = 0.00813498\n",
      "Iteration 29030, loss = 0.00813476\n",
      "Iteration 29031, loss = 0.00813455\n",
      "Iteration 29032, loss = 0.00813433\n",
      "Iteration 29033, loss = 0.00813412\n",
      "Iteration 29034, loss = 0.00813390\n",
      "Iteration 29035, loss = 0.00813369\n",
      "Iteration 29036, loss = 0.00813347\n",
      "Iteration 29037, loss = 0.00813325\n",
      "Iteration 29038, loss = 0.00813304\n",
      "Iteration 29039, loss = 0.00813282\n",
      "Iteration 29040, loss = 0.00813261\n",
      "Iteration 29041, loss = 0.00813239\n",
      "Iteration 29042, loss = 0.00813218\n",
      "Iteration 29043, loss = 0.00813196\n",
      "Iteration 29044, loss = 0.00813175\n",
      "Iteration 29045, loss = 0.00813153\n",
      "Iteration 29046, loss = 0.00813132\n",
      "Iteration 29047, loss = 0.00813110\n",
      "Iteration 29048, loss = 0.00813089\n",
      "Iteration 29049, loss = 0.00813067\n",
      "Iteration 29050, loss = 0.00813046\n",
      "Iteration 29051, loss = 0.00813024\n",
      "Iteration 29052, loss = 0.00813003\n",
      "Iteration 29053, loss = 0.00812981\n",
      "Iteration 29054, loss = 0.00812960\n",
      "Iteration 29055, loss = 0.00812938\n",
      "Iteration 29056, loss = 0.00812917\n",
      "Iteration 29057, loss = 0.00812895\n",
      "Iteration 29058, loss = 0.00812874\n",
      "Iteration 29059, loss = 0.00812852\n",
      "Iteration 29060, loss = 0.00812831\n",
      "Iteration 29061, loss = 0.00812809\n",
      "Iteration 29062, loss = 0.00812788\n",
      "Iteration 29063, loss = 0.00812766\n",
      "Iteration 29064, loss = 0.00812745\n",
      "Iteration 29065, loss = 0.00812723\n",
      "Iteration 29066, loss = 0.00812702\n",
      "Iteration 29067, loss = 0.00812680\n",
      "Iteration 29068, loss = 0.00812659\n",
      "Iteration 29069, loss = 0.00812637\n",
      "Iteration 29070, loss = 0.00812616\n",
      "Iteration 29071, loss = 0.00812594\n",
      "Iteration 29072, loss = 0.00812573\n",
      "Iteration 29073, loss = 0.00812551\n",
      "Iteration 29074, loss = 0.00812530\n",
      "Iteration 29075, loss = 0.00812509\n",
      "Iteration 29076, loss = 0.00812487\n",
      "Iteration 29077, loss = 0.00812466\n",
      "Iteration 29078, loss = 0.00812444\n",
      "Iteration 29079, loss = 0.00812423\n",
      "Iteration 29080, loss = 0.00812401\n",
      "Iteration 29081, loss = 0.00812380\n",
      "Iteration 29082, loss = 0.00812358\n",
      "Iteration 29083, loss = 0.00812337\n",
      "Iteration 29084, loss = 0.00812316\n",
      "Iteration 29085, loss = 0.00812294\n",
      "Iteration 29086, loss = 0.00812273\n",
      "Iteration 29087, loss = 0.00812251\n",
      "Iteration 29088, loss = 0.00812230\n",
      "Iteration 29089, loss = 0.00812208\n",
      "Iteration 29090, loss = 0.00812187\n",
      "Iteration 29091, loss = 0.00812165\n",
      "Iteration 29092, loss = 0.00812144\n",
      "Iteration 29093, loss = 0.00812123\n",
      "Iteration 29094, loss = 0.00812101\n",
      "Iteration 29095, loss = 0.00812080\n",
      "Iteration 29096, loss = 0.00812058\n",
      "Iteration 29097, loss = 0.00812037\n",
      "Iteration 29098, loss = 0.00812016\n",
      "Iteration 29099, loss = 0.00811994\n",
      "Iteration 29100, loss = 0.00811973\n",
      "Iteration 29101, loss = 0.00811951\n",
      "Iteration 29102, loss = 0.00811930\n",
      "Iteration 29103, loss = 0.00811908\n",
      "Iteration 29104, loss = 0.00811887\n",
      "Iteration 29105, loss = 0.00811866\n",
      "Iteration 29106, loss = 0.00811844\n",
      "Iteration 29107, loss = 0.00811823\n",
      "Iteration 29108, loss = 0.00811802\n",
      "Iteration 29109, loss = 0.00811780\n",
      "Iteration 29110, loss = 0.00811759\n",
      "Iteration 29111, loss = 0.00811737\n",
      "Iteration 29112, loss = 0.00811716\n",
      "Iteration 29113, loss = 0.00811695\n",
      "Iteration 29114, loss = 0.00811673\n",
      "Iteration 29115, loss = 0.00811652\n",
      "Iteration 29116, loss = 0.00811630\n",
      "Iteration 29117, loss = 0.00811609\n",
      "Iteration 29118, loss = 0.00811588\n",
      "Iteration 29119, loss = 0.00811566\n",
      "Iteration 29120, loss = 0.00811545\n",
      "Iteration 29121, loss = 0.00811524\n",
      "Iteration 29122, loss = 0.00811502\n",
      "Iteration 29123, loss = 0.00811481\n",
      "Iteration 29124, loss = 0.00811459\n",
      "Iteration 29125, loss = 0.00811438\n",
      "Iteration 29126, loss = 0.00811417\n",
      "Iteration 29127, loss = 0.00811395\n",
      "Iteration 29128, loss = 0.00811374\n",
      "Iteration 29129, loss = 0.00811353\n",
      "Iteration 29130, loss = 0.00811331\n",
      "Iteration 29131, loss = 0.00811310\n",
      "Iteration 29132, loss = 0.00811289\n",
      "Iteration 29133, loss = 0.00811267\n",
      "Iteration 29134, loss = 0.00811246\n",
      "Iteration 29135, loss = 0.00811225\n",
      "Iteration 29136, loss = 0.00811203\n",
      "Iteration 29137, loss = 0.00811182\n",
      "Iteration 29138, loss = 0.00811161\n",
      "Iteration 29139, loss = 0.00811139\n",
      "Iteration 29140, loss = 0.00811118\n",
      "Iteration 29141, loss = 0.00811097\n",
      "Iteration 29142, loss = 0.00811075\n",
      "Iteration 29143, loss = 0.00811054\n",
      "Iteration 29144, loss = 0.00811033\n",
      "Iteration 29145, loss = 0.00811011\n",
      "Iteration 29146, loss = 0.00810990\n",
      "Iteration 29147, loss = 0.00810969\n",
      "Iteration 29148, loss = 0.00810947\n",
      "Iteration 29149, loss = 0.00810926\n",
      "Iteration 29150, loss = 0.00810905\n",
      "Iteration 29151, loss = 0.00810884\n",
      "Iteration 29152, loss = 0.00810862\n",
      "Iteration 29153, loss = 0.00810841\n",
      "Iteration 29154, loss = 0.00810820\n",
      "Iteration 29155, loss = 0.00810798\n",
      "Iteration 29156, loss = 0.00810777\n",
      "Iteration 29157, loss = 0.00810756\n",
      "Iteration 29158, loss = 0.00810734\n",
      "Iteration 29159, loss = 0.00810713\n",
      "Iteration 29160, loss = 0.00810692\n",
      "Iteration 29161, loss = 0.00810671\n",
      "Iteration 29162, loss = 0.00810649\n",
      "Iteration 29163, loss = 0.00810628\n",
      "Iteration 29164, loss = 0.00810607\n",
      "Iteration 29165, loss = 0.00810585\n",
      "Iteration 29166, loss = 0.00810564\n",
      "Iteration 29167, loss = 0.00810543\n",
      "Iteration 29168, loss = 0.00810522\n",
      "Iteration 29169, loss = 0.00810500\n",
      "Iteration 29170, loss = 0.00810479\n",
      "Iteration 29171, loss = 0.00810458\n",
      "Iteration 29172, loss = 0.00810437\n",
      "Iteration 29173, loss = 0.00810415\n",
      "Iteration 29174, loss = 0.00810394\n",
      "Iteration 29175, loss = 0.00810373\n",
      "Iteration 29176, loss = 0.00810352\n",
      "Iteration 29177, loss = 0.00810330\n",
      "Iteration 29178, loss = 0.00810309\n",
      "Iteration 29179, loss = 0.00810288\n",
      "Iteration 29180, loss = 0.00810267\n",
      "Iteration 29181, loss = 0.00810245\n",
      "Iteration 29182, loss = 0.00810224\n",
      "Iteration 29183, loss = 0.00810203\n",
      "Iteration 29184, loss = 0.00810182\n",
      "Iteration 29185, loss = 0.00810160\n",
      "Iteration 29186, loss = 0.00810139\n",
      "Iteration 29187, loss = 0.00810118\n",
      "Iteration 29188, loss = 0.00810097\n",
      "Iteration 29189, loss = 0.00810075\n",
      "Iteration 29190, loss = 0.00810054\n",
      "Iteration 29191, loss = 0.00810033\n",
      "Iteration 29192, loss = 0.00810012\n",
      "Iteration 29193, loss = 0.00809990\n",
      "Iteration 29194, loss = 0.00809969\n",
      "Iteration 29195, loss = 0.00809948\n",
      "Iteration 29196, loss = 0.00809927\n",
      "Iteration 29197, loss = 0.00809906\n",
      "Iteration 29198, loss = 0.00809884\n",
      "Iteration 29199, loss = 0.00809863\n",
      "Iteration 29200, loss = 0.00809842\n",
      "Iteration 29201, loss = 0.00809821\n",
      "Iteration 29202, loss = 0.00809800\n",
      "Iteration 29203, loss = 0.00809778\n",
      "Iteration 29204, loss = 0.00809757\n",
      "Iteration 29205, loss = 0.00809736\n",
      "Iteration 29206, loss = 0.00809715\n",
      "Iteration 29207, loss = 0.00809694\n",
      "Iteration 29208, loss = 0.00809672\n",
      "Iteration 29209, loss = 0.00809651\n",
      "Iteration 29210, loss = 0.00809630\n",
      "Iteration 29211, loss = 0.00809609\n",
      "Iteration 29212, loss = 0.00809588\n",
      "Iteration 29213, loss = 0.00809566\n",
      "Iteration 29214, loss = 0.00809545\n",
      "Iteration 29215, loss = 0.00809524\n",
      "Iteration 29216, loss = 0.00809503\n",
      "Iteration 29217, loss = 0.00809482\n",
      "Iteration 29218, loss = 0.00809461\n",
      "Iteration 29219, loss = 0.00809439\n",
      "Iteration 29220, loss = 0.00809418\n",
      "Iteration 29221, loss = 0.00809397\n",
      "Iteration 29222, loss = 0.00809376\n",
      "Iteration 29223, loss = 0.00809355\n",
      "Iteration 29224, loss = 0.00809334\n",
      "Iteration 29225, loss = 0.00809312\n",
      "Iteration 29226, loss = 0.00809291\n",
      "Iteration 29227, loss = 0.00809270\n",
      "Iteration 29228, loss = 0.00809249\n",
      "Iteration 29229, loss = 0.00809228\n",
      "Iteration 29230, loss = 0.00809207\n",
      "Iteration 29231, loss = 0.00809186\n",
      "Iteration 29232, loss = 0.00809164\n",
      "Iteration 29233, loss = 0.00809143\n",
      "Iteration 29234, loss = 0.00809122\n",
      "Iteration 29235, loss = 0.00809101\n",
      "Iteration 29236, loss = 0.00809080\n",
      "Iteration 29237, loss = 0.00809059\n",
      "Iteration 29238, loss = 0.00809038\n",
      "Iteration 29239, loss = 0.00809016\n",
      "Iteration 29240, loss = 0.00808995\n",
      "Iteration 29241, loss = 0.00808974\n",
      "Iteration 29242, loss = 0.00808953\n",
      "Iteration 29243, loss = 0.00808932\n",
      "Iteration 29244, loss = 0.00808911\n",
      "Iteration 29245, loss = 0.00808890\n",
      "Iteration 29246, loss = 0.00808869\n",
      "Iteration 29247, loss = 0.00808847\n",
      "Iteration 29248, loss = 0.00808826\n",
      "Iteration 29249, loss = 0.00808805\n",
      "Iteration 29250, loss = 0.00808784\n",
      "Iteration 29251, loss = 0.00808763\n",
      "Iteration 29252, loss = 0.00808742\n",
      "Iteration 29253, loss = 0.00808721\n",
      "Iteration 29254, loss = 0.00808700\n",
      "Iteration 29255, loss = 0.00808679\n",
      "Iteration 29256, loss = 0.00808658\n",
      "Iteration 29257, loss = 0.00808636\n",
      "Iteration 29258, loss = 0.00808615\n",
      "Iteration 29259, loss = 0.00808594\n",
      "Iteration 29260, loss = 0.00808573\n",
      "Iteration 29261, loss = 0.00808552\n",
      "Iteration 29262, loss = 0.00808531\n",
      "Iteration 29263, loss = 0.00808510\n",
      "Iteration 29264, loss = 0.00808489\n",
      "Iteration 29265, loss = 0.00808468\n",
      "Iteration 29266, loss = 0.00808447\n",
      "Iteration 29267, loss = 0.00808426\n",
      "Iteration 29268, loss = 0.00808405\n",
      "Iteration 29269, loss = 0.00808383\n",
      "Iteration 29270, loss = 0.00808362\n",
      "Iteration 29271, loss = 0.00808341\n",
      "Iteration 29272, loss = 0.00808320\n",
      "Iteration 29273, loss = 0.00808299\n",
      "Iteration 29274, loss = 0.00808278\n",
      "Iteration 29275, loss = 0.00808257\n",
      "Iteration 29276, loss = 0.00808236\n",
      "Iteration 29277, loss = 0.00808215\n",
      "Iteration 29278, loss = 0.00808194\n",
      "Iteration 29279, loss = 0.00808173\n",
      "Iteration 29280, loss = 0.00808152\n",
      "Iteration 29281, loss = 0.00808131\n",
      "Iteration 29282, loss = 0.00808110\n",
      "Iteration 29283, loss = 0.00808089\n",
      "Iteration 29284, loss = 0.00808068\n",
      "Iteration 29285, loss = 0.00808047\n",
      "Iteration 29286, loss = 0.00808026\n",
      "Iteration 29287, loss = 0.00808005\n",
      "Iteration 29288, loss = 0.00807983\n",
      "Iteration 29289, loss = 0.00807962\n",
      "Iteration 29290, loss = 0.00807941\n",
      "Iteration 29291, loss = 0.00807920\n",
      "Iteration 29292, loss = 0.00807899\n",
      "Iteration 29293, loss = 0.00807878\n",
      "Iteration 29294, loss = 0.00807857\n",
      "Iteration 29295, loss = 0.00807836\n",
      "Iteration 29296, loss = 0.00807815\n",
      "Iteration 29297, loss = 0.00807794\n",
      "Iteration 29298, loss = 0.00807773\n",
      "Iteration 29299, loss = 0.00807752\n",
      "Iteration 29300, loss = 0.00807731\n",
      "Iteration 29301, loss = 0.00807710\n",
      "Iteration 29302, loss = 0.00807689\n",
      "Iteration 29303, loss = 0.00807668\n",
      "Iteration 29304, loss = 0.00807647\n",
      "Iteration 29305, loss = 0.00807626\n",
      "Iteration 29306, loss = 0.00807605\n",
      "Iteration 29307, loss = 0.00807584\n",
      "Iteration 29308, loss = 0.00807563\n",
      "Iteration 29309, loss = 0.00807542\n",
      "Iteration 29310, loss = 0.00807521\n",
      "Iteration 29311, loss = 0.00807500\n",
      "Iteration 29312, loss = 0.00807479\n",
      "Iteration 29313, loss = 0.00807458\n",
      "Iteration 29314, loss = 0.00807437\n",
      "Iteration 29315, loss = 0.00807416\n",
      "Iteration 29316, loss = 0.00807395\n",
      "Iteration 29317, loss = 0.00807374\n",
      "Iteration 29318, loss = 0.00807353\n",
      "Iteration 29319, loss = 0.00807332\n",
      "Iteration 29320, loss = 0.00807311\n",
      "Iteration 29321, loss = 0.00807291\n",
      "Iteration 29322, loss = 0.00807270\n",
      "Iteration 29323, loss = 0.00807249\n",
      "Iteration 29324, loss = 0.00807228\n",
      "Iteration 29325, loss = 0.00807207\n",
      "Iteration 29326, loss = 0.00807186\n",
      "Iteration 29327, loss = 0.00807165\n",
      "Iteration 29328, loss = 0.00807144\n",
      "Iteration 29329, loss = 0.00807123\n",
      "Iteration 29330, loss = 0.00807102\n",
      "Iteration 29331, loss = 0.00807081\n",
      "Iteration 29332, loss = 0.00807060\n",
      "Iteration 29333, loss = 0.00807039\n",
      "Iteration 29334, loss = 0.00807018\n",
      "Iteration 29335, loss = 0.00806997\n",
      "Iteration 29336, loss = 0.00806976\n",
      "Iteration 29337, loss = 0.00806955\n",
      "Iteration 29338, loss = 0.00806934\n",
      "Iteration 29339, loss = 0.00806913\n",
      "Iteration 29340, loss = 0.00806893\n",
      "Iteration 29341, loss = 0.00806872\n",
      "Iteration 29342, loss = 0.00806851\n",
      "Iteration 29343, loss = 0.00806830\n",
      "Iteration 29344, loss = 0.00806809\n",
      "Iteration 29345, loss = 0.00806788\n",
      "Iteration 29346, loss = 0.00806767\n",
      "Iteration 29347, loss = 0.00806746\n",
      "Iteration 29348, loss = 0.00806725\n",
      "Iteration 29349, loss = 0.00806704\n",
      "Iteration 29350, loss = 0.00806683\n",
      "Iteration 29351, loss = 0.00806662\n",
      "Iteration 29352, loss = 0.00806642\n",
      "Iteration 29353, loss = 0.00806621\n",
      "Iteration 29354, loss = 0.00806600\n",
      "Iteration 29355, loss = 0.00806579\n",
      "Iteration 29356, loss = 0.00806558\n",
      "Iteration 29357, loss = 0.00806537\n",
      "Iteration 29358, loss = 0.00806516\n",
      "Iteration 29359, loss = 0.00806495\n",
      "Iteration 29360, loss = 0.00806474\n",
      "Iteration 29361, loss = 0.00806453\n",
      "Iteration 29362, loss = 0.00806433\n",
      "Iteration 29363, loss = 0.00806412\n",
      "Iteration 29364, loss = 0.00806391\n",
      "Iteration 29365, loss = 0.00806370\n",
      "Iteration 29366, loss = 0.00806349\n",
      "Iteration 29367, loss = 0.00806328\n",
      "Iteration 29368, loss = 0.00806307\n",
      "Iteration 29369, loss = 0.00806286\n",
      "Iteration 29370, loss = 0.00806266\n",
      "Iteration 29371, loss = 0.00806245\n",
      "Iteration 29372, loss = 0.00806224\n",
      "Iteration 29373, loss = 0.00806203\n",
      "Iteration 29374, loss = 0.00806182\n",
      "Iteration 29375, loss = 0.00806161\n",
      "Iteration 29376, loss = 0.00806140\n",
      "Iteration 29377, loss = 0.00806119\n",
      "Iteration 29378, loss = 0.00806099\n",
      "Iteration 29379, loss = 0.00806078\n",
      "Iteration 29380, loss = 0.00806057\n",
      "Iteration 29381, loss = 0.00806036\n",
      "Iteration 29382, loss = 0.00806015\n",
      "Iteration 29383, loss = 0.00805994\n",
      "Iteration 29384, loss = 0.00805974\n",
      "Iteration 29385, loss = 0.00805953\n",
      "Iteration 29386, loss = 0.00805932\n",
      "Iteration 29387, loss = 0.00805911\n",
      "Iteration 29388, loss = 0.00805890\n",
      "Iteration 29389, loss = 0.00805869\n",
      "Iteration 29390, loss = 0.00805849\n",
      "Iteration 29391, loss = 0.00805828\n",
      "Iteration 29392, loss = 0.00805807\n",
      "Iteration 29393, loss = 0.00805786\n",
      "Iteration 29394, loss = 0.00805765\n",
      "Iteration 29395, loss = 0.00805744\n",
      "Iteration 29396, loss = 0.00805724\n",
      "Iteration 29397, loss = 0.00805703\n",
      "Iteration 29398, loss = 0.00805682\n",
      "Iteration 29399, loss = 0.00805661\n",
      "Iteration 29400, loss = 0.00805640\n",
      "Iteration 29401, loss = 0.00805619\n",
      "Iteration 29402, loss = 0.00805599\n",
      "Iteration 29403, loss = 0.00805578\n",
      "Iteration 29404, loss = 0.00805557\n",
      "Iteration 29405, loss = 0.00805536\n",
      "Iteration 29406, loss = 0.00805515\n",
      "Iteration 29407, loss = 0.00805495\n",
      "Iteration 29408, loss = 0.00805474\n",
      "Iteration 29409, loss = 0.00805453\n",
      "Iteration 29410, loss = 0.00805432\n",
      "Iteration 29411, loss = 0.00805411\n",
      "Iteration 29412, loss = 0.00805391\n",
      "Iteration 29413, loss = 0.00805370\n",
      "Iteration 29414, loss = 0.00805349\n",
      "Iteration 29415, loss = 0.00805328\n",
      "Iteration 29416, loss = 0.00805308\n",
      "Iteration 29417, loss = 0.00805287\n",
      "Iteration 29418, loss = 0.00805266\n",
      "Iteration 29419, loss = 0.00805245\n",
      "Iteration 29420, loss = 0.00805224\n",
      "Iteration 29421, loss = 0.00805204\n",
      "Iteration 29422, loss = 0.00805183\n",
      "Iteration 29423, loss = 0.00805162\n",
      "Iteration 29424, loss = 0.00805141\n",
      "Iteration 29425, loss = 0.00805121\n",
      "Iteration 29426, loss = 0.00805100\n",
      "Iteration 29427, loss = 0.00805079\n",
      "Iteration 29428, loss = 0.00805058\n",
      "Iteration 29429, loss = 0.00805038\n",
      "Iteration 29430, loss = 0.00805017\n",
      "Iteration 29431, loss = 0.00804996\n",
      "Iteration 29432, loss = 0.00804975\n",
      "Iteration 29433, loss = 0.00804955\n",
      "Iteration 29434, loss = 0.00804934\n",
      "Iteration 29435, loss = 0.00804913\n",
      "Iteration 29436, loss = 0.00804892\n",
      "Iteration 29437, loss = 0.00804872\n",
      "Iteration 29438, loss = 0.00804851\n",
      "Iteration 29439, loss = 0.00804830\n",
      "Iteration 29440, loss = 0.00804809\n",
      "Iteration 29441, loss = 0.00804789\n",
      "Iteration 29442, loss = 0.00804768\n",
      "Iteration 29443, loss = 0.00804747\n",
      "Iteration 29444, loss = 0.00804726\n",
      "Iteration 29445, loss = 0.00804706\n",
      "Iteration 29446, loss = 0.00804685\n",
      "Iteration 29447, loss = 0.00804664\n",
      "Iteration 29448, loss = 0.00804644\n",
      "Iteration 29449, loss = 0.00804623\n",
      "Iteration 29450, loss = 0.00804602\n",
      "Iteration 29451, loss = 0.00804581\n",
      "Iteration 29452, loss = 0.00804561\n",
      "Iteration 29453, loss = 0.00804540\n",
      "Iteration 29454, loss = 0.00804519\n",
      "Iteration 29455, loss = 0.00804499\n",
      "Iteration 29456, loss = 0.00804478\n",
      "Iteration 29457, loss = 0.00804457\n",
      "Iteration 29458, loss = 0.00804436\n",
      "Iteration 29459, loss = 0.00804416\n",
      "Iteration 29460, loss = 0.00804395\n",
      "Iteration 29461, loss = 0.00804374\n",
      "Iteration 29462, loss = 0.00804354\n",
      "Iteration 29463, loss = 0.00804333\n",
      "Iteration 29464, loss = 0.00804312\n",
      "Iteration 29465, loss = 0.00804292\n",
      "Iteration 29466, loss = 0.00804271\n",
      "Iteration 29467, loss = 0.00804250\n",
      "Iteration 29468, loss = 0.00804230\n",
      "Iteration 29469, loss = 0.00804209\n",
      "Iteration 29470, loss = 0.00804188\n",
      "Iteration 29471, loss = 0.00804168\n",
      "Iteration 29472, loss = 0.00804147\n",
      "Iteration 29473, loss = 0.00804126\n",
      "Iteration 29474, loss = 0.00804106\n",
      "Iteration 29475, loss = 0.00804085\n",
      "Iteration 29476, loss = 0.00804064\n",
      "Iteration 29477, loss = 0.00804044\n",
      "Iteration 29478, loss = 0.00804023\n",
      "Iteration 29479, loss = 0.00804002\n",
      "Iteration 29480, loss = 0.00803982\n",
      "Iteration 29481, loss = 0.00803961\n",
      "Iteration 29482, loss = 0.00803940\n",
      "Iteration 29483, loss = 0.00803920\n",
      "Iteration 29484, loss = 0.00803899\n",
      "Iteration 29485, loss = 0.00803878\n",
      "Iteration 29486, loss = 0.00803858\n",
      "Iteration 29487, loss = 0.00803837\n",
      "Iteration 29488, loss = 0.00803816\n",
      "Iteration 29489, loss = 0.00803796\n",
      "Iteration 29490, loss = 0.00803775\n",
      "Iteration 29491, loss = 0.00803754\n",
      "Iteration 29492, loss = 0.00803734\n",
      "Iteration 29493, loss = 0.00803713\n",
      "Iteration 29494, loss = 0.00803693\n",
      "Iteration 29495, loss = 0.00803672\n",
      "Iteration 29496, loss = 0.00803651\n",
      "Iteration 29497, loss = 0.00803631\n",
      "Iteration 29498, loss = 0.00803610\n",
      "Iteration 29499, loss = 0.00803589\n",
      "Iteration 29500, loss = 0.00803569\n",
      "Iteration 29501, loss = 0.00803548\n",
      "Iteration 29502, loss = 0.00803528\n",
      "Iteration 29503, loss = 0.00803507\n",
      "Iteration 29504, loss = 0.00803486\n",
      "Iteration 29505, loss = 0.00803466\n",
      "Iteration 29506, loss = 0.00803445\n",
      "Iteration 29507, loss = 0.00803425\n",
      "Iteration 29508, loss = 0.00803404\n",
      "Iteration 29509, loss = 0.00803383\n",
      "Iteration 29510, loss = 0.00803363\n",
      "Iteration 29511, loss = 0.00803342\n",
      "Iteration 29512, loss = 0.00803322\n",
      "Iteration 29513, loss = 0.00803301\n",
      "Iteration 29514, loss = 0.00803280\n",
      "Iteration 29515, loss = 0.00803260\n",
      "Iteration 29516, loss = 0.00803239\n",
      "Iteration 29517, loss = 0.00803219\n",
      "Iteration 29518, loss = 0.00803198\n",
      "Iteration 29519, loss = 0.00803177\n",
      "Iteration 29520, loss = 0.00803157\n",
      "Iteration 29521, loss = 0.00803136\n",
      "Iteration 29522, loss = 0.00803116\n",
      "Iteration 29523, loss = 0.00803095\n",
      "Iteration 29524, loss = 0.00803075\n",
      "Iteration 29525, loss = 0.00803054\n",
      "Iteration 29526, loss = 0.00803033\n",
      "Iteration 29527, loss = 0.00803013\n",
      "Iteration 29528, loss = 0.00802992\n",
      "Iteration 29529, loss = 0.00802972\n",
      "Iteration 29530, loss = 0.00802951\n",
      "Iteration 29531, loss = 0.00802931\n",
      "Iteration 29532, loss = 0.00802910\n",
      "Iteration 29533, loss = 0.00802890\n",
      "Iteration 29534, loss = 0.00802869\n",
      "Iteration 29535, loss = 0.00802848\n",
      "Iteration 29536, loss = 0.00802828\n",
      "Iteration 29537, loss = 0.00802807\n",
      "Iteration 29538, loss = 0.00802787\n",
      "Iteration 29539, loss = 0.00802766\n",
      "Iteration 29540, loss = 0.00802746\n",
      "Iteration 29541, loss = 0.00802725\n",
      "Iteration 29542, loss = 0.00802705\n",
      "Iteration 29543, loss = 0.00802684\n",
      "Iteration 29544, loss = 0.00802664\n",
      "Iteration 29545, loss = 0.00802643\n",
      "Iteration 29546, loss = 0.00802622\n",
      "Iteration 29547, loss = 0.00802602\n",
      "Iteration 29548, loss = 0.00802581\n",
      "Iteration 29549, loss = 0.00802561\n",
      "Iteration 29550, loss = 0.00802540\n",
      "Iteration 29551, loss = 0.00802520\n",
      "Iteration 29552, loss = 0.00802499\n",
      "Iteration 29553, loss = 0.00802479\n",
      "Iteration 29554, loss = 0.00802458\n",
      "Iteration 29555, loss = 0.00802438\n",
      "Iteration 29556, loss = 0.00802417\n",
      "Iteration 29557, loss = 0.00802397\n",
      "Iteration 29558, loss = 0.00802376\n",
      "Iteration 29559, loss = 0.00802356\n",
      "Iteration 29560, loss = 0.00802335\n",
      "Iteration 29561, loss = 0.00802315\n",
      "Iteration 29562, loss = 0.00802294\n",
      "Iteration 29563, loss = 0.00802274\n",
      "Iteration 29564, loss = 0.00802253\n",
      "Iteration 29565, loss = 0.00802233\n",
      "Iteration 29566, loss = 0.00802212\n",
      "Iteration 29567, loss = 0.00802192\n",
      "Iteration 29568, loss = 0.00802171\n",
      "Iteration 29569, loss = 0.00802151\n",
      "Iteration 29570, loss = 0.00802130\n",
      "Iteration 29571, loss = 0.00802110\n",
      "Iteration 29572, loss = 0.00802089\n",
      "Iteration 29573, loss = 0.00802069\n",
      "Iteration 29574, loss = 0.00802048\n",
      "Iteration 29575, loss = 0.00802028\n",
      "Iteration 29576, loss = 0.00802007\n",
      "Iteration 29577, loss = 0.00801987\n",
      "Iteration 29578, loss = 0.00801966\n",
      "Iteration 29579, loss = 0.00801946\n",
      "Iteration 29580, loss = 0.00801926\n",
      "Iteration 29581, loss = 0.00801905\n",
      "Iteration 29582, loss = 0.00801885\n",
      "Iteration 29583, loss = 0.00801864\n",
      "Iteration 29584, loss = 0.00801844\n",
      "Iteration 29585, loss = 0.00801823\n",
      "Iteration 29586, loss = 0.00801803\n",
      "Iteration 29587, loss = 0.00801782\n",
      "Iteration 29588, loss = 0.00801762\n",
      "Iteration 29589, loss = 0.00801741\n",
      "Iteration 29590, loss = 0.00801721\n",
      "Iteration 29591, loss = 0.00801701\n",
      "Iteration 29592, loss = 0.00801680\n",
      "Iteration 29593, loss = 0.00801660\n",
      "Iteration 29594, loss = 0.00801639\n",
      "Iteration 29595, loss = 0.00801619\n",
      "Iteration 29596, loss = 0.00801598\n",
      "Iteration 29597, loss = 0.00801578\n",
      "Iteration 29598, loss = 0.00801557\n",
      "Iteration 29599, loss = 0.00801537\n",
      "Iteration 29600, loss = 0.00801517\n",
      "Iteration 29601, loss = 0.00801496\n",
      "Iteration 29602, loss = 0.00801476\n",
      "Iteration 29603, loss = 0.00801455\n",
      "Iteration 29604, loss = 0.00801435\n",
      "Iteration 29605, loss = 0.00801415\n",
      "Iteration 29606, loss = 0.00801394\n",
      "Iteration 29607, loss = 0.00801374\n",
      "Iteration 29608, loss = 0.00801353\n",
      "Iteration 29609, loss = 0.00801333\n",
      "Iteration 29610, loss = 0.00801312\n",
      "Iteration 29611, loss = 0.00801292\n",
      "Iteration 29612, loss = 0.00801272\n",
      "Iteration 29613, loss = 0.00801251\n",
      "Iteration 29614, loss = 0.00801231\n",
      "Iteration 29615, loss = 0.00801210\n",
      "Iteration 29616, loss = 0.00801190\n",
      "Iteration 29617, loss = 0.00801170\n",
      "Iteration 29618, loss = 0.00801149\n",
      "Iteration 29619, loss = 0.00801129\n",
      "Iteration 29620, loss = 0.00801108\n",
      "Iteration 29621, loss = 0.00801088\n",
      "Iteration 29622, loss = 0.00801068\n",
      "Iteration 29623, loss = 0.00801047\n",
      "Iteration 29624, loss = 0.00801027\n",
      "Iteration 29625, loss = 0.00801007\n",
      "Iteration 29626, loss = 0.00800986\n",
      "Iteration 29627, loss = 0.00800966\n",
      "Iteration 29628, loss = 0.00800945\n",
      "Iteration 29629, loss = 0.00800925\n",
      "Iteration 29630, loss = 0.00800905\n",
      "Iteration 29631, loss = 0.00800884\n",
      "Iteration 29632, loss = 0.00800864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29633, loss = 0.00800844\n",
      "Iteration 29634, loss = 0.00800823\n",
      "Iteration 29635, loss = 0.00800803\n",
      "Iteration 29636, loss = 0.00800783\n",
      "Iteration 29637, loss = 0.00800762\n",
      "Iteration 29638, loss = 0.00800742\n",
      "Iteration 29639, loss = 0.00800721\n",
      "Iteration 29640, loss = 0.00800701\n",
      "Iteration 29641, loss = 0.00800681\n",
      "Iteration 29642, loss = 0.00800660\n",
      "Iteration 29643, loss = 0.00800640\n",
      "Iteration 29644, loss = 0.00800620\n",
      "Iteration 29645, loss = 0.00800599\n",
      "Iteration 29646, loss = 0.00800579\n",
      "Iteration 29647, loss = 0.00800559\n",
      "Iteration 29648, loss = 0.00800538\n",
      "Iteration 29649, loss = 0.00800518\n",
      "Iteration 29650, loss = 0.00800498\n",
      "Iteration 29651, loss = 0.00800477\n",
      "Iteration 29652, loss = 0.00800457\n",
      "Iteration 29653, loss = 0.00800437\n",
      "Iteration 29654, loss = 0.00800416\n",
      "Iteration 29655, loss = 0.00800396\n",
      "Iteration 29656, loss = 0.00800376\n",
      "Iteration 29657, loss = 0.00800355\n",
      "Iteration 29658, loss = 0.00800335\n",
      "Iteration 29659, loss = 0.00800315\n",
      "Iteration 29660, loss = 0.00800294\n",
      "Iteration 29661, loss = 0.00800274\n",
      "Iteration 29662, loss = 0.00800254\n",
      "Iteration 29663, loss = 0.00800234\n",
      "Iteration 29664, loss = 0.00800213\n",
      "Iteration 29665, loss = 0.00800193\n",
      "Iteration 29666, loss = 0.00800173\n",
      "Iteration 29667, loss = 0.00800152\n",
      "Iteration 29668, loss = 0.00800132\n",
      "Iteration 29669, loss = 0.00800112\n",
      "Iteration 29670, loss = 0.00800091\n",
      "Iteration 29671, loss = 0.00800071\n",
      "Iteration 29672, loss = 0.00800051\n",
      "Iteration 29673, loss = 0.00800031\n",
      "Iteration 29674, loss = 0.00800010\n",
      "Iteration 29675, loss = 0.00799990\n",
      "Iteration 29676, loss = 0.00799970\n",
      "Iteration 29677, loss = 0.00799949\n",
      "Iteration 29678, loss = 0.00799929\n",
      "Iteration 29679, loss = 0.00799909\n",
      "Iteration 29680, loss = 0.00799889\n",
      "Iteration 29681, loss = 0.00799868\n",
      "Iteration 29682, loss = 0.00799848\n",
      "Iteration 29683, loss = 0.00799828\n",
      "Iteration 29684, loss = 0.00799807\n",
      "Iteration 29685, loss = 0.00799787\n",
      "Iteration 29686, loss = 0.00799767\n",
      "Iteration 29687, loss = 0.00799747\n",
      "Iteration 29688, loss = 0.00799726\n",
      "Iteration 29689, loss = 0.00799706\n",
      "Iteration 29690, loss = 0.00799686\n",
      "Iteration 29691, loss = 0.00799666\n",
      "Iteration 29692, loss = 0.00799645\n",
      "Iteration 29693, loss = 0.00799625\n",
      "Iteration 29694, loss = 0.00799605\n",
      "Iteration 29695, loss = 0.00799585\n",
      "Iteration 29696, loss = 0.00799564\n",
      "Iteration 29697, loss = 0.00799544\n",
      "Iteration 29698, loss = 0.00799524\n",
      "Iteration 29699, loss = 0.00799504\n",
      "Iteration 29700, loss = 0.00799483\n",
      "Iteration 29701, loss = 0.00799463\n",
      "Iteration 29702, loss = 0.00799443\n",
      "Iteration 29703, loss = 0.00799423\n",
      "Iteration 29704, loss = 0.00799403\n",
      "Iteration 29705, loss = 0.00799382\n",
      "Iteration 29706, loss = 0.00799362\n",
      "Iteration 29707, loss = 0.00799342\n",
      "Iteration 29708, loss = 0.00799322\n",
      "Iteration 29709, loss = 0.00799301\n",
      "Iteration 29710, loss = 0.00799281\n",
      "Iteration 29711, loss = 0.00799261\n",
      "Iteration 29712, loss = 0.00799241\n",
      "Iteration 29713, loss = 0.00799221\n",
      "Iteration 29714, loss = 0.00799200\n",
      "Iteration 29715, loss = 0.00799180\n",
      "Iteration 29716, loss = 0.00799160\n",
      "Iteration 29717, loss = 0.00799140\n",
      "Iteration 29718, loss = 0.00799119\n",
      "Iteration 29719, loss = 0.00799099\n",
      "Iteration 29720, loss = 0.00799079\n",
      "Iteration 29721, loss = 0.00799059\n",
      "Iteration 29722, loss = 0.00799039\n",
      "Iteration 29723, loss = 0.00799018\n",
      "Iteration 29724, loss = 0.00798998\n",
      "Iteration 29725, loss = 0.00798978\n",
      "Iteration 29726, loss = 0.00798958\n",
      "Iteration 29727, loss = 0.00798938\n",
      "Iteration 29728, loss = 0.00798918\n",
      "Iteration 29729, loss = 0.00798897\n",
      "Iteration 29730, loss = 0.00798877\n",
      "Iteration 29731, loss = 0.00798857\n",
      "Iteration 29732, loss = 0.00798837\n",
      "Iteration 29733, loss = 0.00798817\n",
      "Iteration 29734, loss = 0.00798796\n",
      "Iteration 29735, loss = 0.00798776\n",
      "Iteration 29736, loss = 0.00798756\n",
      "Iteration 29737, loss = 0.00798736\n",
      "Iteration 29738, loss = 0.00798716\n",
      "Iteration 29739, loss = 0.00798696\n",
      "Iteration 29740, loss = 0.00798675\n",
      "Iteration 29741, loss = 0.00798655\n",
      "Iteration 29742, loss = 0.00798635\n",
      "Iteration 29743, loss = 0.00798615\n",
      "Iteration 29744, loss = 0.00798595\n",
      "Iteration 29745, loss = 0.00798575\n",
      "Iteration 29746, loss = 0.00798554\n",
      "Iteration 29747, loss = 0.00798534\n",
      "Iteration 29748, loss = 0.00798514\n",
      "Iteration 29749, loss = 0.00798494\n",
      "Iteration 29750, loss = 0.00798474\n",
      "Iteration 29751, loss = 0.00798454\n",
      "Iteration 29752, loss = 0.00798434\n",
      "Iteration 29753, loss = 0.00798413\n",
      "Iteration 29754, loss = 0.00798393\n",
      "Iteration 29755, loss = 0.00798373\n",
      "Iteration 29756, loss = 0.00798353\n",
      "Iteration 29757, loss = 0.00798333\n",
      "Iteration 29758, loss = 0.00798313\n",
      "Iteration 29759, loss = 0.00798293\n",
      "Iteration 29760, loss = 0.00798273\n",
      "Iteration 29761, loss = 0.00798252\n",
      "Iteration 29762, loss = 0.00798232\n",
      "Iteration 29763, loss = 0.00798212\n",
      "Iteration 29764, loss = 0.00798192\n",
      "Iteration 29765, loss = 0.00798172\n",
      "Iteration 29766, loss = 0.00798152\n",
      "Iteration 29767, loss = 0.00798132\n",
      "Iteration 29768, loss = 0.00798112\n",
      "Iteration 29769, loss = 0.00798091\n",
      "Iteration 29770, loss = 0.00798071\n",
      "Iteration 29771, loss = 0.00798051\n",
      "Iteration 29772, loss = 0.00798031\n",
      "Iteration 29773, loss = 0.00798011\n",
      "Iteration 29774, loss = 0.00797991\n",
      "Iteration 29775, loss = 0.00797971\n",
      "Iteration 29776, loss = 0.00797951\n",
      "Iteration 29777, loss = 0.00797931\n",
      "Iteration 29778, loss = 0.00797911\n",
      "Iteration 29779, loss = 0.00797890\n",
      "Iteration 29780, loss = 0.00797870\n",
      "Iteration 29781, loss = 0.00797850\n",
      "Iteration 29782, loss = 0.00797830\n",
      "Iteration 29783, loss = 0.00797810\n",
      "Iteration 29784, loss = 0.00797790\n",
      "Iteration 29785, loss = 0.00797770\n",
      "Iteration 29786, loss = 0.00797750\n",
      "Iteration 29787, loss = 0.00797730\n",
      "Iteration 29788, loss = 0.00797710\n",
      "Iteration 29789, loss = 0.00797690\n",
      "Iteration 29790, loss = 0.00797670\n",
      "Iteration 29791, loss = 0.00797650\n",
      "Iteration 29792, loss = 0.00797629\n",
      "Iteration 29793, loss = 0.00797609\n",
      "Iteration 29794, loss = 0.00797589\n",
      "Iteration 29795, loss = 0.00797569\n",
      "Iteration 29796, loss = 0.00797549\n",
      "Iteration 29797, loss = 0.00797529\n",
      "Iteration 29798, loss = 0.00797509\n",
      "Iteration 29799, loss = 0.00797489\n",
      "Iteration 29800, loss = 0.00797469\n",
      "Iteration 29801, loss = 0.00797449\n",
      "Iteration 29802, loss = 0.00797429\n",
      "Iteration 29803, loss = 0.00797409\n",
      "Iteration 29804, loss = 0.00797389\n",
      "Iteration 29805, loss = 0.00797369\n",
      "Iteration 29806, loss = 0.00797349\n",
      "Iteration 29807, loss = 0.00797329\n",
      "Iteration 29808, loss = 0.00797309\n",
      "Iteration 29809, loss = 0.00797289\n",
      "Iteration 29810, loss = 0.00797269\n",
      "Iteration 29811, loss = 0.00797249\n",
      "Iteration 29812, loss = 0.00797229\n",
      "Iteration 29813, loss = 0.00797209\n",
      "Iteration 29814, loss = 0.00797188\n",
      "Iteration 29815, loss = 0.00797168\n",
      "Iteration 29816, loss = 0.00797148\n",
      "Iteration 29817, loss = 0.00797128\n",
      "Iteration 29818, loss = 0.00797108\n",
      "Iteration 29819, loss = 0.00797088\n",
      "Iteration 29820, loss = 0.00797068\n",
      "Iteration 29821, loss = 0.00797048\n",
      "Iteration 29822, loss = 0.00797028\n",
      "Iteration 29823, loss = 0.00797008\n",
      "Iteration 29824, loss = 0.00796988\n",
      "Iteration 29825, loss = 0.00796968\n",
      "Iteration 29826, loss = 0.00796948\n",
      "Iteration 29827, loss = 0.00796928\n",
      "Iteration 29828, loss = 0.00796908\n",
      "Iteration 29829, loss = 0.00796888\n",
      "Iteration 29830, loss = 0.00796868\n",
      "Iteration 29831, loss = 0.00796848\n",
      "Iteration 29832, loss = 0.00796828\n",
      "Iteration 29833, loss = 0.00796808\n",
      "Iteration 29834, loss = 0.00796788\n",
      "Iteration 29835, loss = 0.00796768\n",
      "Iteration 29836, loss = 0.00796748\n",
      "Iteration 29837, loss = 0.00796728\n",
      "Iteration 29838, loss = 0.00796708\n",
      "Iteration 29839, loss = 0.00796688\n",
      "Iteration 29840, loss = 0.00796668\n",
      "Iteration 29841, loss = 0.00796648\n",
      "Iteration 29842, loss = 0.00796628\n",
      "Iteration 29843, loss = 0.00796609\n",
      "Iteration 29844, loss = 0.00796589\n",
      "Iteration 29845, loss = 0.00796569\n",
      "Iteration 29846, loss = 0.00796549\n",
      "Iteration 29847, loss = 0.00796529\n",
      "Iteration 29848, loss = 0.00796509\n",
      "Iteration 29849, loss = 0.00796489\n",
      "Iteration 29850, loss = 0.00796469\n",
      "Iteration 29851, loss = 0.00796449\n",
      "Iteration 29852, loss = 0.00796429\n",
      "Iteration 29853, loss = 0.00796409\n",
      "Iteration 29854, loss = 0.00796389\n",
      "Iteration 29855, loss = 0.00796369\n",
      "Iteration 29856, loss = 0.00796349\n",
      "Iteration 29857, loss = 0.00796329\n",
      "Iteration 29858, loss = 0.00796309\n",
      "Iteration 29859, loss = 0.00796289\n",
      "Iteration 29860, loss = 0.00796269\n",
      "Iteration 29861, loss = 0.00796249\n",
      "Iteration 29862, loss = 0.00796229\n",
      "Iteration 29863, loss = 0.00796209\n",
      "Iteration 29864, loss = 0.00796190\n",
      "Iteration 29865, loss = 0.00796170\n",
      "Iteration 29866, loss = 0.00796150\n",
      "Iteration 29867, loss = 0.00796130\n",
      "Iteration 29868, loss = 0.00796110\n",
      "Iteration 29869, loss = 0.00796090\n",
      "Iteration 29870, loss = 0.00796070\n",
      "Iteration 29871, loss = 0.00796050\n",
      "Iteration 29872, loss = 0.00796030\n",
      "Iteration 29873, loss = 0.00796010\n",
      "Iteration 29874, loss = 0.00795990\n",
      "Iteration 29875, loss = 0.00795970\n",
      "Iteration 29876, loss = 0.00795950\n",
      "Iteration 29877, loss = 0.00795931\n",
      "Iteration 29878, loss = 0.00795911\n",
      "Iteration 29879, loss = 0.00795891\n",
      "Iteration 29880, loss = 0.00795871\n",
      "Iteration 29881, loss = 0.00795851\n",
      "Iteration 29882, loss = 0.00795831\n",
      "Iteration 29883, loss = 0.00795811\n",
      "Iteration 29884, loss = 0.00795791\n",
      "Iteration 29885, loss = 0.00795771\n",
      "Iteration 29886, loss = 0.00795751\n",
      "Iteration 29887, loss = 0.00795732\n",
      "Iteration 29888, loss = 0.00795712\n",
      "Iteration 29889, loss = 0.00795692\n",
      "Iteration 29890, loss = 0.00795672\n",
      "Iteration 29891, loss = 0.00795652\n",
      "Iteration 29892, loss = 0.00795632\n",
      "Iteration 29893, loss = 0.00795612\n",
      "Iteration 29894, loss = 0.00795592\n",
      "Iteration 29895, loss = 0.00795572\n",
      "Iteration 29896, loss = 0.00795553\n",
      "Iteration 29897, loss = 0.00795533\n",
      "Iteration 29898, loss = 0.00795513\n",
      "Iteration 29899, loss = 0.00795493\n",
      "Iteration 29900, loss = 0.00795473\n",
      "Iteration 29901, loss = 0.00795453\n",
      "Iteration 29902, loss = 0.00795433\n",
      "Iteration 29903, loss = 0.00795414\n",
      "Iteration 29904, loss = 0.00795394\n",
      "Iteration 29905, loss = 0.00795374\n",
      "Iteration 29906, loss = 0.00795354\n",
      "Iteration 29907, loss = 0.00795334\n",
      "Iteration 29908, loss = 0.00795314\n",
      "Iteration 29909, loss = 0.00795294\n",
      "Iteration 29910, loss = 0.00795275\n",
      "Iteration 29911, loss = 0.00795255\n",
      "Iteration 29912, loss = 0.00795235\n",
      "Iteration 29913, loss = 0.00795215\n",
      "Iteration 29914, loss = 0.00795195\n",
      "Iteration 29915, loss = 0.00795175\n",
      "Iteration 29916, loss = 0.00795155\n",
      "Iteration 29917, loss = 0.00795136\n",
      "Iteration 29918, loss = 0.00795116\n",
      "Iteration 29919, loss = 0.00795096\n",
      "Iteration 29920, loss = 0.00795076\n",
      "Iteration 29921, loss = 0.00795056\n",
      "Iteration 29922, loss = 0.00795036\n",
      "Iteration 29923, loss = 0.00795017\n",
      "Iteration 29924, loss = 0.00794997\n",
      "Iteration 29925, loss = 0.00794977\n",
      "Iteration 29926, loss = 0.00794957\n",
      "Iteration 29927, loss = 0.00794937\n",
      "Iteration 29928, loss = 0.00794918\n",
      "Iteration 29929, loss = 0.00794898\n",
      "Iteration 29930, loss = 0.00794878\n",
      "Iteration 29931, loss = 0.00794858\n",
      "Iteration 29932, loss = 0.00794838\n",
      "Iteration 29933, loss = 0.00794818\n",
      "Iteration 29934, loss = 0.00794799\n",
      "Iteration 29935, loss = 0.00794779\n",
      "Iteration 29936, loss = 0.00794759\n",
      "Iteration 29937, loss = 0.00794739\n",
      "Iteration 29938, loss = 0.00794719\n",
      "Iteration 29939, loss = 0.00794700\n",
      "Iteration 29940, loss = 0.00794680\n",
      "Iteration 29941, loss = 0.00794660\n",
      "Iteration 29942, loss = 0.00794640\n",
      "Iteration 29943, loss = 0.00794620\n",
      "Iteration 29944, loss = 0.00794601\n",
      "Iteration 29945, loss = 0.00794581\n",
      "Iteration 29946, loss = 0.00794561\n",
      "Iteration 29947, loss = 0.00794541\n",
      "Iteration 29948, loss = 0.00794522\n",
      "Iteration 29949, loss = 0.00794502\n",
      "Iteration 29950, loss = 0.00794482\n",
      "Iteration 29951, loss = 0.00794462\n",
      "Iteration 29952, loss = 0.00794442\n",
      "Iteration 29953, loss = 0.00794423\n",
      "Iteration 29954, loss = 0.00794403\n",
      "Iteration 29955, loss = 0.00794383\n",
      "Iteration 29956, loss = 0.00794363\n",
      "Iteration 29957, loss = 0.00794344\n",
      "Iteration 29958, loss = 0.00794324\n",
      "Iteration 29959, loss = 0.00794304\n",
      "Iteration 29960, loss = 0.00794284\n",
      "Iteration 29961, loss = 0.00794265\n",
      "Iteration 29962, loss = 0.00794245\n",
      "Iteration 29963, loss = 0.00794225\n",
      "Iteration 29964, loss = 0.00794205\n",
      "Iteration 29965, loss = 0.00794186\n",
      "Iteration 29966, loss = 0.00794166\n",
      "Iteration 29967, loss = 0.00794146\n",
      "Iteration 29968, loss = 0.00794126\n",
      "Iteration 29969, loss = 0.00794107\n",
      "Iteration 29970, loss = 0.00794087\n",
      "Iteration 29971, loss = 0.00794067\n",
      "Iteration 29972, loss = 0.00794047\n",
      "Iteration 29973, loss = 0.00794028\n",
      "Iteration 29974, loss = 0.00794008\n",
      "Iteration 29975, loss = 0.00793988\n",
      "Iteration 29976, loss = 0.00793968\n",
      "Iteration 29977, loss = 0.00793949\n",
      "Iteration 29978, loss = 0.00793929\n",
      "Iteration 29979, loss = 0.00793909\n",
      "Iteration 29980, loss = 0.00793890\n",
      "Iteration 29981, loss = 0.00793870\n",
      "Iteration 29982, loss = 0.00793850\n",
      "Iteration 29983, loss = 0.00793830\n",
      "Iteration 29984, loss = 0.00793811\n",
      "Iteration 29985, loss = 0.00793791\n",
      "Iteration 29986, loss = 0.00793771\n",
      "Iteration 29987, loss = 0.00793752\n",
      "Iteration 29988, loss = 0.00793732\n",
      "Iteration 29989, loss = 0.00793712\n",
      "Iteration 29990, loss = 0.00793692\n",
      "Iteration 29991, loss = 0.00793673\n",
      "Iteration 29992, loss = 0.00793653\n",
      "Iteration 29993, loss = 0.00793633\n",
      "Iteration 29994, loss = 0.00793614\n",
      "Iteration 29995, loss = 0.00793594\n",
      "Iteration 29996, loss = 0.00793574\n",
      "Iteration 29997, loss = 0.00793555\n",
      "Iteration 29998, loss = 0.00793535\n",
      "Iteration 29999, loss = 0.00793515\n",
      "Iteration 30000, loss = 0.00793495\n",
      "Iteration 30001, loss = 0.00793476\n",
      "Iteration 30002, loss = 0.00793456\n",
      "Iteration 30003, loss = 0.00793436\n",
      "Iteration 30004, loss = 0.00793417\n",
      "Iteration 30005, loss = 0.00793397\n",
      "Iteration 30006, loss = 0.00793377\n",
      "Iteration 30007, loss = 0.00793358\n",
      "Iteration 30008, loss = 0.00793338\n",
      "Iteration 30009, loss = 0.00793318\n",
      "Iteration 30010, loss = 0.00793299\n",
      "Iteration 30011, loss = 0.00793279\n",
      "Iteration 30012, loss = 0.00793259\n",
      "Iteration 30013, loss = 0.00793240\n",
      "Iteration 30014, loss = 0.00793220\n",
      "Iteration 30015, loss = 0.00793200\n",
      "Iteration 30016, loss = 0.00793181\n",
      "Iteration 30017, loss = 0.00793161\n",
      "Iteration 30018, loss = 0.00793141\n",
      "Iteration 30019, loss = 0.00793122\n",
      "Iteration 30020, loss = 0.00793102\n",
      "Iteration 30021, loss = 0.00793082\n",
      "Iteration 30022, loss = 0.00793063\n",
      "Iteration 30023, loss = 0.00793043\n",
      "Iteration 30024, loss = 0.00793024\n",
      "Iteration 30025, loss = 0.00793004\n",
      "Iteration 30026, loss = 0.00792984\n",
      "Iteration 30027, loss = 0.00792965\n",
      "Iteration 30028, loss = 0.00792945\n",
      "Iteration 30029, loss = 0.00792925\n",
      "Iteration 30030, loss = 0.00792906\n",
      "Iteration 30031, loss = 0.00792886\n",
      "Iteration 30032, loss = 0.00792866\n",
      "Iteration 30033, loss = 0.00792847\n",
      "Iteration 30034, loss = 0.00792827\n",
      "Iteration 30035, loss = 0.00792808\n",
      "Iteration 30036, loss = 0.00792788\n",
      "Iteration 30037, loss = 0.00792768\n",
      "Iteration 30038, loss = 0.00792749\n",
      "Iteration 30039, loss = 0.00792729\n",
      "Iteration 30040, loss = 0.00792710\n",
      "Iteration 30041, loss = 0.00792690\n",
      "Iteration 30042, loss = 0.00792670\n",
      "Iteration 30043, loss = 0.00792651\n",
      "Iteration 30044, loss = 0.00792631\n",
      "Iteration 30045, loss = 0.00792611\n",
      "Iteration 30046, loss = 0.00792592\n",
      "Iteration 30047, loss = 0.00792572\n",
      "Iteration 30048, loss = 0.00792553\n",
      "Iteration 30049, loss = 0.00792533\n",
      "Iteration 30050, loss = 0.00792513\n",
      "Iteration 30051, loss = 0.00792494\n",
      "Iteration 30052, loss = 0.00792474\n",
      "Iteration 30053, loss = 0.00792455\n",
      "Iteration 30054, loss = 0.00792435\n",
      "Iteration 30055, loss = 0.00792416\n",
      "Iteration 30056, loss = 0.00792396\n",
      "Iteration 30057, loss = 0.00792376\n",
      "Iteration 30058, loss = 0.00792357\n",
      "Iteration 30059, loss = 0.00792337\n",
      "Iteration 30060, loss = 0.00792318\n",
      "Iteration 30061, loss = 0.00792298\n",
      "Iteration 30062, loss = 0.00792278\n",
      "Iteration 30063, loss = 0.00792259\n",
      "Iteration 30064, loss = 0.00792239\n",
      "Iteration 30065, loss = 0.00792220\n",
      "Iteration 30066, loss = 0.00792200\n",
      "Iteration 30067, loss = 0.00792181\n",
      "Iteration 30068, loss = 0.00792161\n",
      "Iteration 30069, loss = 0.00792141\n",
      "Iteration 30070, loss = 0.00792122\n",
      "Iteration 30071, loss = 0.00792102\n",
      "Iteration 30072, loss = 0.00792083\n",
      "Iteration 30073, loss = 0.00792063\n",
      "Iteration 30074, loss = 0.00792044\n",
      "Iteration 30075, loss = 0.00792024\n",
      "Iteration 30076, loss = 0.00792005\n",
      "Iteration 30077, loss = 0.00791985\n",
      "Iteration 30078, loss = 0.00791965\n",
      "Iteration 30079, loss = 0.00791946\n",
      "Iteration 30080, loss = 0.00791926\n",
      "Iteration 30081, loss = 0.00791907\n",
      "Iteration 30082, loss = 0.00791887\n",
      "Iteration 30083, loss = 0.00791868\n",
      "Iteration 30084, loss = 0.00791848\n",
      "Iteration 30085, loss = 0.00791829\n",
      "Iteration 30086, loss = 0.00791809\n",
      "Iteration 30087, loss = 0.00791790\n",
      "Iteration 30088, loss = 0.00791770\n",
      "Iteration 30089, loss = 0.00791751\n",
      "Iteration 30090, loss = 0.00791731\n",
      "Iteration 30091, loss = 0.00791712\n",
      "Iteration 30092, loss = 0.00791692\n",
      "Iteration 30093, loss = 0.00791673\n",
      "Iteration 30094, loss = 0.00791653\n",
      "Iteration 30095, loss = 0.00791633\n",
      "Iteration 30096, loss = 0.00791614\n",
      "Iteration 30097, loss = 0.00791594\n",
      "Iteration 30098, loss = 0.00791575\n",
      "Iteration 30099, loss = 0.00791555\n",
      "Iteration 30100, loss = 0.00791536\n",
      "Iteration 30101, loss = 0.00791516\n",
      "Iteration 30102, loss = 0.00791497\n",
      "Iteration 30103, loss = 0.00791477\n",
      "Iteration 30104, loss = 0.00791458\n",
      "Iteration 30105, loss = 0.00791438\n",
      "Iteration 30106, loss = 0.00791419\n",
      "Iteration 30107, loss = 0.00791399\n",
      "Iteration 30108, loss = 0.00791380\n",
      "Iteration 30109, loss = 0.00791360\n",
      "Iteration 30110, loss = 0.00791341\n",
      "Iteration 30111, loss = 0.00791321\n",
      "Iteration 30112, loss = 0.00791302\n",
      "Iteration 30113, loss = 0.00791282\n",
      "Iteration 30114, loss = 0.00791263\n",
      "Iteration 30115, loss = 0.00791244\n",
      "Iteration 30116, loss = 0.00791224\n",
      "Iteration 30117, loss = 0.00791205\n",
      "Iteration 30118, loss = 0.00791185\n",
      "Iteration 30119, loss = 0.00791166\n",
      "Iteration 30120, loss = 0.00791146\n",
      "Iteration 30121, loss = 0.00791127\n",
      "Iteration 30122, loss = 0.00791107\n",
      "Iteration 30123, loss = 0.00791088\n",
      "Iteration 30124, loss = 0.00791068\n",
      "Iteration 30125, loss = 0.00791049\n",
      "Iteration 30126, loss = 0.00791029\n",
      "Iteration 30127, loss = 0.00791010\n",
      "Iteration 30128, loss = 0.00790990\n",
      "Iteration 30129, loss = 0.00790971\n",
      "Iteration 30130, loss = 0.00790952\n",
      "Iteration 30131, loss = 0.00790932\n",
      "Iteration 30132, loss = 0.00790913\n",
      "Iteration 30133, loss = 0.00790893\n",
      "Iteration 30134, loss = 0.00790874\n",
      "Iteration 30135, loss = 0.00790854\n",
      "Iteration 30136, loss = 0.00790835\n",
      "Iteration 30137, loss = 0.00790815\n",
      "Iteration 30138, loss = 0.00790796\n",
      "Iteration 30139, loss = 0.00790777\n",
      "Iteration 30140, loss = 0.00790757\n",
      "Iteration 30141, loss = 0.00790738\n",
      "Iteration 30142, loss = 0.00790718\n",
      "Iteration 30143, loss = 0.00790699\n",
      "Iteration 30144, loss = 0.00790679\n",
      "Iteration 30145, loss = 0.00790660\n",
      "Iteration 30146, loss = 0.00790641\n",
      "Iteration 30147, loss = 0.00790621\n",
      "Iteration 30148, loss = 0.00790602\n",
      "Iteration 30149, loss = 0.00790582\n",
      "Iteration 30150, loss = 0.00790563\n",
      "Iteration 30151, loss = 0.00790543\n",
      "Iteration 30152, loss = 0.00790524\n",
      "Iteration 30153, loss = 0.00790505\n",
      "Iteration 30154, loss = 0.00790485\n",
      "Iteration 30155, loss = 0.00790466\n",
      "Iteration 30156, loss = 0.00790446\n",
      "Iteration 30157, loss = 0.00790427\n",
      "Iteration 30158, loss = 0.00790408\n",
      "Iteration 30159, loss = 0.00790388\n",
      "Iteration 30160, loss = 0.00790369\n",
      "Iteration 30161, loss = 0.00790349\n",
      "Iteration 30162, loss = 0.00790330\n",
      "Iteration 30163, loss = 0.00790311\n",
      "Iteration 30164, loss = 0.00790291\n",
      "Iteration 30165, loss = 0.00790272\n",
      "Iteration 30166, loss = 0.00790252\n",
      "Iteration 30167, loss = 0.00790233\n",
      "Iteration 30168, loss = 0.00790214\n",
      "Iteration 30169, loss = 0.00790194\n",
      "Iteration 30170, loss = 0.00790175\n",
      "Iteration 30171, loss = 0.00790155\n",
      "Iteration 30172, loss = 0.00790136\n",
      "Iteration 30173, loss = 0.00790117\n",
      "Iteration 30174, loss = 0.00790097\n",
      "Iteration 30175, loss = 0.00790078\n",
      "Iteration 30176, loss = 0.00790059\n",
      "Iteration 30177, loss = 0.00790039\n",
      "Iteration 30178, loss = 0.00790020\n",
      "Iteration 30179, loss = 0.00790000\n",
      "Iteration 30180, loss = 0.00789981\n",
      "Iteration 30181, loss = 0.00789962\n",
      "Iteration 30182, loss = 0.00789942\n",
      "Iteration 30183, loss = 0.00789923\n",
      "Iteration 30184, loss = 0.00789904\n",
      "Iteration 30185, loss = 0.00789884\n",
      "Iteration 30186, loss = 0.00789865\n",
      "Iteration 30187, loss = 0.00789846\n",
      "Iteration 30188, loss = 0.00789826\n",
      "Iteration 30189, loss = 0.00789807\n",
      "Iteration 30190, loss = 0.00789788\n",
      "Iteration 30191, loss = 0.00789768\n",
      "Iteration 30192, loss = 0.00789749\n",
      "Iteration 30193, loss = 0.00789730\n",
      "Iteration 30194, loss = 0.00789710\n",
      "Iteration 30195, loss = 0.00789691\n",
      "Iteration 30196, loss = 0.00789672\n",
      "Iteration 30197, loss = 0.00789652\n",
      "Iteration 30198, loss = 0.00789633\n",
      "Iteration 30199, loss = 0.00789614\n",
      "Iteration 30200, loss = 0.00789594\n",
      "Iteration 30201, loss = 0.00789575\n",
      "Iteration 30202, loss = 0.00789556\n",
      "Iteration 30203, loss = 0.00789536\n",
      "Iteration 30204, loss = 0.00789517\n",
      "Iteration 30205, loss = 0.00789498\n",
      "Iteration 30206, loss = 0.00789478\n",
      "Iteration 30207, loss = 0.00789459\n",
      "Iteration 30208, loss = 0.00789440\n",
      "Iteration 30209, loss = 0.00789420\n",
      "Iteration 30210, loss = 0.00789401\n",
      "Iteration 30211, loss = 0.00789382\n",
      "Iteration 30212, loss = 0.00789362\n",
      "Iteration 30213, loss = 0.00789343\n",
      "Iteration 30214, loss = 0.00789324\n",
      "Iteration 30215, loss = 0.00789304\n",
      "Iteration 30216, loss = 0.00789285\n",
      "Iteration 30217, loss = 0.00789266\n",
      "Iteration 30218, loss = 0.00789247\n",
      "Iteration 30219, loss = 0.00789227\n",
      "Iteration 30220, loss = 0.00789208\n",
      "Iteration 30221, loss = 0.00789189\n",
      "Iteration 30222, loss = 0.00789169\n",
      "Iteration 30223, loss = 0.00789150\n",
      "Iteration 30224, loss = 0.00789131\n",
      "Iteration 30225, loss = 0.00789112\n",
      "Iteration 30226, loss = 0.00789092\n",
      "Iteration 30227, loss = 0.00789073\n",
      "Iteration 30228, loss = 0.00789054\n",
      "Iteration 30229, loss = 0.00789034\n",
      "Iteration 30230, loss = 0.00789015\n",
      "Iteration 30231, loss = 0.00788996\n",
      "Iteration 30232, loss = 0.00788977\n",
      "Iteration 30233, loss = 0.00788957\n",
      "Iteration 30234, loss = 0.00788938\n",
      "Iteration 30235, loss = 0.00788919\n",
      "Iteration 30236, loss = 0.00788899\n",
      "Iteration 30237, loss = 0.00788880\n",
      "Iteration 30238, loss = 0.00788861\n",
      "Iteration 30239, loss = 0.00788842\n",
      "Iteration 30240, loss = 0.00788822\n",
      "Iteration 30241, loss = 0.00788803\n",
      "Iteration 30242, loss = 0.00788784\n",
      "Iteration 30243, loss = 0.00788765\n",
      "Iteration 30244, loss = 0.00788745\n",
      "Iteration 30245, loss = 0.00788726\n",
      "Iteration 30246, loss = 0.00788707\n",
      "Iteration 30247, loss = 0.00788688\n",
      "Iteration 30248, loss = 0.00788668\n",
      "Iteration 30249, loss = 0.00788649\n",
      "Iteration 30250, loss = 0.00788630\n",
      "Iteration 30251, loss = 0.00788611\n",
      "Iteration 30252, loss = 0.00788591\n",
      "Iteration 30253, loss = 0.00788572\n",
      "Iteration 30254, loss = 0.00788553\n",
      "Iteration 30255, loss = 0.00788534\n",
      "Iteration 30256, loss = 0.00788515\n",
      "Iteration 30257, loss = 0.00788495\n",
      "Iteration 30258, loss = 0.00788476\n",
      "Iteration 30259, loss = 0.00788457\n",
      "Iteration 30260, loss = 0.00788438\n",
      "Iteration 30261, loss = 0.00788418\n",
      "Iteration 30262, loss = 0.00788399\n",
      "Iteration 30263, loss = 0.00788380\n",
      "Iteration 30264, loss = 0.00788361\n",
      "Iteration 30265, loss = 0.00788342\n",
      "Iteration 30266, loss = 0.00788322\n",
      "Iteration 30267, loss = 0.00788303\n",
      "Iteration 30268, loss = 0.00788284\n",
      "Iteration 30269, loss = 0.00788265\n",
      "Iteration 30270, loss = 0.00788245\n",
      "Iteration 30271, loss = 0.00788226\n",
      "Iteration 30272, loss = 0.00788207\n",
      "Iteration 30273, loss = 0.00788188\n",
      "Iteration 30274, loss = 0.00788169\n",
      "Iteration 30275, loss = 0.00788149\n",
      "Iteration 30276, loss = 0.00788130\n",
      "Iteration 30277, loss = 0.00788111\n",
      "Iteration 30278, loss = 0.00788092\n",
      "Iteration 30279, loss = 0.00788073\n",
      "Iteration 30280, loss = 0.00788054\n",
      "Iteration 30281, loss = 0.00788034\n",
      "Iteration 30282, loss = 0.00788015\n",
      "Iteration 30283, loss = 0.00787996\n",
      "Iteration 30284, loss = 0.00787977\n",
      "Iteration 30285, loss = 0.00787958\n",
      "Iteration 30286, loss = 0.00787938\n",
      "Iteration 30287, loss = 0.00787919\n",
      "Iteration 30288, loss = 0.00787900\n",
      "Iteration 30289, loss = 0.00787881\n",
      "Iteration 30290, loss = 0.00787862\n",
      "Iteration 30291, loss = 0.00787843\n",
      "Iteration 30292, loss = 0.00787823\n",
      "Iteration 30293, loss = 0.00787804\n",
      "Iteration 30294, loss = 0.00787785\n",
      "Iteration 30295, loss = 0.00787766\n",
      "Iteration 30296, loss = 0.00787747\n",
      "Iteration 30297, loss = 0.00787728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30298, loss = 0.00787708\n",
      "Iteration 30299, loss = 0.00787689\n",
      "Iteration 30300, loss = 0.00787670\n",
      "Iteration 30301, loss = 0.00787651\n",
      "Iteration 30302, loss = 0.00787632\n",
      "Iteration 30303, loss = 0.00787613\n",
      "Iteration 30304, loss = 0.00787593\n",
      "Iteration 30305, loss = 0.00787574\n",
      "Iteration 30306, loss = 0.00787555\n",
      "Iteration 30307, loss = 0.00787536\n",
      "Iteration 30308, loss = 0.00787517\n",
      "Iteration 30309, loss = 0.00787498\n",
      "Iteration 30310, loss = 0.00787479\n",
      "Iteration 30311, loss = 0.00787459\n",
      "Iteration 30312, loss = 0.00787440\n",
      "Iteration 30313, loss = 0.00787421\n",
      "Iteration 30314, loss = 0.00787402\n",
      "Iteration 30315, loss = 0.00787383\n",
      "Iteration 30316, loss = 0.00787364\n",
      "Iteration 30317, loss = 0.00787345\n",
      "Iteration 30318, loss = 0.00787326\n",
      "Iteration 30319, loss = 0.00787306\n",
      "Iteration 30320, loss = 0.00787287\n",
      "Iteration 30321, loss = 0.00787268\n",
      "Iteration 30322, loss = 0.00787249\n",
      "Iteration 30323, loss = 0.00787230\n",
      "Iteration 30324, loss = 0.00787211\n",
      "Iteration 30325, loss = 0.00787192\n",
      "Iteration 30326, loss = 0.00787173\n",
      "Iteration 30327, loss = 0.00787154\n",
      "Iteration 30328, loss = 0.00787134\n",
      "Iteration 30329, loss = 0.00787115\n",
      "Iteration 30330, loss = 0.00787096\n",
      "Iteration 30331, loss = 0.00787077\n",
      "Iteration 30332, loss = 0.00787058\n",
      "Iteration 30333, loss = 0.00787039\n",
      "Iteration 30334, loss = 0.00787020\n",
      "Iteration 30335, loss = 0.00787001\n",
      "Iteration 30336, loss = 0.00786982\n",
      "Iteration 30337, loss = 0.00786963\n",
      "Iteration 30338, loss = 0.00786943\n",
      "Iteration 30339, loss = 0.00786924\n",
      "Iteration 30340, loss = 0.00786905\n",
      "Iteration 30341, loss = 0.00786886\n",
      "Iteration 30342, loss = 0.00786867\n",
      "Iteration 30343, loss = 0.00786848\n",
      "Iteration 30344, loss = 0.00786829\n",
      "Iteration 30345, loss = 0.00786810\n",
      "Iteration 30346, loss = 0.00786791\n",
      "Iteration 30347, loss = 0.00786772\n",
      "Iteration 30348, loss = 0.00786753\n",
      "Iteration 30349, loss = 0.00786734\n",
      "Iteration 30350, loss = 0.00786715\n",
      "Iteration 30351, loss = 0.00786695\n",
      "Iteration 30352, loss = 0.00786676\n",
      "Iteration 30353, loss = 0.00786657\n",
      "Iteration 30354, loss = 0.00786638\n",
      "Iteration 30355, loss = 0.00786619\n",
      "Iteration 30356, loss = 0.00786600\n",
      "Iteration 30357, loss = 0.00786581\n",
      "Iteration 30358, loss = 0.00786562\n",
      "Iteration 30359, loss = 0.00786543\n",
      "Iteration 30360, loss = 0.00786524\n",
      "Iteration 30361, loss = 0.00786505\n",
      "Iteration 30362, loss = 0.00786486\n",
      "Iteration 30363, loss = 0.00786467\n",
      "Iteration 30364, loss = 0.00786448\n",
      "Iteration 30365, loss = 0.00786429\n",
      "Iteration 30366, loss = 0.00786410\n",
      "Iteration 30367, loss = 0.00786391\n",
      "Iteration 30368, loss = 0.00786372\n",
      "Iteration 30369, loss = 0.00786353\n",
      "Iteration 30370, loss = 0.00786334\n",
      "Iteration 30371, loss = 0.00786315\n",
      "Iteration 30372, loss = 0.00786295\n",
      "Iteration 30373, loss = 0.00786276\n",
      "Iteration 30374, loss = 0.00786257\n",
      "Iteration 30375, loss = 0.00786238\n",
      "Iteration 30376, loss = 0.00786219\n",
      "Iteration 30377, loss = 0.00786200\n",
      "Iteration 30378, loss = 0.00786181\n",
      "Iteration 30379, loss = 0.00786162\n",
      "Iteration 30380, loss = 0.00786143\n",
      "Iteration 30381, loss = 0.00786124\n",
      "Iteration 30382, loss = 0.00786105\n",
      "Iteration 30383, loss = 0.00786086\n",
      "Iteration 30384, loss = 0.00786067\n",
      "Iteration 30385, loss = 0.00786048\n",
      "Iteration 30386, loss = 0.00786029\n",
      "Iteration 30387, loss = 0.00786010\n",
      "Iteration 30388, loss = 0.00785991\n",
      "Iteration 30389, loss = 0.00785972\n",
      "Iteration 30390, loss = 0.00785953\n",
      "Iteration 30391, loss = 0.00785934\n",
      "Iteration 30392, loss = 0.00785915\n",
      "Iteration 30393, loss = 0.00785896\n",
      "Iteration 30394, loss = 0.00785877\n",
      "Iteration 30395, loss = 0.00785858\n",
      "Iteration 30396, loss = 0.00785839\n",
      "Iteration 30397, loss = 0.00785820\n",
      "Iteration 30398, loss = 0.00785801\n",
      "Iteration 30399, loss = 0.00785782\n",
      "Iteration 30400, loss = 0.00785763\n",
      "Iteration 30401, loss = 0.00785744\n",
      "Iteration 30402, loss = 0.00785725\n",
      "Iteration 30403, loss = 0.00785706\n",
      "Iteration 30404, loss = 0.00785687\n",
      "Iteration 30405, loss = 0.00785668\n",
      "Iteration 30406, loss = 0.00785649\n",
      "Iteration 30407, loss = 0.00785631\n",
      "Iteration 30408, loss = 0.00785612\n",
      "Iteration 30409, loss = 0.00785593\n",
      "Iteration 30410, loss = 0.00785574\n",
      "Iteration 30411, loss = 0.00785555\n",
      "Iteration 30412, loss = 0.00785536\n",
      "Iteration 30413, loss = 0.00785517\n",
      "Iteration 30414, loss = 0.00785498\n",
      "Iteration 30415, loss = 0.00785479\n",
      "Iteration 30416, loss = 0.00785460\n",
      "Iteration 30417, loss = 0.00785441\n",
      "Iteration 30418, loss = 0.00785422\n",
      "Iteration 30419, loss = 0.00785403\n",
      "Iteration 30420, loss = 0.00785384\n",
      "Iteration 30421, loss = 0.00785365\n",
      "Iteration 30422, loss = 0.00785346\n",
      "Iteration 30423, loss = 0.00785327\n",
      "Iteration 30424, loss = 0.00785308\n",
      "Iteration 30425, loss = 0.00785289\n",
      "Iteration 30426, loss = 0.00785270\n",
      "Iteration 30427, loss = 0.00785251\n",
      "Iteration 30428, loss = 0.00785233\n",
      "Iteration 30429, loss = 0.00785214\n",
      "Iteration 30430, loss = 0.00785195\n",
      "Iteration 30431, loss = 0.00785176\n",
      "Iteration 30432, loss = 0.00785157\n",
      "Iteration 30433, loss = 0.00785138\n",
      "Iteration 30434, loss = 0.00785119\n",
      "Iteration 30435, loss = 0.00785100\n",
      "Iteration 30436, loss = 0.00785081\n",
      "Iteration 30437, loss = 0.00785062\n",
      "Iteration 30438, loss = 0.00785043\n",
      "Iteration 30439, loss = 0.00785024\n",
      "Iteration 30440, loss = 0.00785005\n",
      "Iteration 30441, loss = 0.00784987\n",
      "Iteration 30442, loss = 0.00784968\n",
      "Iteration 30443, loss = 0.00784949\n",
      "Iteration 30444, loss = 0.00784930\n",
      "Iteration 30445, loss = 0.00784911\n",
      "Iteration 30446, loss = 0.00784892\n",
      "Iteration 30447, loss = 0.00784873\n",
      "Iteration 30448, loss = 0.00784854\n",
      "Iteration 30449, loss = 0.00784835\n",
      "Iteration 30450, loss = 0.00784816\n",
      "Iteration 30451, loss = 0.00784798\n",
      "Iteration 30452, loss = 0.00784779\n",
      "Iteration 30453, loss = 0.00784760\n",
      "Iteration 30454, loss = 0.00784741\n",
      "Iteration 30455, loss = 0.00784722\n",
      "Iteration 30456, loss = 0.00784703\n",
      "Iteration 30457, loss = 0.00784684\n",
      "Iteration 30458, loss = 0.00784665\n",
      "Iteration 30459, loss = 0.00784646\n",
      "Iteration 30460, loss = 0.00784628\n",
      "Iteration 30461, loss = 0.00784609\n",
      "Iteration 30462, loss = 0.00784590\n",
      "Iteration 30463, loss = 0.00784571\n",
      "Iteration 30464, loss = 0.00784552\n",
      "Iteration 30465, loss = 0.00784533\n",
      "Iteration 30466, loss = 0.00784514\n",
      "Iteration 30467, loss = 0.00784495\n",
      "Iteration 30468, loss = 0.00784477\n",
      "Iteration 30469, loss = 0.00784458\n",
      "Iteration 30470, loss = 0.00784439\n",
      "Iteration 30471, loss = 0.00784420\n",
      "Iteration 30472, loss = 0.00784401\n",
      "Iteration 30473, loss = 0.00784382\n",
      "Iteration 30474, loss = 0.00784363\n",
      "Iteration 30475, loss = 0.00784345\n",
      "Iteration 30476, loss = 0.00784326\n",
      "Iteration 30477, loss = 0.00784307\n",
      "Iteration 30478, loss = 0.00784288\n",
      "Iteration 30479, loss = 0.00784269\n",
      "Iteration 30480, loss = 0.00784250\n",
      "Iteration 30481, loss = 0.00784231\n",
      "Iteration 30482, loss = 0.00784213\n",
      "Iteration 30483, loss = 0.00784194\n",
      "Iteration 30484, loss = 0.00784175\n",
      "Iteration 30485, loss = 0.00784156\n",
      "Iteration 30486, loss = 0.00784137\n",
      "Iteration 30487, loss = 0.00784118\n",
      "Iteration 30488, loss = 0.00784100\n",
      "Iteration 30489, loss = 0.00784081\n",
      "Iteration 30490, loss = 0.00784062\n",
      "Iteration 30491, loss = 0.00784043\n",
      "Iteration 30492, loss = 0.00784024\n",
      "Iteration 30493, loss = 0.00784006\n",
      "Iteration 30494, loss = 0.00783987\n",
      "Iteration 30495, loss = 0.00783968\n",
      "Iteration 30496, loss = 0.00783949\n",
      "Iteration 30497, loss = 0.00783930\n",
      "Iteration 30498, loss = 0.00783911\n",
      "Iteration 30499, loss = 0.00783893\n",
      "Iteration 30500, loss = 0.00783874\n",
      "Iteration 30501, loss = 0.00783855\n",
      "Iteration 30502, loss = 0.00783836\n",
      "Iteration 30503, loss = 0.00783817\n",
      "Iteration 30504, loss = 0.00783799\n",
      "Iteration 30505, loss = 0.00783780\n",
      "Iteration 30506, loss = 0.00783761\n",
      "Iteration 30507, loss = 0.00783742\n",
      "Iteration 30508, loss = 0.00783723\n",
      "Iteration 30509, loss = 0.00783705\n",
      "Iteration 30510, loss = 0.00783686\n",
      "Iteration 30511, loss = 0.00783667\n",
      "Iteration 30512, loss = 0.00783648\n",
      "Iteration 30513, loss = 0.00783629\n",
      "Iteration 30514, loss = 0.00783611\n",
      "Iteration 30515, loss = 0.00783592\n",
      "Iteration 30516, loss = 0.00783573\n",
      "Iteration 30517, loss = 0.00783554\n",
      "Iteration 30518, loss = 0.00783535\n",
      "Iteration 30519, loss = 0.00783517\n",
      "Iteration 30520, loss = 0.00783498\n",
      "Iteration 30521, loss = 0.00783479\n",
      "Iteration 30522, loss = 0.00783460\n",
      "Iteration 30523, loss = 0.00783442\n",
      "Iteration 30524, loss = 0.00783423\n",
      "Iteration 30525, loss = 0.00783404\n",
      "Iteration 30526, loss = 0.00783385\n",
      "Iteration 30527, loss = 0.00783367\n",
      "Iteration 30528, loss = 0.00783348\n",
      "Iteration 30529, loss = 0.00783329\n",
      "Iteration 30530, loss = 0.00783310\n",
      "Iteration 30531, loss = 0.00783291\n",
      "Iteration 30532, loss = 0.00783273\n",
      "Iteration 30533, loss = 0.00783254\n",
      "Iteration 30534, loss = 0.00783235\n",
      "Iteration 30535, loss = 0.00783216\n",
      "Iteration 30536, loss = 0.00783198\n",
      "Iteration 30537, loss = 0.00783179\n",
      "Iteration 30538, loss = 0.00783160\n",
      "Iteration 30539, loss = 0.00783141\n",
      "Iteration 30540, loss = 0.00783123\n",
      "Iteration 30541, loss = 0.00783104\n",
      "Iteration 30542, loss = 0.00783085\n",
      "Iteration 30543, loss = 0.00783067\n",
      "Iteration 30544, loss = 0.00783048\n",
      "Iteration 30545, loss = 0.00783029\n",
      "Iteration 30546, loss = 0.00783010\n",
      "Iteration 30547, loss = 0.00782992\n",
      "Iteration 30548, loss = 0.00782973\n",
      "Iteration 30549, loss = 0.00782954\n",
      "Iteration 30550, loss = 0.00782935\n",
      "Iteration 30551, loss = 0.00782917\n",
      "Iteration 30552, loss = 0.00782898\n",
      "Iteration 30553, loss = 0.00782879\n",
      "Iteration 30554, loss = 0.00782860\n",
      "Iteration 30555, loss = 0.00782842\n",
      "Iteration 30556, loss = 0.00782823\n",
      "Iteration 30557, loss = 0.00782804\n",
      "Iteration 30558, loss = 0.00782786\n",
      "Iteration 30559, loss = 0.00782767\n",
      "Iteration 30560, loss = 0.00782748\n",
      "Iteration 30561, loss = 0.00782730\n",
      "Iteration 30562, loss = 0.00782711\n",
      "Iteration 30563, loss = 0.00782692\n",
      "Iteration 30564, loss = 0.00782673\n",
      "Iteration 30565, loss = 0.00782655\n",
      "Iteration 30566, loss = 0.00782636\n",
      "Iteration 30567, loss = 0.00782617\n",
      "Iteration 30568, loss = 0.00782599\n",
      "Iteration 30569, loss = 0.00782580\n",
      "Iteration 30570, loss = 0.00782561\n",
      "Iteration 30571, loss = 0.00782543\n",
      "Iteration 30572, loss = 0.00782524\n",
      "Iteration 30573, loss = 0.00782505\n",
      "Iteration 30574, loss = 0.00782486\n",
      "Iteration 30575, loss = 0.00782468\n",
      "Iteration 30576, loss = 0.00782449\n",
      "Iteration 30577, loss = 0.00782430\n",
      "Iteration 30578, loss = 0.00782412\n",
      "Iteration 30579, loss = 0.00782393\n",
      "Iteration 30580, loss = 0.00782374\n",
      "Iteration 30581, loss = 0.00782356\n",
      "Iteration 30582, loss = 0.00782337\n",
      "Iteration 30583, loss = 0.00782318\n",
      "Iteration 30584, loss = 0.00782300\n",
      "Iteration 30585, loss = 0.00782281\n",
      "Iteration 30586, loss = 0.00782262\n",
      "Iteration 30587, loss = 0.00782244\n",
      "Iteration 30588, loss = 0.00782225\n",
      "Iteration 30589, loss = 0.00782206\n",
      "Iteration 30590, loss = 0.00782188\n",
      "Iteration 30591, loss = 0.00782169\n",
      "Iteration 30592, loss = 0.00782150\n",
      "Iteration 30593, loss = 0.00782132\n",
      "Iteration 30594, loss = 0.00782113\n",
      "Iteration 30595, loss = 0.00782094\n",
      "Iteration 30596, loss = 0.00782076\n",
      "Iteration 30597, loss = 0.00782057\n",
      "Iteration 30598, loss = 0.00782038\n",
      "Iteration 30599, loss = 0.00782020\n",
      "Iteration 30600, loss = 0.00782001\n",
      "Iteration 30601, loss = 0.00781983\n",
      "Iteration 30602, loss = 0.00781964\n",
      "Iteration 30603, loss = 0.00781945\n",
      "Iteration 30604, loss = 0.00781927\n",
      "Iteration 30605, loss = 0.00781908\n",
      "Iteration 30606, loss = 0.00781889\n",
      "Iteration 30607, loss = 0.00781871\n",
      "Iteration 30608, loss = 0.00781852\n",
      "Iteration 30609, loss = 0.00781833\n",
      "Iteration 30610, loss = 0.00781815\n",
      "Iteration 30611, loss = 0.00781796\n",
      "Iteration 30612, loss = 0.00781778\n",
      "Iteration 30613, loss = 0.00781759\n",
      "Iteration 30614, loss = 0.00781740\n",
      "Iteration 30615, loss = 0.00781722\n",
      "Iteration 30616, loss = 0.00781703\n",
      "Iteration 30617, loss = 0.00781685\n",
      "Iteration 30618, loss = 0.00781666\n",
      "Iteration 30619, loss = 0.00781647\n",
      "Iteration 30620, loss = 0.00781629\n",
      "Iteration 30621, loss = 0.00781610\n",
      "Iteration 30622, loss = 0.00781591\n",
      "Iteration 30623, loss = 0.00781573\n",
      "Iteration 30624, loss = 0.00781554\n",
      "Iteration 30625, loss = 0.00781536\n",
      "Iteration 30626, loss = 0.00781517\n",
      "Iteration 30627, loss = 0.00781498\n",
      "Iteration 30628, loss = 0.00781480\n",
      "Iteration 30629, loss = 0.00781461\n",
      "Iteration 30630, loss = 0.00781443\n",
      "Iteration 30631, loss = 0.00781424\n",
      "Iteration 30632, loss = 0.00781406\n",
      "Iteration 30633, loss = 0.00781387\n",
      "Iteration 30634, loss = 0.00781368\n",
      "Iteration 30635, loss = 0.00781350\n",
      "Iteration 30636, loss = 0.00781331\n",
      "Iteration 30637, loss = 0.00781313\n",
      "Iteration 30638, loss = 0.00781294\n",
      "Iteration 30639, loss = 0.00781275\n",
      "Iteration 30640, loss = 0.00781257\n",
      "Iteration 30641, loss = 0.00781238\n",
      "Iteration 30642, loss = 0.00781220\n",
      "Iteration 30643, loss = 0.00781201\n",
      "Iteration 30644, loss = 0.00781183\n",
      "Iteration 30645, loss = 0.00781164\n",
      "Iteration 30646, loss = 0.00781145\n",
      "Iteration 30647, loss = 0.00781127\n",
      "Iteration 30648, loss = 0.00781108\n",
      "Iteration 30649, loss = 0.00781090\n",
      "Iteration 30650, loss = 0.00781071\n",
      "Iteration 30651, loss = 0.00781053\n",
      "Iteration 30652, loss = 0.00781034\n",
      "Iteration 30653, loss = 0.00781016\n",
      "Iteration 30654, loss = 0.00780997\n",
      "Iteration 30655, loss = 0.00780978\n",
      "Iteration 30656, loss = 0.00780960\n",
      "Iteration 30657, loss = 0.00780941\n",
      "Iteration 30658, loss = 0.00780923\n",
      "Iteration 30659, loss = 0.00780904\n",
      "Iteration 30660, loss = 0.00780886\n",
      "Iteration 30661, loss = 0.00780867\n",
      "Iteration 30662, loss = 0.00780849\n",
      "Iteration 30663, loss = 0.00780830\n",
      "Iteration 30664, loss = 0.00780812\n",
      "Iteration 30665, loss = 0.00780793\n",
      "Iteration 30666, loss = 0.00780774\n",
      "Iteration 30667, loss = 0.00780756\n",
      "Iteration 30668, loss = 0.00780737\n",
      "Iteration 30669, loss = 0.00780719\n",
      "Iteration 30670, loss = 0.00780700\n",
      "Iteration 30671, loss = 0.00780682\n",
      "Iteration 30672, loss = 0.00780663\n",
      "Iteration 30673, loss = 0.00780645\n",
      "Iteration 30674, loss = 0.00780626\n",
      "Iteration 30675, loss = 0.00780608\n",
      "Iteration 30676, loss = 0.00780589\n",
      "Iteration 30677, loss = 0.00780571\n",
      "Iteration 30678, loss = 0.00780552\n",
      "Iteration 30679, loss = 0.00780534\n",
      "Iteration 30680, loss = 0.00780515\n",
      "Iteration 30681, loss = 0.00780497\n",
      "Iteration 30682, loss = 0.00780478\n",
      "Iteration 30683, loss = 0.00780460\n",
      "Iteration 30684, loss = 0.00780441\n",
      "Iteration 30685, loss = 0.00780423\n",
      "Iteration 30686, loss = 0.00780404\n",
      "Iteration 30687, loss = 0.00780386\n",
      "Iteration 30688, loss = 0.00780367\n",
      "Iteration 30689, loss = 0.00780349\n",
      "Iteration 30690, loss = 0.00780330\n",
      "Iteration 30691, loss = 0.00780312\n",
      "Iteration 30692, loss = 0.00780293\n",
      "Iteration 30693, loss = 0.00780275\n",
      "Iteration 30694, loss = 0.00780256\n",
      "Iteration 30695, loss = 0.00780238\n",
      "Iteration 30696, loss = 0.00780219\n",
      "Iteration 30697, loss = 0.00780201\n",
      "Iteration 30698, loss = 0.00780182\n",
      "Iteration 30699, loss = 0.00780164\n",
      "Iteration 30700, loss = 0.00780145\n",
      "Iteration 30701, loss = 0.00780127\n",
      "Iteration 30702, loss = 0.00780108\n",
      "Iteration 30703, loss = 0.00780090\n",
      "Iteration 30704, loss = 0.00780071\n",
      "Iteration 30705, loss = 0.00780053\n",
      "Iteration 30706, loss = 0.00780035\n",
      "Iteration 30707, loss = 0.00780016\n",
      "Iteration 30708, loss = 0.00779998\n",
      "Iteration 30709, loss = 0.00779979\n",
      "Iteration 30710, loss = 0.00779961\n",
      "Iteration 30711, loss = 0.00779942\n",
      "Iteration 30712, loss = 0.00779924\n",
      "Iteration 30713, loss = 0.00779905\n",
      "Iteration 30714, loss = 0.00779887\n",
      "Iteration 30715, loss = 0.00779868\n",
      "Iteration 30716, loss = 0.00779850\n",
      "Iteration 30717, loss = 0.00779832\n",
      "Iteration 30718, loss = 0.00779813\n",
      "Iteration 30719, loss = 0.00779795\n",
      "Iteration 30720, loss = 0.00779776\n",
      "Iteration 30721, loss = 0.00779758\n",
      "Iteration 30722, loss = 0.00779739\n",
      "Iteration 30723, loss = 0.00779721\n",
      "Iteration 30724, loss = 0.00779702\n",
      "Iteration 30725, loss = 0.00779684\n",
      "Iteration 30726, loss = 0.00779666\n",
      "Iteration 30727, loss = 0.00779647\n",
      "Iteration 30728, loss = 0.00779629\n",
      "Iteration 30729, loss = 0.00779610\n",
      "Iteration 30730, loss = 0.00779592\n",
      "Iteration 30731, loss = 0.00779573\n",
      "Iteration 30732, loss = 0.00779555\n",
      "Iteration 30733, loss = 0.00779537\n",
      "Iteration 30734, loss = 0.00779518\n",
      "Iteration 30735, loss = 0.00779500\n",
      "Iteration 30736, loss = 0.00779481\n",
      "Iteration 30737, loss = 0.00779463\n",
      "Iteration 30738, loss = 0.00779444\n",
      "Iteration 30739, loss = 0.00779426\n",
      "Iteration 30740, loss = 0.00779408\n",
      "Iteration 30741, loss = 0.00779389\n",
      "Iteration 30742, loss = 0.00779371\n",
      "Iteration 30743, loss = 0.00779352\n",
      "Iteration 30744, loss = 0.00779334\n",
      "Iteration 30745, loss = 0.00779316\n",
      "Iteration 30746, loss = 0.00779297\n",
      "Iteration 30747, loss = 0.00779279\n",
      "Iteration 30748, loss = 0.00779260\n",
      "Iteration 30749, loss = 0.00779242\n",
      "Iteration 30750, loss = 0.00779224\n",
      "Iteration 30751, loss = 0.00779205\n",
      "Iteration 30752, loss = 0.00779187\n",
      "Iteration 30753, loss = 0.00779168\n",
      "Iteration 30754, loss = 0.00779150\n",
      "Iteration 30755, loss = 0.00779132\n",
      "Iteration 30756, loss = 0.00779113\n",
      "Iteration 30757, loss = 0.00779095\n",
      "Iteration 30758, loss = 0.00779077\n",
      "Iteration 30759, loss = 0.00779058\n",
      "Iteration 30760, loss = 0.00779040\n",
      "Iteration 30761, loss = 0.00779021\n",
      "Iteration 30762, loss = 0.00779003\n",
      "Iteration 30763, loss = 0.00778985\n",
      "Iteration 30764, loss = 0.00778966\n",
      "Iteration 30765, loss = 0.00778948\n",
      "Iteration 30766, loss = 0.00778930\n",
      "Iteration 30767, loss = 0.00778911\n",
      "Iteration 30768, loss = 0.00778893\n",
      "Iteration 30769, loss = 0.00778874\n",
      "Iteration 30770, loss = 0.00778856\n",
      "Iteration 30771, loss = 0.00778838\n",
      "Iteration 30772, loss = 0.00778819\n",
      "Iteration 30773, loss = 0.00778801\n",
      "Iteration 30774, loss = 0.00778783\n",
      "Iteration 30775, loss = 0.00778764\n",
      "Iteration 30776, loss = 0.00778746\n",
      "Iteration 30777, loss = 0.00778728\n",
      "Iteration 30778, loss = 0.00778709\n",
      "Iteration 30779, loss = 0.00778691\n",
      "Iteration 30780, loss = 0.00778673\n",
      "Iteration 30781, loss = 0.00778654\n",
      "Iteration 30782, loss = 0.00778636\n",
      "Iteration 30783, loss = 0.00778618\n",
      "Iteration 30784, loss = 0.00778599\n",
      "Iteration 30785, loss = 0.00778581\n",
      "Iteration 30786, loss = 0.00778563\n",
      "Iteration 30787, loss = 0.00778544\n",
      "Iteration 30788, loss = 0.00778526\n",
      "Iteration 30789, loss = 0.00778508\n",
      "Iteration 30790, loss = 0.00778489\n",
      "Iteration 30791, loss = 0.00778471\n",
      "Iteration 30792, loss = 0.00778453\n",
      "Iteration 30793, loss = 0.00778434\n",
      "Iteration 30794, loss = 0.00778416\n",
      "Iteration 30795, loss = 0.00778398\n",
      "Iteration 30796, loss = 0.00778379\n",
      "Iteration 30797, loss = 0.00778361\n",
      "Iteration 30798, loss = 0.00778343\n",
      "Iteration 30799, loss = 0.00778324\n",
      "Iteration 30800, loss = 0.00778306\n",
      "Iteration 30801, loss = 0.00778288\n",
      "Iteration 30802, loss = 0.00778269\n",
      "Iteration 30803, loss = 0.00778251\n",
      "Iteration 30804, loss = 0.00778233\n",
      "Iteration 30805, loss = 0.00778215\n",
      "Iteration 30806, loss = 0.00778196\n",
      "Iteration 30807, loss = 0.00778178\n",
      "Iteration 30808, loss = 0.00778160\n",
      "Iteration 30809, loss = 0.00778141\n",
      "Iteration 30810, loss = 0.00778123\n",
      "Iteration 30811, loss = 0.00778105\n",
      "Iteration 30812, loss = 0.00778086\n",
      "Iteration 30813, loss = 0.00778068\n",
      "Iteration 30814, loss = 0.00778050\n",
      "Iteration 30815, loss = 0.00778032\n",
      "Iteration 30816, loss = 0.00778013\n",
      "Iteration 30817, loss = 0.00777995\n",
      "Iteration 30818, loss = 0.00777977\n",
      "Iteration 30819, loss = 0.00777958\n",
      "Iteration 30820, loss = 0.00777940\n",
      "Iteration 30821, loss = 0.00777922\n",
      "Iteration 30822, loss = 0.00777904\n",
      "Iteration 30823, loss = 0.00777885\n",
      "Iteration 30824, loss = 0.00777867\n",
      "Iteration 30825, loss = 0.00777849\n",
      "Iteration 30826, loss = 0.00777831\n",
      "Iteration 30827, loss = 0.00777812\n",
      "Iteration 30828, loss = 0.00777794\n",
      "Iteration 30829, loss = 0.00777776\n",
      "Iteration 30830, loss = 0.00777757\n",
      "Iteration 30831, loss = 0.00777739\n",
      "Iteration 30832, loss = 0.00777721\n",
      "Iteration 30833, loss = 0.00777703\n",
      "Iteration 30834, loss = 0.00777684\n",
      "Iteration 30835, loss = 0.00777666\n",
      "Iteration 30836, loss = 0.00777648\n",
      "Iteration 30837, loss = 0.00777630\n",
      "Iteration 30838, loss = 0.00777611\n",
      "Iteration 30839, loss = 0.00777593\n",
      "Iteration 30840, loss = 0.00777575\n",
      "Iteration 30841, loss = 0.00777557\n",
      "Iteration 30842, loss = 0.00777538\n",
      "Iteration 30843, loss = 0.00777520\n",
      "Iteration 30844, loss = 0.00777502\n",
      "Iteration 30845, loss = 0.00777484\n",
      "Iteration 30846, loss = 0.00777465\n",
      "Iteration 30847, loss = 0.00777447\n",
      "Iteration 30848, loss = 0.00777429\n",
      "Iteration 30849, loss = 0.00777411\n",
      "Iteration 30850, loss = 0.00777393\n",
      "Iteration 30851, loss = 0.00777374\n",
      "Iteration 30852, loss = 0.00777356\n",
      "Iteration 30853, loss = 0.00777338\n",
      "Iteration 30854, loss = 0.00777320\n",
      "Iteration 30855, loss = 0.00777301\n",
      "Iteration 30856, loss = 0.00777283\n",
      "Iteration 30857, loss = 0.00777265\n",
      "Iteration 30858, loss = 0.00777247\n",
      "Iteration 30859, loss = 0.00777229\n",
      "Iteration 30860, loss = 0.00777210\n",
      "Iteration 30861, loss = 0.00777192\n",
      "Iteration 30862, loss = 0.00777174\n",
      "Iteration 30863, loss = 0.00777156\n",
      "Iteration 30864, loss = 0.00777137\n",
      "Iteration 30865, loss = 0.00777119\n",
      "Iteration 30866, loss = 0.00777101\n",
      "Iteration 30867, loss = 0.00777083\n",
      "Iteration 30868, loss = 0.00777065\n",
      "Iteration 30869, loss = 0.00777046\n",
      "Iteration 30870, loss = 0.00777028\n",
      "Iteration 30871, loss = 0.00777010\n",
      "Iteration 30872, loss = 0.00776992\n",
      "Iteration 30873, loss = 0.00776974\n",
      "Iteration 30874, loss = 0.00776955\n",
      "Iteration 30875, loss = 0.00776937\n",
      "Iteration 30876, loss = 0.00776919\n",
      "Iteration 30877, loss = 0.00776901\n",
      "Iteration 30878, loss = 0.00776883\n",
      "Iteration 30879, loss = 0.00776865\n",
      "Iteration 30880, loss = 0.00776846\n",
      "Iteration 30881, loss = 0.00776828\n",
      "Iteration 30882, loss = 0.00776810\n",
      "Iteration 30883, loss = 0.00776792\n",
      "Iteration 30884, loss = 0.00776774\n",
      "Iteration 30885, loss = 0.00776755\n",
      "Iteration 30886, loss = 0.00776737\n",
      "Iteration 30887, loss = 0.00776719\n",
      "Iteration 30888, loss = 0.00776701\n",
      "Iteration 30889, loss = 0.00776683\n",
      "Iteration 30890, loss = 0.00776665\n",
      "Iteration 30891, loss = 0.00776646\n",
      "Iteration 30892, loss = 0.00776628\n",
      "Iteration 30893, loss = 0.00776610\n",
      "Iteration 30894, loss = 0.00776592\n",
      "Iteration 30895, loss = 0.00776574\n",
      "Iteration 30896, loss = 0.00776556\n",
      "Iteration 30897, loss = 0.00776538\n",
      "Iteration 30898, loss = 0.00776519\n",
      "Iteration 30899, loss = 0.00776501\n",
      "Iteration 30900, loss = 0.00776483\n",
      "Iteration 30901, loss = 0.00776465\n",
      "Iteration 30902, loss = 0.00776447\n",
      "Iteration 30903, loss = 0.00776429\n",
      "Iteration 30904, loss = 0.00776410\n",
      "Iteration 30905, loss = 0.00776392\n",
      "Iteration 30906, loss = 0.00776374\n",
      "Iteration 30907, loss = 0.00776356\n",
      "Iteration 30908, loss = 0.00776338\n",
      "Iteration 30909, loss = 0.00776320\n",
      "Iteration 30910, loss = 0.00776302\n",
      "Iteration 30911, loss = 0.00776284\n",
      "Iteration 30912, loss = 0.00776265\n",
      "Iteration 30913, loss = 0.00776247\n",
      "Iteration 30914, loss = 0.00776229\n",
      "Iteration 30915, loss = 0.00776211\n",
      "Iteration 30916, loss = 0.00776193\n",
      "Iteration 30917, loss = 0.00776175\n",
      "Iteration 30918, loss = 0.00776157\n",
      "Iteration 30919, loss = 0.00776139\n",
      "Iteration 30920, loss = 0.00776120\n",
      "Iteration 30921, loss = 0.00776102\n",
      "Iteration 30922, loss = 0.00776084\n",
      "Iteration 30923, loss = 0.00776066\n",
      "Iteration 30924, loss = 0.00776048\n",
      "Iteration 30925, loss = 0.00776030\n",
      "Iteration 30926, loss = 0.00776012\n",
      "Iteration 30927, loss = 0.00775994\n",
      "Iteration 30928, loss = 0.00775976\n",
      "Iteration 30929, loss = 0.00775957\n",
      "Iteration 30930, loss = 0.00775939\n",
      "Iteration 30931, loss = 0.00775921\n",
      "Iteration 30932, loss = 0.00775903\n",
      "Iteration 30933, loss = 0.00775885\n",
      "Iteration 30934, loss = 0.00775867\n",
      "Iteration 30935, loss = 0.00775849\n",
      "Iteration 30936, loss = 0.00775831\n",
      "Iteration 30937, loss = 0.00775813\n",
      "Iteration 30938, loss = 0.00775795\n",
      "Iteration 30939, loss = 0.00775777\n",
      "Iteration 30940, loss = 0.00775758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30941, loss = 0.00775740\n",
      "Iteration 30942, loss = 0.00775722\n",
      "Iteration 30943, loss = 0.00775704\n",
      "Iteration 30944, loss = 0.00775686\n",
      "Iteration 30945, loss = 0.00775668\n",
      "Iteration 30946, loss = 0.00775650\n",
      "Iteration 30947, loss = 0.00775632\n",
      "Iteration 30948, loss = 0.00775614\n",
      "Iteration 30949, loss = 0.00775596\n",
      "Iteration 30950, loss = 0.00775578\n",
      "Iteration 30951, loss = 0.00775560\n",
      "Iteration 30952, loss = 0.00775542\n",
      "Iteration 30953, loss = 0.00775523\n",
      "Iteration 30954, loss = 0.00775505\n",
      "Iteration 30955, loss = 0.00775487\n",
      "Iteration 30956, loss = 0.00775469\n",
      "Iteration 30957, loss = 0.00775451\n",
      "Iteration 30958, loss = 0.00775433\n",
      "Iteration 30959, loss = 0.00775415\n",
      "Iteration 30960, loss = 0.00775397\n",
      "Iteration 30961, loss = 0.00775379\n",
      "Iteration 30962, loss = 0.00775361\n",
      "Iteration 30963, loss = 0.00775343\n",
      "Iteration 30964, loss = 0.00775325\n",
      "Iteration 30965, loss = 0.00775307\n",
      "Iteration 30966, loss = 0.00775289\n",
      "Iteration 30967, loss = 0.00775271\n",
      "Iteration 30968, loss = 0.00775253\n",
      "Iteration 30969, loss = 0.00775235\n",
      "Iteration 30970, loss = 0.00775217\n",
      "Iteration 30971, loss = 0.00775199\n",
      "Iteration 30972, loss = 0.00775181\n",
      "Iteration 30973, loss = 0.00775163\n",
      "Iteration 30974, loss = 0.00775144\n",
      "Iteration 30975, loss = 0.00775126\n",
      "Iteration 30976, loss = 0.00775108\n",
      "Iteration 30977, loss = 0.00775090\n",
      "Iteration 30978, loss = 0.00775072\n",
      "Iteration 30979, loss = 0.00775054\n",
      "Iteration 30980, loss = 0.00775036\n",
      "Iteration 30981, loss = 0.00775018\n",
      "Iteration 30982, loss = 0.00775000\n",
      "Iteration 30983, loss = 0.00774982\n",
      "Iteration 30984, loss = 0.00774964\n",
      "Iteration 30985, loss = 0.00774946\n",
      "Iteration 30986, loss = 0.00774928\n",
      "Iteration 30987, loss = 0.00774910\n",
      "Iteration 30988, loss = 0.00774892\n",
      "Iteration 30989, loss = 0.00774874\n",
      "Iteration 30990, loss = 0.00774856\n",
      "Iteration 30991, loss = 0.00774838\n",
      "Iteration 30992, loss = 0.00774820\n",
      "Iteration 30993, loss = 0.00774802\n",
      "Iteration 30994, loss = 0.00774784\n",
      "Iteration 30995, loss = 0.00774766\n",
      "Iteration 30996, loss = 0.00774748\n",
      "Iteration 30997, loss = 0.00774730\n",
      "Iteration 30998, loss = 0.00774712\n",
      "Iteration 30999, loss = 0.00774694\n",
      "Iteration 31000, loss = 0.00774676\n",
      "Iteration 31001, loss = 0.00774658\n",
      "Iteration 31002, loss = 0.00774640\n",
      "Iteration 31003, loss = 0.00774622\n",
      "Iteration 31004, loss = 0.00774604\n",
      "Iteration 31005, loss = 0.00774586\n",
      "Iteration 31006, loss = 0.00774568\n",
      "Iteration 31007, loss = 0.00774550\n",
      "Iteration 31008, loss = 0.00774532\n",
      "Iteration 31009, loss = 0.00774514\n",
      "Iteration 31010, loss = 0.00774496\n",
      "Iteration 31011, loss = 0.00774478\n",
      "Iteration 31012, loss = 0.00774461\n",
      "Iteration 31013, loss = 0.00774443\n",
      "Iteration 31014, loss = 0.00774425\n",
      "Iteration 31015, loss = 0.00774407\n",
      "Iteration 31016, loss = 0.00774389\n",
      "Iteration 31017, loss = 0.00774371\n",
      "Iteration 31018, loss = 0.00774353\n",
      "Iteration 31019, loss = 0.00774335\n",
      "Iteration 31020, loss = 0.00774317\n",
      "Iteration 31021, loss = 0.00774299\n",
      "Iteration 31022, loss = 0.00774281\n",
      "Iteration 31023, loss = 0.00774263\n",
      "Iteration 31024, loss = 0.00774245\n",
      "Iteration 31025, loss = 0.00774227\n",
      "Iteration 31026, loss = 0.00774209\n",
      "Iteration 31027, loss = 0.00774191\n",
      "Iteration 31028, loss = 0.00774173\n",
      "Iteration 31029, loss = 0.00774155\n",
      "Iteration 31030, loss = 0.00774137\n",
      "Iteration 31031, loss = 0.00774119\n",
      "Iteration 31032, loss = 0.00774101\n",
      "Iteration 31033, loss = 0.00774084\n",
      "Iteration 31034, loss = 0.00774066\n",
      "Iteration 31035, loss = 0.00774048\n",
      "Iteration 31036, loss = 0.00774030\n",
      "Iteration 31037, loss = 0.00774012\n",
      "Iteration 31038, loss = 0.00773994\n",
      "Iteration 31039, loss = 0.00773976\n",
      "Iteration 31040, loss = 0.00773958\n",
      "Iteration 31041, loss = 0.00773940\n",
      "Iteration 31042, loss = 0.00773922\n",
      "Iteration 31043, loss = 0.00773904\n",
      "Iteration 31044, loss = 0.00773886\n",
      "Iteration 31045, loss = 0.00773868\n",
      "Iteration 31046, loss = 0.00773850\n",
      "Iteration 31047, loss = 0.00773833\n",
      "Iteration 31048, loss = 0.00773815\n",
      "Iteration 31049, loss = 0.00773797\n",
      "Iteration 31050, loss = 0.00773779\n",
      "Iteration 31051, loss = 0.00773761\n",
      "Iteration 31052, loss = 0.00773743\n",
      "Iteration 31053, loss = 0.00773725\n",
      "Iteration 31054, loss = 0.00773707\n",
      "Iteration 31055, loss = 0.00773689\n",
      "Iteration 31056, loss = 0.00773671\n",
      "Iteration 31057, loss = 0.00773654\n",
      "Iteration 31058, loss = 0.00773636\n",
      "Iteration 31059, loss = 0.00773618\n",
      "Iteration 31060, loss = 0.00773600\n",
      "Iteration 31061, loss = 0.00773582\n",
      "Iteration 31062, loss = 0.00773564\n",
      "Iteration 31063, loss = 0.00773546\n",
      "Iteration 31064, loss = 0.00773528\n",
      "Iteration 31065, loss = 0.00773510\n",
      "Iteration 31066, loss = 0.00773492\n",
      "Iteration 31067, loss = 0.00773475\n",
      "Iteration 31068, loss = 0.00773457\n",
      "Iteration 31069, loss = 0.00773439\n",
      "Iteration 31070, loss = 0.00773421\n",
      "Iteration 31071, loss = 0.00773403\n",
      "Iteration 31072, loss = 0.00773385\n",
      "Iteration 31073, loss = 0.00773367\n",
      "Iteration 31074, loss = 0.00773349\n",
      "Iteration 31075, loss = 0.00773332\n",
      "Iteration 31076, loss = 0.00773314\n",
      "Iteration 31077, loss = 0.00773296\n",
      "Iteration 31078, loss = 0.00773278\n",
      "Iteration 31079, loss = 0.00773260\n",
      "Iteration 31080, loss = 0.00773242\n",
      "Iteration 31081, loss = 0.00773224\n",
      "Iteration 31082, loss = 0.00773207\n",
      "Iteration 31083, loss = 0.00773189\n",
      "Iteration 31084, loss = 0.00773171\n",
      "Iteration 31085, loss = 0.00773153\n",
      "Iteration 31086, loss = 0.00773135\n",
      "Iteration 31087, loss = 0.00773117\n",
      "Iteration 31088, loss = 0.00773099\n",
      "Iteration 31089, loss = 0.00773082\n",
      "Iteration 31090, loss = 0.00773064\n",
      "Iteration 31091, loss = 0.00773046\n",
      "Iteration 31092, loss = 0.00773028\n",
      "Iteration 31093, loss = 0.00773010\n",
      "Iteration 31094, loss = 0.00772992\n",
      "Iteration 31095, loss = 0.00772975\n",
      "Iteration 31096, loss = 0.00772957\n",
      "Iteration 31097, loss = 0.00772939\n",
      "Iteration 31098, loss = 0.00772921\n",
      "Iteration 31099, loss = 0.00772903\n",
      "Iteration 31100, loss = 0.00772885\n",
      "Iteration 31101, loss = 0.00772868\n",
      "Iteration 31102, loss = 0.00772850\n",
      "Iteration 31103, loss = 0.00772832\n",
      "Iteration 31104, loss = 0.00772814\n",
      "Iteration 31105, loss = 0.00772796\n",
      "Iteration 31106, loss = 0.00772778\n",
      "Iteration 31107, loss = 0.00772761\n",
      "Iteration 31108, loss = 0.00772743\n",
      "Iteration 31109, loss = 0.00772725\n",
      "Iteration 31110, loss = 0.00772707\n",
      "Iteration 31111, loss = 0.00772689\n",
      "Iteration 31112, loss = 0.00772672\n",
      "Iteration 31113, loss = 0.00772654\n",
      "Iteration 31114, loss = 0.00772636\n",
      "Iteration 31115, loss = 0.00772618\n",
      "Iteration 31116, loss = 0.00772600\n",
      "Iteration 31117, loss = 0.00772582\n",
      "Iteration 31118, loss = 0.00772565\n",
      "Iteration 31119, loss = 0.00772547\n",
      "Iteration 31120, loss = 0.00772529\n",
      "Iteration 31121, loss = 0.00772511\n",
      "Iteration 31122, loss = 0.00772493\n",
      "Iteration 31123, loss = 0.00772476\n",
      "Iteration 31124, loss = 0.00772458\n",
      "Iteration 31125, loss = 0.00772440\n",
      "Iteration 31126, loss = 0.00772422\n",
      "Iteration 31127, loss = 0.00772405\n",
      "Iteration 31128, loss = 0.00772387\n",
      "Iteration 31129, loss = 0.00772369\n",
      "Iteration 31130, loss = 0.00772351\n",
      "Iteration 31131, loss = 0.00772333\n",
      "Iteration 31132, loss = 0.00772316\n",
      "Iteration 31133, loss = 0.00772298\n",
      "Iteration 31134, loss = 0.00772280\n",
      "Iteration 31135, loss = 0.00772262\n",
      "Iteration 31136, loss = 0.00772245\n",
      "Iteration 31137, loss = 0.00772227\n",
      "Iteration 31138, loss = 0.00772209\n",
      "Iteration 31139, loss = 0.00772191\n",
      "Iteration 31140, loss = 0.00772173\n",
      "Iteration 31141, loss = 0.00772156\n",
      "Iteration 31142, loss = 0.00772138\n",
      "Iteration 31143, loss = 0.00772120\n",
      "Iteration 31144, loss = 0.00772102\n",
      "Iteration 31145, loss = 0.00772085\n",
      "Iteration 31146, loss = 0.00772067\n",
      "Iteration 31147, loss = 0.00772049\n",
      "Iteration 31148, loss = 0.00772031\n",
      "Iteration 31149, loss = 0.00772014\n",
      "Iteration 31150, loss = 0.00771996\n",
      "Iteration 31151, loss = 0.00771978\n",
      "Iteration 31152, loss = 0.00771960\n",
      "Iteration 31153, loss = 0.00771943\n",
      "Iteration 31154, loss = 0.00771925\n",
      "Iteration 31155, loss = 0.00771907\n",
      "Iteration 31156, loss = 0.00771889\n",
      "Iteration 31157, loss = 0.00771872\n",
      "Iteration 31158, loss = 0.00771854\n",
      "Iteration 31159, loss = 0.00771836\n",
      "Iteration 31160, loss = 0.00771818\n",
      "Iteration 31161, loss = 0.00771801\n",
      "Iteration 31162, loss = 0.00771783\n",
      "Iteration 31163, loss = 0.00771765\n",
      "Iteration 31164, loss = 0.00771747\n",
      "Iteration 31165, loss = 0.00771730\n",
      "Iteration 31166, loss = 0.00771712\n",
      "Iteration 31167, loss = 0.00771694\n",
      "Iteration 31168, loss = 0.00771677\n",
      "Iteration 31169, loss = 0.00771659\n",
      "Iteration 31170, loss = 0.00771641\n",
      "Iteration 31171, loss = 0.00771623\n",
      "Iteration 31172, loss = 0.00771606\n",
      "Iteration 31173, loss = 0.00771588\n",
      "Iteration 31174, loss = 0.00771570\n",
      "Iteration 31175, loss = 0.00771553\n",
      "Iteration 31176, loss = 0.00771535\n",
      "Iteration 31177, loss = 0.00771517\n",
      "Iteration 31178, loss = 0.00771499\n",
      "Iteration 31179, loss = 0.00771482\n",
      "Iteration 31180, loss = 0.00771464\n",
      "Iteration 31181, loss = 0.00771446\n",
      "Iteration 31182, loss = 0.00771429\n",
      "Iteration 31183, loss = 0.00771411\n",
      "Iteration 31184, loss = 0.00771393\n",
      "Iteration 31185, loss = 0.00771376\n",
      "Iteration 31186, loss = 0.00771358\n",
      "Iteration 31187, loss = 0.00771340\n",
      "Iteration 31188, loss = 0.00771322\n",
      "Iteration 31189, loss = 0.00771305\n",
      "Iteration 31190, loss = 0.00771287\n",
      "Iteration 31191, loss = 0.00771269\n",
      "Iteration 31192, loss = 0.00771252\n",
      "Iteration 31193, loss = 0.00771234\n",
      "Iteration 31194, loss = 0.00771216\n",
      "Iteration 31195, loss = 0.00771199\n",
      "Iteration 31196, loss = 0.00771181\n",
      "Iteration 31197, loss = 0.00771163\n",
      "Iteration 31198, loss = 0.00771146\n",
      "Iteration 31199, loss = 0.00771128\n",
      "Iteration 31200, loss = 0.00771110\n",
      "Iteration 31201, loss = 0.00771093\n",
      "Iteration 31202, loss = 0.00771075\n",
      "Iteration 31203, loss = 0.00771057\n",
      "Iteration 31204, loss = 0.00771040\n",
      "Iteration 31205, loss = 0.00771022\n",
      "Iteration 31206, loss = 0.00771004\n",
      "Iteration 31207, loss = 0.00770987\n",
      "Iteration 31208, loss = 0.00770969\n",
      "Iteration 31209, loss = 0.00770951\n",
      "Iteration 31210, loss = 0.00770934\n",
      "Iteration 31211, loss = 0.00770916\n",
      "Iteration 31212, loss = 0.00770898\n",
      "Iteration 31213, loss = 0.00770881\n",
      "Iteration 31214, loss = 0.00770863\n",
      "Iteration 31215, loss = 0.00770845\n",
      "Iteration 31216, loss = 0.00770828\n",
      "Iteration 31217, loss = 0.00770810\n",
      "Iteration 31218, loss = 0.00770792\n",
      "Iteration 31219, loss = 0.00770775\n",
      "Iteration 31220, loss = 0.00770757\n",
      "Iteration 31221, loss = 0.00770739\n",
      "Iteration 31222, loss = 0.00770722\n",
      "Iteration 31223, loss = 0.00770704\n",
      "Iteration 31224, loss = 0.00770687\n",
      "Iteration 31225, loss = 0.00770669\n",
      "Iteration 31226, loss = 0.00770651\n",
      "Iteration 31227, loss = 0.00770634\n",
      "Iteration 31228, loss = 0.00770616\n",
      "Iteration 31229, loss = 0.00770598\n",
      "Iteration 31230, loss = 0.00770581\n",
      "Iteration 31231, loss = 0.00770563\n",
      "Iteration 31232, loss = 0.00770545\n",
      "Iteration 31233, loss = 0.00770528\n",
      "Iteration 31234, loss = 0.00770510\n",
      "Iteration 31235, loss = 0.00770493\n",
      "Iteration 31236, loss = 0.00770475\n",
      "Iteration 31237, loss = 0.00770457\n",
      "Iteration 31238, loss = 0.00770440\n",
      "Iteration 31239, loss = 0.00770422\n",
      "Iteration 31240, loss = 0.00770405\n",
      "Iteration 31241, loss = 0.00770387\n",
      "Iteration 31242, loss = 0.00770369\n",
      "Iteration 31243, loss = 0.00770352\n",
      "Iteration 31244, loss = 0.00770334\n",
      "Iteration 31245, loss = 0.00770317\n",
      "Iteration 31246, loss = 0.00770299\n",
      "Iteration 31247, loss = 0.00770281\n",
      "Iteration 31248, loss = 0.00770264\n",
      "Iteration 31249, loss = 0.00770246\n",
      "Iteration 31250, loss = 0.00770229\n",
      "Iteration 31251, loss = 0.00770211\n",
      "Iteration 31252, loss = 0.00770193\n",
      "Iteration 31253, loss = 0.00770176\n",
      "Iteration 31254, loss = 0.00770158\n",
      "Iteration 31255, loss = 0.00770141\n",
      "Iteration 31256, loss = 0.00770123\n",
      "Iteration 31257, loss = 0.00770105\n",
      "Iteration 31258, loss = 0.00770088\n",
      "Iteration 31259, loss = 0.00770070\n",
      "Iteration 31260, loss = 0.00770053\n",
      "Iteration 31261, loss = 0.00770035\n",
      "Iteration 31262, loss = 0.00770017\n",
      "Iteration 31263, loss = 0.00770000\n",
      "Iteration 31264, loss = 0.00769982\n",
      "Iteration 31265, loss = 0.00769965\n",
      "Iteration 31266, loss = 0.00769947\n",
      "Iteration 31267, loss = 0.00769930\n",
      "Iteration 31268, loss = 0.00769912\n",
      "Iteration 31269, loss = 0.00769894\n",
      "Iteration 31270, loss = 0.00769877\n",
      "Iteration 31271, loss = 0.00769859\n",
      "Iteration 31272, loss = 0.00769842\n",
      "Iteration 31273, loss = 0.00769824\n",
      "Iteration 31274, loss = 0.00769807\n",
      "Iteration 31275, loss = 0.00769789\n",
      "Iteration 31276, loss = 0.00769772\n",
      "Iteration 31277, loss = 0.00769754\n",
      "Iteration 31278, loss = 0.00769736\n",
      "Iteration 31279, loss = 0.00769719\n",
      "Iteration 31280, loss = 0.00769701\n",
      "Iteration 31281, loss = 0.00769684\n",
      "Iteration 31282, loss = 0.00769666\n",
      "Iteration 31283, loss = 0.00769649\n",
      "Iteration 31284, loss = 0.00769631\n",
      "Iteration 31285, loss = 0.00769614\n",
      "Iteration 31286, loss = 0.00769596\n",
      "Iteration 31287, loss = 0.00769579\n",
      "Iteration 31288, loss = 0.00769561\n",
      "Iteration 31289, loss = 0.00769543\n",
      "Iteration 31290, loss = 0.00769526\n",
      "Iteration 31291, loss = 0.00769508\n",
      "Iteration 31292, loss = 0.00769491\n",
      "Iteration 31293, loss = 0.00769473\n",
      "Iteration 31294, loss = 0.00769456\n",
      "Iteration 31295, loss = 0.00769438\n",
      "Iteration 31296, loss = 0.00769421\n",
      "Iteration 31297, loss = 0.00769403\n",
      "Iteration 31298, loss = 0.00769386\n",
      "Iteration 31299, loss = 0.00769368\n",
      "Iteration 31300, loss = 0.00769351\n",
      "Iteration 31301, loss = 0.00769333\n",
      "Iteration 31302, loss = 0.00769316\n",
      "Iteration 31303, loss = 0.00769298\n",
      "Iteration 31304, loss = 0.00769281\n",
      "Iteration 31305, loss = 0.00769263\n",
      "Iteration 31306, loss = 0.00769246\n",
      "Iteration 31307, loss = 0.00769228\n",
      "Iteration 31308, loss = 0.00769211\n",
      "Iteration 31309, loss = 0.00769193\n",
      "Iteration 31310, loss = 0.00769176\n",
      "Iteration 31311, loss = 0.00769158\n",
      "Iteration 31312, loss = 0.00769141\n",
      "Iteration 31313, loss = 0.00769123\n",
      "Iteration 31314, loss = 0.00769106\n",
      "Iteration 31315, loss = 0.00769088\n",
      "Iteration 31316, loss = 0.00769071\n",
      "Iteration 31317, loss = 0.00769053\n",
      "Iteration 31318, loss = 0.00769036\n",
      "Iteration 31319, loss = 0.00769018\n",
      "Iteration 31320, loss = 0.00769001\n",
      "Iteration 31321, loss = 0.00768983\n",
      "Iteration 31322, loss = 0.00768966\n",
      "Iteration 31323, loss = 0.00768948\n",
      "Iteration 31324, loss = 0.00768931\n",
      "Iteration 31325, loss = 0.00768913\n",
      "Iteration 31326, loss = 0.00768896\n",
      "Iteration 31327, loss = 0.00768878\n",
      "Iteration 31328, loss = 0.00768861\n",
      "Iteration 31329, loss = 0.00768843\n",
      "Iteration 31330, loss = 0.00768826\n",
      "Iteration 31331, loss = 0.00768808\n",
      "Iteration 31332, loss = 0.00768791\n",
      "Iteration 31333, loss = 0.00768773\n",
      "Iteration 31334, loss = 0.00768756\n",
      "Iteration 31335, loss = 0.00768738\n",
      "Iteration 31336, loss = 0.00768721\n",
      "Iteration 31337, loss = 0.00768704\n",
      "Iteration 31338, loss = 0.00768686\n",
      "Iteration 31339, loss = 0.00768669\n",
      "Iteration 31340, loss = 0.00768651\n",
      "Iteration 31341, loss = 0.00768634\n",
      "Iteration 31342, loss = 0.00768616\n",
      "Iteration 31343, loss = 0.00768599\n",
      "Iteration 31344, loss = 0.00768581\n",
      "Iteration 31345, loss = 0.00768564\n",
      "Iteration 31346, loss = 0.00768546\n",
      "Iteration 31347, loss = 0.00768529\n",
      "Iteration 31348, loss = 0.00768512\n",
      "Iteration 31349, loss = 0.00768494\n",
      "Iteration 31350, loss = 0.00768477\n",
      "Iteration 31351, loss = 0.00768459\n",
      "Iteration 31352, loss = 0.00768442\n",
      "Iteration 31353, loss = 0.00768424\n",
      "Iteration 31354, loss = 0.00768407\n",
      "Iteration 31355, loss = 0.00768389\n",
      "Iteration 31356, loss = 0.00768372\n",
      "Iteration 31357, loss = 0.00768355\n",
      "Iteration 31358, loss = 0.00768337\n",
      "Iteration 31359, loss = 0.00768320\n",
      "Iteration 31360, loss = 0.00768302\n",
      "Iteration 31361, loss = 0.00768285\n",
      "Iteration 31362, loss = 0.00768267\n",
      "Iteration 31363, loss = 0.00768250\n",
      "Iteration 31364, loss = 0.00768233\n",
      "Iteration 31365, loss = 0.00768215\n",
      "Iteration 31366, loss = 0.00768198\n",
      "Iteration 31367, loss = 0.00768180\n",
      "Iteration 31368, loss = 0.00768163\n",
      "Iteration 31369, loss = 0.00768146\n",
      "Iteration 31370, loss = 0.00768128\n",
      "Iteration 31371, loss = 0.00768111\n",
      "Iteration 31372, loss = 0.00768093\n",
      "Iteration 31373, loss = 0.00768076\n",
      "Iteration 31374, loss = 0.00768058\n",
      "Iteration 31375, loss = 0.00768041\n",
      "Iteration 31376, loss = 0.00768024\n",
      "Iteration 31377, loss = 0.00768006\n",
      "Iteration 31378, loss = 0.00767989\n",
      "Iteration 31379, loss = 0.00767971\n",
      "Iteration 31380, loss = 0.00767954\n",
      "Iteration 31381, loss = 0.00767937\n",
      "Iteration 31382, loss = 0.00767919\n",
      "Iteration 31383, loss = 0.00767902\n",
      "Iteration 31384, loss = 0.00767885\n",
      "Iteration 31385, loss = 0.00767867\n",
      "Iteration 31386, loss = 0.00767850\n",
      "Iteration 31387, loss = 0.00767832\n",
      "Iteration 31388, loss = 0.00767815\n",
      "Iteration 31389, loss = 0.00767798\n",
      "Iteration 31390, loss = 0.00767780\n",
      "Iteration 31391, loss = 0.00767763\n",
      "Iteration 31392, loss = 0.00767745\n",
      "Iteration 31393, loss = 0.00767728\n",
      "Iteration 31394, loss = 0.00767711\n",
      "Iteration 31395, loss = 0.00767693\n",
      "Iteration 31396, loss = 0.00767676\n",
      "Iteration 31397, loss = 0.00767659\n",
      "Iteration 31398, loss = 0.00767641\n",
      "Iteration 31399, loss = 0.00767624\n",
      "Iteration 31400, loss = 0.00767606\n",
      "Iteration 31401, loss = 0.00767589\n",
      "Iteration 31402, loss = 0.00767572\n",
      "Iteration 31403, loss = 0.00767554\n",
      "Iteration 31404, loss = 0.00767537\n",
      "Iteration 31405, loss = 0.00767520\n",
      "Iteration 31406, loss = 0.00767502\n",
      "Iteration 31407, loss = 0.00767485\n",
      "Iteration 31408, loss = 0.00767468\n",
      "Iteration 31409, loss = 0.00767450\n",
      "Iteration 31410, loss = 0.00767433\n",
      "Iteration 31411, loss = 0.00767416\n",
      "Iteration 31412, loss = 0.00767398\n",
      "Iteration 31413, loss = 0.00767381\n",
      "Iteration 31414, loss = 0.00767364\n",
      "Iteration 31415, loss = 0.00767346\n",
      "Iteration 31416, loss = 0.00767329\n",
      "Iteration 31417, loss = 0.00767311\n",
      "Iteration 31418, loss = 0.00767294\n",
      "Iteration 31419, loss = 0.00767277\n",
      "Iteration 31420, loss = 0.00767259\n",
      "Iteration 31421, loss = 0.00767242\n",
      "Iteration 31422, loss = 0.00767225\n",
      "Iteration 31423, loss = 0.00767207\n",
      "Iteration 31424, loss = 0.00767190\n",
      "Iteration 31425, loss = 0.00767173\n",
      "Iteration 31426, loss = 0.00767155\n",
      "Iteration 31427, loss = 0.00767138\n",
      "Iteration 31428, loss = 0.00767121\n",
      "Iteration 31429, loss = 0.00767104\n",
      "Iteration 31430, loss = 0.00767086\n",
      "Iteration 31431, loss = 0.00767069\n",
      "Iteration 31432, loss = 0.00767052\n",
      "Iteration 31433, loss = 0.00767034\n",
      "Iteration 31434, loss = 0.00767017\n",
      "Iteration 31435, loss = 0.00767000\n",
      "Iteration 31436, loss = 0.00766982\n",
      "Iteration 31437, loss = 0.00766965\n",
      "Iteration 31438, loss = 0.00766948\n",
      "Iteration 31439, loss = 0.00766930\n",
      "Iteration 31440, loss = 0.00766913\n",
      "Iteration 31441, loss = 0.00766896\n",
      "Iteration 31442, loss = 0.00766878\n",
      "Iteration 31443, loss = 0.00766861\n",
      "Iteration 31444, loss = 0.00766844\n",
      "Iteration 31445, loss = 0.00766827\n",
      "Iteration 31446, loss = 0.00766809\n",
      "Iteration 31447, loss = 0.00766792\n",
      "Iteration 31448, loss = 0.00766775\n",
      "Iteration 31449, loss = 0.00766757\n",
      "Iteration 31450, loss = 0.00766740\n",
      "Iteration 31451, loss = 0.00766723\n",
      "Iteration 31452, loss = 0.00766706\n",
      "Iteration 31453, loss = 0.00766688\n",
      "Iteration 31454, loss = 0.00766671\n",
      "Iteration 31455, loss = 0.00766654\n",
      "Iteration 31456, loss = 0.00766636\n",
      "Iteration 31457, loss = 0.00766619\n",
      "Iteration 31458, loss = 0.00766602\n",
      "Iteration 31459, loss = 0.00766585\n",
      "Iteration 31460, loss = 0.00766567\n",
      "Iteration 31461, loss = 0.00766550\n",
      "Iteration 31462, loss = 0.00766533\n",
      "Iteration 31463, loss = 0.00766515\n",
      "Iteration 31464, loss = 0.00766498\n",
      "Iteration 31465, loss = 0.00766481\n",
      "Iteration 31466, loss = 0.00766464\n",
      "Iteration 31467, loss = 0.00766446\n",
      "Iteration 31468, loss = 0.00766429\n",
      "Iteration 31469, loss = 0.00766412\n",
      "Iteration 31470, loss = 0.00766395\n",
      "Iteration 31471, loss = 0.00766377\n",
      "Iteration 31472, loss = 0.00766360\n",
      "Iteration 31473, loss = 0.00766343\n",
      "Iteration 31474, loss = 0.00766326\n",
      "Iteration 31475, loss = 0.00766308\n",
      "Iteration 31476, loss = 0.00766291\n",
      "Iteration 31477, loss = 0.00766274\n",
      "Iteration 31478, loss = 0.00766257\n",
      "Iteration 31479, loss = 0.00766239\n",
      "Iteration 31480, loss = 0.00766222\n",
      "Iteration 31481, loss = 0.00766205\n",
      "Iteration 31482, loss = 0.00766188\n",
      "Iteration 31483, loss = 0.00766170\n",
      "Iteration 31484, loss = 0.00766153\n",
      "Iteration 31485, loss = 0.00766136\n",
      "Iteration 31486, loss = 0.00766119\n",
      "Iteration 31487, loss = 0.00766101\n",
      "Iteration 31488, loss = 0.00766084\n",
      "Iteration 31489, loss = 0.00766067\n",
      "Iteration 31490, loss = 0.00766050\n",
      "Iteration 31491, loss = 0.00766032\n",
      "Iteration 31492, loss = 0.00766015\n",
      "Iteration 31493, loss = 0.00765998\n",
      "Iteration 31494, loss = 0.00765981\n",
      "Iteration 31495, loss = 0.00765964\n",
      "Iteration 31496, loss = 0.00765946\n",
      "Iteration 31497, loss = 0.00765929\n",
      "Iteration 31498, loss = 0.00765912\n",
      "Iteration 31499, loss = 0.00765895\n",
      "Iteration 31500, loss = 0.00765877\n",
      "Iteration 31501, loss = 0.00765860\n",
      "Iteration 31502, loss = 0.00765843\n",
      "Iteration 31503, loss = 0.00765826\n",
      "Iteration 31504, loss = 0.00765809\n",
      "Iteration 31505, loss = 0.00765791\n",
      "Iteration 31506, loss = 0.00765774\n",
      "Iteration 31507, loss = 0.00765757\n",
      "Iteration 31508, loss = 0.00765740\n",
      "Iteration 31509, loss = 0.00765723\n",
      "Iteration 31510, loss = 0.00765705\n",
      "Iteration 31511, loss = 0.00765688\n",
      "Iteration 31512, loss = 0.00765671\n",
      "Iteration 31513, loss = 0.00765654\n",
      "Iteration 31514, loss = 0.00765637\n",
      "Iteration 31515, loss = 0.00765619\n",
      "Iteration 31516, loss = 0.00765602\n",
      "Iteration 31517, loss = 0.00765585\n",
      "Iteration 31518, loss = 0.00765568\n",
      "Iteration 31519, loss = 0.00765551\n",
      "Iteration 31520, loss = 0.00765533\n",
      "Iteration 31521, loss = 0.00765516\n",
      "Iteration 31522, loss = 0.00765499\n",
      "Iteration 31523, loss = 0.00765482\n",
      "Iteration 31524, loss = 0.00765465\n",
      "Iteration 31525, loss = 0.00765448\n",
      "Iteration 31526, loss = 0.00765430\n",
      "Iteration 31527, loss = 0.00765413\n",
      "Iteration 31528, loss = 0.00765396\n",
      "Iteration 31529, loss = 0.00765379\n",
      "Iteration 31530, loss = 0.00765362\n",
      "Iteration 31531, loss = 0.00765345\n",
      "Iteration 31532, loss = 0.00765327\n",
      "Iteration 31533, loss = 0.00765310\n",
      "Iteration 31534, loss = 0.00765293\n",
      "Iteration 31535, loss = 0.00765276\n",
      "Iteration 31536, loss = 0.00765259\n",
      "Iteration 31537, loss = 0.00765242\n",
      "Iteration 31538, loss = 0.00765224\n",
      "Iteration 31539, loss = 0.00765207\n",
      "Iteration 31540, loss = 0.00765190\n",
      "Iteration 31541, loss = 0.00765173\n",
      "Iteration 31542, loss = 0.00765156\n",
      "Iteration 31543, loss = 0.00765139\n",
      "Iteration 31544, loss = 0.00765121\n",
      "Iteration 31545, loss = 0.00765104\n",
      "Iteration 31546, loss = 0.00765087\n",
      "Iteration 31547, loss = 0.00765070\n",
      "Iteration 31548, loss = 0.00765053\n",
      "Iteration 31549, loss = 0.00765036\n",
      "Iteration 31550, loss = 0.00765019\n",
      "Iteration 31551, loss = 0.00765001\n",
      "Iteration 31552, loss = 0.00764984\n",
      "Iteration 31553, loss = 0.00764967\n",
      "Iteration 31554, loss = 0.00764950\n",
      "Iteration 31555, loss = 0.00764933\n",
      "Iteration 31556, loss = 0.00764916\n",
      "Iteration 31557, loss = 0.00764899\n",
      "Iteration 31558, loss = 0.00764882\n",
      "Iteration 31559, loss = 0.00764864\n",
      "Iteration 31560, loss = 0.00764847\n",
      "Iteration 31561, loss = 0.00764830\n",
      "Iteration 31562, loss = 0.00764813\n",
      "Iteration 31563, loss = 0.00764796\n",
      "Iteration 31564, loss = 0.00764779\n",
      "Iteration 31565, loss = 0.00764762\n",
      "Iteration 31566, loss = 0.00764745\n",
      "Iteration 31567, loss = 0.00764727\n",
      "Iteration 31568, loss = 0.00764710\n",
      "Iteration 31569, loss = 0.00764693\n",
      "Iteration 31570, loss = 0.00764676\n",
      "Iteration 31571, loss = 0.00764659\n",
      "Iteration 31572, loss = 0.00764642\n",
      "Iteration 31573, loss = 0.00764625\n",
      "Iteration 31574, loss = 0.00764608\n",
      "Iteration 31575, loss = 0.00764591\n",
      "Iteration 31576, loss = 0.00764573\n",
      "Iteration 31577, loss = 0.00764556\n",
      "Iteration 31578, loss = 0.00764539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31579, loss = 0.00764522\n",
      "Iteration 31580, loss = 0.00764505\n",
      "Iteration 31581, loss = 0.00764488\n",
      "Iteration 31582, loss = 0.00764471\n",
      "Iteration 31583, loss = 0.00764454\n",
      "Iteration 31584, loss = 0.00764437\n",
      "Iteration 31585, loss = 0.00764420\n",
      "Iteration 31586, loss = 0.00764403\n",
      "Iteration 31587, loss = 0.00764385\n",
      "Iteration 31588, loss = 0.00764368\n",
      "Iteration 31589, loss = 0.00764351\n",
      "Iteration 31590, loss = 0.00764334\n",
      "Iteration 31591, loss = 0.00764317\n",
      "Iteration 31592, loss = 0.00764300\n",
      "Iteration 31593, loss = 0.00764283\n",
      "Iteration 31594, loss = 0.00764266\n",
      "Iteration 31595, loss = 0.00764249\n",
      "Iteration 31596, loss = 0.00764232\n",
      "Iteration 31597, loss = 0.00764215\n",
      "Iteration 31598, loss = 0.00764198\n",
      "Iteration 31599, loss = 0.00764181\n",
      "Iteration 31600, loss = 0.00764163\n",
      "Iteration 31601, loss = 0.00764146\n",
      "Iteration 31602, loss = 0.00764129\n",
      "Iteration 31603, loss = 0.00764112\n",
      "Iteration 31604, loss = 0.00764095\n",
      "Iteration 31605, loss = 0.00764078\n",
      "Iteration 31606, loss = 0.00764061\n",
      "Iteration 31607, loss = 0.00764044\n",
      "Iteration 31608, loss = 0.00764027\n",
      "Iteration 31609, loss = 0.00764010\n",
      "Iteration 31610, loss = 0.00763993\n",
      "Iteration 31611, loss = 0.00763976\n",
      "Iteration 31612, loss = 0.00763959\n",
      "Iteration 31613, loss = 0.00763942\n",
      "Iteration 31614, loss = 0.00763925\n",
      "Iteration 31615, loss = 0.00763908\n",
      "Iteration 31616, loss = 0.00763891\n",
      "Iteration 31617, loss = 0.00763874\n",
      "Iteration 31618, loss = 0.00763857\n",
      "Iteration 31619, loss = 0.00763839\n",
      "Iteration 31620, loss = 0.00763822\n",
      "Iteration 31621, loss = 0.00763805\n",
      "Iteration 31622, loss = 0.00763788\n",
      "Iteration 31623, loss = 0.00763771\n",
      "Iteration 31624, loss = 0.00763754\n",
      "Iteration 31625, loss = 0.00763737\n",
      "Iteration 31626, loss = 0.00763720\n",
      "Iteration 31627, loss = 0.00763703\n",
      "Iteration 31628, loss = 0.00763686\n",
      "Iteration 31629, loss = 0.00763669\n",
      "Iteration 31630, loss = 0.00763652\n",
      "Iteration 31631, loss = 0.00763635\n",
      "Iteration 31632, loss = 0.00763618\n",
      "Iteration 31633, loss = 0.00763601\n",
      "Iteration 31634, loss = 0.00763584\n",
      "Iteration 31635, loss = 0.00763567\n",
      "Iteration 31636, loss = 0.00763550\n",
      "Iteration 31637, loss = 0.00763533\n",
      "Iteration 31638, loss = 0.00763516\n",
      "Iteration 31639, loss = 0.00763499\n",
      "Iteration 31640, loss = 0.00763482\n",
      "Iteration 31641, loss = 0.00763465\n",
      "Iteration 31642, loss = 0.00763448\n",
      "Iteration 31643, loss = 0.00763431\n",
      "Iteration 31644, loss = 0.00763414\n",
      "Iteration 31645, loss = 0.00763397\n",
      "Iteration 31646, loss = 0.00763380\n",
      "Iteration 31647, loss = 0.00763363\n",
      "Iteration 31648, loss = 0.00763346\n",
      "Iteration 31649, loss = 0.00763329\n",
      "Iteration 31650, loss = 0.00763312\n",
      "Iteration 31651, loss = 0.00763295\n",
      "Iteration 31652, loss = 0.00763278\n",
      "Iteration 31653, loss = 0.00763261\n",
      "Iteration 31654, loss = 0.00763244\n",
      "Iteration 31655, loss = 0.00763227\n",
      "Iteration 31656, loss = 0.00763210\n",
      "Iteration 31657, loss = 0.00763193\n",
      "Iteration 31658, loss = 0.00763176\n",
      "Iteration 31659, loss = 0.00763159\n",
      "Iteration 31660, loss = 0.00763142\n",
      "Iteration 31661, loss = 0.00763125\n",
      "Iteration 31662, loss = 0.00763108\n",
      "Iteration 31663, loss = 0.00763091\n",
      "Iteration 31664, loss = 0.00763074\n",
      "Iteration 31665, loss = 0.00763057\n",
      "Iteration 31666, loss = 0.00763040\n",
      "Iteration 31667, loss = 0.00763023\n",
      "Iteration 31668, loss = 0.00763006\n",
      "Iteration 31669, loss = 0.00762989\n",
      "Iteration 31670, loss = 0.00762972\n",
      "Iteration 31671, loss = 0.00762956\n",
      "Iteration 31672, loss = 0.00762939\n",
      "Iteration 31673, loss = 0.00762922\n",
      "Iteration 31674, loss = 0.00762905\n",
      "Iteration 31675, loss = 0.00762888\n",
      "Iteration 31676, loss = 0.00762871\n",
      "Iteration 31677, loss = 0.00762854\n",
      "Iteration 31678, loss = 0.00762837\n",
      "Iteration 31679, loss = 0.00762820\n",
      "Iteration 31680, loss = 0.00762803\n",
      "Iteration 31681, loss = 0.00762786\n",
      "Iteration 31682, loss = 0.00762769\n",
      "Iteration 31683, loss = 0.00762752\n",
      "Iteration 31684, loss = 0.00762735\n",
      "Iteration 31685, loss = 0.00762718\n",
      "Iteration 31686, loss = 0.00762701\n",
      "Iteration 31687, loss = 0.00762684\n",
      "Iteration 31688, loss = 0.00762667\n",
      "Iteration 31689, loss = 0.00762650\n",
      "Iteration 31690, loss = 0.00762634\n",
      "Iteration 31691, loss = 0.00762617\n",
      "Iteration 31692, loss = 0.00762600\n",
      "Iteration 31693, loss = 0.00762583\n",
      "Iteration 31694, loss = 0.00762566\n",
      "Iteration 31695, loss = 0.00762549\n",
      "Iteration 31696, loss = 0.00762532\n",
      "Iteration 31697, loss = 0.00762515\n",
      "Iteration 31698, loss = 0.00762498\n",
      "Iteration 31699, loss = 0.00762481\n",
      "Iteration 31700, loss = 0.00762464\n",
      "Iteration 31701, loss = 0.00762447\n",
      "Iteration 31702, loss = 0.00762430\n",
      "Iteration 31703, loss = 0.00762414\n",
      "Iteration 31704, loss = 0.00762397\n",
      "Iteration 31705, loss = 0.00762380\n",
      "Iteration 31706, loss = 0.00762363\n",
      "Iteration 31707, loss = 0.00762346\n",
      "Iteration 31708, loss = 0.00762329\n",
      "Iteration 31709, loss = 0.00762312\n",
      "Iteration 31710, loss = 0.00762295\n",
      "Iteration 31711, loss = 0.00762278\n",
      "Iteration 31712, loss = 0.00762261\n",
      "Iteration 31713, loss = 0.00762244\n",
      "Iteration 31714, loss = 0.00762228\n",
      "Iteration 31715, loss = 0.00762211\n",
      "Iteration 31716, loss = 0.00762194\n",
      "Iteration 31717, loss = 0.00762177\n",
      "Iteration 31718, loss = 0.00762160\n",
      "Iteration 31719, loss = 0.00762143\n",
      "Iteration 31720, loss = 0.00762126\n",
      "Iteration 31721, loss = 0.00762109\n",
      "Iteration 31722, loss = 0.00762092\n",
      "Iteration 31723, loss = 0.00762076\n",
      "Iteration 31724, loss = 0.00762059\n",
      "Iteration 31725, loss = 0.00762042\n",
      "Iteration 31726, loss = 0.00762025\n",
      "Iteration 31727, loss = 0.00762008\n",
      "Iteration 31728, loss = 0.00761991\n",
      "Iteration 31729, loss = 0.00761974\n",
      "Iteration 31730, loss = 0.00761957\n",
      "Iteration 31731, loss = 0.00761941\n",
      "Iteration 31732, loss = 0.00761924\n",
      "Iteration 31733, loss = 0.00761907\n",
      "Iteration 31734, loss = 0.00761890\n",
      "Iteration 31735, loss = 0.00761873\n",
      "Iteration 31736, loss = 0.00761856\n",
      "Iteration 31737, loss = 0.00761839\n",
      "Iteration 31738, loss = 0.00761822\n",
      "Iteration 31739, loss = 0.00761806\n",
      "Iteration 31740, loss = 0.00761789\n",
      "Iteration 31741, loss = 0.00761772\n",
      "Iteration 31742, loss = 0.00761755\n",
      "Iteration 31743, loss = 0.00761738\n",
      "Iteration 31744, loss = 0.00761721\n",
      "Iteration 31745, loss = 0.00761704\n",
      "Iteration 31746, loss = 0.00761688\n",
      "Iteration 31747, loss = 0.00761671\n",
      "Iteration 31748, loss = 0.00761654\n",
      "Iteration 31749, loss = 0.00761637\n",
      "Iteration 31750, loss = 0.00761620\n",
      "Iteration 31751, loss = 0.00761603\n",
      "Iteration 31752, loss = 0.00761587\n",
      "Iteration 31753, loss = 0.00761570\n",
      "Iteration 31754, loss = 0.00761553\n",
      "Iteration 31755, loss = 0.00761536\n",
      "Iteration 31756, loss = 0.00761519\n",
      "Iteration 31757, loss = 0.00761502\n",
      "Iteration 31758, loss = 0.00761485\n",
      "Iteration 31759, loss = 0.00761469\n",
      "Iteration 31760, loss = 0.00761452\n",
      "Iteration 31761, loss = 0.00761435\n",
      "Iteration 31762, loss = 0.00761418\n",
      "Iteration 31763, loss = 0.00761401\n",
      "Iteration 31764, loss = 0.00761385\n",
      "Iteration 31765, loss = 0.00761368\n",
      "Iteration 31766, loss = 0.00761351\n",
      "Iteration 31767, loss = 0.00761334\n",
      "Iteration 31768, loss = 0.00761317\n",
      "Iteration 31769, loss = 0.00761300\n",
      "Iteration 31770, loss = 0.00761284\n",
      "Iteration 31771, loss = 0.00761267\n",
      "Iteration 31772, loss = 0.00761250\n",
      "Iteration 31773, loss = 0.00761233\n",
      "Iteration 31774, loss = 0.00761216\n",
      "Iteration 31775, loss = 0.00761200\n",
      "Iteration 31776, loss = 0.00761183\n",
      "Iteration 31777, loss = 0.00761166\n",
      "Iteration 31778, loss = 0.00761149\n",
      "Iteration 31779, loss = 0.00761132\n",
      "Iteration 31780, loss = 0.00761116\n",
      "Iteration 31781, loss = 0.00761099\n",
      "Iteration 31782, loss = 0.00761082\n",
      "Iteration 31783, loss = 0.00761065\n",
      "Iteration 31784, loss = 0.00761048\n",
      "Iteration 31785, loss = 0.00761032\n",
      "Iteration 31786, loss = 0.00761015\n",
      "Iteration 31787, loss = 0.00760998\n",
      "Iteration 31788, loss = 0.00760981\n",
      "Iteration 31789, loss = 0.00760964\n",
      "Iteration 31790, loss = 0.00760948\n",
      "Iteration 31791, loss = 0.00760931\n",
      "Iteration 31792, loss = 0.00760914\n",
      "Iteration 31793, loss = 0.00760897\n",
      "Iteration 31794, loss = 0.00760880\n",
      "Iteration 31795, loss = 0.00760864\n",
      "Iteration 31796, loss = 0.00760847\n",
      "Iteration 31797, loss = 0.00760830\n",
      "Iteration 31798, loss = 0.00760813\n",
      "Iteration 31799, loss = 0.00760797\n",
      "Iteration 31800, loss = 0.00760780\n",
      "Iteration 31801, loss = 0.00760763\n",
      "Iteration 31802, loss = 0.00760746\n",
      "Iteration 31803, loss = 0.00760729\n",
      "Iteration 31804, loss = 0.00760713\n",
      "Iteration 31805, loss = 0.00760696\n",
      "Iteration 31806, loss = 0.00760679\n",
      "Iteration 31807, loss = 0.00760662\n",
      "Iteration 31808, loss = 0.00760646\n",
      "Iteration 31809, loss = 0.00760629\n",
      "Iteration 31810, loss = 0.00760612\n",
      "Iteration 31811, loss = 0.00760595\n",
      "Iteration 31812, loss = 0.00760579\n",
      "Iteration 31813, loss = 0.00760562\n",
      "Iteration 31814, loss = 0.00760545\n",
      "Iteration 31815, loss = 0.00760528\n",
      "Iteration 31816, loss = 0.00760512\n",
      "Iteration 31817, loss = 0.00760495\n",
      "Iteration 31818, loss = 0.00760478\n",
      "Iteration 31819, loss = 0.00760461\n",
      "Iteration 31820, loss = 0.00760445\n",
      "Iteration 31821, loss = 0.00760428\n",
      "Iteration 31822, loss = 0.00760411\n",
      "Iteration 31823, loss = 0.00760394\n",
      "Iteration 31824, loss = 0.00760378\n",
      "Iteration 31825, loss = 0.00760361\n",
      "Iteration 31826, loss = 0.00760344\n",
      "Iteration 31827, loss = 0.00760327\n",
      "Iteration 31828, loss = 0.00760311\n",
      "Iteration 31829, loss = 0.00760294\n",
      "Iteration 31830, loss = 0.00760277\n",
      "Iteration 31831, loss = 0.00760261\n",
      "Iteration 31832, loss = 0.00760244\n",
      "Iteration 31833, loss = 0.00760227\n",
      "Iteration 31834, loss = 0.00760210\n",
      "Iteration 31835, loss = 0.00760194\n",
      "Iteration 31836, loss = 0.00760177\n",
      "Iteration 31837, loss = 0.00760160\n",
      "Iteration 31838, loss = 0.00760143\n",
      "Iteration 31839, loss = 0.00760127\n",
      "Iteration 31840, loss = 0.00760110\n",
      "Iteration 31841, loss = 0.00760093\n",
      "Iteration 31842, loss = 0.00760077\n",
      "Iteration 31843, loss = 0.00760060\n",
      "Iteration 31844, loss = 0.00760043\n",
      "Iteration 31845, loss = 0.00760026\n",
      "Iteration 31846, loss = 0.00760010\n",
      "Iteration 31847, loss = 0.00759993\n",
      "Iteration 31848, loss = 0.00759976\n",
      "Iteration 31849, loss = 0.00759960\n",
      "Iteration 31850, loss = 0.00759943\n",
      "Iteration 31851, loss = 0.00759926\n",
      "Iteration 31852, loss = 0.00759910\n",
      "Iteration 31853, loss = 0.00759893\n",
      "Iteration 31854, loss = 0.00759876\n",
      "Iteration 31855, loss = 0.00759859\n",
      "Iteration 31856, loss = 0.00759843\n",
      "Iteration 31857, loss = 0.00759826\n",
      "Iteration 31858, loss = 0.00759809\n",
      "Iteration 31859, loss = 0.00759793\n",
      "Iteration 31860, loss = 0.00759776\n",
      "Iteration 31861, loss = 0.00759759\n",
      "Iteration 31862, loss = 0.00759743\n",
      "Iteration 31863, loss = 0.00759726\n",
      "Iteration 31864, loss = 0.00759709\n",
      "Iteration 31865, loss = 0.00759693\n",
      "Iteration 31866, loss = 0.00759676\n",
      "Iteration 31867, loss = 0.00759659\n",
      "Iteration 31868, loss = 0.00759643\n",
      "Iteration 31869, loss = 0.00759626\n",
      "Iteration 31870, loss = 0.00759609\n",
      "Iteration 31871, loss = 0.00759593\n",
      "Iteration 31872, loss = 0.00759576\n",
      "Iteration 31873, loss = 0.00759559\n",
      "Iteration 31874, loss = 0.00759543\n",
      "Iteration 31875, loss = 0.00759526\n",
      "Iteration 31876, loss = 0.00759509\n",
      "Iteration 31877, loss = 0.00759493\n",
      "Iteration 31878, loss = 0.00759476\n",
      "Iteration 31879, loss = 0.00759459\n",
      "Iteration 31880, loss = 0.00759443\n",
      "Iteration 31881, loss = 0.00759426\n",
      "Iteration 31882, loss = 0.00759409\n",
      "Iteration 31883, loss = 0.00759393\n",
      "Iteration 31884, loss = 0.00759376\n",
      "Iteration 31885, loss = 0.00759359\n",
      "Iteration 31886, loss = 0.00759343\n",
      "Iteration 31887, loss = 0.00759326\n",
      "Iteration 31888, loss = 0.00759309\n",
      "Iteration 31889, loss = 0.00759293\n",
      "Iteration 31890, loss = 0.00759276\n",
      "Iteration 31891, loss = 0.00759259\n",
      "Iteration 31892, loss = 0.00759243\n",
      "Iteration 31893, loss = 0.00759226\n",
      "Iteration 31894, loss = 0.00759210\n",
      "Iteration 31895, loss = 0.00759193\n",
      "Iteration 31896, loss = 0.00759176\n",
      "Iteration 31897, loss = 0.00759160\n",
      "Iteration 31898, loss = 0.00759143\n",
      "Iteration 31899, loss = 0.00759126\n",
      "Iteration 31900, loss = 0.00759110\n",
      "Iteration 31901, loss = 0.00759093\n",
      "Iteration 31902, loss = 0.00759076\n",
      "Iteration 31903, loss = 0.00759060\n",
      "Iteration 31904, loss = 0.00759043\n",
      "Iteration 31905, loss = 0.00759027\n",
      "Iteration 31906, loss = 0.00759010\n",
      "Iteration 31907, loss = 0.00758993\n",
      "Iteration 31908, loss = 0.00758977\n",
      "Iteration 31909, loss = 0.00758960\n",
      "Iteration 31910, loss = 0.00758943\n",
      "Iteration 31911, loss = 0.00758927\n",
      "Iteration 31912, loss = 0.00758910\n",
      "Iteration 31913, loss = 0.00758894\n",
      "Iteration 31914, loss = 0.00758877\n",
      "Iteration 31915, loss = 0.00758860\n",
      "Iteration 31916, loss = 0.00758844\n",
      "Iteration 31917, loss = 0.00758827\n",
      "Iteration 31918, loss = 0.00758811\n",
      "Iteration 31919, loss = 0.00758794\n",
      "Iteration 31920, loss = 0.00758777\n",
      "Iteration 31921, loss = 0.00758761\n",
      "Iteration 31922, loss = 0.00758744\n",
      "Iteration 31923, loss = 0.00758728\n",
      "Iteration 31924, loss = 0.00758711\n",
      "Iteration 31925, loss = 0.00758694\n",
      "Iteration 31926, loss = 0.00758678\n",
      "Iteration 31927, loss = 0.00758661\n",
      "Iteration 31928, loss = 0.00758645\n",
      "Iteration 31929, loss = 0.00758628\n",
      "Iteration 31930, loss = 0.00758611\n",
      "Iteration 31931, loss = 0.00758595\n",
      "Iteration 31932, loss = 0.00758578\n",
      "Iteration 31933, loss = 0.00758562\n",
      "Iteration 31934, loss = 0.00758545\n",
      "Iteration 31935, loss = 0.00758529\n",
      "Iteration 31936, loss = 0.00758512\n",
      "Iteration 31937, loss = 0.00758495\n",
      "Iteration 31938, loss = 0.00758479\n",
      "Iteration 31939, loss = 0.00758462\n",
      "Iteration 31940, loss = 0.00758446\n",
      "Iteration 31941, loss = 0.00758429\n",
      "Iteration 31942, loss = 0.00758413\n",
      "Iteration 31943, loss = 0.00758396\n",
      "Iteration 31944, loss = 0.00758379\n",
      "Iteration 31945, loss = 0.00758363\n",
      "Iteration 31946, loss = 0.00758346\n",
      "Iteration 31947, loss = 0.00758330\n",
      "Iteration 31948, loss = 0.00758313\n",
      "Iteration 31949, loss = 0.00758297\n",
      "Iteration 31950, loss = 0.00758280\n",
      "Iteration 31951, loss = 0.00758264\n",
      "Iteration 31952, loss = 0.00758247\n",
      "Iteration 31953, loss = 0.00758230\n",
      "Iteration 31954, loss = 0.00758214\n",
      "Iteration 31955, loss = 0.00758197\n",
      "Iteration 31956, loss = 0.00758181\n",
      "Iteration 31957, loss = 0.00758164\n",
      "Iteration 31958, loss = 0.00758148\n",
      "Iteration 31959, loss = 0.00758131\n",
      "Iteration 31960, loss = 0.00758115\n",
      "Iteration 31961, loss = 0.00758098\n",
      "Iteration 31962, loss = 0.00758081\n",
      "Iteration 31963, loss = 0.00758065\n",
      "Iteration 31964, loss = 0.00758048\n",
      "Iteration 31965, loss = 0.00758032\n",
      "Iteration 31966, loss = 0.00758015\n",
      "Iteration 31967, loss = 0.00757999\n",
      "Iteration 31968, loss = 0.00757982\n",
      "Iteration 31969, loss = 0.00757966\n",
      "Iteration 31970, loss = 0.00757949\n",
      "Iteration 31971, loss = 0.00757933\n",
      "Iteration 31972, loss = 0.00757916\n",
      "Iteration 31973, loss = 0.00757900\n",
      "Iteration 31974, loss = 0.00757883\n",
      "Iteration 31975, loss = 0.00757867\n",
      "Iteration 31976, loss = 0.00757850\n",
      "Iteration 31977, loss = 0.00757834\n",
      "Iteration 31978, loss = 0.00757817\n",
      "Iteration 31979, loss = 0.00757800\n",
      "Iteration 31980, loss = 0.00757784\n",
      "Iteration 31981, loss = 0.00757767\n",
      "Iteration 31982, loss = 0.00757751\n",
      "Iteration 31983, loss = 0.00757734\n",
      "Iteration 31984, loss = 0.00757718\n",
      "Iteration 31985, loss = 0.00757701\n",
      "Iteration 31986, loss = 0.00757685\n",
      "Iteration 31987, loss = 0.00757668\n",
      "Iteration 31988, loss = 0.00757652\n",
      "Iteration 31989, loss = 0.00757635\n",
      "Iteration 31990, loss = 0.00757619\n",
      "Iteration 31991, loss = 0.00757602\n",
      "Iteration 31992, loss = 0.00757586\n",
      "Iteration 31993, loss = 0.00757569\n",
      "Iteration 31994, loss = 0.00757553\n",
      "Iteration 31995, loss = 0.00757536\n",
      "Iteration 31996, loss = 0.00757520\n",
      "Iteration 31997, loss = 0.00757503\n",
      "Iteration 31998, loss = 0.00757487\n",
      "Iteration 31999, loss = 0.00757470\n",
      "Iteration 32000, loss = 0.00757454\n",
      "Iteration 32001, loss = 0.00757437\n",
      "Iteration 32002, loss = 0.00757421\n",
      "Iteration 32003, loss = 0.00757405\n",
      "Iteration 32004, loss = 0.00757388\n",
      "Iteration 32005, loss = 0.00757372\n",
      "Iteration 32006, loss = 0.00757355\n",
      "Iteration 32007, loss = 0.00757339\n",
      "Iteration 32008, loss = 0.00757322\n",
      "Iteration 32009, loss = 0.00757306\n",
      "Iteration 32010, loss = 0.00757289\n",
      "Iteration 32011, loss = 0.00757273\n",
      "Iteration 32012, loss = 0.00757256\n",
      "Iteration 32013, loss = 0.00757240\n",
      "Iteration 32014, loss = 0.00757223\n",
      "Iteration 32015, loss = 0.00757207\n",
      "Iteration 32016, loss = 0.00757190\n",
      "Iteration 32017, loss = 0.00757174\n",
      "Iteration 32018, loss = 0.00757157\n",
      "Iteration 32019, loss = 0.00757141\n",
      "Iteration 32020, loss = 0.00757125\n",
      "Iteration 32021, loss = 0.00757108\n",
      "Iteration 32022, loss = 0.00757092\n",
      "Iteration 32023, loss = 0.00757075\n",
      "Iteration 32024, loss = 0.00757059\n",
      "Iteration 32025, loss = 0.00757042\n",
      "Iteration 32026, loss = 0.00757026\n",
      "Iteration 32027, loss = 0.00757009\n",
      "Iteration 32028, loss = 0.00756993\n",
      "Iteration 32029, loss = 0.00756976\n",
      "Iteration 32030, loss = 0.00756960\n",
      "Iteration 32031, loss = 0.00756944\n",
      "Iteration 32032, loss = 0.00756927\n",
      "Iteration 32033, loss = 0.00756911\n",
      "Iteration 32034, loss = 0.00756894\n",
      "Iteration 32035, loss = 0.00756878\n",
      "Iteration 32036, loss = 0.00756861\n",
      "Iteration 32037, loss = 0.00756845\n",
      "Iteration 32038, loss = 0.00756829\n",
      "Iteration 32039, loss = 0.00756812\n",
      "Iteration 32040, loss = 0.00756796\n",
      "Iteration 32041, loss = 0.00756779\n",
      "Iteration 32042, loss = 0.00756763\n",
      "Iteration 32043, loss = 0.00756746\n",
      "Iteration 32044, loss = 0.00756730\n",
      "Iteration 32045, loss = 0.00756714\n",
      "Iteration 32046, loss = 0.00756697\n",
      "Iteration 32047, loss = 0.00756681\n",
      "Iteration 32048, loss = 0.00756664\n",
      "Iteration 32049, loss = 0.00756648\n",
      "Iteration 32050, loss = 0.00756631\n",
      "Iteration 32051, loss = 0.00756615\n",
      "Iteration 32052, loss = 0.00756599\n",
      "Iteration 32053, loss = 0.00756582\n",
      "Iteration 32054, loss = 0.00756566\n",
      "Iteration 32055, loss = 0.00756549\n",
      "Iteration 32056, loss = 0.00756533\n",
      "Iteration 32057, loss = 0.00756517\n",
      "Iteration 32058, loss = 0.00756500\n",
      "Iteration 32059, loss = 0.00756484\n",
      "Iteration 32060, loss = 0.00756467\n",
      "Iteration 32061, loss = 0.00756451\n",
      "Iteration 32062, loss = 0.00756435\n",
      "Iteration 32063, loss = 0.00756418\n",
      "Iteration 32064, loss = 0.00756402\n",
      "Iteration 32065, loss = 0.00756385\n",
      "Iteration 32066, loss = 0.00756369\n",
      "Iteration 32067, loss = 0.00756353\n",
      "Iteration 32068, loss = 0.00756336\n",
      "Iteration 32069, loss = 0.00756320\n",
      "Iteration 32070, loss = 0.00756303\n",
      "Iteration 32071, loss = 0.00756287\n",
      "Iteration 32072, loss = 0.00756271\n",
      "Iteration 32073, loss = 0.00756254\n",
      "Iteration 32074, loss = 0.00756238\n",
      "Iteration 32075, loss = 0.00756221\n",
      "Iteration 32076, loss = 0.00756205\n",
      "Iteration 32077, loss = 0.00756189\n",
      "Iteration 32078, loss = 0.00756172\n",
      "Iteration 32079, loss = 0.00756156\n",
      "Iteration 32080, loss = 0.00756140\n",
      "Iteration 32081, loss = 0.00756123\n",
      "Iteration 32082, loss = 0.00756107\n",
      "Iteration 32083, loss = 0.00756090\n",
      "Iteration 32084, loss = 0.00756074\n",
      "Iteration 32085, loss = 0.00756058\n",
      "Iteration 32086, loss = 0.00756041\n",
      "Iteration 32087, loss = 0.00756025\n",
      "Iteration 32088, loss = 0.00756009\n",
      "Iteration 32089, loss = 0.00755992\n",
      "Iteration 32090, loss = 0.00755976\n",
      "Iteration 32091, loss = 0.00755960\n",
      "Iteration 32092, loss = 0.00755943\n",
      "Iteration 32093, loss = 0.00755927\n",
      "Iteration 32094, loss = 0.00755910\n",
      "Iteration 32095, loss = 0.00755894\n",
      "Iteration 32096, loss = 0.00755878\n",
      "Iteration 32097, loss = 0.00755861\n",
      "Iteration 32098, loss = 0.00755845\n",
      "Iteration 32099, loss = 0.00755829\n",
      "Iteration 32100, loss = 0.00755812\n",
      "Iteration 32101, loss = 0.00755796\n",
      "Iteration 32102, loss = 0.00755780\n",
      "Iteration 32103, loss = 0.00755763\n",
      "Iteration 32104, loss = 0.00755747\n",
      "Iteration 32105, loss = 0.00755731\n",
      "Iteration 32106, loss = 0.00755714\n",
      "Iteration 32107, loss = 0.00755698\n",
      "Iteration 32108, loss = 0.00755682\n",
      "Iteration 32109, loss = 0.00755665\n",
      "Iteration 32110, loss = 0.00755649\n",
      "Iteration 32111, loss = 0.00755633\n",
      "Iteration 32112, loss = 0.00755616\n",
      "Iteration 32113, loss = 0.00755600\n",
      "Iteration 32114, loss = 0.00755584\n",
      "Iteration 32115, loss = 0.00755567\n",
      "Iteration 32116, loss = 0.00755551\n",
      "Iteration 32117, loss = 0.00755535\n",
      "Iteration 32118, loss = 0.00755518\n",
      "Iteration 32119, loss = 0.00755502\n",
      "Iteration 32120, loss = 0.00755486\n",
      "Iteration 32121, loss = 0.00755469\n",
      "Iteration 32122, loss = 0.00755453\n",
      "Iteration 32123, loss = 0.00755437\n",
      "Iteration 32124, loss = 0.00755420\n",
      "Iteration 32125, loss = 0.00755404\n",
      "Iteration 32126, loss = 0.00755388\n",
      "Iteration 32127, loss = 0.00755372\n",
      "Iteration 32128, loss = 0.00755355\n",
      "Iteration 32129, loss = 0.00755339\n",
      "Iteration 32130, loss = 0.00755323\n",
      "Iteration 32131, loss = 0.00755306\n",
      "Iteration 32132, loss = 0.00755290\n",
      "Iteration 32133, loss = 0.00755274\n",
      "Iteration 32134, loss = 0.00755257\n",
      "Iteration 32135, loss = 0.00755241\n",
      "Iteration 32136, loss = 0.00755225\n",
      "Iteration 32137, loss = 0.00755209\n",
      "Iteration 32138, loss = 0.00755192\n",
      "Iteration 32139, loss = 0.00755176\n",
      "Iteration 32140, loss = 0.00755160\n",
      "Iteration 32141, loss = 0.00755143\n",
      "Iteration 32142, loss = 0.00755127\n",
      "Iteration 32143, loss = 0.00755111\n",
      "Iteration 32144, loss = 0.00755094\n",
      "Iteration 32145, loss = 0.00755078\n",
      "Iteration 32146, loss = 0.00755062\n",
      "Iteration 32147, loss = 0.00755046\n",
      "Iteration 32148, loss = 0.00755029\n",
      "Iteration 32149, loss = 0.00755013\n",
      "Iteration 32150, loss = 0.00754997\n",
      "Iteration 32151, loss = 0.00754981\n",
      "Iteration 32152, loss = 0.00754964\n",
      "Iteration 32153, loss = 0.00754948\n",
      "Iteration 32154, loss = 0.00754932\n",
      "Iteration 32155, loss = 0.00754915\n",
      "Iteration 32156, loss = 0.00754899\n",
      "Iteration 32157, loss = 0.00754883\n",
      "Iteration 32158, loss = 0.00754867\n",
      "Iteration 32159, loss = 0.00754850\n",
      "Iteration 32160, loss = 0.00754834\n",
      "Iteration 32161, loss = 0.00754818\n",
      "Iteration 32162, loss = 0.00754802\n",
      "Iteration 32163, loss = 0.00754785\n",
      "Iteration 32164, loss = 0.00754769\n",
      "Iteration 32165, loss = 0.00754753\n",
      "Iteration 32166, loss = 0.00754737\n",
      "Iteration 32167, loss = 0.00754720\n",
      "Iteration 32168, loss = 0.00754704\n",
      "Iteration 32169, loss = 0.00754688\n",
      "Iteration 32170, loss = 0.00754672\n",
      "Iteration 32171, loss = 0.00754655\n",
      "Iteration 32172, loss = 0.00754639\n",
      "Iteration 32173, loss = 0.00754623\n",
      "Iteration 32174, loss = 0.00754607\n",
      "Iteration 32175, loss = 0.00754590\n",
      "Iteration 32176, loss = 0.00754574\n",
      "Iteration 32177, loss = 0.00754558\n",
      "Iteration 32178, loss = 0.00754542\n",
      "Iteration 32179, loss = 0.00754525\n",
      "Iteration 32180, loss = 0.00754509\n",
      "Iteration 32181, loss = 0.00754493\n",
      "Iteration 32182, loss = 0.00754477\n",
      "Iteration 32183, loss = 0.00754460\n",
      "Iteration 32184, loss = 0.00754444\n",
      "Iteration 32185, loss = 0.00754428\n",
      "Iteration 32186, loss = 0.00754412\n",
      "Iteration 32187, loss = 0.00754396\n",
      "Iteration 32188, loss = 0.00754379\n",
      "Iteration 32189, loss = 0.00754363\n",
      "Iteration 32190, loss = 0.00754347\n",
      "Iteration 32191, loss = 0.00754331\n",
      "Iteration 32192, loss = 0.00754314\n",
      "Iteration 32193, loss = 0.00754298\n",
      "Iteration 32194, loss = 0.00754282\n",
      "Iteration 32195, loss = 0.00754266\n",
      "Iteration 32196, loss = 0.00754250\n",
      "Iteration 32197, loss = 0.00754233\n",
      "Iteration 32198, loss = 0.00754217\n",
      "Iteration 32199, loss = 0.00754201\n",
      "Iteration 32200, loss = 0.00754185\n",
      "Iteration 32201, loss = 0.00754169\n",
      "Iteration 32202, loss = 0.00754152\n",
      "Iteration 32203, loss = 0.00754136\n",
      "Iteration 32204, loss = 0.00754120\n",
      "Iteration 32205, loss = 0.00754104\n",
      "Iteration 32206, loss = 0.00754088\n",
      "Iteration 32207, loss = 0.00754071\n",
      "Iteration 32208, loss = 0.00754055\n",
      "Iteration 32209, loss = 0.00754039\n",
      "Iteration 32210, loss = 0.00754023\n",
      "Iteration 32211, loss = 0.00754007\n",
      "Iteration 32212, loss = 0.00753990\n",
      "Iteration 32213, loss = 0.00753974\n",
      "Iteration 32214, loss = 0.00753958\n",
      "Iteration 32215, loss = 0.00753942\n",
      "Iteration 32216, loss = 0.00753926\n",
      "Iteration 32217, loss = 0.00753909\n",
      "Iteration 32218, loss = 0.00753893\n",
      "Iteration 32219, loss = 0.00753877\n",
      "Iteration 32220, loss = 0.00753861\n",
      "Iteration 32221, loss = 0.00753845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32222, loss = 0.00753829\n",
      "Iteration 32223, loss = 0.00753812\n",
      "Iteration 32224, loss = 0.00753796\n",
      "Iteration 32225, loss = 0.00753780\n",
      "Iteration 32226, loss = 0.00753764\n",
      "Iteration 32227, loss = 0.00753748\n",
      "Iteration 32228, loss = 0.00753731\n",
      "Iteration 32229, loss = 0.00753715\n",
      "Iteration 32230, loss = 0.00753699\n",
      "Iteration 32231, loss = 0.00753683\n",
      "Iteration 32232, loss = 0.00753667\n",
      "Iteration 32233, loss = 0.00753651\n",
      "Iteration 32234, loss = 0.00753634\n",
      "Iteration 32235, loss = 0.00753618\n",
      "Iteration 32236, loss = 0.00753602\n",
      "Iteration 32237, loss = 0.00753586\n",
      "Iteration 32238, loss = 0.00753570\n",
      "Iteration 32239, loss = 0.00753554\n",
      "Iteration 32240, loss = 0.00753538\n",
      "Iteration 32241, loss = 0.00753521\n",
      "Iteration 32242, loss = 0.00753505\n",
      "Iteration 32243, loss = 0.00753489\n",
      "Iteration 32244, loss = 0.00753473\n",
      "Iteration 32245, loss = 0.00753457\n",
      "Iteration 32246, loss = 0.00753441\n",
      "Iteration 32247, loss = 0.00753425\n",
      "Iteration 32248, loss = 0.00753408\n",
      "Iteration 32249, loss = 0.00753392\n",
      "Iteration 32250, loss = 0.00753376\n",
      "Iteration 32251, loss = 0.00753360\n",
      "Iteration 32252, loss = 0.00753344\n",
      "Iteration 32253, loss = 0.00753328\n",
      "Iteration 32254, loss = 0.00753312\n",
      "Iteration 32255, loss = 0.00753295\n",
      "Iteration 32256, loss = 0.00753279\n",
      "Iteration 32257, loss = 0.00753263\n",
      "Iteration 32258, loss = 0.00753247\n",
      "Iteration 32259, loss = 0.00753231\n",
      "Iteration 32260, loss = 0.00753215\n",
      "Iteration 32261, loss = 0.00753199\n",
      "Iteration 32262, loss = 0.00753183\n",
      "Iteration 32263, loss = 0.00753166\n",
      "Iteration 32264, loss = 0.00753150\n",
      "Iteration 32265, loss = 0.00753134\n",
      "Iteration 32266, loss = 0.00753118\n",
      "Iteration 32267, loss = 0.00753102\n",
      "Iteration 32268, loss = 0.00753086\n",
      "Iteration 32269, loss = 0.00753070\n",
      "Iteration 32270, loss = 0.00753054\n",
      "Iteration 32271, loss = 0.00753037\n",
      "Iteration 32272, loss = 0.00753021\n",
      "Iteration 32273, loss = 0.00753005\n",
      "Iteration 32274, loss = 0.00752989\n",
      "Iteration 32275, loss = 0.00752973\n",
      "Iteration 32276, loss = 0.00752957\n",
      "Iteration 32277, loss = 0.00752941\n",
      "Iteration 32278, loss = 0.00752925\n",
      "Iteration 32279, loss = 0.00752909\n",
      "Iteration 32280, loss = 0.00752893\n",
      "Iteration 32281, loss = 0.00752876\n",
      "Iteration 32282, loss = 0.00752860\n",
      "Iteration 32283, loss = 0.00752844\n",
      "Iteration 32284, loss = 0.00752828\n",
      "Iteration 32285, loss = 0.00752812\n",
      "Iteration 32286, loss = 0.00752796\n",
      "Iteration 32287, loss = 0.00752780\n",
      "Iteration 32288, loss = 0.00752764\n",
      "Iteration 32289, loss = 0.00752748\n",
      "Iteration 32290, loss = 0.00752732\n",
      "Iteration 32291, loss = 0.00752716\n",
      "Iteration 32292, loss = 0.00752700\n",
      "Iteration 32293, loss = 0.00752683\n",
      "Iteration 32294, loss = 0.00752667\n",
      "Iteration 32295, loss = 0.00752651\n",
      "Iteration 32296, loss = 0.00752635\n",
      "Iteration 32297, loss = 0.00752619\n",
      "Iteration 32298, loss = 0.00752603\n",
      "Iteration 32299, loss = 0.00752587\n",
      "Iteration 32300, loss = 0.00752571\n",
      "Iteration 32301, loss = 0.00752555\n",
      "Iteration 32302, loss = 0.00752539\n",
      "Iteration 32303, loss = 0.00752523\n",
      "Iteration 32304, loss = 0.00752507\n",
      "Iteration 32305, loss = 0.00752491\n",
      "Iteration 32306, loss = 0.00752475\n",
      "Iteration 32307, loss = 0.00752458\n",
      "Iteration 32308, loss = 0.00752442\n",
      "Iteration 32309, loss = 0.00752426\n",
      "Iteration 32310, loss = 0.00752410\n",
      "Iteration 32311, loss = 0.00752394\n",
      "Iteration 32312, loss = 0.00752378\n",
      "Iteration 32313, loss = 0.00752362\n",
      "Iteration 32314, loss = 0.00752346\n",
      "Iteration 32315, loss = 0.00752330\n",
      "Iteration 32316, loss = 0.00752314\n",
      "Iteration 32317, loss = 0.00752298\n",
      "Iteration 32318, loss = 0.00752282\n",
      "Iteration 32319, loss = 0.00752266\n",
      "Iteration 32320, loss = 0.00752250\n",
      "Iteration 32321, loss = 0.00752234\n",
      "Iteration 32322, loss = 0.00752218\n",
      "Iteration 32323, loss = 0.00752202\n",
      "Iteration 32324, loss = 0.00752186\n",
      "Iteration 32325, loss = 0.00752170\n",
      "Iteration 32326, loss = 0.00752154\n",
      "Iteration 32327, loss = 0.00752138\n",
      "Iteration 32328, loss = 0.00752121\n",
      "Iteration 32329, loss = 0.00752105\n",
      "Iteration 32330, loss = 0.00752089\n",
      "Iteration 32331, loss = 0.00752073\n",
      "Iteration 32332, loss = 0.00752057\n",
      "Iteration 32333, loss = 0.00752041\n",
      "Iteration 32334, loss = 0.00752025\n",
      "Iteration 32335, loss = 0.00752009\n",
      "Iteration 32336, loss = 0.00751993\n",
      "Iteration 32337, loss = 0.00751977\n",
      "Iteration 32338, loss = 0.00751961\n",
      "Iteration 32339, loss = 0.00751945\n",
      "Iteration 32340, loss = 0.00751929\n",
      "Iteration 32341, loss = 0.00751913\n",
      "Iteration 32342, loss = 0.00751897\n",
      "Iteration 32343, loss = 0.00751881\n",
      "Iteration 32344, loss = 0.00751865\n",
      "Iteration 32345, loss = 0.00751849\n",
      "Iteration 32346, loss = 0.00751833\n",
      "Iteration 32347, loss = 0.00751817\n",
      "Iteration 32348, loss = 0.00751801\n",
      "Iteration 32349, loss = 0.00751785\n",
      "Iteration 32350, loss = 0.00751769\n",
      "Iteration 32351, loss = 0.00751753\n",
      "Iteration 32352, loss = 0.00751737\n",
      "Iteration 32353, loss = 0.00751721\n",
      "Iteration 32354, loss = 0.00751705\n",
      "Iteration 32355, loss = 0.00751689\n",
      "Iteration 32356, loss = 0.00751673\n",
      "Iteration 32357, loss = 0.00751657\n",
      "Iteration 32358, loss = 0.00751641\n",
      "Iteration 32359, loss = 0.00751625\n",
      "Iteration 32360, loss = 0.00751609\n",
      "Iteration 32361, loss = 0.00751593\n",
      "Iteration 32362, loss = 0.00751577\n",
      "Iteration 32363, loss = 0.00751561\n",
      "Iteration 32364, loss = 0.00751545\n",
      "Iteration 32365, loss = 0.00751529\n",
      "Iteration 32366, loss = 0.00751513\n",
      "Iteration 32367, loss = 0.00751497\n",
      "Iteration 32368, loss = 0.00751481\n",
      "Iteration 32369, loss = 0.00751465\n",
      "Iteration 32370, loss = 0.00751449\n",
      "Iteration 32371, loss = 0.00751433\n",
      "Iteration 32372, loss = 0.00751417\n",
      "Iteration 32373, loss = 0.00751401\n",
      "Iteration 32374, loss = 0.00751385\n",
      "Iteration 32375, loss = 0.00751369\n",
      "Iteration 32376, loss = 0.00751354\n",
      "Iteration 32377, loss = 0.00751338\n",
      "Iteration 32378, loss = 0.00751322\n",
      "Iteration 32379, loss = 0.00751306\n",
      "Iteration 32380, loss = 0.00751290\n",
      "Iteration 32381, loss = 0.00751274\n",
      "Iteration 32382, loss = 0.00751258\n",
      "Iteration 32383, loss = 0.00751242\n",
      "Iteration 32384, loss = 0.00751226\n",
      "Iteration 32385, loss = 0.00751210\n",
      "Iteration 32386, loss = 0.00751194\n",
      "Iteration 32387, loss = 0.00751178\n",
      "Iteration 32388, loss = 0.00751162\n",
      "Iteration 32389, loss = 0.00751146\n",
      "Iteration 32390, loss = 0.00751130\n",
      "Iteration 32391, loss = 0.00751114\n",
      "Iteration 32392, loss = 0.00751098\n",
      "Iteration 32393, loss = 0.00751082\n",
      "Iteration 32394, loss = 0.00751066\n",
      "Iteration 32395, loss = 0.00751050\n",
      "Iteration 32396, loss = 0.00751034\n",
      "Iteration 32397, loss = 0.00751019\n",
      "Iteration 32398, loss = 0.00751003\n",
      "Iteration 32399, loss = 0.00750987\n",
      "Iteration 32400, loss = 0.00750971\n",
      "Iteration 32401, loss = 0.00750955\n",
      "Iteration 32402, loss = 0.00750939\n",
      "Iteration 32403, loss = 0.00750923\n",
      "Iteration 32404, loss = 0.00750907\n",
      "Iteration 32405, loss = 0.00750891\n",
      "Iteration 32406, loss = 0.00750875\n",
      "Iteration 32407, loss = 0.00750859\n",
      "Iteration 32408, loss = 0.00750843\n",
      "Iteration 32409, loss = 0.00750827\n",
      "Iteration 32410, loss = 0.00750811\n",
      "Iteration 32411, loss = 0.00750796\n",
      "Iteration 32412, loss = 0.00750780\n",
      "Iteration 32413, loss = 0.00750764\n",
      "Iteration 32414, loss = 0.00750748\n",
      "Iteration 32415, loss = 0.00750732\n",
      "Iteration 32416, loss = 0.00750716\n",
      "Iteration 32417, loss = 0.00750700\n",
      "Iteration 32418, loss = 0.00750684\n",
      "Iteration 32419, loss = 0.00750668\n",
      "Iteration 32420, loss = 0.00750652\n",
      "Iteration 32421, loss = 0.00750636\n",
      "Iteration 32422, loss = 0.00750621\n",
      "Iteration 32423, loss = 0.00750605\n",
      "Iteration 32424, loss = 0.00750589\n",
      "Iteration 32425, loss = 0.00750573\n",
      "Iteration 32426, loss = 0.00750557\n",
      "Iteration 32427, loss = 0.00750541\n",
      "Iteration 32428, loss = 0.00750525\n",
      "Iteration 32429, loss = 0.00750509\n",
      "Iteration 32430, loss = 0.00750493\n",
      "Iteration 32431, loss = 0.00750477\n",
      "Iteration 32432, loss = 0.00750462\n",
      "Iteration 32433, loss = 0.00750446\n",
      "Iteration 32434, loss = 0.00750430\n",
      "Iteration 32435, loss = 0.00750414\n",
      "Iteration 32436, loss = 0.00750398\n",
      "Iteration 32437, loss = 0.00750382\n",
      "Iteration 32438, loss = 0.00750366\n",
      "Iteration 32439, loss = 0.00750350\n",
      "Iteration 32440, loss = 0.00750334\n",
      "Iteration 32441, loss = 0.00750319\n",
      "Iteration 32442, loss = 0.00750303\n",
      "Iteration 32443, loss = 0.00750287\n",
      "Iteration 32444, loss = 0.00750271\n",
      "Iteration 32445, loss = 0.00750255\n",
      "Iteration 32446, loss = 0.00750239\n",
      "Iteration 32447, loss = 0.00750223\n",
      "Iteration 32448, loss = 0.00750207\n",
      "Iteration 32449, loss = 0.00750192\n",
      "Iteration 32450, loss = 0.00750176\n",
      "Iteration 32451, loss = 0.00750160\n",
      "Iteration 32452, loss = 0.00750144\n",
      "Iteration 32453, loss = 0.00750128\n",
      "Iteration 32454, loss = 0.00750112\n",
      "Iteration 32455, loss = 0.00750096\n",
      "Iteration 32456, loss = 0.00750081\n",
      "Iteration 32457, loss = 0.00750065\n",
      "Iteration 32458, loss = 0.00750049\n",
      "Iteration 32459, loss = 0.00750033\n",
      "Iteration 32460, loss = 0.00750017\n",
      "Iteration 32461, loss = 0.00750001\n",
      "Iteration 32462, loss = 0.00749985\n",
      "Iteration 32463, loss = 0.00749970\n",
      "Iteration 32464, loss = 0.00749954\n",
      "Iteration 32465, loss = 0.00749938\n",
      "Iteration 32466, loss = 0.00749922\n",
      "Iteration 32467, loss = 0.00749906\n",
      "Iteration 32468, loss = 0.00749890\n",
      "Iteration 32469, loss = 0.00749874\n",
      "Iteration 32470, loss = 0.00749859\n",
      "Iteration 32471, loss = 0.00749843\n",
      "Iteration 32472, loss = 0.00749827\n",
      "Iteration 32473, loss = 0.00749811\n",
      "Iteration 32474, loss = 0.00749795\n",
      "Iteration 32475, loss = 0.00749779\n",
      "Iteration 32476, loss = 0.00749764\n",
      "Iteration 32477, loss = 0.00749748\n",
      "Iteration 32478, loss = 0.00749732\n",
      "Iteration 32479, loss = 0.00749716\n",
      "Iteration 32480, loss = 0.00749700\n",
      "Iteration 32481, loss = 0.00749684\n",
      "Iteration 32482, loss = 0.00749669\n",
      "Iteration 32483, loss = 0.00749653\n",
      "Iteration 32484, loss = 0.00749637\n",
      "Iteration 32485, loss = 0.00749621\n",
      "Iteration 32486, loss = 0.00749605\n",
      "Iteration 32487, loss = 0.00749590\n",
      "Iteration 32488, loss = 0.00749574\n",
      "Iteration 32489, loss = 0.00749558\n",
      "Iteration 32490, loss = 0.00749542\n",
      "Iteration 32491, loss = 0.00749526\n",
      "Iteration 32492, loss = 0.00749510\n",
      "Iteration 32493, loss = 0.00749495\n",
      "Iteration 32494, loss = 0.00749479\n",
      "Iteration 32495, loss = 0.00749463\n",
      "Iteration 32496, loss = 0.00749447\n",
      "Iteration 32497, loss = 0.00749431\n",
      "Iteration 32498, loss = 0.00749416\n",
      "Iteration 32499, loss = 0.00749400\n",
      "Iteration 32500, loss = 0.00749384\n",
      "Iteration 32501, loss = 0.00749368\n",
      "Iteration 32502, loss = 0.00749352\n",
      "Iteration 32503, loss = 0.00749337\n",
      "Iteration 32504, loss = 0.00749321\n",
      "Iteration 32505, loss = 0.00749305\n",
      "Iteration 32506, loss = 0.00749289\n",
      "Iteration 32507, loss = 0.00749273\n",
      "Iteration 32508, loss = 0.00749258\n",
      "Iteration 32509, loss = 0.00749242\n",
      "Iteration 32510, loss = 0.00749226\n",
      "Iteration 32511, loss = 0.00749210\n",
      "Iteration 32512, loss = 0.00749195\n",
      "Iteration 32513, loss = 0.00749179\n",
      "Iteration 32514, loss = 0.00749163\n",
      "Iteration 32515, loss = 0.00749147\n",
      "Iteration 32516, loss = 0.00749131\n",
      "Iteration 32517, loss = 0.00749116\n",
      "Iteration 32518, loss = 0.00749100\n",
      "Iteration 32519, loss = 0.00749084\n",
      "Iteration 32520, loss = 0.00749068\n",
      "Iteration 32521, loss = 0.00749053\n",
      "Iteration 32522, loss = 0.00749037\n",
      "Iteration 32523, loss = 0.00749021\n",
      "Iteration 32524, loss = 0.00749005\n",
      "Iteration 32525, loss = 0.00748989\n",
      "Iteration 32526, loss = 0.00748974\n",
      "Iteration 32527, loss = 0.00748958\n",
      "Iteration 32528, loss = 0.00748942\n",
      "Iteration 32529, loss = 0.00748926\n",
      "Iteration 32530, loss = 0.00748911\n",
      "Iteration 32531, loss = 0.00748895\n",
      "Iteration 32532, loss = 0.00748879\n",
      "Iteration 32533, loss = 0.00748863\n",
      "Iteration 32534, loss = 0.00748848\n",
      "Iteration 32535, loss = 0.00748832\n",
      "Iteration 32536, loss = 0.00748816\n",
      "Iteration 32537, loss = 0.00748800\n",
      "Iteration 32538, loss = 0.00748785\n",
      "Iteration 32539, loss = 0.00748769\n",
      "Iteration 32540, loss = 0.00748753\n",
      "Iteration 32541, loss = 0.00748737\n",
      "Iteration 32542, loss = 0.00748722\n",
      "Iteration 32543, loss = 0.00748706\n",
      "Iteration 32544, loss = 0.00748690\n",
      "Iteration 32545, loss = 0.00748674\n",
      "Iteration 32546, loss = 0.00748659\n",
      "Iteration 32547, loss = 0.00748643\n",
      "Iteration 32548, loss = 0.00748627\n",
      "Iteration 32549, loss = 0.00748611\n",
      "Iteration 32550, loss = 0.00748596\n",
      "Iteration 32551, loss = 0.00748580\n",
      "Iteration 32552, loss = 0.00748564\n",
      "Iteration 32553, loss = 0.00748549\n",
      "Iteration 32554, loss = 0.00748533\n",
      "Iteration 32555, loss = 0.00748517\n",
      "Iteration 32556, loss = 0.00748501\n",
      "Iteration 32557, loss = 0.00748486\n",
      "Iteration 32558, loss = 0.00748470\n",
      "Iteration 32559, loss = 0.00748454\n",
      "Iteration 32560, loss = 0.00748438\n",
      "Iteration 32561, loss = 0.00748423\n",
      "Iteration 32562, loss = 0.00748407\n",
      "Iteration 32563, loss = 0.00748391\n",
      "Iteration 32564, loss = 0.00748376\n",
      "Iteration 32565, loss = 0.00748360\n",
      "Iteration 32566, loss = 0.00748344\n",
      "Iteration 32567, loss = 0.00748328\n",
      "Iteration 32568, loss = 0.00748313\n",
      "Iteration 32569, loss = 0.00748297\n",
      "Iteration 32570, loss = 0.00748281\n",
      "Iteration 32571, loss = 0.00748266\n",
      "Iteration 32572, loss = 0.00748250\n",
      "Iteration 32573, loss = 0.00748234\n",
      "Iteration 32574, loss = 0.00748218\n",
      "Iteration 32575, loss = 0.00748203\n",
      "Iteration 32576, loss = 0.00748187\n",
      "Iteration 32577, loss = 0.00748171\n",
      "Iteration 32578, loss = 0.00748156\n",
      "Iteration 32579, loss = 0.00748140\n",
      "Iteration 32580, loss = 0.00748124\n",
      "Iteration 32581, loss = 0.00748109\n",
      "Iteration 32582, loss = 0.00748093\n",
      "Iteration 32583, loss = 0.00748077\n",
      "Iteration 32584, loss = 0.00748062\n",
      "Iteration 32585, loss = 0.00748046\n",
      "Iteration 32586, loss = 0.00748030\n",
      "Iteration 32587, loss = 0.00748014\n",
      "Iteration 32588, loss = 0.00747999\n",
      "Iteration 32589, loss = 0.00747983\n",
      "Iteration 32590, loss = 0.00747967\n",
      "Iteration 32591, loss = 0.00747952\n",
      "Iteration 32592, loss = 0.00747936\n",
      "Iteration 32593, loss = 0.00747920\n",
      "Iteration 32594, loss = 0.00747905\n",
      "Iteration 32595, loss = 0.00747889\n",
      "Iteration 32596, loss = 0.00747873\n",
      "Iteration 32597, loss = 0.00747858\n",
      "Iteration 32598, loss = 0.00747842\n",
      "Iteration 32599, loss = 0.00747826\n",
      "Iteration 32600, loss = 0.00747811\n",
      "Iteration 32601, loss = 0.00747795\n",
      "Iteration 32602, loss = 0.00747779\n",
      "Iteration 32603, loss = 0.00747764\n",
      "Iteration 32604, loss = 0.00747748\n",
      "Iteration 32605, loss = 0.00747732\n",
      "Iteration 32606, loss = 0.00747717\n",
      "Iteration 32607, loss = 0.00747701\n",
      "Iteration 32608, loss = 0.00747685\n",
      "Iteration 32609, loss = 0.00747670\n",
      "Iteration 32610, loss = 0.00747654\n",
      "Iteration 32611, loss = 0.00747638\n",
      "Iteration 32612, loss = 0.00747623\n",
      "Iteration 32613, loss = 0.00747607\n",
      "Iteration 32614, loss = 0.00747591\n",
      "Iteration 32615, loss = 0.00747576\n",
      "Iteration 32616, loss = 0.00747560\n",
      "Iteration 32617, loss = 0.00747545\n",
      "Iteration 32618, loss = 0.00747529\n",
      "Iteration 32619, loss = 0.00747513\n",
      "Iteration 32620, loss = 0.00747498\n",
      "Iteration 32621, loss = 0.00747482\n",
      "Iteration 32622, loss = 0.00747466\n",
      "Iteration 32623, loss = 0.00747451\n",
      "Iteration 32624, loss = 0.00747435\n",
      "Iteration 32625, loss = 0.00747419\n",
      "Iteration 32626, loss = 0.00747404\n",
      "Iteration 32627, loss = 0.00747388\n",
      "Iteration 32628, loss = 0.00747373\n",
      "Iteration 32629, loss = 0.00747357\n",
      "Iteration 32630, loss = 0.00747341\n",
      "Iteration 32631, loss = 0.00747326\n",
      "Iteration 32632, loss = 0.00747310\n",
      "Iteration 32633, loss = 0.00747294\n",
      "Iteration 32634, loss = 0.00747279\n",
      "Iteration 32635, loss = 0.00747263\n",
      "Iteration 32636, loss = 0.00747248\n",
      "Iteration 32637, loss = 0.00747232\n",
      "Iteration 32638, loss = 0.00747216\n",
      "Iteration 32639, loss = 0.00747201\n",
      "Iteration 32640, loss = 0.00747185\n",
      "Iteration 32641, loss = 0.00747169\n",
      "Iteration 32642, loss = 0.00747154\n",
      "Iteration 32643, loss = 0.00747138\n",
      "Iteration 32644, loss = 0.00747123\n",
      "Iteration 32645, loss = 0.00747107\n",
      "Iteration 32646, loss = 0.00747091\n",
      "Iteration 32647, loss = 0.00747076\n",
      "Iteration 32648, loss = 0.00747060\n",
      "Iteration 32649, loss = 0.00747045\n",
      "Iteration 32650, loss = 0.00747029\n",
      "Iteration 32651, loss = 0.00747013\n",
      "Iteration 32652, loss = 0.00746998\n",
      "Iteration 32653, loss = 0.00746982\n",
      "Iteration 32654, loss = 0.00746967\n",
      "Iteration 32655, loss = 0.00746951\n",
      "Iteration 32656, loss = 0.00746935\n",
      "Iteration 32657, loss = 0.00746920\n",
      "Iteration 32658, loss = 0.00746904\n",
      "Iteration 32659, loss = 0.00746889\n",
      "Iteration 32660, loss = 0.00746873\n",
      "Iteration 32661, loss = 0.00746857\n",
      "Iteration 32662, loss = 0.00746842\n",
      "Iteration 32663, loss = 0.00746826\n",
      "Iteration 32664, loss = 0.00746811\n",
      "Iteration 32665, loss = 0.00746795\n",
      "Iteration 32666, loss = 0.00746779\n",
      "Iteration 32667, loss = 0.00746764\n",
      "Iteration 32668, loss = 0.00746748\n",
      "Iteration 32669, loss = 0.00746733\n",
      "Iteration 32670, loss = 0.00746717\n",
      "Iteration 32671, loss = 0.00746702\n",
      "Iteration 32672, loss = 0.00746686\n",
      "Iteration 32673, loss = 0.00746670\n",
      "Iteration 32674, loss = 0.00746655\n",
      "Iteration 32675, loss = 0.00746639\n",
      "Iteration 32676, loss = 0.00746624\n",
      "Iteration 32677, loss = 0.00746608\n",
      "Iteration 32678, loss = 0.00746593\n",
      "Iteration 32679, loss = 0.00746577\n",
      "Iteration 32680, loss = 0.00746561\n",
      "Iteration 32681, loss = 0.00746546\n",
      "Iteration 32682, loss = 0.00746530\n",
      "Iteration 32683, loss = 0.00746515\n",
      "Iteration 32684, loss = 0.00746499\n",
      "Iteration 32685, loss = 0.00746484\n",
      "Iteration 32686, loss = 0.00746468\n",
      "Iteration 32687, loss = 0.00746453\n",
      "Iteration 32688, loss = 0.00746437\n",
      "Iteration 32689, loss = 0.00746421\n",
      "Iteration 32690, loss = 0.00746406\n",
      "Iteration 32691, loss = 0.00746390\n",
      "Iteration 32692, loss = 0.00746375\n",
      "Iteration 32693, loss = 0.00746359\n",
      "Iteration 32694, loss = 0.00746344\n",
      "Iteration 32695, loss = 0.00746328\n",
      "Iteration 32696, loss = 0.00746313\n",
      "Iteration 32697, loss = 0.00746297\n",
      "Iteration 32698, loss = 0.00746282\n",
      "Iteration 32699, loss = 0.00746266\n",
      "Iteration 32700, loss = 0.00746251\n",
      "Iteration 32701, loss = 0.00746235\n",
      "Iteration 32702, loss = 0.00746219\n",
      "Iteration 32703, loss = 0.00746204\n",
      "Iteration 32704, loss = 0.00746188\n",
      "Iteration 32705, loss = 0.00746173\n",
      "Iteration 32706, loss = 0.00746157\n",
      "Iteration 32707, loss = 0.00746142\n",
      "Iteration 32708, loss = 0.00746126\n",
      "Iteration 32709, loss = 0.00746111\n",
      "Iteration 32710, loss = 0.00746095\n",
      "Iteration 32711, loss = 0.00746080\n",
      "Iteration 32712, loss = 0.00746064\n",
      "Iteration 32713, loss = 0.00746049\n",
      "Iteration 32714, loss = 0.00746033\n",
      "Iteration 32715, loss = 0.00746018\n",
      "Iteration 32716, loss = 0.00746002\n",
      "Iteration 32717, loss = 0.00745987\n",
      "Iteration 32718, loss = 0.00745971\n",
      "Iteration 32719, loss = 0.00745956\n",
      "Iteration 32720, loss = 0.00745940\n",
      "Iteration 32721, loss = 0.00745925\n",
      "Iteration 32722, loss = 0.00745909\n",
      "Iteration 32723, loss = 0.00745894\n",
      "Iteration 32724, loss = 0.00745878\n",
      "Iteration 32725, loss = 0.00745862\n",
      "Iteration 32726, loss = 0.00745847\n",
      "Iteration 32727, loss = 0.00745831\n",
      "Iteration 32728, loss = 0.00745816\n",
      "Iteration 32729, loss = 0.00745800\n",
      "Iteration 32730, loss = 0.00745785\n",
      "Iteration 32731, loss = 0.00745769\n",
      "Iteration 32732, loss = 0.00745754\n",
      "Iteration 32733, loss = 0.00745739\n",
      "Iteration 32734, loss = 0.00745723\n",
      "Iteration 32735, loss = 0.00745708\n",
      "Iteration 32736, loss = 0.00745692\n",
      "Iteration 32737, loss = 0.00745677\n",
      "Iteration 32738, loss = 0.00745661\n",
      "Iteration 32739, loss = 0.00745646\n",
      "Iteration 32740, loss = 0.00745630\n",
      "Iteration 32741, loss = 0.00745615\n",
      "Iteration 32742, loss = 0.00745599\n",
      "Iteration 32743, loss = 0.00745584\n",
      "Iteration 32744, loss = 0.00745568\n",
      "Iteration 32745, loss = 0.00745553\n",
      "Iteration 32746, loss = 0.00745537\n",
      "Iteration 32747, loss = 0.00745522\n",
      "Iteration 32748, loss = 0.00745506\n",
      "Iteration 32749, loss = 0.00745491\n",
      "Iteration 32750, loss = 0.00745475\n",
      "Iteration 32751, loss = 0.00745460\n",
      "Iteration 32752, loss = 0.00745444\n",
      "Iteration 32753, loss = 0.00745429\n",
      "Iteration 32754, loss = 0.00745413\n",
      "Iteration 32755, loss = 0.00745398\n",
      "Iteration 32756, loss = 0.00745383\n",
      "Iteration 32757, loss = 0.00745367\n",
      "Iteration 32758, loss = 0.00745352\n",
      "Iteration 32759, loss = 0.00745336\n",
      "Iteration 32760, loss = 0.00745321\n",
      "Iteration 32761, loss = 0.00745305\n",
      "Iteration 32762, loss = 0.00745290\n",
      "Iteration 32763, loss = 0.00745274\n",
      "Iteration 32764, loss = 0.00745259\n",
      "Iteration 32765, loss = 0.00745243\n",
      "Iteration 32766, loss = 0.00745228\n",
      "Iteration 32767, loss = 0.00745212\n",
      "Iteration 32768, loss = 0.00745197\n",
      "Iteration 32769, loss = 0.00745182\n",
      "Iteration 32770, loss = 0.00745166\n",
      "Iteration 32771, loss = 0.00745151\n",
      "Iteration 32772, loss = 0.00745135\n",
      "Iteration 32773, loss = 0.00745120\n",
      "Iteration 32774, loss = 0.00745104\n",
      "Iteration 32775, loss = 0.00745089\n",
      "Iteration 32776, loss = 0.00745073\n",
      "Iteration 32777, loss = 0.00745058\n",
      "Iteration 32778, loss = 0.00745043\n",
      "Iteration 32779, loss = 0.00745027\n",
      "Iteration 32780, loss = 0.00745012\n",
      "Iteration 32781, loss = 0.00744996\n",
      "Iteration 32782, loss = 0.00744981\n",
      "Iteration 32783, loss = 0.00744965\n",
      "Iteration 32784, loss = 0.00744950\n",
      "Iteration 32785, loss = 0.00744935\n",
      "Iteration 32786, loss = 0.00744919\n",
      "Iteration 32787, loss = 0.00744904\n",
      "Iteration 32788, loss = 0.00744888\n",
      "Iteration 32789, loss = 0.00744873\n",
      "Iteration 32790, loss = 0.00744857\n",
      "Iteration 32791, loss = 0.00744842\n",
      "Iteration 32792, loss = 0.00744827\n",
      "Iteration 32793, loss = 0.00744811\n",
      "Iteration 32794, loss = 0.00744796\n",
      "Iteration 32795, loss = 0.00744780\n",
      "Iteration 32796, loss = 0.00744765\n",
      "Iteration 32797, loss = 0.00744750\n",
      "Iteration 32798, loss = 0.00744734\n",
      "Iteration 32799, loss = 0.00744719\n",
      "Iteration 32800, loss = 0.00744703\n",
      "Iteration 32801, loss = 0.00744688\n",
      "Iteration 32802, loss = 0.00744673\n",
      "Iteration 32803, loss = 0.00744657\n",
      "Iteration 32804, loss = 0.00744642\n",
      "Iteration 32805, loss = 0.00744626\n",
      "Iteration 32806, loss = 0.00744611\n",
      "Iteration 32807, loss = 0.00744596\n",
      "Iteration 32808, loss = 0.00744580\n",
      "Iteration 32809, loss = 0.00744565\n",
      "Iteration 32810, loss = 0.00744549\n",
      "Iteration 32811, loss = 0.00744534\n",
      "Iteration 32812, loss = 0.00744519\n",
      "Iteration 32813, loss = 0.00744503\n",
      "Iteration 32814, loss = 0.00744488\n",
      "Iteration 32815, loss = 0.00744472\n",
      "Iteration 32816, loss = 0.00744457\n",
      "Iteration 32817, loss = 0.00744442\n",
      "Iteration 32818, loss = 0.00744426\n",
      "Iteration 32819, loss = 0.00744411\n",
      "Iteration 32820, loss = 0.00744395\n",
      "Iteration 32821, loss = 0.00744380\n",
      "Iteration 32822, loss = 0.00744365\n",
      "Iteration 32823, loss = 0.00744349\n",
      "Iteration 32824, loss = 0.00744334\n",
      "Iteration 32825, loss = 0.00744319\n",
      "Iteration 32826, loss = 0.00744303\n",
      "Iteration 32827, loss = 0.00744288\n",
      "Iteration 32828, loss = 0.00744272\n",
      "Iteration 32829, loss = 0.00744257\n",
      "Iteration 32830, loss = 0.00744242\n",
      "Iteration 32831, loss = 0.00744226\n",
      "Iteration 32832, loss = 0.00744211\n",
      "Iteration 32833, loss = 0.00744196\n",
      "Iteration 32834, loss = 0.00744180\n",
      "Iteration 32835, loss = 0.00744165\n",
      "Iteration 32836, loss = 0.00744150\n",
      "Iteration 32837, loss = 0.00744134\n",
      "Iteration 32838, loss = 0.00744119\n",
      "Iteration 32839, loss = 0.00744103\n",
      "Iteration 32840, loss = 0.00744088\n",
      "Iteration 32841, loss = 0.00744073\n",
      "Iteration 32842, loss = 0.00744057\n",
      "Iteration 32843, loss = 0.00744042\n",
      "Iteration 32844, loss = 0.00744027\n",
      "Iteration 32845, loss = 0.00744011\n",
      "Iteration 32846, loss = 0.00743996\n",
      "Iteration 32847, loss = 0.00743981\n",
      "Iteration 32848, loss = 0.00743965\n",
      "Iteration 32849, loss = 0.00743950\n",
      "Iteration 32850, loss = 0.00743935\n",
      "Iteration 32851, loss = 0.00743919\n",
      "Iteration 32852, loss = 0.00743904\n",
      "Iteration 32853, loss = 0.00743889\n",
      "Iteration 32854, loss = 0.00743873\n",
      "Iteration 32855, loss = 0.00743858\n",
      "Iteration 32856, loss = 0.00743843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32857, loss = 0.00743827\n",
      "Iteration 32858, loss = 0.00743812\n",
      "Iteration 32859, loss = 0.00743797\n",
      "Iteration 32860, loss = 0.00743781\n",
      "Iteration 32861, loss = 0.00743766\n",
      "Iteration 32862, loss = 0.00743751\n",
      "Iteration 32863, loss = 0.00743735\n",
      "Iteration 32864, loss = 0.00743720\n",
      "Iteration 32865, loss = 0.00743705\n",
      "Iteration 32866, loss = 0.00743689\n",
      "Iteration 32867, loss = 0.00743674\n",
      "Iteration 32868, loss = 0.00743659\n",
      "Iteration 32869, loss = 0.00743643\n",
      "Iteration 32870, loss = 0.00743628\n",
      "Iteration 32871, loss = 0.00743613\n",
      "Iteration 32872, loss = 0.00743597\n",
      "Iteration 32873, loss = 0.00743582\n",
      "Iteration 32874, loss = 0.00743567\n",
      "Iteration 32875, loss = 0.00743551\n",
      "Iteration 32876, loss = 0.00743536\n",
      "Iteration 32877, loss = 0.00743521\n",
      "Iteration 32878, loss = 0.00743506\n",
      "Iteration 32879, loss = 0.00743490\n",
      "Iteration 32880, loss = 0.00743475\n",
      "Iteration 32881, loss = 0.00743460\n",
      "Iteration 32882, loss = 0.00743444\n",
      "Iteration 32883, loss = 0.00743429\n",
      "Iteration 32884, loss = 0.00743414\n",
      "Iteration 32885, loss = 0.00743398\n",
      "Iteration 32886, loss = 0.00743383\n",
      "Iteration 32887, loss = 0.00743368\n",
      "Iteration 32888, loss = 0.00743353\n",
      "Iteration 32889, loss = 0.00743337\n",
      "Iteration 32890, loss = 0.00743322\n",
      "Iteration 32891, loss = 0.00743307\n",
      "Iteration 32892, loss = 0.00743291\n",
      "Iteration 32893, loss = 0.00743276\n",
      "Iteration 32894, loss = 0.00743261\n",
      "Iteration 32895, loss = 0.00743245\n",
      "Iteration 32896, loss = 0.00743230\n",
      "Iteration 32897, loss = 0.00743215\n",
      "Iteration 32898, loss = 0.00743200\n",
      "Iteration 32899, loss = 0.00743184\n",
      "Iteration 32900, loss = 0.00743169\n",
      "Iteration 32901, loss = 0.00743154\n",
      "Iteration 32902, loss = 0.00743139\n",
      "Iteration 32903, loss = 0.00743123\n",
      "Iteration 32904, loss = 0.00743108\n",
      "Iteration 32905, loss = 0.00743093\n",
      "Iteration 32906, loss = 0.00743077\n",
      "Iteration 32907, loss = 0.00743062\n",
      "Iteration 32908, loss = 0.00743047\n",
      "Iteration 32909, loss = 0.00743032\n",
      "Iteration 32910, loss = 0.00743016\n",
      "Iteration 32911, loss = 0.00743001\n",
      "Iteration 32912, loss = 0.00742986\n",
      "Iteration 32913, loss = 0.00742971\n",
      "Iteration 32914, loss = 0.00742955\n",
      "Iteration 32915, loss = 0.00742940\n",
      "Iteration 32916, loss = 0.00742925\n",
      "Iteration 32917, loss = 0.00742910\n",
      "Iteration 32918, loss = 0.00742894\n",
      "Iteration 32919, loss = 0.00742879\n",
      "Iteration 32920, loss = 0.00742864\n",
      "Iteration 32921, loss = 0.00742849\n",
      "Iteration 32922, loss = 0.00742833\n",
      "Iteration 32923, loss = 0.00742818\n",
      "Iteration 32924, loss = 0.00742803\n",
      "Iteration 32925, loss = 0.00742788\n",
      "Iteration 32926, loss = 0.00742772\n",
      "Iteration 32927, loss = 0.00742757\n",
      "Iteration 32928, loss = 0.00742742\n",
      "Iteration 32929, loss = 0.00742727\n",
      "Iteration 32930, loss = 0.00742711\n",
      "Iteration 32931, loss = 0.00742696\n",
      "Iteration 32932, loss = 0.00742681\n",
      "Iteration 32933, loss = 0.00742666\n",
      "Iteration 32934, loss = 0.00742650\n",
      "Iteration 32935, loss = 0.00742635\n",
      "Iteration 32936, loss = 0.00742620\n",
      "Iteration 32937, loss = 0.00742605\n",
      "Iteration 32938, loss = 0.00742589\n",
      "Iteration 32939, loss = 0.00742574\n",
      "Iteration 32940, loss = 0.00742559\n",
      "Iteration 32941, loss = 0.00742544\n",
      "Iteration 32942, loss = 0.00742528\n",
      "Iteration 32943, loss = 0.00742513\n",
      "Iteration 32944, loss = 0.00742498\n",
      "Iteration 32945, loss = 0.00742483\n",
      "Iteration 32946, loss = 0.00742468\n",
      "Iteration 32947, loss = 0.00742452\n",
      "Iteration 32948, loss = 0.00742437\n",
      "Iteration 32949, loss = 0.00742422\n",
      "Iteration 32950, loss = 0.00742407\n",
      "Iteration 32951, loss = 0.00742392\n",
      "Iteration 32952, loss = 0.00742376\n",
      "Iteration 32953, loss = 0.00742361\n",
      "Iteration 32954, loss = 0.00742346\n",
      "Iteration 32955, loss = 0.00742331\n",
      "Iteration 32956, loss = 0.00742315\n",
      "Iteration 32957, loss = 0.00742300\n",
      "Iteration 32958, loss = 0.00742285\n",
      "Iteration 32959, loss = 0.00742270\n",
      "Iteration 32960, loss = 0.00742255\n",
      "Iteration 32961, loss = 0.00742239\n",
      "Iteration 32962, loss = 0.00742224\n",
      "Iteration 32963, loss = 0.00742209\n",
      "Iteration 32964, loss = 0.00742194\n",
      "Iteration 32965, loss = 0.00742179\n",
      "Iteration 32966, loss = 0.00742163\n",
      "Iteration 32967, loss = 0.00742148\n",
      "Iteration 32968, loss = 0.00742133\n",
      "Iteration 32969, loss = 0.00742118\n",
      "Iteration 32970, loss = 0.00742103\n",
      "Iteration 32971, loss = 0.00742087\n",
      "Iteration 32972, loss = 0.00742072\n",
      "Iteration 32973, loss = 0.00742057\n",
      "Iteration 32974, loss = 0.00742042\n",
      "Iteration 32975, loss = 0.00742027\n",
      "Iteration 32976, loss = 0.00742012\n",
      "Iteration 32977, loss = 0.00741996\n",
      "Iteration 32978, loss = 0.00741981\n",
      "Iteration 32979, loss = 0.00741966\n",
      "Iteration 32980, loss = 0.00741951\n",
      "Iteration 32981, loss = 0.00741936\n",
      "Iteration 32982, loss = 0.00741920\n",
      "Iteration 32983, loss = 0.00741905\n",
      "Iteration 32984, loss = 0.00741890\n",
      "Iteration 32985, loss = 0.00741875\n",
      "Iteration 32986, loss = 0.00741860\n",
      "Iteration 32987, loss = 0.00741845\n",
      "Iteration 32988, loss = 0.00741829\n",
      "Iteration 32989, loss = 0.00741814\n",
      "Iteration 32990, loss = 0.00741799\n",
      "Iteration 32991, loss = 0.00741784\n",
      "Iteration 32992, loss = 0.00741769\n",
      "Iteration 32993, loss = 0.00741754\n",
      "Iteration 32994, loss = 0.00741738\n",
      "Iteration 32995, loss = 0.00741723\n",
      "Iteration 32996, loss = 0.00741708\n",
      "Iteration 32997, loss = 0.00741693\n",
      "Iteration 32998, loss = 0.00741678\n",
      "Iteration 32999, loss = 0.00741663\n",
      "Iteration 33000, loss = 0.00741648\n",
      "Iteration 33001, loss = 0.00741632\n",
      "Iteration 33002, loss = 0.00741617\n",
      "Iteration 33003, loss = 0.00741602\n",
      "Iteration 33004, loss = 0.00741587\n",
      "Iteration 33005, loss = 0.00741572\n",
      "Iteration 33006, loss = 0.00741557\n",
      "Iteration 33007, loss = 0.00741542\n",
      "Iteration 33008, loss = 0.00741526\n",
      "Iteration 33009, loss = 0.00741511\n",
      "Iteration 33010, loss = 0.00741496\n",
      "Iteration 33011, loss = 0.00741481\n",
      "Iteration 33012, loss = 0.00741466\n",
      "Iteration 33013, loss = 0.00741451\n",
      "Iteration 33014, loss = 0.00741436\n",
      "Iteration 33015, loss = 0.00741420\n",
      "Iteration 33016, loss = 0.00741405\n",
      "Iteration 33017, loss = 0.00741390\n",
      "Iteration 33018, loss = 0.00741375\n",
      "Iteration 33019, loss = 0.00741360\n",
      "Iteration 33020, loss = 0.00741345\n",
      "Iteration 33021, loss = 0.00741330\n",
      "Iteration 33022, loss = 0.00741314\n",
      "Iteration 33023, loss = 0.00741299\n",
      "Iteration 33024, loss = 0.00741284\n",
      "Iteration 33025, loss = 0.00741269\n",
      "Iteration 33026, loss = 0.00741254\n",
      "Iteration 33027, loss = 0.00741239\n",
      "Iteration 33028, loss = 0.00741224\n",
      "Iteration 33029, loss = 0.00741209\n",
      "Iteration 33030, loss = 0.00741194\n",
      "Iteration 33031, loss = 0.00741178\n",
      "Iteration 33032, loss = 0.00741163\n",
      "Iteration 33033, loss = 0.00741148\n",
      "Iteration 33034, loss = 0.00741133\n",
      "Iteration 33035, loss = 0.00741118\n",
      "Iteration 33036, loss = 0.00741103\n",
      "Iteration 33037, loss = 0.00741088\n",
      "Iteration 33038, loss = 0.00741073\n",
      "Iteration 33039, loss = 0.00741058\n",
      "Iteration 33040, loss = 0.00741042\n",
      "Iteration 33041, loss = 0.00741027\n",
      "Iteration 33042, loss = 0.00741012\n",
      "Iteration 33043, loss = 0.00740997\n",
      "Iteration 33044, loss = 0.00740982\n",
      "Iteration 33045, loss = 0.00740967\n",
      "Iteration 33046, loss = 0.00740952\n",
      "Iteration 33047, loss = 0.00740937\n",
      "Iteration 33048, loss = 0.00740922\n",
      "Iteration 33049, loss = 0.00740907\n",
      "Iteration 33050, loss = 0.00740892\n",
      "Iteration 33051, loss = 0.00740876\n",
      "Iteration 33052, loss = 0.00740861\n",
      "Iteration 33053, loss = 0.00740846\n",
      "Iteration 33054, loss = 0.00740831\n",
      "Iteration 33055, loss = 0.00740816\n",
      "Iteration 33056, loss = 0.00740801\n",
      "Iteration 33057, loss = 0.00740786\n",
      "Iteration 33058, loss = 0.00740771\n",
      "Iteration 33059, loss = 0.00740756\n",
      "Iteration 33060, loss = 0.00740741\n",
      "Iteration 33061, loss = 0.00740726\n",
      "Iteration 33062, loss = 0.00740711\n",
      "Iteration 33063, loss = 0.00740695\n",
      "Iteration 33064, loss = 0.00740680\n",
      "Iteration 33065, loss = 0.00740665\n",
      "Iteration 33066, loss = 0.00740650\n",
      "Iteration 33067, loss = 0.00740635\n",
      "Iteration 33068, loss = 0.00740620\n",
      "Iteration 33069, loss = 0.00740605\n",
      "Iteration 33070, loss = 0.00740590\n",
      "Iteration 33071, loss = 0.00740575\n",
      "Iteration 33072, loss = 0.00740560\n",
      "Iteration 33073, loss = 0.00740545\n",
      "Iteration 33074, loss = 0.00740530\n",
      "Iteration 33075, loss = 0.00740515\n",
      "Iteration 33076, loss = 0.00740500\n",
      "Iteration 33077, loss = 0.00740485\n",
      "Iteration 33078, loss = 0.00740469\n",
      "Iteration 33079, loss = 0.00740454\n",
      "Iteration 33080, loss = 0.00740439\n",
      "Iteration 33081, loss = 0.00740424\n",
      "Iteration 33082, loss = 0.00740409\n",
      "Iteration 33083, loss = 0.00740394\n",
      "Iteration 33084, loss = 0.00740379\n",
      "Iteration 33085, loss = 0.00740364\n",
      "Iteration 33086, loss = 0.00740349\n",
      "Iteration 33087, loss = 0.00740334\n",
      "Iteration 33088, loss = 0.00740319\n",
      "Iteration 33089, loss = 0.00740304\n",
      "Iteration 33090, loss = 0.00740289\n",
      "Iteration 33091, loss = 0.00740274\n",
      "Iteration 33092, loss = 0.00740259\n",
      "Iteration 33093, loss = 0.00740244\n",
      "Iteration 33094, loss = 0.00740229\n",
      "Iteration 33095, loss = 0.00740214\n",
      "Iteration 33096, loss = 0.00740199\n",
      "Iteration 33097, loss = 0.00740184\n",
      "Iteration 33098, loss = 0.00740169\n",
      "Iteration 33099, loss = 0.00740154\n",
      "Iteration 33100, loss = 0.00740139\n",
      "Iteration 33101, loss = 0.00740124\n",
      "Iteration 33102, loss = 0.00740109\n",
      "Iteration 33103, loss = 0.00740094\n",
      "Iteration 33104, loss = 0.00740078\n",
      "Iteration 33105, loss = 0.00740063\n",
      "Iteration 33106, loss = 0.00740048\n",
      "Iteration 33107, loss = 0.00740033\n",
      "Iteration 33108, loss = 0.00740018\n",
      "Iteration 33109, loss = 0.00740003\n",
      "Iteration 33110, loss = 0.00739988\n",
      "Iteration 33111, loss = 0.00739973\n",
      "Iteration 33112, loss = 0.00739958\n",
      "Iteration 33113, loss = 0.00739943\n",
      "Iteration 33114, loss = 0.00739928\n",
      "Iteration 33115, loss = 0.00739913\n",
      "Iteration 33116, loss = 0.00739898\n",
      "Iteration 33117, loss = 0.00739883\n",
      "Iteration 33118, loss = 0.00739868\n",
      "Iteration 33119, loss = 0.00739853\n",
      "Iteration 33120, loss = 0.00739838\n",
      "Iteration 33121, loss = 0.00739823\n",
      "Iteration 33122, loss = 0.00739808\n",
      "Iteration 33123, loss = 0.00739793\n",
      "Iteration 33124, loss = 0.00739778\n",
      "Iteration 33125, loss = 0.00739763\n",
      "Iteration 33126, loss = 0.00739748\n",
      "Iteration 33127, loss = 0.00739733\n",
      "Iteration 33128, loss = 0.00739718\n",
      "Iteration 33129, loss = 0.00739703\n",
      "Iteration 33130, loss = 0.00739688\n",
      "Iteration 33131, loss = 0.00739673\n",
      "Iteration 33132, loss = 0.00739658\n",
      "Iteration 33133, loss = 0.00739643\n",
      "Iteration 33134, loss = 0.00739628\n",
      "Iteration 33135, loss = 0.00739613\n",
      "Iteration 33136, loss = 0.00739598\n",
      "Iteration 33137, loss = 0.00739583\n",
      "Iteration 33138, loss = 0.00739568\n",
      "Iteration 33139, loss = 0.00739553\n",
      "Iteration 33140, loss = 0.00739538\n",
      "Iteration 33141, loss = 0.00739524\n",
      "Iteration 33142, loss = 0.00739509\n",
      "Iteration 33143, loss = 0.00739494\n",
      "Iteration 33144, loss = 0.00739479\n",
      "Iteration 33145, loss = 0.00739464\n",
      "Iteration 33146, loss = 0.00739449\n",
      "Iteration 33147, loss = 0.00739434\n",
      "Iteration 33148, loss = 0.00739419\n",
      "Iteration 33149, loss = 0.00739404\n",
      "Iteration 33150, loss = 0.00739389\n",
      "Iteration 33151, loss = 0.00739374\n",
      "Iteration 33152, loss = 0.00739359\n",
      "Iteration 33153, loss = 0.00739344\n",
      "Iteration 33154, loss = 0.00739329\n",
      "Iteration 33155, loss = 0.00739314\n",
      "Iteration 33156, loss = 0.00739299\n",
      "Iteration 33157, loss = 0.00739284\n",
      "Iteration 33158, loss = 0.00739269\n",
      "Iteration 33159, loss = 0.00739254\n",
      "Iteration 33160, loss = 0.00739239\n",
      "Iteration 33161, loss = 0.00739224\n",
      "Iteration 33162, loss = 0.00739209\n",
      "Iteration 33163, loss = 0.00739194\n",
      "Iteration 33164, loss = 0.00739179\n",
      "Iteration 33165, loss = 0.00739164\n",
      "Iteration 33166, loss = 0.00739150\n",
      "Iteration 33167, loss = 0.00739135\n",
      "Iteration 33168, loss = 0.00739120\n",
      "Iteration 33169, loss = 0.00739105\n",
      "Iteration 33170, loss = 0.00739090\n",
      "Iteration 33171, loss = 0.00739075\n",
      "Iteration 33172, loss = 0.00739060\n",
      "Iteration 33173, loss = 0.00739045\n",
      "Iteration 33174, loss = 0.00739030\n",
      "Iteration 33175, loss = 0.00739015\n",
      "Iteration 33176, loss = 0.00739000\n",
      "Iteration 33177, loss = 0.00738985\n",
      "Iteration 33178, loss = 0.00738970\n",
      "Iteration 33179, loss = 0.00738955\n",
      "Iteration 33180, loss = 0.00738940\n",
      "Iteration 33181, loss = 0.00738925\n",
      "Iteration 33182, loss = 0.00738911\n",
      "Iteration 33183, loss = 0.00738896\n",
      "Iteration 33184, loss = 0.00738881\n",
      "Iteration 33185, loss = 0.00738866\n",
      "Iteration 33186, loss = 0.00738851\n",
      "Iteration 33187, loss = 0.00738836\n",
      "Iteration 33188, loss = 0.00738821\n",
      "Iteration 33189, loss = 0.00738806\n",
      "Iteration 33190, loss = 0.00738791\n",
      "Iteration 33191, loss = 0.00738776\n",
      "Iteration 33192, loss = 0.00738761\n",
      "Iteration 33193, loss = 0.00738746\n",
      "Iteration 33194, loss = 0.00738732\n",
      "Iteration 33195, loss = 0.00738717\n",
      "Iteration 33196, loss = 0.00738702\n",
      "Iteration 33197, loss = 0.00738687\n",
      "Iteration 33198, loss = 0.00738672\n",
      "Iteration 33199, loss = 0.00738657\n",
      "Iteration 33200, loss = 0.00738642\n",
      "Iteration 33201, loss = 0.00738627\n",
      "Iteration 33202, loss = 0.00738612\n",
      "Iteration 33203, loss = 0.00738597\n",
      "Iteration 33204, loss = 0.00738582\n",
      "Iteration 33205, loss = 0.00738568\n",
      "Iteration 33206, loss = 0.00738553\n",
      "Iteration 33207, loss = 0.00738538\n",
      "Iteration 33208, loss = 0.00738523\n",
      "Iteration 33209, loss = 0.00738508\n",
      "Iteration 33210, loss = 0.00738493\n",
      "Iteration 33211, loss = 0.00738478\n",
      "Iteration 33212, loss = 0.00738463\n",
      "Iteration 33213, loss = 0.00738448\n",
      "Iteration 33214, loss = 0.00738434\n",
      "Iteration 33215, loss = 0.00738419\n",
      "Iteration 33216, loss = 0.00738404\n",
      "Iteration 33217, loss = 0.00738389\n",
      "Iteration 33218, loss = 0.00738374\n",
      "Iteration 33219, loss = 0.00738359\n",
      "Iteration 33220, loss = 0.00738344\n",
      "Iteration 33221, loss = 0.00738329\n",
      "Iteration 33222, loss = 0.00738315\n",
      "Iteration 33223, loss = 0.00738300\n",
      "Iteration 33224, loss = 0.00738285\n",
      "Iteration 33225, loss = 0.00738270\n",
      "Iteration 33226, loss = 0.00738255\n",
      "Iteration 33227, loss = 0.00738240\n",
      "Iteration 33228, loss = 0.00738225\n",
      "Iteration 33229, loss = 0.00738210\n",
      "Iteration 33230, loss = 0.00738196\n",
      "Iteration 33231, loss = 0.00738181\n",
      "Iteration 33232, loss = 0.00738166\n",
      "Iteration 33233, loss = 0.00738151\n",
      "Iteration 33234, loss = 0.00738136\n",
      "Iteration 33235, loss = 0.00738121\n",
      "Iteration 33236, loss = 0.00738106\n",
      "Iteration 33237, loss = 0.00738091\n",
      "Iteration 33238, loss = 0.00738077\n",
      "Iteration 33239, loss = 0.00738062\n",
      "Iteration 33240, loss = 0.00738047\n",
      "Iteration 33241, loss = 0.00738032\n",
      "Iteration 33242, loss = 0.00738017\n",
      "Iteration 33243, loss = 0.00738002\n",
      "Iteration 33244, loss = 0.00737988\n",
      "Iteration 33245, loss = 0.00737973\n",
      "Iteration 33246, loss = 0.00737958\n",
      "Iteration 33247, loss = 0.00737943\n",
      "Iteration 33248, loss = 0.00737928\n",
      "Iteration 33249, loss = 0.00737913\n",
      "Iteration 33250, loss = 0.00737898\n",
      "Iteration 33251, loss = 0.00737884\n",
      "Iteration 33252, loss = 0.00737869\n",
      "Iteration 33253, loss = 0.00737854\n",
      "Iteration 33254, loss = 0.00737839\n",
      "Iteration 33255, loss = 0.00737824\n",
      "Iteration 33256, loss = 0.00737809\n",
      "Iteration 33257, loss = 0.00737795\n",
      "Iteration 33258, loss = 0.00737780\n",
      "Iteration 33259, loss = 0.00737765\n",
      "Iteration 33260, loss = 0.00737750\n",
      "Iteration 33261, loss = 0.00737735\n",
      "Iteration 33262, loss = 0.00737720\n",
      "Iteration 33263, loss = 0.00737706\n",
      "Iteration 33264, loss = 0.00737691\n",
      "Iteration 33265, loss = 0.00737676\n",
      "Iteration 33266, loss = 0.00737661\n",
      "Iteration 33267, loss = 0.00737646\n",
      "Iteration 33268, loss = 0.00737631\n",
      "Iteration 33269, loss = 0.00737617\n",
      "Iteration 33270, loss = 0.00737602\n",
      "Iteration 33271, loss = 0.00737587\n",
      "Iteration 33272, loss = 0.00737572\n",
      "Iteration 33273, loss = 0.00737557\n",
      "Iteration 33274, loss = 0.00737543\n",
      "Iteration 33275, loss = 0.00737528\n",
      "Iteration 33276, loss = 0.00737513\n",
      "Iteration 33277, loss = 0.00737498\n",
      "Iteration 33278, loss = 0.00737483\n",
      "Iteration 33279, loss = 0.00737469\n",
      "Iteration 33280, loss = 0.00737454\n",
      "Iteration 33281, loss = 0.00737439\n",
      "Iteration 33282, loss = 0.00737424\n",
      "Iteration 33283, loss = 0.00737409\n",
      "Iteration 33284, loss = 0.00737395\n",
      "Iteration 33285, loss = 0.00737380\n",
      "Iteration 33286, loss = 0.00737365\n",
      "Iteration 33287, loss = 0.00737350\n",
      "Iteration 33288, loss = 0.00737335\n",
      "Iteration 33289, loss = 0.00737321\n",
      "Iteration 33290, loss = 0.00737306\n",
      "Iteration 33291, loss = 0.00737291\n",
      "Iteration 33292, loss = 0.00737276\n",
      "Iteration 33293, loss = 0.00737261\n",
      "Iteration 33294, loss = 0.00737247\n",
      "Iteration 33295, loss = 0.00737232\n",
      "Iteration 33296, loss = 0.00737217\n",
      "Iteration 33297, loss = 0.00737202\n",
      "Iteration 33298, loss = 0.00737187\n",
      "Iteration 33299, loss = 0.00737173\n",
      "Iteration 33300, loss = 0.00737158\n",
      "Iteration 33301, loss = 0.00737143\n",
      "Iteration 33302, loss = 0.00737128\n",
      "Iteration 33303, loss = 0.00737114\n",
      "Iteration 33304, loss = 0.00737099\n",
      "Iteration 33305, loss = 0.00737084\n",
      "Iteration 33306, loss = 0.00737069\n",
      "Iteration 33307, loss = 0.00737054\n",
      "Iteration 33308, loss = 0.00737040\n",
      "Iteration 33309, loss = 0.00737025\n",
      "Iteration 33310, loss = 0.00737010\n",
      "Iteration 33311, loss = 0.00736995\n",
      "Iteration 33312, loss = 0.00736981\n",
      "Iteration 33313, loss = 0.00736966\n",
      "Iteration 33314, loss = 0.00736951\n",
      "Iteration 33315, loss = 0.00736936\n",
      "Iteration 33316, loss = 0.00736922\n",
      "Iteration 33317, loss = 0.00736907\n",
      "Iteration 33318, loss = 0.00736892\n",
      "Iteration 33319, loss = 0.00736877\n",
      "Iteration 33320, loss = 0.00736862\n",
      "Iteration 33321, loss = 0.00736848\n",
      "Iteration 33322, loss = 0.00736833\n",
      "Iteration 33323, loss = 0.00736818\n",
      "Iteration 33324, loss = 0.00736803\n",
      "Iteration 33325, loss = 0.00736789\n",
      "Iteration 33326, loss = 0.00736774\n",
      "Iteration 33327, loss = 0.00736759\n",
      "Iteration 33328, loss = 0.00736744\n",
      "Iteration 33329, loss = 0.00736730\n",
      "Iteration 33330, loss = 0.00736715\n",
      "Iteration 33331, loss = 0.00736700\n",
      "Iteration 33332, loss = 0.00736685\n",
      "Iteration 33333, loss = 0.00736671\n",
      "Iteration 33334, loss = 0.00736656\n",
      "Iteration 33335, loss = 0.00736641\n",
      "Iteration 33336, loss = 0.00736627\n",
      "Iteration 33337, loss = 0.00736612\n",
      "Iteration 33338, loss = 0.00736597\n",
      "Iteration 33339, loss = 0.00736582\n",
      "Iteration 33340, loss = 0.00736568\n",
      "Iteration 33341, loss = 0.00736553\n",
      "Iteration 33342, loss = 0.00736538\n",
      "Iteration 33343, loss = 0.00736523\n",
      "Iteration 33344, loss = 0.00736509\n",
      "Iteration 33345, loss = 0.00736494\n",
      "Iteration 33346, loss = 0.00736479\n",
      "Iteration 33347, loss = 0.00736465\n",
      "Iteration 33348, loss = 0.00736450\n",
      "Iteration 33349, loss = 0.00736435\n",
      "Iteration 33350, loss = 0.00736420\n",
      "Iteration 33351, loss = 0.00736406\n",
      "Iteration 33352, loss = 0.00736391\n",
      "Iteration 33353, loss = 0.00736376\n",
      "Iteration 33354, loss = 0.00736361\n",
      "Iteration 33355, loss = 0.00736347\n",
      "Iteration 33356, loss = 0.00736332\n",
      "Iteration 33357, loss = 0.00736317\n",
      "Iteration 33358, loss = 0.00736303\n",
      "Iteration 33359, loss = 0.00736288\n",
      "Iteration 33360, loss = 0.00736273\n",
      "Iteration 33361, loss = 0.00736258\n",
      "Iteration 33362, loss = 0.00736244\n",
      "Iteration 33363, loss = 0.00736229\n",
      "Iteration 33364, loss = 0.00736214\n",
      "Iteration 33365, loss = 0.00736200\n",
      "Iteration 33366, loss = 0.00736185\n",
      "Iteration 33367, loss = 0.00736170\n",
      "Iteration 33368, loss = 0.00736156\n",
      "Iteration 33369, loss = 0.00736141\n",
      "Iteration 33370, loss = 0.00736126\n",
      "Iteration 33371, loss = 0.00736111\n",
      "Iteration 33372, loss = 0.00736097\n",
      "Iteration 33373, loss = 0.00736082\n",
      "Iteration 33374, loss = 0.00736067\n",
      "Iteration 33375, loss = 0.00736053\n",
      "Iteration 33376, loss = 0.00736038\n",
      "Iteration 33377, loss = 0.00736023\n",
      "Iteration 33378, loss = 0.00736009\n",
      "Iteration 33379, loss = 0.00735994\n",
      "Iteration 33380, loss = 0.00735979\n",
      "Iteration 33381, loss = 0.00735965\n",
      "Iteration 33382, loss = 0.00735950\n",
      "Iteration 33383, loss = 0.00735935\n",
      "Iteration 33384, loss = 0.00735921\n",
      "Iteration 33385, loss = 0.00735906\n",
      "Iteration 33386, loss = 0.00735891\n",
      "Iteration 33387, loss = 0.00735877\n",
      "Iteration 33388, loss = 0.00735862\n",
      "Iteration 33389, loss = 0.00735847\n",
      "Iteration 33390, loss = 0.00735832\n",
      "Iteration 33391, loss = 0.00735818\n",
      "Iteration 33392, loss = 0.00735803\n",
      "Iteration 33393, loss = 0.00735788\n",
      "Iteration 33394, loss = 0.00735774\n",
      "Iteration 33395, loss = 0.00735759\n",
      "Iteration 33396, loss = 0.00735744\n",
      "Iteration 33397, loss = 0.00735730\n",
      "Iteration 33398, loss = 0.00735715\n",
      "Iteration 33399, loss = 0.00735700\n",
      "Iteration 33400, loss = 0.00735686\n",
      "Iteration 33401, loss = 0.00735671\n",
      "Iteration 33402, loss = 0.00735657\n",
      "Iteration 33403, loss = 0.00735642\n",
      "Iteration 33404, loss = 0.00735627\n",
      "Iteration 33405, loss = 0.00735613\n",
      "Iteration 33406, loss = 0.00735598\n",
      "Iteration 33407, loss = 0.00735583\n",
      "Iteration 33408, loss = 0.00735569\n",
      "Iteration 33409, loss = 0.00735554\n",
      "Iteration 33410, loss = 0.00735539\n",
      "Iteration 33411, loss = 0.00735525\n",
      "Iteration 33412, loss = 0.00735510\n",
      "Iteration 33413, loss = 0.00735495\n",
      "Iteration 33414, loss = 0.00735481\n",
      "Iteration 33415, loss = 0.00735466\n",
      "Iteration 33416, loss = 0.00735451\n",
      "Iteration 33417, loss = 0.00735437\n",
      "Iteration 33418, loss = 0.00735422\n",
      "Iteration 33419, loss = 0.00735408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33420, loss = 0.00735393\n",
      "Iteration 33421, loss = 0.00735378\n",
      "Iteration 33422, loss = 0.00735364\n",
      "Iteration 33423, loss = 0.00735349\n",
      "Iteration 33424, loss = 0.00735334\n",
      "Iteration 33425, loss = 0.00735320\n",
      "Iteration 33426, loss = 0.00735305\n",
      "Iteration 33427, loss = 0.00735290\n",
      "Iteration 33428, loss = 0.00735276\n",
      "Iteration 33429, loss = 0.00735261\n",
      "Iteration 33430, loss = 0.00735247\n",
      "Iteration 33431, loss = 0.00735232\n",
      "Iteration 33432, loss = 0.00735217\n",
      "Iteration 33433, loss = 0.00735203\n",
      "Iteration 33434, loss = 0.00735188\n",
      "Iteration 33435, loss = 0.00735173\n",
      "Iteration 33436, loss = 0.00735159\n",
      "Iteration 33437, loss = 0.00735144\n",
      "Iteration 33438, loss = 0.00735130\n",
      "Iteration 33439, loss = 0.00735115\n",
      "Iteration 33440, loss = 0.00735100\n",
      "Iteration 33441, loss = 0.00735086\n",
      "Iteration 33442, loss = 0.00735071\n",
      "Iteration 33443, loss = 0.00735057\n",
      "Iteration 33444, loss = 0.00735042\n",
      "Iteration 33445, loss = 0.00735027\n",
      "Iteration 33446, loss = 0.00735013\n",
      "Iteration 33447, loss = 0.00734998\n",
      "Iteration 33448, loss = 0.00734984\n",
      "Iteration 33449, loss = 0.00734969\n",
      "Iteration 33450, loss = 0.00734954\n",
      "Iteration 33451, loss = 0.00734940\n",
      "Iteration 33452, loss = 0.00734925\n",
      "Iteration 33453, loss = 0.00734911\n",
      "Iteration 33454, loss = 0.00734896\n",
      "Iteration 33455, loss = 0.00734881\n",
      "Iteration 33456, loss = 0.00734867\n",
      "Iteration 33457, loss = 0.00734852\n",
      "Iteration 33458, loss = 0.00734838\n",
      "Iteration 33459, loss = 0.00734823\n",
      "Iteration 33460, loss = 0.00734808\n",
      "Iteration 33461, loss = 0.00734794\n",
      "Iteration 33462, loss = 0.00734779\n",
      "Iteration 33463, loss = 0.00734765\n",
      "Iteration 33464, loss = 0.00734750\n",
      "Iteration 33465, loss = 0.00734735\n",
      "Iteration 33466, loss = 0.00734721\n",
      "Iteration 33467, loss = 0.00734706\n",
      "Iteration 33468, loss = 0.00734692\n",
      "Iteration 33469, loss = 0.00734677\n",
      "Iteration 33470, loss = 0.00734663\n",
      "Iteration 33471, loss = 0.00734648\n",
      "Iteration 33472, loss = 0.00734633\n",
      "Iteration 33473, loss = 0.00734619\n",
      "Iteration 33474, loss = 0.00734604\n",
      "Iteration 33475, loss = 0.00734590\n",
      "Iteration 33476, loss = 0.00734575\n",
      "Iteration 33477, loss = 0.00734561\n",
      "Iteration 33478, loss = 0.00734546\n",
      "Iteration 33479, loss = 0.00734531\n",
      "Iteration 33480, loss = 0.00734517\n",
      "Iteration 33481, loss = 0.00734502\n",
      "Iteration 33482, loss = 0.00734488\n",
      "Iteration 33483, loss = 0.00734473\n",
      "Iteration 33484, loss = 0.00734459\n",
      "Iteration 33485, loss = 0.00734444\n",
      "Iteration 33486, loss = 0.00734429\n",
      "Iteration 33487, loss = 0.00734415\n",
      "Iteration 33488, loss = 0.00734400\n",
      "Iteration 33489, loss = 0.00734386\n",
      "Iteration 33490, loss = 0.00734371\n",
      "Iteration 33491, loss = 0.00734357\n",
      "Iteration 33492, loss = 0.00734342\n",
      "Iteration 33493, loss = 0.00734328\n",
      "Iteration 33494, loss = 0.00734313\n",
      "Iteration 33495, loss = 0.00734299\n",
      "Iteration 33496, loss = 0.00734284\n",
      "Iteration 33497, loss = 0.00734269\n",
      "Iteration 33498, loss = 0.00734255\n",
      "Iteration 33499, loss = 0.00734240\n",
      "Iteration 33500, loss = 0.00734226\n",
      "Iteration 33501, loss = 0.00734211\n",
      "Iteration 33502, loss = 0.00734197\n",
      "Iteration 33503, loss = 0.00734182\n",
      "Iteration 33504, loss = 0.00734168\n",
      "Iteration 33505, loss = 0.00734153\n",
      "Iteration 33506, loss = 0.00734139\n",
      "Iteration 33507, loss = 0.00734124\n",
      "Iteration 33508, loss = 0.00734110\n",
      "Iteration 33509, loss = 0.00734095\n",
      "Iteration 33510, loss = 0.00734080\n",
      "Iteration 33511, loss = 0.00734066\n",
      "Iteration 33512, loss = 0.00734051\n",
      "Iteration 33513, loss = 0.00734037\n",
      "Iteration 33514, loss = 0.00734022\n",
      "Iteration 33515, loss = 0.00734008\n",
      "Iteration 33516, loss = 0.00733993\n",
      "Iteration 33517, loss = 0.00733979\n",
      "Iteration 33518, loss = 0.00733964\n",
      "Iteration 33519, loss = 0.00733950\n",
      "Iteration 33520, loss = 0.00733935\n",
      "Iteration 33521, loss = 0.00733921\n",
      "Iteration 33522, loss = 0.00733906\n",
      "Iteration 33523, loss = 0.00733892\n",
      "Iteration 33524, loss = 0.00733877\n",
      "Iteration 33525, loss = 0.00733863\n",
      "Iteration 33526, loss = 0.00733848\n",
      "Iteration 33527, loss = 0.00733834\n",
      "Iteration 33528, loss = 0.00733819\n",
      "Iteration 33529, loss = 0.00733805\n",
      "Iteration 33530, loss = 0.00733790\n",
      "Iteration 33531, loss = 0.00733776\n",
      "Iteration 33532, loss = 0.00733761\n",
      "Iteration 33533, loss = 0.00733747\n",
      "Iteration 33534, loss = 0.00733732\n",
      "Iteration 33535, loss = 0.00733718\n",
      "Iteration 33536, loss = 0.00733703\n",
      "Iteration 33537, loss = 0.00733689\n",
      "Iteration 33538, loss = 0.00733674\n",
      "Iteration 33539, loss = 0.00733660\n",
      "Iteration 33540, loss = 0.00733645\n",
      "Iteration 33541, loss = 0.00733631\n",
      "Iteration 33542, loss = 0.00733616\n",
      "Iteration 33543, loss = 0.00733602\n",
      "Iteration 33544, loss = 0.00733587\n",
      "Iteration 33545, loss = 0.00733573\n",
      "Iteration 33546, loss = 0.00733558\n",
      "Iteration 33547, loss = 0.00733544\n",
      "Iteration 33548, loss = 0.00733529\n",
      "Iteration 33549, loss = 0.00733515\n",
      "Iteration 33550, loss = 0.00733500\n",
      "Iteration 33551, loss = 0.00733486\n",
      "Iteration 33552, loss = 0.00733471\n",
      "Iteration 33553, loss = 0.00733457\n",
      "Iteration 33554, loss = 0.00733442\n",
      "Iteration 33555, loss = 0.00733428\n",
      "Iteration 33556, loss = 0.00733413\n",
      "Iteration 33557, loss = 0.00733399\n",
      "Iteration 33558, loss = 0.00733384\n",
      "Iteration 33559, loss = 0.00733370\n",
      "Iteration 33560, loss = 0.00733356\n",
      "Iteration 33561, loss = 0.00733341\n",
      "Iteration 33562, loss = 0.00733327\n",
      "Iteration 33563, loss = 0.00733312\n",
      "Iteration 33564, loss = 0.00733298\n",
      "Iteration 33565, loss = 0.00733283\n",
      "Iteration 33566, loss = 0.00733269\n",
      "Iteration 33567, loss = 0.00733254\n",
      "Iteration 33568, loss = 0.00733240\n",
      "Iteration 33569, loss = 0.00733225\n",
      "Iteration 33570, loss = 0.00733211\n",
      "Iteration 33571, loss = 0.00733196\n",
      "Iteration 33572, loss = 0.00733182\n",
      "Iteration 33573, loss = 0.00733168\n",
      "Iteration 33574, loss = 0.00733153\n",
      "Iteration 33575, loss = 0.00733139\n",
      "Iteration 33576, loss = 0.00733124\n",
      "Iteration 33577, loss = 0.00733110\n",
      "Iteration 33578, loss = 0.00733095\n",
      "Iteration 33579, loss = 0.00733081\n",
      "Iteration 33580, loss = 0.00733066\n",
      "Iteration 33581, loss = 0.00733052\n",
      "Iteration 33582, loss = 0.00733037\n",
      "Iteration 33583, loss = 0.00733023\n",
      "Iteration 33584, loss = 0.00733009\n",
      "Iteration 33585, loss = 0.00732994\n",
      "Iteration 33586, loss = 0.00732980\n",
      "Iteration 33587, loss = 0.00732965\n",
      "Iteration 33588, loss = 0.00732951\n",
      "Iteration 33589, loss = 0.00732936\n",
      "Iteration 33590, loss = 0.00732922\n",
      "Iteration 33591, loss = 0.00732908\n",
      "Iteration 33592, loss = 0.00732893\n",
      "Iteration 33593, loss = 0.00732879\n",
      "Iteration 33594, loss = 0.00732864\n",
      "Iteration 33595, loss = 0.00732850\n",
      "Iteration 33596, loss = 0.00732835\n",
      "Iteration 33597, loss = 0.00732821\n",
      "Iteration 33598, loss = 0.00732807\n",
      "Iteration 33599, loss = 0.00732792\n",
      "Iteration 33600, loss = 0.00732778\n",
      "Iteration 33601, loss = 0.00732763\n",
      "Iteration 33602, loss = 0.00732749\n",
      "Iteration 33603, loss = 0.00732734\n",
      "Iteration 33604, loss = 0.00732720\n",
      "Iteration 33605, loss = 0.00732706\n",
      "Iteration 33606, loss = 0.00732691\n",
      "Iteration 33607, loss = 0.00732677\n",
      "Iteration 33608, loss = 0.00732662\n",
      "Iteration 33609, loss = 0.00732648\n",
      "Iteration 33610, loss = 0.00732634\n",
      "Iteration 33611, loss = 0.00732619\n",
      "Iteration 33612, loss = 0.00732605\n",
      "Iteration 33613, loss = 0.00732590\n",
      "Iteration 33614, loss = 0.00732576\n",
      "Iteration 33615, loss = 0.00732561\n",
      "Iteration 33616, loss = 0.00732547\n",
      "Iteration 33617, loss = 0.00732533\n",
      "Iteration 33618, loss = 0.00732518\n",
      "Iteration 33619, loss = 0.00732504\n",
      "Iteration 33620, loss = 0.00732489\n",
      "Iteration 33621, loss = 0.00732475\n",
      "Iteration 33622, loss = 0.00732461\n",
      "Iteration 33623, loss = 0.00732446\n",
      "Iteration 33624, loss = 0.00732432\n",
      "Iteration 33625, loss = 0.00732417\n",
      "Iteration 33626, loss = 0.00732403\n",
      "Iteration 33627, loss = 0.00732389\n",
      "Iteration 33628, loss = 0.00732374\n",
      "Iteration 33629, loss = 0.00732360\n",
      "Iteration 33630, loss = 0.00732346\n",
      "Iteration 33631, loss = 0.00732331\n",
      "Iteration 33632, loss = 0.00732317\n",
      "Iteration 33633, loss = 0.00732302\n",
      "Iteration 33634, loss = 0.00732288\n",
      "Iteration 33635, loss = 0.00732274\n",
      "Iteration 33636, loss = 0.00732259\n",
      "Iteration 33637, loss = 0.00732245\n",
      "Iteration 33638, loss = 0.00732230\n",
      "Iteration 33639, loss = 0.00732216\n",
      "Iteration 33640, loss = 0.00732202\n",
      "Iteration 33641, loss = 0.00732187\n",
      "Iteration 33642, loss = 0.00732173\n",
      "Iteration 33643, loss = 0.00732159\n",
      "Iteration 33644, loss = 0.00732144\n",
      "Iteration 33645, loss = 0.00732130\n",
      "Iteration 33646, loss = 0.00732116\n",
      "Iteration 33647, loss = 0.00732101\n",
      "Iteration 33648, loss = 0.00732087\n",
      "Iteration 33649, loss = 0.00732072\n",
      "Iteration 33650, loss = 0.00732058\n",
      "Iteration 33651, loss = 0.00732044\n",
      "Iteration 33652, loss = 0.00732029\n",
      "Iteration 33653, loss = 0.00732015\n",
      "Iteration 33654, loss = 0.00732001\n",
      "Iteration 33655, loss = 0.00731986\n",
      "Iteration 33656, loss = 0.00731972\n",
      "Iteration 33657, loss = 0.00731958\n",
      "Iteration 33658, loss = 0.00731943\n",
      "Iteration 33659, loss = 0.00731929\n",
      "Iteration 33660, loss = 0.00731914\n",
      "Iteration 33661, loss = 0.00731900\n",
      "Iteration 33662, loss = 0.00731886\n",
      "Iteration 33663, loss = 0.00731871\n",
      "Iteration 33664, loss = 0.00731857\n",
      "Iteration 33665, loss = 0.00731843\n",
      "Iteration 33666, loss = 0.00731828\n",
      "Iteration 33667, loss = 0.00731814\n",
      "Iteration 33668, loss = 0.00731800\n",
      "Iteration 33669, loss = 0.00731785\n",
      "Iteration 33670, loss = 0.00731771\n",
      "Iteration 33671, loss = 0.00731757\n",
      "Iteration 33672, loss = 0.00731742\n",
      "Iteration 33673, loss = 0.00731728\n",
      "Iteration 33674, loss = 0.00731714\n",
      "Iteration 33675, loss = 0.00731699\n",
      "Iteration 33676, loss = 0.00731685\n",
      "Iteration 33677, loss = 0.00731671\n",
      "Iteration 33678, loss = 0.00731656\n",
      "Iteration 33679, loss = 0.00731642\n",
      "Iteration 33680, loss = 0.00731628\n",
      "Iteration 33681, loss = 0.00731613\n",
      "Iteration 33682, loss = 0.00731599\n",
      "Iteration 33683, loss = 0.00731585\n",
      "Iteration 33684, loss = 0.00731570\n",
      "Iteration 33685, loss = 0.00731556\n",
      "Iteration 33686, loss = 0.00731542\n",
      "Iteration 33687, loss = 0.00731527\n",
      "Iteration 33688, loss = 0.00731513\n",
      "Iteration 33689, loss = 0.00731499\n",
      "Iteration 33690, loss = 0.00731484\n",
      "Iteration 33691, loss = 0.00731470\n",
      "Iteration 33692, loss = 0.00731456\n",
      "Iteration 33693, loss = 0.00731441\n",
      "Iteration 33694, loss = 0.00731427\n",
      "Iteration 33695, loss = 0.00731413\n",
      "Iteration 33696, loss = 0.00731399\n",
      "Iteration 33697, loss = 0.00731384\n",
      "Iteration 33698, loss = 0.00731370\n",
      "Iteration 33699, loss = 0.00731356\n",
      "Iteration 33700, loss = 0.00731341\n",
      "Iteration 33701, loss = 0.00731327\n",
      "Iteration 33702, loss = 0.00731313\n",
      "Iteration 33703, loss = 0.00731298\n",
      "Iteration 33704, loss = 0.00731284\n",
      "Iteration 33705, loss = 0.00731270\n",
      "Iteration 33706, loss = 0.00731256\n",
      "Iteration 33707, loss = 0.00731241\n",
      "Iteration 33708, loss = 0.00731227\n",
      "Iteration 33709, loss = 0.00731213\n",
      "Iteration 33710, loss = 0.00731198\n",
      "Iteration 33711, loss = 0.00731184\n",
      "Iteration 33712, loss = 0.00731170\n",
      "Iteration 33713, loss = 0.00731155\n",
      "Iteration 33714, loss = 0.00731141\n",
      "Iteration 33715, loss = 0.00731127\n",
      "Iteration 33716, loss = 0.00731113\n",
      "Iteration 33717, loss = 0.00731098\n",
      "Iteration 33718, loss = 0.00731084\n",
      "Iteration 33719, loss = 0.00731070\n",
      "Iteration 33720, loss = 0.00731055\n",
      "Iteration 33721, loss = 0.00731041\n",
      "Iteration 33722, loss = 0.00731027\n",
      "Iteration 33723, loss = 0.00731013\n",
      "Iteration 33724, loss = 0.00730998\n",
      "Iteration 33725, loss = 0.00730984\n",
      "Iteration 33726, loss = 0.00730970\n",
      "Iteration 33727, loss = 0.00730956\n",
      "Iteration 33728, loss = 0.00730941\n",
      "Iteration 33729, loss = 0.00730927\n",
      "Iteration 33730, loss = 0.00730913\n",
      "Iteration 33731, loss = 0.00730898\n",
      "Iteration 33732, loss = 0.00730884\n",
      "Iteration 33733, loss = 0.00730870\n",
      "Iteration 33734, loss = 0.00730856\n",
      "Iteration 33735, loss = 0.00730841\n",
      "Iteration 33736, loss = 0.00730827\n",
      "Iteration 33737, loss = 0.00730813\n",
      "Iteration 33738, loss = 0.00730799\n",
      "Iteration 33739, loss = 0.00730784\n",
      "Iteration 33740, loss = 0.00730770\n",
      "Iteration 33741, loss = 0.00730756\n",
      "Iteration 33742, loss = 0.00730742\n",
      "Iteration 33743, loss = 0.00730727\n",
      "Iteration 33744, loss = 0.00730713\n",
      "Iteration 33745, loss = 0.00730699\n",
      "Iteration 33746, loss = 0.00730685\n",
      "Iteration 33747, loss = 0.00730670\n",
      "Iteration 33748, loss = 0.00730656\n",
      "Iteration 33749, loss = 0.00730642\n",
      "Iteration 33750, loss = 0.00730628\n",
      "Iteration 33751, loss = 0.00730613\n",
      "Iteration 33752, loss = 0.00730599\n",
      "Iteration 33753, loss = 0.00730585\n",
      "Iteration 33754, loss = 0.00730571\n",
      "Iteration 33755, loss = 0.00730556\n",
      "Iteration 33756, loss = 0.00730542\n",
      "Iteration 33757, loss = 0.00730528\n",
      "Iteration 33758, loss = 0.00730514\n",
      "Iteration 33759, loss = 0.00730499\n",
      "Iteration 33760, loss = 0.00730485\n",
      "Iteration 33761, loss = 0.00730471\n",
      "Iteration 33762, loss = 0.00730457\n",
      "Iteration 33763, loss = 0.00730442\n",
      "Iteration 33764, loss = 0.00730428\n",
      "Iteration 33765, loss = 0.00730414\n",
      "Iteration 33766, loss = 0.00730400\n",
      "Iteration 33767, loss = 0.00730385\n",
      "Iteration 33768, loss = 0.00730371\n",
      "Iteration 33769, loss = 0.00730357\n",
      "Iteration 33770, loss = 0.00730343\n",
      "Iteration 33771, loss = 0.00730329\n",
      "Iteration 33772, loss = 0.00730314\n",
      "Iteration 33773, loss = 0.00730300\n",
      "Iteration 33774, loss = 0.00730286\n",
      "Iteration 33775, loss = 0.00730272\n",
      "Iteration 33776, loss = 0.00730257\n",
      "Iteration 33777, loss = 0.00730243\n",
      "Iteration 33778, loss = 0.00730229\n",
      "Iteration 33779, loss = 0.00730215\n",
      "Iteration 33780, loss = 0.00730201\n",
      "Iteration 33781, loss = 0.00730186\n",
      "Iteration 33782, loss = 0.00730172\n",
      "Iteration 33783, loss = 0.00730158\n",
      "Iteration 33784, loss = 0.00730144\n",
      "Iteration 33785, loss = 0.00730130\n",
      "Iteration 33786, loss = 0.00730115\n",
      "Iteration 33787, loss = 0.00730101\n",
      "Iteration 33788, loss = 0.00730087\n",
      "Iteration 33789, loss = 0.00730073\n",
      "Iteration 33790, loss = 0.00730059\n",
      "Iteration 33791, loss = 0.00730044\n",
      "Iteration 33792, loss = 0.00730030\n",
      "Iteration 33793, loss = 0.00730016\n",
      "Iteration 33794, loss = 0.00730002\n",
      "Iteration 33795, loss = 0.00729988\n",
      "Iteration 33796, loss = 0.00729973\n",
      "Iteration 33797, loss = 0.00729959\n",
      "Iteration 33798, loss = 0.00729945\n",
      "Iteration 33799, loss = 0.00729931\n",
      "Iteration 33800, loss = 0.00729917\n",
      "Iteration 33801, loss = 0.00729902\n",
      "Iteration 33802, loss = 0.00729888\n",
      "Iteration 33803, loss = 0.00729874\n",
      "Iteration 33804, loss = 0.00729860\n",
      "Iteration 33805, loss = 0.00729846\n",
      "Iteration 33806, loss = 0.00729831\n",
      "Iteration 33807, loss = 0.00729817\n",
      "Iteration 33808, loss = 0.00729803\n",
      "Iteration 33809, loss = 0.00729789\n",
      "Iteration 33810, loss = 0.00729775\n",
      "Iteration 33811, loss = 0.00729761\n",
      "Iteration 33812, loss = 0.00729746\n",
      "Iteration 33813, loss = 0.00729732\n",
      "Iteration 33814, loss = 0.00729718\n",
      "Iteration 33815, loss = 0.00729704\n",
      "Iteration 33816, loss = 0.00729690\n",
      "Iteration 33817, loss = 0.00729676\n",
      "Iteration 33818, loss = 0.00729661\n",
      "Iteration 33819, loss = 0.00729647\n",
      "Iteration 33820, loss = 0.00729633\n",
      "Iteration 33821, loss = 0.00729619\n",
      "Iteration 33822, loss = 0.00729605\n",
      "Iteration 33823, loss = 0.00729591\n",
      "Iteration 33824, loss = 0.00729576\n",
      "Iteration 33825, loss = 0.00729562\n",
      "Iteration 33826, loss = 0.00729548\n",
      "Iteration 33827, loss = 0.00729534\n",
      "Iteration 33828, loss = 0.00729520\n",
      "Iteration 33829, loss = 0.00729506\n",
      "Iteration 33830, loss = 0.00729491\n",
      "Iteration 33831, loss = 0.00729477\n",
      "Iteration 33832, loss = 0.00729463\n",
      "Iteration 33833, loss = 0.00729449\n",
      "Iteration 33834, loss = 0.00729435\n",
      "Iteration 33835, loss = 0.00729421\n",
      "Iteration 33836, loss = 0.00729407\n",
      "Iteration 33837, loss = 0.00729392\n",
      "Iteration 33838, loss = 0.00729378\n",
      "Iteration 33839, loss = 0.00729364\n",
      "Iteration 33840, loss = 0.00729350\n",
      "Iteration 33841, loss = 0.00729336\n",
      "Iteration 33842, loss = 0.00729322\n",
      "Iteration 33843, loss = 0.00729307\n",
      "Iteration 33844, loss = 0.00729293\n",
      "Iteration 33845, loss = 0.00729279\n",
      "Iteration 33846, loss = 0.00729265\n",
      "Iteration 33847, loss = 0.00729251\n",
      "Iteration 33848, loss = 0.00729237\n",
      "Iteration 33849, loss = 0.00729223\n",
      "Iteration 33850, loss = 0.00729209\n",
      "Iteration 33851, loss = 0.00729194\n",
      "Iteration 33852, loss = 0.00729180\n",
      "Iteration 33853, loss = 0.00729166\n",
      "Iteration 33854, loss = 0.00729152\n",
      "Iteration 33855, loss = 0.00729138\n",
      "Iteration 33856, loss = 0.00729124\n",
      "Iteration 33857, loss = 0.00729110\n",
      "Iteration 33858, loss = 0.00729096\n",
      "Iteration 33859, loss = 0.00729081\n",
      "Iteration 33860, loss = 0.00729067\n",
      "Iteration 33861, loss = 0.00729053\n",
      "Iteration 33862, loss = 0.00729039\n",
      "Iteration 33863, loss = 0.00729025\n",
      "Iteration 33864, loss = 0.00729011\n",
      "Iteration 33865, loss = 0.00728997\n",
      "Iteration 33866, loss = 0.00728983\n",
      "Iteration 33867, loss = 0.00728968\n",
      "Iteration 33868, loss = 0.00728954\n",
      "Iteration 33869, loss = 0.00728940\n",
      "Iteration 33870, loss = 0.00728926\n",
      "Iteration 33871, loss = 0.00728912\n",
      "Iteration 33872, loss = 0.00728898\n",
      "Iteration 33873, loss = 0.00728884\n",
      "Iteration 33874, loss = 0.00728870\n",
      "Iteration 33875, loss = 0.00728856\n",
      "Iteration 33876, loss = 0.00728841\n",
      "Iteration 33877, loss = 0.00728827\n",
      "Iteration 33878, loss = 0.00728813\n",
      "Iteration 33879, loss = 0.00728799\n",
      "Iteration 33880, loss = 0.00728785\n",
      "Iteration 33881, loss = 0.00728771\n",
      "Iteration 33882, loss = 0.00728757\n",
      "Iteration 33883, loss = 0.00728743\n",
      "Iteration 33884, loss = 0.00728729\n",
      "Iteration 33885, loss = 0.00728715\n",
      "Iteration 33886, loss = 0.00728701\n",
      "Iteration 33887, loss = 0.00728686\n",
      "Iteration 33888, loss = 0.00728672\n",
      "Iteration 33889, loss = 0.00728658\n",
      "Iteration 33890, loss = 0.00728644\n",
      "Iteration 33891, loss = 0.00728630\n",
      "Iteration 33892, loss = 0.00728616\n",
      "Iteration 33893, loss = 0.00728602\n",
      "Iteration 33894, loss = 0.00728588\n",
      "Iteration 33895, loss = 0.00728574\n",
      "Iteration 33896, loss = 0.00728560\n",
      "Iteration 33897, loss = 0.00728546\n",
      "Iteration 33898, loss = 0.00728532\n",
      "Iteration 33899, loss = 0.00728517\n",
      "Iteration 33900, loss = 0.00728503\n",
      "Iteration 33901, loss = 0.00728489\n",
      "Iteration 33902, loss = 0.00728475\n",
      "Iteration 33903, loss = 0.00728461\n",
      "Iteration 33904, loss = 0.00728447\n",
      "Iteration 33905, loss = 0.00728433\n",
      "Iteration 33906, loss = 0.00728419\n",
      "Iteration 33907, loss = 0.00728405\n",
      "Iteration 33908, loss = 0.00728391\n",
      "Iteration 33909, loss = 0.00728377\n",
      "Iteration 33910, loss = 0.00728363\n",
      "Iteration 33911, loss = 0.00728349\n",
      "Iteration 33912, loss = 0.00728335\n",
      "Iteration 33913, loss = 0.00728320\n",
      "Iteration 33914, loss = 0.00728306\n",
      "Iteration 33915, loss = 0.00728292\n",
      "Iteration 33916, loss = 0.00728278\n",
      "Iteration 33917, loss = 0.00728264\n",
      "Iteration 33918, loss = 0.00728250\n",
      "Iteration 33919, loss = 0.00728236\n",
      "Iteration 33920, loss = 0.00728222\n",
      "Iteration 33921, loss = 0.00728208\n",
      "Iteration 33922, loss = 0.00728194\n",
      "Iteration 33923, loss = 0.00728180\n",
      "Iteration 33924, loss = 0.00728166\n",
      "Iteration 33925, loss = 0.00728152\n",
      "Iteration 33926, loss = 0.00728138\n",
      "Iteration 33927, loss = 0.00728124\n",
      "Iteration 33928, loss = 0.00728110\n",
      "Iteration 33929, loss = 0.00728096\n",
      "Iteration 33930, loss = 0.00728082\n",
      "Iteration 33931, loss = 0.00728068\n",
      "Iteration 33932, loss = 0.00728054\n",
      "Iteration 33933, loss = 0.00728039\n",
      "Iteration 33934, loss = 0.00728025\n",
      "Iteration 33935, loss = 0.00728011\n",
      "Iteration 33936, loss = 0.00727997\n",
      "Iteration 33937, loss = 0.00727983\n",
      "Iteration 33938, loss = 0.00727969\n",
      "Iteration 33939, loss = 0.00727955\n",
      "Iteration 33940, loss = 0.00727941\n",
      "Iteration 33941, loss = 0.00727927\n",
      "Iteration 33942, loss = 0.00727913\n",
      "Iteration 33943, loss = 0.00727899\n",
      "Iteration 33944, loss = 0.00727885\n",
      "Iteration 33945, loss = 0.00727871\n",
      "Iteration 33946, loss = 0.00727857\n",
      "Iteration 33947, loss = 0.00727843\n",
      "Iteration 33948, loss = 0.00727829\n",
      "Iteration 33949, loss = 0.00727815\n",
      "Iteration 33950, loss = 0.00727801\n",
      "Iteration 33951, loss = 0.00727787\n",
      "Iteration 33952, loss = 0.00727773\n",
      "Iteration 33953, loss = 0.00727759\n",
      "Iteration 33954, loss = 0.00727745\n",
      "Iteration 33955, loss = 0.00727731\n",
      "Iteration 33956, loss = 0.00727717\n",
      "Iteration 33957, loss = 0.00727703\n",
      "Iteration 33958, loss = 0.00727689\n",
      "Iteration 33959, loss = 0.00727675\n",
      "Iteration 33960, loss = 0.00727661\n",
      "Iteration 33961, loss = 0.00727647\n",
      "Iteration 33962, loss = 0.00727633\n",
      "Iteration 33963, loss = 0.00727619\n",
      "Iteration 33964, loss = 0.00727605\n",
      "Iteration 33965, loss = 0.00727591\n",
      "Iteration 33966, loss = 0.00727577\n",
      "Iteration 33967, loss = 0.00727563\n",
      "Iteration 33968, loss = 0.00727549\n",
      "Iteration 33969, loss = 0.00727535\n",
      "Iteration 33970, loss = 0.00727521\n",
      "Iteration 33971, loss = 0.00727507\n",
      "Iteration 33972, loss = 0.00727493\n",
      "Iteration 33973, loss = 0.00727479\n",
      "Iteration 33974, loss = 0.00727465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33975, loss = 0.00727451\n",
      "Iteration 33976, loss = 0.00727437\n",
      "Iteration 33977, loss = 0.00727423\n",
      "Iteration 33978, loss = 0.00727409\n",
      "Iteration 33979, loss = 0.00727395\n",
      "Iteration 33980, loss = 0.00727381\n",
      "Iteration 33981, loss = 0.00727367\n",
      "Iteration 33982, loss = 0.00727353\n",
      "Iteration 33983, loss = 0.00727339\n",
      "Iteration 33984, loss = 0.00727325\n",
      "Iteration 33985, loss = 0.00727311\n",
      "Iteration 33986, loss = 0.00727297\n",
      "Iteration 33987, loss = 0.00727283\n",
      "Iteration 33988, loss = 0.00727269\n",
      "Iteration 33989, loss = 0.00727255\n",
      "Iteration 33990, loss = 0.00727241\n",
      "Iteration 33991, loss = 0.00727227\n",
      "Iteration 33992, loss = 0.00727213\n",
      "Iteration 33993, loss = 0.00727199\n",
      "Iteration 33994, loss = 0.00727185\n",
      "Iteration 33995, loss = 0.00727171\n",
      "Iteration 33996, loss = 0.00727157\n",
      "Iteration 33997, loss = 0.00727143\n",
      "Iteration 33998, loss = 0.00727129\n",
      "Iteration 33999, loss = 0.00727115\n",
      "Iteration 34000, loss = 0.00727101\n",
      "Iteration 34001, loss = 0.00727088\n",
      "Iteration 34002, loss = 0.00727074\n",
      "Iteration 34003, loss = 0.00727060\n",
      "Iteration 34004, loss = 0.00727046\n",
      "Iteration 34005, loss = 0.00727032\n",
      "Iteration 34006, loss = 0.00727018\n",
      "Iteration 34007, loss = 0.00727004\n",
      "Iteration 34008, loss = 0.00726990\n",
      "Iteration 34009, loss = 0.00726976\n",
      "Iteration 34010, loss = 0.00726962\n",
      "Iteration 34011, loss = 0.00726948\n",
      "Iteration 34012, loss = 0.00726934\n",
      "Iteration 34013, loss = 0.00726920\n",
      "Iteration 34014, loss = 0.00726906\n",
      "Iteration 34015, loss = 0.00726892\n",
      "Iteration 34016, loss = 0.00726878\n",
      "Iteration 34017, loss = 0.00726864\n",
      "Iteration 34018, loss = 0.00726850\n",
      "Iteration 34019, loss = 0.00726836\n",
      "Iteration 34020, loss = 0.00726822\n",
      "Iteration 34021, loss = 0.00726809\n",
      "Iteration 34022, loss = 0.00726795\n",
      "Iteration 34023, loss = 0.00726781\n",
      "Iteration 34024, loss = 0.00726767\n",
      "Iteration 34025, loss = 0.00726753\n",
      "Iteration 34026, loss = 0.00726739\n",
      "Iteration 34027, loss = 0.00726725\n",
      "Iteration 34028, loss = 0.00726711\n",
      "Iteration 34029, loss = 0.00726697\n",
      "Iteration 34030, loss = 0.00726683\n",
      "Iteration 34031, loss = 0.00726669\n",
      "Iteration 34032, loss = 0.00726655\n",
      "Iteration 34033, loss = 0.00726641\n",
      "Iteration 34034, loss = 0.00726627\n",
      "Iteration 34035, loss = 0.00726613\n",
      "Iteration 34036, loss = 0.00726600\n",
      "Iteration 34037, loss = 0.00726586\n",
      "Iteration 34038, loss = 0.00726572\n",
      "Iteration 34039, loss = 0.00726558\n",
      "Iteration 34040, loss = 0.00726544\n",
      "Iteration 34041, loss = 0.00726530\n",
      "Iteration 34042, loss = 0.00726516\n",
      "Iteration 34043, loss = 0.00726502\n",
      "Iteration 34044, loss = 0.00726488\n",
      "Iteration 34045, loss = 0.00726474\n",
      "Iteration 34046, loss = 0.00726460\n",
      "Iteration 34047, loss = 0.00726447\n",
      "Iteration 34048, loss = 0.00726433\n",
      "Iteration 34049, loss = 0.00726419\n",
      "Iteration 34050, loss = 0.00726405\n",
      "Iteration 34051, loss = 0.00726391\n",
      "Iteration 34052, loss = 0.00726377\n",
      "Iteration 34053, loss = 0.00726363\n",
      "Iteration 34054, loss = 0.00726349\n",
      "Iteration 34055, loss = 0.00726335\n",
      "Iteration 34056, loss = 0.00726321\n",
      "Iteration 34057, loss = 0.00726307\n",
      "Iteration 34058, loss = 0.00726294\n",
      "Iteration 34059, loss = 0.00726280\n",
      "Iteration 34060, loss = 0.00726266\n",
      "Iteration 34061, loss = 0.00726252\n",
      "Iteration 34062, loss = 0.00726238\n",
      "Iteration 34063, loss = 0.00726224\n",
      "Iteration 34064, loss = 0.00726210\n",
      "Iteration 34065, loss = 0.00726196\n",
      "Iteration 34066, loss = 0.00726182\n",
      "Iteration 34067, loss = 0.00726169\n",
      "Iteration 34068, loss = 0.00726155\n",
      "Iteration 34069, loss = 0.00726141\n",
      "Iteration 34070, loss = 0.00726127\n",
      "Iteration 34071, loss = 0.00726113\n",
      "Iteration 34072, loss = 0.00726099\n",
      "Iteration 34073, loss = 0.00726085\n",
      "Iteration 34074, loss = 0.00726071\n",
      "Iteration 34075, loss = 0.00726057\n",
      "Iteration 34076, loss = 0.00726044\n",
      "Iteration 34077, loss = 0.00726030\n",
      "Iteration 34078, loss = 0.00726016\n",
      "Iteration 34079, loss = 0.00726002\n",
      "Iteration 34080, loss = 0.00725988\n",
      "Iteration 34081, loss = 0.00725974\n",
      "Iteration 34082, loss = 0.00725960\n",
      "Iteration 34083, loss = 0.00725946\n",
      "Iteration 34084, loss = 0.00725933\n",
      "Iteration 34085, loss = 0.00725919\n",
      "Iteration 34086, loss = 0.00725905\n",
      "Iteration 34087, loss = 0.00725891\n",
      "Iteration 34088, loss = 0.00725877\n",
      "Iteration 34089, loss = 0.00725863\n",
      "Iteration 34090, loss = 0.00725849\n",
      "Iteration 34091, loss = 0.00725836\n",
      "Iteration 34092, loss = 0.00725822\n",
      "Iteration 34093, loss = 0.00725808\n",
      "Iteration 34094, loss = 0.00725794\n",
      "Iteration 34095, loss = 0.00725780\n",
      "Iteration 34096, loss = 0.00725766\n",
      "Iteration 34097, loss = 0.00725752\n",
      "Iteration 34098, loss = 0.00725739\n",
      "Iteration 34099, loss = 0.00725725\n",
      "Iteration 34100, loss = 0.00725711\n",
      "Iteration 34101, loss = 0.00725697\n",
      "Iteration 34102, loss = 0.00725683\n",
      "Iteration 34103, loss = 0.00725669\n",
      "Iteration 34104, loss = 0.00725655\n",
      "Iteration 34105, loss = 0.00725642\n",
      "Iteration 34106, loss = 0.00725628\n",
      "Iteration 34107, loss = 0.00725614\n",
      "Iteration 34108, loss = 0.00725600\n",
      "Iteration 34109, loss = 0.00725586\n",
      "Iteration 34110, loss = 0.00725572\n",
      "Iteration 34111, loss = 0.00725559\n",
      "Iteration 34112, loss = 0.00725545\n",
      "Iteration 34113, loss = 0.00725531\n",
      "Iteration 34114, loss = 0.00725517\n",
      "Iteration 34115, loss = 0.00725503\n",
      "Iteration 34116, loss = 0.00725489\n",
      "Iteration 34117, loss = 0.00725476\n",
      "Iteration 34118, loss = 0.00725462\n",
      "Iteration 34119, loss = 0.00725448\n",
      "Iteration 34120, loss = 0.00725434\n",
      "Iteration 34121, loss = 0.00725420\n",
      "Iteration 34122, loss = 0.00725406\n",
      "Iteration 34123, loss = 0.00725393\n",
      "Iteration 34124, loss = 0.00725379\n",
      "Iteration 34125, loss = 0.00725365\n",
      "Iteration 34126, loss = 0.00725351\n",
      "Iteration 34127, loss = 0.00725337\n",
      "Iteration 34128, loss = 0.00725323\n",
      "Iteration 34129, loss = 0.00725310\n",
      "Iteration 34130, loss = 0.00725296\n",
      "Iteration 34131, loss = 0.00725282\n",
      "Iteration 34132, loss = 0.00725268\n",
      "Iteration 34133, loss = 0.00725254\n",
      "Iteration 34134, loss = 0.00725241\n",
      "Iteration 34135, loss = 0.00725227\n",
      "Iteration 34136, loss = 0.00725213\n",
      "Iteration 34137, loss = 0.00725199\n",
      "Iteration 34138, loss = 0.00725185\n",
      "Iteration 34139, loss = 0.00725172\n",
      "Iteration 34140, loss = 0.00725158\n",
      "Iteration 34141, loss = 0.00725144\n",
      "Iteration 34142, loss = 0.00725130\n",
      "Iteration 34143, loss = 0.00725116\n",
      "Iteration 34144, loss = 0.00725103\n",
      "Iteration 34145, loss = 0.00725089\n",
      "Iteration 34146, loss = 0.00725075\n",
      "Iteration 34147, loss = 0.00725061\n",
      "Iteration 34148, loss = 0.00725047\n",
      "Iteration 34149, loss = 0.00725034\n",
      "Iteration 34150, loss = 0.00725020\n",
      "Iteration 34151, loss = 0.00725006\n",
      "Iteration 34152, loss = 0.00724992\n",
      "Iteration 34153, loss = 0.00724978\n",
      "Iteration 34154, loss = 0.00724965\n",
      "Iteration 34155, loss = 0.00724951\n",
      "Iteration 34156, loss = 0.00724937\n",
      "Iteration 34157, loss = 0.00724923\n",
      "Iteration 34158, loss = 0.00724909\n",
      "Iteration 34159, loss = 0.00724896\n",
      "Iteration 34160, loss = 0.00724882\n",
      "Iteration 34161, loss = 0.00724868\n",
      "Iteration 34162, loss = 0.00724854\n",
      "Iteration 34163, loss = 0.00724840\n",
      "Iteration 34164, loss = 0.00724827\n",
      "Iteration 34165, loss = 0.00724813\n",
      "Iteration 34166, loss = 0.00724799\n",
      "Iteration 34167, loss = 0.00724785\n",
      "Iteration 34168, loss = 0.00724772\n",
      "Iteration 34169, loss = 0.00724758\n",
      "Iteration 34170, loss = 0.00724744\n",
      "Iteration 34171, loss = 0.00724730\n",
      "Iteration 34172, loss = 0.00724717\n",
      "Iteration 34173, loss = 0.00724703\n",
      "Iteration 34174, loss = 0.00724689\n",
      "Iteration 34175, loss = 0.00724675\n",
      "Iteration 34176, loss = 0.00724661\n",
      "Iteration 34177, loss = 0.00724648\n",
      "Iteration 34178, loss = 0.00724634\n",
      "Iteration 34179, loss = 0.00724620\n",
      "Iteration 34180, loss = 0.00724606\n",
      "Iteration 34181, loss = 0.00724593\n",
      "Iteration 34182, loss = 0.00724579\n",
      "Iteration 34183, loss = 0.00724565\n",
      "Iteration 34184, loss = 0.00724551\n",
      "Iteration 34185, loss = 0.00724538\n",
      "Iteration 34186, loss = 0.00724524\n",
      "Iteration 34187, loss = 0.00724510\n",
      "Iteration 34188, loss = 0.00724496\n",
      "Iteration 34189, loss = 0.00724483\n",
      "Iteration 34190, loss = 0.00724469\n",
      "Iteration 34191, loss = 0.00724455\n",
      "Iteration 34192, loss = 0.00724441\n",
      "Iteration 34193, loss = 0.00724428\n",
      "Iteration 34194, loss = 0.00724414\n",
      "Iteration 34195, loss = 0.00724400\n",
      "Iteration 34196, loss = 0.00724386\n",
      "Iteration 34197, loss = 0.00724373\n",
      "Iteration 34198, loss = 0.00724359\n",
      "Iteration 34199, loss = 0.00724345\n",
      "Iteration 34200, loss = 0.00724331\n",
      "Iteration 34201, loss = 0.00724318\n",
      "Iteration 34202, loss = 0.00724304\n",
      "Iteration 34203, loss = 0.00724290\n",
      "Iteration 34204, loss = 0.00724276\n",
      "Iteration 34205, loss = 0.00724263\n",
      "Iteration 34206, loss = 0.00724249\n",
      "Iteration 34207, loss = 0.00724235\n",
      "Iteration 34208, loss = 0.00724221\n",
      "Iteration 34209, loss = 0.00724208\n",
      "Iteration 34210, loss = 0.00724194\n",
      "Iteration 34211, loss = 0.00724180\n",
      "Iteration 34212, loss = 0.00724167\n",
      "Iteration 34213, loss = 0.00724153\n",
      "Iteration 34214, loss = 0.00724139\n",
      "Iteration 34215, loss = 0.00724125\n",
      "Iteration 34216, loss = 0.00724112\n",
      "Iteration 34217, loss = 0.00724098\n",
      "Iteration 34218, loss = 0.00724084\n",
      "Iteration 34219, loss = 0.00724071\n",
      "Iteration 34220, loss = 0.00724057\n",
      "Iteration 34221, loss = 0.00724043\n",
      "Iteration 34222, loss = 0.00724029\n",
      "Iteration 34223, loss = 0.00724016\n",
      "Iteration 34224, loss = 0.00724002\n",
      "Iteration 34225, loss = 0.00723988\n",
      "Iteration 34226, loss = 0.00723975\n",
      "Iteration 34227, loss = 0.00723961\n",
      "Iteration 34228, loss = 0.00723947\n",
      "Iteration 34229, loss = 0.00723933\n",
      "Iteration 34230, loss = 0.00723920\n",
      "Iteration 34231, loss = 0.00723906\n",
      "Iteration 34232, loss = 0.00723892\n",
      "Iteration 34233, loss = 0.00723879\n",
      "Iteration 34234, loss = 0.00723865\n",
      "Iteration 34235, loss = 0.00723851\n",
      "Iteration 34236, loss = 0.00723837\n",
      "Iteration 34237, loss = 0.00723824\n",
      "Iteration 34238, loss = 0.00723810\n",
      "Iteration 34239, loss = 0.00723796\n",
      "Iteration 34240, loss = 0.00723783\n",
      "Iteration 34241, loss = 0.00723769\n",
      "Iteration 34242, loss = 0.00723755\n",
      "Iteration 34243, loss = 0.00723742\n",
      "Iteration 34244, loss = 0.00723728\n",
      "Iteration 34245, loss = 0.00723714\n",
      "Iteration 34246, loss = 0.00723701\n",
      "Iteration 34247, loss = 0.00723687\n",
      "Iteration 34248, loss = 0.00723673\n",
      "Iteration 34249, loss = 0.00723659\n",
      "Iteration 34250, loss = 0.00723646\n",
      "Iteration 34251, loss = 0.00723632\n",
      "Iteration 34252, loss = 0.00723618\n",
      "Iteration 34253, loss = 0.00723605\n",
      "Iteration 34254, loss = 0.00723591\n",
      "Iteration 34255, loss = 0.00723577\n",
      "Iteration 34256, loss = 0.00723564\n",
      "Iteration 34257, loss = 0.00723550\n",
      "Iteration 34258, loss = 0.00723536\n",
      "Iteration 34259, loss = 0.00723523\n",
      "Iteration 34260, loss = 0.00723509\n",
      "Iteration 34261, loss = 0.00723495\n",
      "Iteration 34262, loss = 0.00723482\n",
      "Iteration 34263, loss = 0.00723468\n",
      "Iteration 34264, loss = 0.00723454\n",
      "Iteration 34265, loss = 0.00723441\n",
      "Iteration 34266, loss = 0.00723427\n",
      "Iteration 34267, loss = 0.00723413\n",
      "Iteration 34268, loss = 0.00723400\n",
      "Iteration 34269, loss = 0.00723386\n",
      "Iteration 34270, loss = 0.00723372\n",
      "Iteration 34271, loss = 0.00723359\n",
      "Iteration 34272, loss = 0.00723345\n",
      "Iteration 34273, loss = 0.00723331\n",
      "Iteration 34274, loss = 0.00723318\n",
      "Iteration 34275, loss = 0.00723304\n",
      "Iteration 34276, loss = 0.00723290\n",
      "Iteration 34277, loss = 0.00723277\n",
      "Iteration 34278, loss = 0.00723263\n",
      "Iteration 34279, loss = 0.00723249\n",
      "Iteration 34280, loss = 0.00723236\n",
      "Iteration 34281, loss = 0.00723222\n",
      "Iteration 34282, loss = 0.00723208\n",
      "Iteration 34283, loss = 0.00723195\n",
      "Iteration 34284, loss = 0.00723181\n",
      "Iteration 34285, loss = 0.00723167\n",
      "Iteration 34286, loss = 0.00723154\n",
      "Iteration 34287, loss = 0.00723140\n",
      "Iteration 34288, loss = 0.00723127\n",
      "Iteration 34289, loss = 0.00723113\n",
      "Iteration 34290, loss = 0.00723099\n",
      "Iteration 34291, loss = 0.00723086\n",
      "Iteration 34292, loss = 0.00723072\n",
      "Iteration 34293, loss = 0.00723058\n",
      "Iteration 34294, loss = 0.00723045\n",
      "Iteration 34295, loss = 0.00723031\n",
      "Iteration 34296, loss = 0.00723017\n",
      "Iteration 34297, loss = 0.00723004\n",
      "Iteration 34298, loss = 0.00722990\n",
      "Iteration 34299, loss = 0.00722977\n",
      "Iteration 34300, loss = 0.00722963\n",
      "Iteration 34301, loss = 0.00722949\n",
      "Iteration 34302, loss = 0.00722936\n",
      "Iteration 34303, loss = 0.00722922\n",
      "Iteration 34304, loss = 0.00722908\n",
      "Iteration 34305, loss = 0.00722895\n",
      "Iteration 34306, loss = 0.00722881\n",
      "Iteration 34307, loss = 0.00722867\n",
      "Iteration 34308, loss = 0.00722854\n",
      "Iteration 34309, loss = 0.00722840\n",
      "Iteration 34310, loss = 0.00722827\n",
      "Iteration 34311, loss = 0.00722813\n",
      "Iteration 34312, loss = 0.00722799\n",
      "Iteration 34313, loss = 0.00722786\n",
      "Iteration 34314, loss = 0.00722772\n",
      "Iteration 34315, loss = 0.00722759\n",
      "Iteration 34316, loss = 0.00722745\n",
      "Iteration 34317, loss = 0.00722731\n",
      "Iteration 34318, loss = 0.00722718\n",
      "Iteration 34319, loss = 0.00722704\n",
      "Iteration 34320, loss = 0.00722690\n",
      "Iteration 34321, loss = 0.00722677\n",
      "Iteration 34322, loss = 0.00722663\n",
      "Iteration 34323, loss = 0.00722650\n",
      "Iteration 34324, loss = 0.00722636\n",
      "Iteration 34325, loss = 0.00722622\n",
      "Iteration 34326, loss = 0.00722609\n",
      "Iteration 34327, loss = 0.00722595\n",
      "Iteration 34328, loss = 0.00722582\n",
      "Iteration 34329, loss = 0.00722568\n",
      "Iteration 34330, loss = 0.00722554\n",
      "Iteration 34331, loss = 0.00722541\n",
      "Iteration 34332, loss = 0.00722527\n",
      "Iteration 34333, loss = 0.00722514\n",
      "Iteration 34334, loss = 0.00722500\n",
      "Iteration 34335, loss = 0.00722486\n",
      "Iteration 34336, loss = 0.00722473\n",
      "Iteration 34337, loss = 0.00722459\n",
      "Iteration 34338, loss = 0.00722446\n",
      "Iteration 34339, loss = 0.00722432\n",
      "Iteration 34340, loss = 0.00722419\n",
      "Iteration 34341, loss = 0.00722405\n",
      "Iteration 34342, loss = 0.00722391\n",
      "Iteration 34343, loss = 0.00722378\n",
      "Iteration 34344, loss = 0.00722364\n",
      "Iteration 34345, loss = 0.00722351\n",
      "Iteration 34346, loss = 0.00722337\n",
      "Iteration 34347, loss = 0.00722323\n",
      "Iteration 34348, loss = 0.00722310\n",
      "Iteration 34349, loss = 0.00722296\n",
      "Iteration 34350, loss = 0.00722283\n",
      "Iteration 34351, loss = 0.00722269\n",
      "Iteration 34352, loss = 0.00722256\n",
      "Iteration 34353, loss = 0.00722242\n",
      "Iteration 34354, loss = 0.00722228\n",
      "Iteration 34355, loss = 0.00722215\n",
      "Iteration 34356, loss = 0.00722201\n",
      "Iteration 34357, loss = 0.00722188\n",
      "Iteration 34358, loss = 0.00722174\n",
      "Iteration 34359, loss = 0.00722161\n",
      "Iteration 34360, loss = 0.00722147\n",
      "Iteration 34361, loss = 0.00722133\n",
      "Iteration 34362, loss = 0.00722120\n",
      "Iteration 34363, loss = 0.00722106\n",
      "Iteration 34364, loss = 0.00722093\n",
      "Iteration 34365, loss = 0.00722079\n",
      "Iteration 34366, loss = 0.00722066\n",
      "Iteration 34367, loss = 0.00722052\n",
      "Iteration 34368, loss = 0.00722039\n",
      "Iteration 34369, loss = 0.00722025\n",
      "Iteration 34370, loss = 0.00722011\n",
      "Iteration 34371, loss = 0.00721998\n",
      "Iteration 34372, loss = 0.00721984\n",
      "Iteration 34373, loss = 0.00721971\n",
      "Iteration 34374, loss = 0.00721957\n",
      "Iteration 34375, loss = 0.00721944\n",
      "Iteration 34376, loss = 0.00721930\n",
      "Iteration 34377, loss = 0.00721917\n",
      "Iteration 34378, loss = 0.00721903\n",
      "Iteration 34379, loss = 0.00721889\n",
      "Iteration 34380, loss = 0.00721876\n",
      "Iteration 34381, loss = 0.00721862\n",
      "Iteration 34382, loss = 0.00721849\n",
      "Iteration 34383, loss = 0.00721835\n",
      "Iteration 34384, loss = 0.00721822\n",
      "Iteration 34385, loss = 0.00721808\n",
      "Iteration 34386, loss = 0.00721795\n",
      "Iteration 34387, loss = 0.00721781\n",
      "Iteration 34388, loss = 0.00721768\n",
      "Iteration 34389, loss = 0.00721754\n",
      "Iteration 34390, loss = 0.00721741\n",
      "Iteration 34391, loss = 0.00721727\n",
      "Iteration 34392, loss = 0.00721713\n",
      "Iteration 34393, loss = 0.00721700\n",
      "Iteration 34394, loss = 0.00721686\n",
      "Iteration 34395, loss = 0.00721673\n",
      "Iteration 34396, loss = 0.00721659\n",
      "Iteration 34397, loss = 0.00721646\n",
      "Iteration 34398, loss = 0.00721632\n",
      "Iteration 34399, loss = 0.00721619\n",
      "Iteration 34400, loss = 0.00721605\n",
      "Iteration 34401, loss = 0.00721592\n",
      "Iteration 34402, loss = 0.00721578\n",
      "Iteration 34403, loss = 0.00721565\n",
      "Iteration 34404, loss = 0.00721551\n",
      "Iteration 34405, loss = 0.00721538\n",
      "Iteration 34406, loss = 0.00721524\n",
      "Iteration 34407, loss = 0.00721511\n",
      "Iteration 34408, loss = 0.00721497\n",
      "Iteration 34409, loss = 0.00721484\n",
      "Iteration 34410, loss = 0.00721470\n",
      "Iteration 34411, loss = 0.00721457\n",
      "Iteration 34412, loss = 0.00721443\n",
      "Iteration 34413, loss = 0.00721430\n",
      "Iteration 34414, loss = 0.00721416\n",
      "Iteration 34415, loss = 0.00721403\n",
      "Iteration 34416, loss = 0.00721389\n",
      "Iteration 34417, loss = 0.00721376\n",
      "Iteration 34418, loss = 0.00721362\n",
      "Iteration 34419, loss = 0.00721349\n",
      "Iteration 34420, loss = 0.00721335\n",
      "Iteration 34421, loss = 0.00721322\n",
      "Iteration 34422, loss = 0.00721308\n",
      "Iteration 34423, loss = 0.00721295\n",
      "Iteration 34424, loss = 0.00721281\n",
      "Iteration 34425, loss = 0.00721268\n",
      "Iteration 34426, loss = 0.00721254\n",
      "Iteration 34427, loss = 0.00721241\n",
      "Iteration 34428, loss = 0.00721227\n",
      "Iteration 34429, loss = 0.00721214\n",
      "Iteration 34430, loss = 0.00721200\n",
      "Iteration 34431, loss = 0.00721187\n",
      "Iteration 34432, loss = 0.00721173\n",
      "Iteration 34433, loss = 0.00721160\n",
      "Iteration 34434, loss = 0.00721146\n",
      "Iteration 34435, loss = 0.00721133\n",
      "Iteration 34436, loss = 0.00721119\n",
      "Iteration 34437, loss = 0.00721106\n",
      "Iteration 34438, loss = 0.00721092\n",
      "Iteration 34439, loss = 0.00721079\n",
      "Iteration 34440, loss = 0.00721065\n",
      "Iteration 34441, loss = 0.00721052\n",
      "Iteration 34442, loss = 0.00721038\n",
      "Iteration 34443, loss = 0.00721025\n",
      "Iteration 34444, loss = 0.00721011\n",
      "Iteration 34445, loss = 0.00720998\n",
      "Iteration 34446, loss = 0.00720984\n",
      "Iteration 34447, loss = 0.00720971\n",
      "Iteration 34448, loss = 0.00720957\n",
      "Iteration 34449, loss = 0.00720944\n",
      "Iteration 34450, loss = 0.00720931\n",
      "Iteration 34451, loss = 0.00720917\n",
      "Iteration 34452, loss = 0.00720904\n",
      "Iteration 34453, loss = 0.00720890\n",
      "Iteration 34454, loss = 0.00720877\n",
      "Iteration 34455, loss = 0.00720863\n",
      "Iteration 34456, loss = 0.00720850\n",
      "Iteration 34457, loss = 0.00720836\n",
      "Iteration 34458, loss = 0.00720823\n",
      "Iteration 34459, loss = 0.00720809\n",
      "Iteration 34460, loss = 0.00720796\n",
      "Iteration 34461, loss = 0.00720782\n",
      "Iteration 34462, loss = 0.00720769\n",
      "Iteration 34463, loss = 0.00720756\n",
      "Iteration 34464, loss = 0.00720742\n",
      "Iteration 34465, loss = 0.00720729\n",
      "Iteration 34466, loss = 0.00720715\n",
      "Iteration 34467, loss = 0.00720702\n",
      "Iteration 34468, loss = 0.00720688\n",
      "Iteration 34469, loss = 0.00720675\n",
      "Iteration 34470, loss = 0.00720661\n",
      "Iteration 34471, loss = 0.00720648\n",
      "Iteration 34472, loss = 0.00720634\n",
      "Iteration 34473, loss = 0.00720621\n",
      "Iteration 34474, loss = 0.00720608\n",
      "Iteration 34475, loss = 0.00720594\n",
      "Iteration 34476, loss = 0.00720581\n",
      "Iteration 34477, loss = 0.00720567\n",
      "Iteration 34478, loss = 0.00720554\n",
      "Iteration 34479, loss = 0.00720540\n",
      "Iteration 34480, loss = 0.00720527\n",
      "Iteration 34481, loss = 0.00720514\n",
      "Iteration 34482, loss = 0.00720500\n",
      "Iteration 34483, loss = 0.00720487\n",
      "Iteration 34484, loss = 0.00720473\n",
      "Iteration 34485, loss = 0.00720460\n",
      "Iteration 34486, loss = 0.00720446\n",
      "Iteration 34487, loss = 0.00720433\n",
      "Iteration 34488, loss = 0.00720419\n",
      "Iteration 34489, loss = 0.00720406\n",
      "Iteration 34490, loss = 0.00720393\n",
      "Iteration 34491, loss = 0.00720379\n",
      "Iteration 34492, loss = 0.00720366\n",
      "Iteration 34493, loss = 0.00720352\n",
      "Iteration 34494, loss = 0.00720339\n",
      "Iteration 34495, loss = 0.00720326\n",
      "Iteration 34496, loss = 0.00720312\n",
      "Iteration 34497, loss = 0.00720299\n",
      "Iteration 34498, loss = 0.00720285\n",
      "Iteration 34499, loss = 0.00720272\n",
      "Iteration 34500, loss = 0.00720258\n",
      "Iteration 34501, loss = 0.00720245\n",
      "Iteration 34502, loss = 0.00720232\n",
      "Iteration 34503, loss = 0.00720218\n",
      "Iteration 34504, loss = 0.00720205\n",
      "Iteration 34505, loss = 0.00720191\n",
      "Iteration 34506, loss = 0.00720178\n",
      "Iteration 34507, loss = 0.00720165\n",
      "Iteration 34508, loss = 0.00720151\n",
      "Iteration 34509, loss = 0.00720138\n",
      "Iteration 34510, loss = 0.00720124\n",
      "Iteration 34511, loss = 0.00720111\n",
      "Iteration 34512, loss = 0.00720098\n",
      "Iteration 34513, loss = 0.00720084\n",
      "Iteration 34514, loss = 0.00720071\n",
      "Iteration 34515, loss = 0.00720057\n",
      "Iteration 34516, loss = 0.00720044\n",
      "Iteration 34517, loss = 0.00720031\n",
      "Iteration 34518, loss = 0.00720017\n",
      "Iteration 34519, loss = 0.00720004\n",
      "Iteration 34520, loss = 0.00719990\n",
      "Iteration 34521, loss = 0.00719977\n",
      "Iteration 34522, loss = 0.00719964\n",
      "Iteration 34523, loss = 0.00719950\n",
      "Iteration 34524, loss = 0.00719937\n",
      "Iteration 34525, loss = 0.00719923\n",
      "Iteration 34526, loss = 0.00719910\n",
      "Iteration 34527, loss = 0.00719897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34528, loss = 0.00719883\n",
      "Iteration 34529, loss = 0.00719870\n",
      "Iteration 34530, loss = 0.00719856\n",
      "Iteration 34531, loss = 0.00719843\n",
      "Iteration 34532, loss = 0.00719830\n",
      "Iteration 34533, loss = 0.00719816\n",
      "Iteration 34534, loss = 0.00719803\n",
      "Iteration 34535, loss = 0.00719790\n",
      "Iteration 34536, loss = 0.00719776\n",
      "Iteration 34537, loss = 0.00719763\n",
      "Iteration 34538, loss = 0.00719749\n",
      "Iteration 34539, loss = 0.00719736\n",
      "Iteration 34540, loss = 0.00719723\n",
      "Iteration 34541, loss = 0.00719709\n",
      "Iteration 34542, loss = 0.00719696\n",
      "Iteration 34543, loss = 0.00719683\n",
      "Iteration 34544, loss = 0.00719669\n",
      "Iteration 34545, loss = 0.00719656\n",
      "Iteration 34546, loss = 0.00719642\n",
      "Iteration 34547, loss = 0.00719629\n",
      "Iteration 34548, loss = 0.00719616\n",
      "Iteration 34549, loss = 0.00719602\n",
      "Iteration 34550, loss = 0.00719589\n",
      "Iteration 34551, loss = 0.00719576\n",
      "Iteration 34552, loss = 0.00719562\n",
      "Iteration 34553, loss = 0.00719549\n",
      "Iteration 34554, loss = 0.00719536\n",
      "Iteration 34555, loss = 0.00719522\n",
      "Iteration 34556, loss = 0.00719509\n",
      "Iteration 34557, loss = 0.00719495\n",
      "Iteration 34558, loss = 0.00719482\n",
      "Iteration 34559, loss = 0.00719469\n",
      "Iteration 34560, loss = 0.00719455\n",
      "Iteration 34561, loss = 0.00719442\n",
      "Iteration 34562, loss = 0.00719429\n",
      "Iteration 34563, loss = 0.00719415\n",
      "Iteration 34564, loss = 0.00719402\n",
      "Iteration 34565, loss = 0.00719389\n",
      "Iteration 34566, loss = 0.00719375\n",
      "Iteration 34567, loss = 0.00719362\n",
      "Iteration 34568, loss = 0.00719349\n",
      "Iteration 34569, loss = 0.00719335\n",
      "Iteration 34570, loss = 0.00719322\n",
      "Iteration 34571, loss = 0.00719309\n",
      "Iteration 34572, loss = 0.00719295\n",
      "Iteration 34573, loss = 0.00719282\n",
      "Iteration 34574, loss = 0.00719269\n",
      "Iteration 34575, loss = 0.00719255\n",
      "Iteration 34576, loss = 0.00719242\n",
      "Iteration 34577, loss = 0.00719229\n",
      "Iteration 34578, loss = 0.00719215\n",
      "Iteration 34579, loss = 0.00719202\n",
      "Iteration 34580, loss = 0.00719189\n",
      "Iteration 34581, loss = 0.00719175\n",
      "Iteration 34582, loss = 0.00719162\n",
      "Iteration 34583, loss = 0.00719149\n",
      "Iteration 34584, loss = 0.00719135\n",
      "Iteration 34585, loss = 0.00719122\n",
      "Iteration 34586, loss = 0.00719109\n",
      "Iteration 34587, loss = 0.00719095\n",
      "Iteration 34588, loss = 0.00719082\n",
      "Iteration 34589, loss = 0.00719069\n",
      "Iteration 34590, loss = 0.00719055\n",
      "Iteration 34591, loss = 0.00719042\n",
      "Iteration 34592, loss = 0.00719029\n",
      "Iteration 34593, loss = 0.00719015\n",
      "Iteration 34594, loss = 0.00719002\n",
      "Iteration 34595, loss = 0.00718989\n",
      "Iteration 34596, loss = 0.00718975\n",
      "Iteration 34597, loss = 0.00718962\n",
      "Iteration 34598, loss = 0.00718949\n",
      "Iteration 34599, loss = 0.00718936\n",
      "Iteration 34600, loss = 0.00718922\n",
      "Iteration 34601, loss = 0.00718909\n",
      "Iteration 34602, loss = 0.00718896\n",
      "Iteration 34603, loss = 0.00718882\n",
      "Iteration 34604, loss = 0.00718869\n",
      "Iteration 34605, loss = 0.00718856\n",
      "Iteration 34606, loss = 0.00718842\n",
      "Iteration 34607, loss = 0.00718829\n",
      "Iteration 34608, loss = 0.00718816\n",
      "Iteration 34609, loss = 0.00718802\n",
      "Iteration 34610, loss = 0.00718789\n",
      "Iteration 34611, loss = 0.00718776\n",
      "Iteration 34612, loss = 0.00718763\n",
      "Iteration 34613, loss = 0.00718749\n",
      "Iteration 34614, loss = 0.00718736\n",
      "Iteration 34615, loss = 0.00718723\n",
      "Iteration 34616, loss = 0.00718709\n",
      "Iteration 34617, loss = 0.00718696\n",
      "Iteration 34618, loss = 0.00718683\n",
      "Iteration 34619, loss = 0.00718670\n",
      "Iteration 34620, loss = 0.00718656\n",
      "Iteration 34621, loss = 0.00718643\n",
      "Iteration 34622, loss = 0.00718630\n",
      "Iteration 34623, loss = 0.00718616\n",
      "Iteration 34624, loss = 0.00718603\n",
      "Iteration 34625, loss = 0.00718590\n",
      "Iteration 34626, loss = 0.00718577\n",
      "Iteration 34627, loss = 0.00718563\n",
      "Iteration 34628, loss = 0.00718550\n",
      "Iteration 34629, loss = 0.00718537\n",
      "Iteration 34630, loss = 0.00718523\n",
      "Iteration 34631, loss = 0.00718510\n",
      "Iteration 34632, loss = 0.00718497\n",
      "Iteration 34633, loss = 0.00718484\n",
      "Iteration 34634, loss = 0.00718470\n",
      "Iteration 34635, loss = 0.00718457\n",
      "Iteration 34636, loss = 0.00718444\n",
      "Iteration 34637, loss = 0.00718431\n",
      "Iteration 34638, loss = 0.00718417\n",
      "Iteration 34639, loss = 0.00718404\n",
      "Iteration 34640, loss = 0.00718391\n",
      "Iteration 34641, loss = 0.00718377\n",
      "Iteration 34642, loss = 0.00718364\n",
      "Iteration 34643, loss = 0.00718351\n",
      "Iteration 34644, loss = 0.00718338\n",
      "Iteration 34645, loss = 0.00718324\n",
      "Iteration 34646, loss = 0.00718311\n",
      "Iteration 34647, loss = 0.00718298\n",
      "Iteration 34648, loss = 0.00718285\n",
      "Iteration 34649, loss = 0.00718271\n",
      "Iteration 34650, loss = 0.00718258\n",
      "Iteration 34651, loss = 0.00718245\n",
      "Iteration 34652, loss = 0.00718232\n",
      "Iteration 34653, loss = 0.00718218\n",
      "Iteration 34654, loss = 0.00718205\n",
      "Iteration 34655, loss = 0.00718192\n",
      "Iteration 34656, loss = 0.00718179\n",
      "Iteration 34657, loss = 0.00718165\n",
      "Iteration 34658, loss = 0.00718152\n",
      "Iteration 34659, loss = 0.00718139\n",
      "Iteration 34660, loss = 0.00718126\n",
      "Iteration 34661, loss = 0.00718112\n",
      "Iteration 34662, loss = 0.00718099\n",
      "Iteration 34663, loss = 0.00718086\n",
      "Iteration 34664, loss = 0.00718073\n",
      "Iteration 34665, loss = 0.00718059\n",
      "Iteration 34666, loss = 0.00718046\n",
      "Iteration 34667, loss = 0.00718033\n",
      "Iteration 34668, loss = 0.00718020\n",
      "Iteration 34669, loss = 0.00718006\n",
      "Iteration 34670, loss = 0.00717993\n",
      "Iteration 34671, loss = 0.00717980\n",
      "Iteration 34672, loss = 0.00717967\n",
      "Iteration 34673, loss = 0.00717953\n",
      "Iteration 34674, loss = 0.00717940\n",
      "Iteration 34675, loss = 0.00717927\n",
      "Iteration 34676, loss = 0.00717914\n",
      "Iteration 34677, loss = 0.00717901\n",
      "Iteration 34678, loss = 0.00717887\n",
      "Iteration 34679, loss = 0.00717874\n",
      "Iteration 34680, loss = 0.00717861\n",
      "Iteration 34681, loss = 0.00717848\n",
      "Iteration 34682, loss = 0.00717834\n",
      "Iteration 34683, loss = 0.00717821\n",
      "Iteration 34684, loss = 0.00717808\n",
      "Iteration 34685, loss = 0.00717795\n",
      "Iteration 34686, loss = 0.00717782\n",
      "Iteration 34687, loss = 0.00717768\n",
      "Iteration 34688, loss = 0.00717755\n",
      "Iteration 34689, loss = 0.00717742\n",
      "Iteration 34690, loss = 0.00717729\n",
      "Iteration 34691, loss = 0.00717715\n",
      "Iteration 34692, loss = 0.00717702\n",
      "Iteration 34693, loss = 0.00717689\n",
      "Iteration 34694, loss = 0.00717676\n",
      "Iteration 34695, loss = 0.00717663\n",
      "Iteration 34696, loss = 0.00717649\n",
      "Iteration 34697, loss = 0.00717636\n",
      "Iteration 34698, loss = 0.00717623\n",
      "Iteration 34699, loss = 0.00717610\n",
      "Iteration 34700, loss = 0.00717597\n",
      "Iteration 34701, loss = 0.00717583\n",
      "Iteration 34702, loss = 0.00717570\n",
      "Iteration 34703, loss = 0.00717557\n",
      "Iteration 34704, loss = 0.00717544\n",
      "Iteration 34705, loss = 0.00717531\n",
      "Iteration 34706, loss = 0.00717517\n",
      "Iteration 34707, loss = 0.00717504\n",
      "Iteration 34708, loss = 0.00717491\n",
      "Iteration 34709, loss = 0.00717478\n",
      "Iteration 34710, loss = 0.00717465\n",
      "Iteration 34711, loss = 0.00717451\n",
      "Iteration 34712, loss = 0.00717438\n",
      "Iteration 34713, loss = 0.00717425\n",
      "Iteration 34714, loss = 0.00717412\n",
      "Iteration 34715, loss = 0.00717399\n",
      "Iteration 34716, loss = 0.00717385\n",
      "Iteration 34717, loss = 0.00717372\n",
      "Iteration 34718, loss = 0.00717359\n",
      "Iteration 34719, loss = 0.00717346\n",
      "Iteration 34720, loss = 0.00717333\n",
      "Iteration 34721, loss = 0.00717319\n",
      "Iteration 34722, loss = 0.00717306\n",
      "Iteration 34723, loss = 0.00717293\n",
      "Iteration 34724, loss = 0.00717280\n",
      "Iteration 34725, loss = 0.00717267\n",
      "Iteration 34726, loss = 0.00717254\n",
      "Iteration 34727, loss = 0.00717240\n",
      "Iteration 34728, loss = 0.00717227\n",
      "Iteration 34729, loss = 0.00717214\n",
      "Iteration 34730, loss = 0.00717201\n",
      "Iteration 34731, loss = 0.00717188\n",
      "Iteration 34732, loss = 0.00717175\n",
      "Iteration 34733, loss = 0.00717161\n",
      "Iteration 34734, loss = 0.00717148\n",
      "Iteration 34735, loss = 0.00717135\n",
      "Iteration 34736, loss = 0.00717122\n",
      "Iteration 34737, loss = 0.00717109\n",
      "Iteration 34738, loss = 0.00717096\n",
      "Iteration 34739, loss = 0.00717082\n",
      "Iteration 34740, loss = 0.00717069\n",
      "Iteration 34741, loss = 0.00717056\n",
      "Iteration 34742, loss = 0.00717043\n",
      "Iteration 34743, loss = 0.00717030\n",
      "Iteration 34744, loss = 0.00717017\n",
      "Iteration 34745, loss = 0.00717003\n",
      "Iteration 34746, loss = 0.00716990\n",
      "Iteration 34747, loss = 0.00716977\n",
      "Iteration 34748, loss = 0.00716964\n",
      "Iteration 34749, loss = 0.00716951\n",
      "Iteration 34750, loss = 0.00716938\n",
      "Iteration 34751, loss = 0.00716924\n",
      "Iteration 34752, loss = 0.00716911\n",
      "Iteration 34753, loss = 0.00716898\n",
      "Iteration 34754, loss = 0.00716885\n",
      "Iteration 34755, loss = 0.00716872\n",
      "Iteration 34756, loss = 0.00716859\n",
      "Iteration 34757, loss = 0.00716846\n",
      "Iteration 34758, loss = 0.00716832\n",
      "Iteration 34759, loss = 0.00716819\n",
      "Iteration 34760, loss = 0.00716806\n",
      "Iteration 34761, loss = 0.00716793\n",
      "Iteration 34762, loss = 0.00716780\n",
      "Iteration 34763, loss = 0.00716767\n",
      "Iteration 34764, loss = 0.00716754\n",
      "Iteration 34765, loss = 0.00716740\n",
      "Iteration 34766, loss = 0.00716727\n",
      "Iteration 34767, loss = 0.00716714\n",
      "Iteration 34768, loss = 0.00716701\n",
      "Iteration 34769, loss = 0.00716688\n",
      "Iteration 34770, loss = 0.00716675\n",
      "Iteration 34771, loss = 0.00716662\n",
      "Iteration 34772, loss = 0.00716649\n",
      "Iteration 34773, loss = 0.00716635\n",
      "Iteration 34774, loss = 0.00716622\n",
      "Iteration 34775, loss = 0.00716609\n",
      "Iteration 34776, loss = 0.00716596\n",
      "Iteration 34777, loss = 0.00716583\n",
      "Iteration 34778, loss = 0.00716570\n",
      "Iteration 34779, loss = 0.00716557\n",
      "Iteration 34780, loss = 0.00716544\n",
      "Iteration 34781, loss = 0.00716530\n",
      "Iteration 34782, loss = 0.00716517\n",
      "Iteration 34783, loss = 0.00716504\n",
      "Iteration 34784, loss = 0.00716491\n",
      "Iteration 34785, loss = 0.00716478\n",
      "Iteration 34786, loss = 0.00716465\n",
      "Iteration 34787, loss = 0.00716452\n",
      "Iteration 34788, loss = 0.00716439\n",
      "Iteration 34789, loss = 0.00716425\n",
      "Iteration 34790, loss = 0.00716412\n",
      "Iteration 34791, loss = 0.00716399\n",
      "Iteration 34792, loss = 0.00716386\n",
      "Iteration 34793, loss = 0.00716373\n",
      "Iteration 34794, loss = 0.00716360\n",
      "Iteration 34795, loss = 0.00716347\n",
      "Iteration 34796, loss = 0.00716334\n",
      "Iteration 34797, loss = 0.00716321\n",
      "Iteration 34798, loss = 0.00716308\n",
      "Iteration 34799, loss = 0.00716294\n",
      "Iteration 34800, loss = 0.00716281\n",
      "Iteration 34801, loss = 0.00716268\n",
      "Iteration 34802, loss = 0.00716255\n",
      "Iteration 34803, loss = 0.00716242\n",
      "Iteration 34804, loss = 0.00716229\n",
      "Iteration 34805, loss = 0.00716216\n",
      "Iteration 34806, loss = 0.00716203\n",
      "Iteration 34807, loss = 0.00716190\n",
      "Iteration 34808, loss = 0.00716177\n",
      "Iteration 34809, loss = 0.00716163\n",
      "Iteration 34810, loss = 0.00716150\n",
      "Iteration 34811, loss = 0.00716137\n",
      "Iteration 34812, loss = 0.00716124\n",
      "Iteration 34813, loss = 0.00716111\n",
      "Iteration 34814, loss = 0.00716098\n",
      "Iteration 34815, loss = 0.00716085\n",
      "Iteration 34816, loss = 0.00716072\n",
      "Iteration 34817, loss = 0.00716059\n",
      "Iteration 34818, loss = 0.00716046\n",
      "Iteration 34819, loss = 0.00716033\n",
      "Iteration 34820, loss = 0.00716019\n",
      "Iteration 34821, loss = 0.00716006\n",
      "Iteration 34822, loss = 0.00715993\n",
      "Iteration 34823, loss = 0.00715980\n",
      "Iteration 34824, loss = 0.00715967\n",
      "Iteration 34825, loss = 0.00715954\n",
      "Iteration 34826, loss = 0.00715941\n",
      "Iteration 34827, loss = 0.00715928\n",
      "Iteration 34828, loss = 0.00715915\n",
      "Iteration 34829, loss = 0.00715902\n",
      "Iteration 34830, loss = 0.00715889\n",
      "Iteration 34831, loss = 0.00715876\n",
      "Iteration 34832, loss = 0.00715863\n",
      "Iteration 34833, loss = 0.00715850\n",
      "Iteration 34834, loss = 0.00715836\n",
      "Iteration 34835, loss = 0.00715823\n",
      "Iteration 34836, loss = 0.00715810\n",
      "Iteration 34837, loss = 0.00715797\n",
      "Iteration 34838, loss = 0.00715784\n",
      "Iteration 34839, loss = 0.00715771\n",
      "Iteration 34840, loss = 0.00715758\n",
      "Iteration 34841, loss = 0.00715745\n",
      "Iteration 34842, loss = 0.00715732\n",
      "Iteration 34843, loss = 0.00715719\n",
      "Iteration 34844, loss = 0.00715706\n",
      "Iteration 34845, loss = 0.00715693\n",
      "Iteration 34846, loss = 0.00715680\n",
      "Iteration 34847, loss = 0.00715667\n",
      "Iteration 34848, loss = 0.00715654\n",
      "Iteration 34849, loss = 0.00715641\n",
      "Iteration 34850, loss = 0.00715628\n",
      "Iteration 34851, loss = 0.00715615\n",
      "Iteration 34852, loss = 0.00715601\n",
      "Iteration 34853, loss = 0.00715588\n",
      "Iteration 34854, loss = 0.00715575\n",
      "Iteration 34855, loss = 0.00715562\n",
      "Iteration 34856, loss = 0.00715549\n",
      "Iteration 34857, loss = 0.00715536\n",
      "Iteration 34858, loss = 0.00715523\n",
      "Iteration 34859, loss = 0.00715510\n",
      "Iteration 34860, loss = 0.00715497\n",
      "Iteration 34861, loss = 0.00715484\n",
      "Iteration 34862, loss = 0.00715471\n",
      "Iteration 34863, loss = 0.00715458\n",
      "Iteration 34864, loss = 0.00715445\n",
      "Iteration 34865, loss = 0.00715432\n",
      "Iteration 34866, loss = 0.00715419\n",
      "Iteration 34867, loss = 0.00715406\n",
      "Iteration 34868, loss = 0.00715393\n",
      "Iteration 34869, loss = 0.00715380\n",
      "Iteration 34870, loss = 0.00715367\n",
      "Iteration 34871, loss = 0.00715354\n",
      "Iteration 34872, loss = 0.00715341\n",
      "Iteration 34873, loss = 0.00715328\n",
      "Iteration 34874, loss = 0.00715315\n",
      "Iteration 34875, loss = 0.00715302\n",
      "Iteration 34876, loss = 0.00715289\n",
      "Iteration 34877, loss = 0.00715276\n",
      "Iteration 34878, loss = 0.00715263\n",
      "Iteration 34879, loss = 0.00715250\n",
      "Iteration 34880, loss = 0.00715237\n",
      "Iteration 34881, loss = 0.00715224\n",
      "Iteration 34882, loss = 0.00715210\n",
      "Iteration 34883, loss = 0.00715197\n",
      "Iteration 34884, loss = 0.00715184\n",
      "Iteration 34885, loss = 0.00715171\n",
      "Iteration 34886, loss = 0.00715158\n",
      "Iteration 34887, loss = 0.00715145\n",
      "Iteration 34888, loss = 0.00715132\n",
      "Iteration 34889, loss = 0.00715119\n",
      "Iteration 34890, loss = 0.00715106\n",
      "Iteration 34891, loss = 0.00715093\n",
      "Iteration 34892, loss = 0.00715080\n",
      "Iteration 34893, loss = 0.00715067\n",
      "Iteration 34894, loss = 0.00715054\n",
      "Iteration 34895, loss = 0.00715041\n",
      "Iteration 34896, loss = 0.00715028\n",
      "Iteration 34897, loss = 0.00715015\n",
      "Iteration 34898, loss = 0.00715002\n",
      "Iteration 34899, loss = 0.00714989\n",
      "Iteration 34900, loss = 0.00714976\n",
      "Iteration 34901, loss = 0.00714963\n",
      "Iteration 34902, loss = 0.00714950\n",
      "Iteration 34903, loss = 0.00714937\n",
      "Iteration 34904, loss = 0.00714924\n",
      "Iteration 34905, loss = 0.00714911\n",
      "Iteration 34906, loss = 0.00714898\n",
      "Iteration 34907, loss = 0.00714885\n",
      "Iteration 34908, loss = 0.00714872\n",
      "Iteration 34909, loss = 0.00714859\n",
      "Iteration 34910, loss = 0.00714846\n",
      "Iteration 34911, loss = 0.00714833\n",
      "Iteration 34912, loss = 0.00714820\n",
      "Iteration 34913, loss = 0.00714807\n",
      "Iteration 34914, loss = 0.00714794\n",
      "Iteration 34915, loss = 0.00714782\n",
      "Iteration 34916, loss = 0.00714769\n",
      "Iteration 34917, loss = 0.00714756\n",
      "Iteration 34918, loss = 0.00714743\n",
      "Iteration 34919, loss = 0.00714730\n",
      "Iteration 34920, loss = 0.00714717\n",
      "Iteration 34921, loss = 0.00714704\n",
      "Iteration 34922, loss = 0.00714691\n",
      "Iteration 34923, loss = 0.00714678\n",
      "Iteration 34924, loss = 0.00714665\n",
      "Iteration 34925, loss = 0.00714652\n",
      "Iteration 34926, loss = 0.00714639\n",
      "Iteration 34927, loss = 0.00714626\n",
      "Iteration 34928, loss = 0.00714613\n",
      "Iteration 34929, loss = 0.00714600\n",
      "Iteration 34930, loss = 0.00714587\n",
      "Iteration 34931, loss = 0.00714574\n",
      "Iteration 34932, loss = 0.00714561\n",
      "Iteration 34933, loss = 0.00714548\n",
      "Iteration 34934, loss = 0.00714535\n",
      "Iteration 34935, loss = 0.00714522\n",
      "Iteration 34936, loss = 0.00714509\n",
      "Iteration 34937, loss = 0.00714496\n",
      "Iteration 34938, loss = 0.00714483\n",
      "Iteration 34939, loss = 0.00714470\n",
      "Iteration 34940, loss = 0.00714457\n",
      "Iteration 34941, loss = 0.00714444\n",
      "Iteration 34942, loss = 0.00714431\n",
      "Iteration 34943, loss = 0.00714418\n",
      "Iteration 34944, loss = 0.00714405\n",
      "Iteration 34945, loss = 0.00714392\n",
      "Iteration 34946, loss = 0.00714380\n",
      "Iteration 34947, loss = 0.00714367\n",
      "Iteration 34948, loss = 0.00714354\n",
      "Iteration 34949, loss = 0.00714341\n",
      "Iteration 34950, loss = 0.00714328\n",
      "Iteration 34951, loss = 0.00714315\n",
      "Iteration 34952, loss = 0.00714302\n",
      "Iteration 34953, loss = 0.00714289\n",
      "Iteration 34954, loss = 0.00714276\n",
      "Iteration 34955, loss = 0.00714263\n",
      "Iteration 34956, loss = 0.00714250\n",
      "Iteration 34957, loss = 0.00714237\n",
      "Iteration 34958, loss = 0.00714224\n",
      "Iteration 34959, loss = 0.00714211\n",
      "Iteration 34960, loss = 0.00714198\n",
      "Iteration 34961, loss = 0.00714185\n",
      "Iteration 34962, loss = 0.00714172\n",
      "Iteration 34963, loss = 0.00714159\n",
      "Iteration 34964, loss = 0.00714147\n",
      "Iteration 34965, loss = 0.00714134\n",
      "Iteration 34966, loss = 0.00714121\n",
      "Iteration 34967, loss = 0.00714108\n",
      "Iteration 34968, loss = 0.00714095\n",
      "Iteration 34969, loss = 0.00714082\n",
      "Iteration 34970, loss = 0.00714069\n",
      "Iteration 34971, loss = 0.00714056\n",
      "Iteration 34972, loss = 0.00714043\n",
      "Iteration 34973, loss = 0.00714030\n",
      "Iteration 34974, loss = 0.00714017\n",
      "Iteration 34975, loss = 0.00714004\n",
      "Iteration 34976, loss = 0.00713991\n",
      "Iteration 34977, loss = 0.00713979\n",
      "Iteration 34978, loss = 0.00713966\n",
      "Iteration 34979, loss = 0.00713953\n",
      "Iteration 34980, loss = 0.00713940\n",
      "Iteration 34981, loss = 0.00713927\n",
      "Iteration 34982, loss = 0.00713914\n",
      "Iteration 34983, loss = 0.00713901\n",
      "Iteration 34984, loss = 0.00713888\n",
      "Iteration 34985, loss = 0.00713875\n",
      "Iteration 34986, loss = 0.00713862\n",
      "Iteration 34987, loss = 0.00713849\n",
      "Iteration 34988, loss = 0.00713836\n",
      "Iteration 34989, loss = 0.00713824\n",
      "Iteration 34990, loss = 0.00713811\n",
      "Iteration 34991, loss = 0.00713798\n",
      "Iteration 34992, loss = 0.00713785\n",
      "Iteration 34993, loss = 0.00713772\n",
      "Iteration 34994, loss = 0.00713759\n",
      "Iteration 34995, loss = 0.00713746\n",
      "Iteration 34996, loss = 0.00713733\n",
      "Iteration 34997, loss = 0.00713720\n",
      "Iteration 34998, loss = 0.00713707\n",
      "Iteration 34999, loss = 0.00713695\n",
      "Iteration 35000, loss = 0.00713682\n",
      "Iteration 35001, loss = 0.00713669\n",
      "Iteration 35002, loss = 0.00713656\n",
      "Iteration 35003, loss = 0.00713643\n",
      "Iteration 35004, loss = 0.00713630\n",
      "Iteration 35005, loss = 0.00713617\n",
      "Iteration 35006, loss = 0.00713604\n",
      "Iteration 35007, loss = 0.00713591\n",
      "Iteration 35008, loss = 0.00713578\n",
      "Iteration 35009, loss = 0.00713566\n",
      "Iteration 35010, loss = 0.00713553\n",
      "Iteration 35011, loss = 0.00713540\n",
      "Iteration 35012, loss = 0.00713527\n",
      "Iteration 35013, loss = 0.00713514\n",
      "Iteration 35014, loss = 0.00713501\n",
      "Iteration 35015, loss = 0.00713488\n",
      "Iteration 35016, loss = 0.00713475\n",
      "Iteration 35017, loss = 0.00713463\n",
      "Iteration 35018, loss = 0.00713450\n",
      "Iteration 35019, loss = 0.00713437\n",
      "Iteration 35020, loss = 0.00713424\n",
      "Iteration 35021, loss = 0.00713411\n",
      "Iteration 35022, loss = 0.00713398\n",
      "Iteration 35023, loss = 0.00713385\n",
      "Iteration 35024, loss = 0.00713372\n",
      "Iteration 35025, loss = 0.00713360\n",
      "Iteration 35026, loss = 0.00713347\n",
      "Iteration 35027, loss = 0.00713334\n",
      "Iteration 35028, loss = 0.00713321\n",
      "Iteration 35029, loss = 0.00713308\n",
      "Iteration 35030, loss = 0.00713295\n",
      "Iteration 35031, loss = 0.00713282\n",
      "Iteration 35032, loss = 0.00713269\n",
      "Iteration 35033, loss = 0.00713257\n",
      "Iteration 35034, loss = 0.00713244\n",
      "Iteration 35035, loss = 0.00713231\n",
      "Iteration 35036, loss = 0.00713218\n",
      "Iteration 35037, loss = 0.00713205\n",
      "Iteration 35038, loss = 0.00713192\n",
      "Iteration 35039, loss = 0.00713179\n",
      "Iteration 35040, loss = 0.00713167\n",
      "Iteration 35041, loss = 0.00713154\n",
      "Iteration 35042, loss = 0.00713141\n",
      "Iteration 35043, loss = 0.00713128\n",
      "Iteration 35044, loss = 0.00713115\n",
      "Iteration 35045, loss = 0.00713102\n",
      "Iteration 35046, loss = 0.00713089\n",
      "Iteration 35047, loss = 0.00713077\n",
      "Iteration 35048, loss = 0.00713064\n",
      "Iteration 35049, loss = 0.00713051\n",
      "Iteration 35050, loss = 0.00713038\n",
      "Iteration 35051, loss = 0.00713025\n",
      "Iteration 35052, loss = 0.00713012\n",
      "Iteration 35053, loss = 0.00712999\n",
      "Iteration 35054, loss = 0.00712987\n",
      "Iteration 35055, loss = 0.00712974\n",
      "Iteration 35056, loss = 0.00712961\n",
      "Iteration 35057, loss = 0.00712948\n",
      "Iteration 35058, loss = 0.00712935\n",
      "Iteration 35059, loss = 0.00712922\n",
      "Iteration 35060, loss = 0.00712910\n",
      "Iteration 35061, loss = 0.00712897\n",
      "Iteration 35062, loss = 0.00712884\n",
      "Iteration 35063, loss = 0.00712871\n",
      "Iteration 35064, loss = 0.00712858\n",
      "Iteration 35065, loss = 0.00712845\n",
      "Iteration 35066, loss = 0.00712833\n",
      "Iteration 35067, loss = 0.00712820\n",
      "Iteration 35068, loss = 0.00712807\n",
      "Iteration 35069, loss = 0.00712794\n",
      "Iteration 35070, loss = 0.00712781\n",
      "Iteration 35071, loss = 0.00712768\n",
      "Iteration 35072, loss = 0.00712756\n",
      "Iteration 35073, loss = 0.00712743\n",
      "Iteration 35074, loss = 0.00712730\n",
      "Iteration 35075, loss = 0.00712717\n",
      "Iteration 35076, loss = 0.00712704\n",
      "Iteration 35077, loss = 0.00712692\n",
      "Iteration 35078, loss = 0.00712679\n",
      "Iteration 35079, loss = 0.00712666\n",
      "Iteration 35080, loss = 0.00712653\n",
      "Iteration 35081, loss = 0.00712640\n",
      "Iteration 35082, loss = 0.00712627\n",
      "Iteration 35083, loss = 0.00712615\n",
      "Iteration 35084, loss = 0.00712602\n",
      "Iteration 35085, loss = 0.00712589\n",
      "Iteration 35086, loss = 0.00712576\n",
      "Iteration 35087, loss = 0.00712563\n",
      "Iteration 35088, loss = 0.00712551\n",
      "Iteration 35089, loss = 0.00712538\n",
      "Iteration 35090, loss = 0.00712525\n",
      "Iteration 35091, loss = 0.00712512\n",
      "Iteration 35092, loss = 0.00712499\n",
      "Iteration 35093, loss = 0.00712487\n",
      "Iteration 35094, loss = 0.00712474\n",
      "Iteration 35095, loss = 0.00712461\n",
      "Iteration 35096, loss = 0.00712448\n",
      "Iteration 35097, loss = 0.00712435\n",
      "Iteration 35098, loss = 0.00712423\n",
      "Iteration 35099, loss = 0.00712410\n",
      "Iteration 35100, loss = 0.00712397\n",
      "Iteration 35101, loss = 0.00712384\n",
      "Iteration 35102, loss = 0.00712371\n",
      "Iteration 35103, loss = 0.00712359\n",
      "Iteration 35104, loss = 0.00712346\n",
      "Iteration 35105, loss = 0.00712333\n",
      "Iteration 35106, loss = 0.00712320\n",
      "Iteration 35107, loss = 0.00712307\n",
      "Iteration 35108, loss = 0.00712295\n",
      "Iteration 35109, loss = 0.00712282\n",
      "Iteration 35110, loss = 0.00712269\n",
      "Iteration 35111, loss = 0.00712256\n",
      "Iteration 35112, loss = 0.00712243\n",
      "Iteration 35113, loss = 0.00712231\n",
      "Iteration 35114, loss = 0.00712218\n",
      "Iteration 35115, loss = 0.00712205\n",
      "Iteration 35116, loss = 0.00712192\n",
      "Iteration 35117, loss = 0.00712180\n",
      "Iteration 35118, loss = 0.00712167\n",
      "Iteration 35119, loss = 0.00712154\n",
      "Iteration 35120, loss = 0.00712141\n",
      "Iteration 35121, loss = 0.00712128\n",
      "Iteration 35122, loss = 0.00712116\n",
      "Iteration 35123, loss = 0.00712103\n",
      "Iteration 35124, loss = 0.00712090\n",
      "Iteration 35125, loss = 0.00712077\n",
      "Iteration 35126, loss = 0.00712065\n",
      "Iteration 35127, loss = 0.00712052\n",
      "Iteration 35128, loss = 0.00712039\n",
      "Iteration 35129, loss = 0.00712026\n",
      "Iteration 35130, loss = 0.00712013\n",
      "Iteration 35131, loss = 0.00712001\n",
      "Iteration 35132, loss = 0.00711988\n",
      "Iteration 35133, loss = 0.00711975\n",
      "Iteration 35134, loss = 0.00711962\n",
      "Iteration 35135, loss = 0.00711950\n",
      "Iteration 35136, loss = 0.00711937\n",
      "Iteration 35137, loss = 0.00711924\n",
      "Iteration 35138, loss = 0.00711911\n",
      "Iteration 35139, loss = 0.00711899\n",
      "Iteration 35140, loss = 0.00711886\n",
      "Iteration 35141, loss = 0.00711873\n",
      "Iteration 35142, loss = 0.00711860\n",
      "Iteration 35143, loss = 0.00711848\n",
      "Iteration 35144, loss = 0.00711835\n",
      "Iteration 35145, loss = 0.00711822\n",
      "Iteration 35146, loss = 0.00711809\n",
      "Iteration 35147, loss = 0.00711797\n",
      "Iteration 35148, loss = 0.00711784\n",
      "Iteration 35149, loss = 0.00711771\n",
      "Iteration 35150, loss = 0.00711758\n",
      "Iteration 35151, loss = 0.00711746\n",
      "Iteration 35152, loss = 0.00711733\n",
      "Iteration 35153, loss = 0.00711720\n",
      "Iteration 35154, loss = 0.00711707\n",
      "Iteration 35155, loss = 0.00711695\n",
      "Iteration 35156, loss = 0.00711682\n",
      "Iteration 35157, loss = 0.00711669\n",
      "Iteration 35158, loss = 0.00711656\n",
      "Iteration 35159, loss = 0.00711644\n",
      "Iteration 35160, loss = 0.00711631\n",
      "Iteration 35161, loss = 0.00711618\n",
      "Iteration 35162, loss = 0.00711605\n",
      "Iteration 35163, loss = 0.00711593\n",
      "Iteration 35164, loss = 0.00711580\n",
      "Iteration 35165, loss = 0.00711567\n",
      "Iteration 35166, loss = 0.00711555\n",
      "Iteration 35167, loss = 0.00711542\n",
      "Iteration 35168, loss = 0.00711529\n",
      "Iteration 35169, loss = 0.00711516\n",
      "Iteration 35170, loss = 0.00711504\n",
      "Iteration 35171, loss = 0.00711491\n",
      "Iteration 35172, loss = 0.00711478\n",
      "Iteration 35173, loss = 0.00711465\n",
      "Iteration 35174, loss = 0.00711453\n",
      "Iteration 35175, loss = 0.00711440\n",
      "Iteration 35176, loss = 0.00711427\n",
      "Iteration 35177, loss = 0.00711415\n",
      "Iteration 35178, loss = 0.00711402\n",
      "Iteration 35179, loss = 0.00711389\n",
      "Iteration 35180, loss = 0.00711376\n",
      "Iteration 35181, loss = 0.00711364\n",
      "Iteration 35182, loss = 0.00711351\n",
      "Iteration 35183, loss = 0.00711338\n",
      "Iteration 35184, loss = 0.00711326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35185, loss = 0.00711313\n",
      "Iteration 35186, loss = 0.00711300\n",
      "Iteration 35187, loss = 0.00711287\n",
      "Iteration 35188, loss = 0.00711275\n",
      "Iteration 35189, loss = 0.00711262\n",
      "Iteration 35190, loss = 0.00711249\n",
      "Iteration 35191, loss = 0.00711237\n",
      "Iteration 35192, loss = 0.00711224\n",
      "Iteration 35193, loss = 0.00711211\n",
      "Iteration 35194, loss = 0.00711198\n",
      "Iteration 35195, loss = 0.00711186\n",
      "Iteration 35196, loss = 0.00711173\n",
      "Iteration 35197, loss = 0.00711160\n",
      "Iteration 35198, loss = 0.00711148\n",
      "Iteration 35199, loss = 0.00711135\n",
      "Iteration 35200, loss = 0.00711122\n",
      "Iteration 35201, loss = 0.00711110\n",
      "Iteration 35202, loss = 0.00711097\n",
      "Iteration 35203, loss = 0.00711084\n",
      "Iteration 35204, loss = 0.00711071\n",
      "Iteration 35205, loss = 0.00711059\n",
      "Iteration 35206, loss = 0.00711046\n",
      "Iteration 35207, loss = 0.00711033\n",
      "Iteration 35208, loss = 0.00711021\n",
      "Iteration 35209, loss = 0.00711008\n",
      "Iteration 35210, loss = 0.00710995\n",
      "Iteration 35211, loss = 0.00710983\n",
      "Iteration 35212, loss = 0.00710970\n",
      "Iteration 35213, loss = 0.00710957\n",
      "Iteration 35214, loss = 0.00710945\n",
      "Iteration 35215, loss = 0.00710932\n",
      "Iteration 35216, loss = 0.00710919\n",
      "Iteration 35217, loss = 0.00710907\n",
      "Iteration 35218, loss = 0.00710894\n",
      "Iteration 35219, loss = 0.00710881\n",
      "Iteration 35220, loss = 0.00710869\n",
      "Iteration 35221, loss = 0.00710856\n",
      "Iteration 35222, loss = 0.00710843\n",
      "Iteration 35223, loss = 0.00710830\n",
      "Iteration 35224, loss = 0.00710818\n",
      "Iteration 35225, loss = 0.00710805\n",
      "Iteration 35226, loss = 0.00710792\n",
      "Iteration 35227, loss = 0.00710780\n",
      "Iteration 35228, loss = 0.00710767\n",
      "Iteration 35229, loss = 0.00710754\n",
      "Iteration 35230, loss = 0.00710742\n",
      "Iteration 35231, loss = 0.00710729\n",
      "Iteration 35232, loss = 0.00710716\n",
      "Iteration 35233, loss = 0.00710704\n",
      "Iteration 35234, loss = 0.00710691\n",
      "Iteration 35235, loss = 0.00710678\n",
      "Iteration 35236, loss = 0.00710666\n",
      "Iteration 35237, loss = 0.00710653\n",
      "Iteration 35238, loss = 0.00710640\n",
      "Iteration 35239, loss = 0.00710628\n",
      "Iteration 35240, loss = 0.00710615\n",
      "Iteration 35241, loss = 0.00710602\n",
      "Iteration 35242, loss = 0.00710590\n",
      "Iteration 35243, loss = 0.00710577\n",
      "Iteration 35244, loss = 0.00710565\n",
      "Iteration 35245, loss = 0.00710552\n",
      "Iteration 35246, loss = 0.00710539\n",
      "Iteration 35247, loss = 0.00710527\n",
      "Iteration 35248, loss = 0.00710514\n",
      "Iteration 35249, loss = 0.00710501\n",
      "Iteration 35250, loss = 0.00710489\n",
      "Iteration 35251, loss = 0.00710476\n",
      "Iteration 35252, loss = 0.00710463\n",
      "Iteration 35253, loss = 0.00710451\n",
      "Iteration 35254, loss = 0.00710438\n",
      "Iteration 35255, loss = 0.00710425\n",
      "Iteration 35256, loss = 0.00710413\n",
      "Iteration 35257, loss = 0.00710400\n",
      "Iteration 35258, loss = 0.00710387\n",
      "Iteration 35259, loss = 0.00710375\n",
      "Iteration 35260, loss = 0.00710362\n",
      "Iteration 35261, loss = 0.00710350\n",
      "Iteration 35262, loss = 0.00710337\n",
      "Iteration 35263, loss = 0.00710324\n",
      "Iteration 35264, loss = 0.00710312\n",
      "Iteration 35265, loss = 0.00710299\n",
      "Iteration 35266, loss = 0.00710286\n",
      "Iteration 35267, loss = 0.00710274\n",
      "Iteration 35268, loss = 0.00710261\n",
      "Iteration 35269, loss = 0.00710248\n",
      "Iteration 35270, loss = 0.00710236\n",
      "Iteration 35271, loss = 0.00710223\n",
      "Iteration 35272, loss = 0.00710211\n",
      "Iteration 35273, loss = 0.00710198\n",
      "Iteration 35274, loss = 0.00710185\n",
      "Iteration 35275, loss = 0.00710173\n",
      "Iteration 35276, loss = 0.00710160\n",
      "Iteration 35277, loss = 0.00710147\n",
      "Iteration 35278, loss = 0.00710135\n",
      "Iteration 35279, loss = 0.00710122\n",
      "Iteration 35280, loss = 0.00710110\n",
      "Iteration 35281, loss = 0.00710097\n",
      "Iteration 35282, loss = 0.00710084\n",
      "Iteration 35283, loss = 0.00710072\n",
      "Iteration 35284, loss = 0.00710059\n",
      "Iteration 35285, loss = 0.00710047\n",
      "Iteration 35286, loss = 0.00710034\n",
      "Iteration 35287, loss = 0.00710021\n",
      "Iteration 35288, loss = 0.00710009\n",
      "Iteration 35289, loss = 0.00709996\n",
      "Iteration 35290, loss = 0.00709983\n",
      "Iteration 35291, loss = 0.00709971\n",
      "Iteration 35292, loss = 0.00709958\n",
      "Iteration 35293, loss = 0.00709946\n",
      "Iteration 35294, loss = 0.00709933\n",
      "Iteration 35295, loss = 0.00709920\n",
      "Iteration 35296, loss = 0.00709908\n",
      "Iteration 35297, loss = 0.00709895\n",
      "Iteration 35298, loss = 0.00709883\n",
      "Iteration 35299, loss = 0.00709870\n",
      "Iteration 35300, loss = 0.00709857\n",
      "Iteration 35301, loss = 0.00709845\n",
      "Iteration 35302, loss = 0.00709832\n",
      "Iteration 35303, loss = 0.00709820\n",
      "Iteration 35304, loss = 0.00709807\n",
      "Iteration 35305, loss = 0.00709794\n",
      "Iteration 35306, loss = 0.00709782\n",
      "Iteration 35307, loss = 0.00709769\n",
      "Iteration 35308, loss = 0.00709757\n",
      "Iteration 35309, loss = 0.00709744\n",
      "Iteration 35310, loss = 0.00709732\n",
      "Iteration 35311, loss = 0.00709719\n",
      "Iteration 35312, loss = 0.00709706\n",
      "Iteration 35313, loss = 0.00709694\n",
      "Iteration 35314, loss = 0.00709681\n",
      "Iteration 35315, loss = 0.00709669\n",
      "Iteration 35316, loss = 0.00709656\n",
      "Iteration 35317, loss = 0.00709643\n",
      "Iteration 35318, loss = 0.00709631\n",
      "Iteration 35319, loss = 0.00709618\n",
      "Iteration 35320, loss = 0.00709606\n",
      "Iteration 35321, loss = 0.00709593\n",
      "Iteration 35322, loss = 0.00709581\n",
      "Iteration 35323, loss = 0.00709568\n",
      "Iteration 35324, loss = 0.00709555\n",
      "Iteration 35325, loss = 0.00709543\n",
      "Iteration 35326, loss = 0.00709530\n",
      "Iteration 35327, loss = 0.00709518\n",
      "Iteration 35328, loss = 0.00709505\n",
      "Iteration 35329, loss = 0.00709492\n",
      "Iteration 35330, loss = 0.00709480\n",
      "Iteration 35331, loss = 0.00709467\n",
      "Iteration 35332, loss = 0.00709455\n",
      "Iteration 35333, loss = 0.00709442\n",
      "Iteration 35334, loss = 0.00709430\n",
      "Iteration 35335, loss = 0.00709417\n",
      "Iteration 35336, loss = 0.00709405\n",
      "Iteration 35337, loss = 0.00709392\n",
      "Iteration 35338, loss = 0.00709379\n",
      "Iteration 35339, loss = 0.00709367\n",
      "Iteration 35340, loss = 0.00709354\n",
      "Iteration 35341, loss = 0.00709342\n",
      "Iteration 35342, loss = 0.00709329\n",
      "Iteration 35343, loss = 0.00709317\n",
      "Iteration 35344, loss = 0.00709304\n",
      "Iteration 35345, loss = 0.00709291\n",
      "Iteration 35346, loss = 0.00709279\n",
      "Iteration 35347, loss = 0.00709266\n",
      "Iteration 35348, loss = 0.00709254\n",
      "Iteration 35349, loss = 0.00709241\n",
      "Iteration 35350, loss = 0.00709229\n",
      "Iteration 35351, loss = 0.00709216\n",
      "Iteration 35352, loss = 0.00709204\n",
      "Iteration 35353, loss = 0.00709191\n",
      "Iteration 35354, loss = 0.00709179\n",
      "Iteration 35355, loss = 0.00709166\n",
      "Iteration 35356, loss = 0.00709153\n",
      "Iteration 35357, loss = 0.00709141\n",
      "Iteration 35358, loss = 0.00709128\n",
      "Iteration 35359, loss = 0.00709116\n",
      "Iteration 35360, loss = 0.00709103\n",
      "Iteration 35361, loss = 0.00709091\n",
      "Iteration 35362, loss = 0.00709078\n",
      "Iteration 35363, loss = 0.00709066\n",
      "Iteration 35364, loss = 0.00709053\n",
      "Iteration 35365, loss = 0.00709041\n",
      "Iteration 35366, loss = 0.00709028\n",
      "Iteration 35367, loss = 0.00709015\n",
      "Iteration 35368, loss = 0.00709003\n",
      "Iteration 35369, loss = 0.00708990\n",
      "Iteration 35370, loss = 0.00708978\n",
      "Iteration 35371, loss = 0.00708965\n",
      "Iteration 35372, loss = 0.00708953\n",
      "Iteration 35373, loss = 0.00708940\n",
      "Iteration 35374, loss = 0.00708928\n",
      "Iteration 35375, loss = 0.00708915\n",
      "Iteration 35376, loss = 0.00708903\n",
      "Iteration 35377, loss = 0.00708890\n",
      "Iteration 35378, loss = 0.00708878\n",
      "Iteration 35379, loss = 0.00708865\n",
      "Iteration 35380, loss = 0.00708853\n",
      "Iteration 35381, loss = 0.00708840\n",
      "Iteration 35382, loss = 0.00708828\n",
      "Iteration 35383, loss = 0.00708815\n",
      "Iteration 35384, loss = 0.00708803\n",
      "Iteration 35385, loss = 0.00708790\n",
      "Iteration 35386, loss = 0.00708778\n",
      "Iteration 35387, loss = 0.00708765\n",
      "Iteration 35388, loss = 0.00708752\n",
      "Iteration 35389, loss = 0.00708740\n",
      "Iteration 35390, loss = 0.00708727\n",
      "Iteration 35391, loss = 0.00708715\n",
      "Iteration 35392, loss = 0.00708702\n",
      "Iteration 35393, loss = 0.00708690\n",
      "Iteration 35394, loss = 0.00708677\n",
      "Iteration 35395, loss = 0.00708665\n",
      "Iteration 35396, loss = 0.00708652\n",
      "Iteration 35397, loss = 0.00708640\n",
      "Iteration 35398, loss = 0.00708627\n",
      "Iteration 35399, loss = 0.00708615\n",
      "Iteration 35400, loss = 0.00708602\n",
      "Iteration 35401, loss = 0.00708590\n",
      "Iteration 35402, loss = 0.00708577\n",
      "Iteration 35403, loss = 0.00708565\n",
      "Iteration 35404, loss = 0.00708552\n",
      "Iteration 35405, loss = 0.00708540\n",
      "Iteration 35406, loss = 0.00708527\n",
      "Iteration 35407, loss = 0.00708515\n",
      "Iteration 35408, loss = 0.00708502\n",
      "Iteration 35409, loss = 0.00708490\n",
      "Iteration 35410, loss = 0.00708477\n",
      "Iteration 35411, loss = 0.00708465\n",
      "Iteration 35412, loss = 0.00708452\n",
      "Iteration 35413, loss = 0.00708440\n",
      "Iteration 35414, loss = 0.00708427\n",
      "Iteration 35415, loss = 0.00708415\n",
      "Iteration 35416, loss = 0.00708402\n",
      "Iteration 35417, loss = 0.00708390\n",
      "Iteration 35418, loss = 0.00708378\n",
      "Iteration 35419, loss = 0.00708365\n",
      "Iteration 35420, loss = 0.00708353\n",
      "Iteration 35421, loss = 0.00708340\n",
      "Iteration 35422, loss = 0.00708328\n",
      "Iteration 35423, loss = 0.00708315\n",
      "Iteration 35424, loss = 0.00708303\n",
      "Iteration 35425, loss = 0.00708290\n",
      "Iteration 35426, loss = 0.00708278\n",
      "Iteration 35427, loss = 0.00708265\n",
      "Iteration 35428, loss = 0.00708253\n",
      "Iteration 35429, loss = 0.00708240\n",
      "Iteration 35430, loss = 0.00708228\n",
      "Iteration 35431, loss = 0.00708215\n",
      "Iteration 35432, loss = 0.00708203\n",
      "Iteration 35433, loss = 0.00708190\n",
      "Iteration 35434, loss = 0.00708178\n",
      "Iteration 35435, loss = 0.00708165\n",
      "Iteration 35436, loss = 0.00708153\n",
      "Iteration 35437, loss = 0.00708140\n",
      "Iteration 35438, loss = 0.00708128\n",
      "Iteration 35439, loss = 0.00708116\n",
      "Iteration 35440, loss = 0.00708103\n",
      "Iteration 35441, loss = 0.00708091\n",
      "Iteration 35442, loss = 0.00708078\n",
      "Iteration 35443, loss = 0.00708066\n",
      "Iteration 35444, loss = 0.00708053\n",
      "Iteration 35445, loss = 0.00708041\n",
      "Iteration 35446, loss = 0.00708028\n",
      "Iteration 35447, loss = 0.00708016\n",
      "Iteration 35448, loss = 0.00708003\n",
      "Iteration 35449, loss = 0.00707991\n",
      "Iteration 35450, loss = 0.00707979\n",
      "Iteration 35451, loss = 0.00707966\n",
      "Iteration 35452, loss = 0.00707954\n",
      "Iteration 35453, loss = 0.00707941\n",
      "Iteration 35454, loss = 0.00707929\n",
      "Iteration 35455, loss = 0.00707916\n",
      "Iteration 35456, loss = 0.00707904\n",
      "Iteration 35457, loss = 0.00707891\n",
      "Iteration 35458, loss = 0.00707879\n",
      "Iteration 35459, loss = 0.00707866\n",
      "Iteration 35460, loss = 0.00707854\n",
      "Iteration 35461, loss = 0.00707842\n",
      "Iteration 35462, loss = 0.00707829\n",
      "Iteration 35463, loss = 0.00707817\n",
      "Iteration 35464, loss = 0.00707804\n",
      "Iteration 35465, loss = 0.00707792\n",
      "Iteration 35466, loss = 0.00707779\n",
      "Iteration 35467, loss = 0.00707767\n",
      "Iteration 35468, loss = 0.00707754\n",
      "Iteration 35469, loss = 0.00707742\n",
      "Iteration 35470, loss = 0.00707730\n",
      "Iteration 35471, loss = 0.00707717\n",
      "Iteration 35472, loss = 0.00707705\n",
      "Iteration 35473, loss = 0.00707692\n",
      "Iteration 35474, loss = 0.00707680\n",
      "Iteration 35475, loss = 0.00707667\n",
      "Iteration 35476, loss = 0.00707655\n",
      "Iteration 35477, loss = 0.00707643\n",
      "Iteration 35478, loss = 0.00707630\n",
      "Iteration 35479, loss = 0.00707618\n",
      "Iteration 35480, loss = 0.00707605\n",
      "Iteration 35481, loss = 0.00707593\n",
      "Iteration 35482, loss = 0.00707580\n",
      "Iteration 35483, loss = 0.00707568\n",
      "Iteration 35484, loss = 0.00707556\n",
      "Iteration 35485, loss = 0.00707543\n",
      "Iteration 35486, loss = 0.00707531\n",
      "Iteration 35487, loss = 0.00707518\n",
      "Iteration 35488, loss = 0.00707506\n",
      "Iteration 35489, loss = 0.00707494\n",
      "Iteration 35490, loss = 0.00707481\n",
      "Iteration 35491, loss = 0.00707469\n",
      "Iteration 35492, loss = 0.00707456\n",
      "Iteration 35493, loss = 0.00707444\n",
      "Iteration 35494, loss = 0.00707431\n",
      "Iteration 35495, loss = 0.00707419\n",
      "Iteration 35496, loss = 0.00707407\n",
      "Iteration 35497, loss = 0.00707394\n",
      "Iteration 35498, loss = 0.00707382\n",
      "Iteration 35499, loss = 0.00707369\n",
      "Iteration 35500, loss = 0.00707357\n",
      "Iteration 35501, loss = 0.00707345\n",
      "Iteration 35502, loss = 0.00707332\n",
      "Iteration 35503, loss = 0.00707320\n",
      "Iteration 35504, loss = 0.00707307\n",
      "Iteration 35505, loss = 0.00707295\n",
      "Iteration 35506, loss = 0.00707283\n",
      "Iteration 35507, loss = 0.00707270\n",
      "Iteration 35508, loss = 0.00707258\n",
      "Iteration 35509, loss = 0.00707245\n",
      "Iteration 35510, loss = 0.00707233\n",
      "Iteration 35511, loss = 0.00707221\n",
      "Iteration 35512, loss = 0.00707208\n",
      "Iteration 35513, loss = 0.00707196\n",
      "Iteration 35514, loss = 0.00707183\n",
      "Iteration 35515, loss = 0.00707171\n",
      "Iteration 35516, loss = 0.00707159\n",
      "Iteration 35517, loss = 0.00707146\n",
      "Iteration 35518, loss = 0.00707134\n",
      "Iteration 35519, loss = 0.00707121\n",
      "Iteration 35520, loss = 0.00707109\n",
      "Iteration 35521, loss = 0.00707097\n",
      "Iteration 35522, loss = 0.00707084\n",
      "Iteration 35523, loss = 0.00707072\n",
      "Iteration 35524, loss = 0.00707060\n",
      "Iteration 35525, loss = 0.00707047\n",
      "Iteration 35526, loss = 0.00707035\n",
      "Iteration 35527, loss = 0.00707022\n",
      "Iteration 35528, loss = 0.00707010\n",
      "Iteration 35529, loss = 0.00706998\n",
      "Iteration 35530, loss = 0.00706985\n",
      "Iteration 35531, loss = 0.00706973\n",
      "Iteration 35532, loss = 0.00706961\n",
      "Iteration 35533, loss = 0.00706948\n",
      "Iteration 35534, loss = 0.00706936\n",
      "Iteration 35535, loss = 0.00706923\n",
      "Iteration 35536, loss = 0.00706911\n",
      "Iteration 35537, loss = 0.00706899\n",
      "Iteration 35538, loss = 0.00706886\n",
      "Iteration 35539, loss = 0.00706874\n",
      "Iteration 35540, loss = 0.00706862\n",
      "Iteration 35541, loss = 0.00706849\n",
      "Iteration 35542, loss = 0.00706837\n",
      "Iteration 35543, loss = 0.00706824\n",
      "Iteration 35544, loss = 0.00706812\n",
      "Iteration 35545, loss = 0.00706800\n",
      "Iteration 35546, loss = 0.00706787\n",
      "Iteration 35547, loss = 0.00706775\n",
      "Iteration 35548, loss = 0.00706763\n",
      "Iteration 35549, loss = 0.00706750\n",
      "Iteration 35550, loss = 0.00706738\n",
      "Iteration 35551, loss = 0.00706726\n",
      "Iteration 35552, loss = 0.00706713\n",
      "Iteration 35553, loss = 0.00706701\n",
      "Iteration 35554, loss = 0.00706688\n",
      "Iteration 35555, loss = 0.00706676\n",
      "Iteration 35556, loss = 0.00706664\n",
      "Iteration 35557, loss = 0.00706651\n",
      "Iteration 35558, loss = 0.00706639\n",
      "Iteration 35559, loss = 0.00706627\n",
      "Iteration 35560, loss = 0.00706614\n",
      "Iteration 35561, loss = 0.00706602\n",
      "Iteration 35562, loss = 0.00706590\n",
      "Iteration 35563, loss = 0.00706577\n",
      "Iteration 35564, loss = 0.00706565\n",
      "Iteration 35565, loss = 0.00706553\n",
      "Iteration 35566, loss = 0.00706540\n",
      "Iteration 35567, loss = 0.00706528\n",
      "Iteration 35568, loss = 0.00706516\n",
      "Iteration 35569, loss = 0.00706503\n",
      "Iteration 35570, loss = 0.00706491\n",
      "Iteration 35571, loss = 0.00706479\n",
      "Iteration 35572, loss = 0.00706466\n",
      "Iteration 35573, loss = 0.00706454\n",
      "Iteration 35574, loss = 0.00706442\n",
      "Iteration 35575, loss = 0.00706429\n",
      "Iteration 35576, loss = 0.00706417\n",
      "Iteration 35577, loss = 0.00706405\n",
      "Iteration 35578, loss = 0.00706392\n",
      "Iteration 35579, loss = 0.00706380\n",
      "Iteration 35580, loss = 0.00706368\n",
      "Iteration 35581, loss = 0.00706355\n",
      "Iteration 35582, loss = 0.00706343\n",
      "Iteration 35583, loss = 0.00706331\n",
      "Iteration 35584, loss = 0.00706318\n",
      "Iteration 35585, loss = 0.00706306\n",
      "Iteration 35586, loss = 0.00706294\n",
      "Iteration 35587, loss = 0.00706281\n",
      "Iteration 35588, loss = 0.00706269\n",
      "Iteration 35589, loss = 0.00706257\n",
      "Iteration 35590, loss = 0.00706244\n",
      "Iteration 35591, loss = 0.00706232\n",
      "Iteration 35592, loss = 0.00706220\n",
      "Iteration 35593, loss = 0.00706207\n",
      "Iteration 35594, loss = 0.00706195\n",
      "Iteration 35595, loss = 0.00706183\n",
      "Iteration 35596, loss = 0.00706170\n",
      "Iteration 35597, loss = 0.00706158\n",
      "Iteration 35598, loss = 0.00706146\n",
      "Iteration 35599, loss = 0.00706134\n",
      "Iteration 35600, loss = 0.00706121\n",
      "Iteration 35601, loss = 0.00706109\n",
      "Iteration 35602, loss = 0.00706097\n",
      "Iteration 35603, loss = 0.00706084\n",
      "Iteration 35604, loss = 0.00706072\n",
      "Iteration 35605, loss = 0.00706060\n",
      "Iteration 35606, loss = 0.00706047\n",
      "Iteration 35607, loss = 0.00706035\n",
      "Iteration 35608, loss = 0.00706023\n",
      "Iteration 35609, loss = 0.00706010\n",
      "Iteration 35610, loss = 0.00705998\n",
      "Iteration 35611, loss = 0.00705986\n",
      "Iteration 35612, loss = 0.00705974\n",
      "Iteration 35613, loss = 0.00705961\n",
      "Iteration 35614, loss = 0.00705949\n",
      "Iteration 35615, loss = 0.00705937\n",
      "Iteration 35616, loss = 0.00705924\n",
      "Iteration 35617, loss = 0.00705912\n",
      "Iteration 35618, loss = 0.00705900\n",
      "Iteration 35619, loss = 0.00705887\n",
      "Iteration 35620, loss = 0.00705875\n",
      "Iteration 35621, loss = 0.00705863\n",
      "Iteration 35622, loss = 0.00705851\n",
      "Iteration 35623, loss = 0.00705838\n",
      "Iteration 35624, loss = 0.00705826\n",
      "Iteration 35625, loss = 0.00705814\n",
      "Iteration 35626, loss = 0.00705801\n",
      "Iteration 35627, loss = 0.00705789\n",
      "Iteration 35628, loss = 0.00705777\n",
      "Iteration 35629, loss = 0.00705765\n",
      "Iteration 35630, loss = 0.00705752\n",
      "Iteration 35631, loss = 0.00705740\n",
      "Iteration 35632, loss = 0.00705728\n",
      "Iteration 35633, loss = 0.00705715\n",
      "Iteration 35634, loss = 0.00705703\n",
      "Iteration 35635, loss = 0.00705691\n",
      "Iteration 35636, loss = 0.00705679\n",
      "Iteration 35637, loss = 0.00705666\n",
      "Iteration 35638, loss = 0.00705654\n",
      "Iteration 35639, loss = 0.00705642\n",
      "Iteration 35640, loss = 0.00705630\n",
      "Iteration 35641, loss = 0.00705617\n",
      "Iteration 35642, loss = 0.00705605\n",
      "Iteration 35643, loss = 0.00705593\n",
      "Iteration 35644, loss = 0.00705580\n",
      "Iteration 35645, loss = 0.00705568\n",
      "Iteration 35646, loss = 0.00705556\n",
      "Iteration 35647, loss = 0.00705544\n",
      "Iteration 35648, loss = 0.00705531\n",
      "Iteration 35649, loss = 0.00705519\n",
      "Iteration 35650, loss = 0.00705507\n",
      "Iteration 35651, loss = 0.00705495\n",
      "Iteration 35652, loss = 0.00705482\n",
      "Iteration 35653, loss = 0.00705470\n",
      "Iteration 35654, loss = 0.00705458\n",
      "Iteration 35655, loss = 0.00705446\n",
      "Iteration 35656, loss = 0.00705433\n",
      "Iteration 35657, loss = 0.00705421\n",
      "Iteration 35658, loss = 0.00705409\n",
      "Iteration 35659, loss = 0.00705397\n",
      "Iteration 35660, loss = 0.00705384\n",
      "Iteration 35661, loss = 0.00705372\n",
      "Iteration 35662, loss = 0.00705360\n",
      "Iteration 35663, loss = 0.00705348\n",
      "Iteration 35664, loss = 0.00705335\n",
      "Iteration 35665, loss = 0.00705323\n",
      "Iteration 35666, loss = 0.00705311\n",
      "Iteration 35667, loss = 0.00705299\n",
      "Iteration 35668, loss = 0.00705286\n",
      "Iteration 35669, loss = 0.00705274\n",
      "Iteration 35670, loss = 0.00705262\n",
      "Iteration 35671, loss = 0.00705250\n",
      "Iteration 35672, loss = 0.00705237\n",
      "Iteration 35673, loss = 0.00705225\n",
      "Iteration 35674, loss = 0.00705213\n",
      "Iteration 35675, loss = 0.00705201\n",
      "Iteration 35676, loss = 0.00705188\n",
      "Iteration 35677, loss = 0.00705176\n",
      "Iteration 35678, loss = 0.00705164\n",
      "Iteration 35679, loss = 0.00705152\n",
      "Iteration 35680, loss = 0.00705139\n",
      "Iteration 35681, loss = 0.00705127\n",
      "Iteration 35682, loss = 0.00705115\n",
      "Iteration 35683, loss = 0.00705103\n",
      "Iteration 35684, loss = 0.00705090\n",
      "Iteration 35685, loss = 0.00705078\n",
      "Iteration 35686, loss = 0.00705066\n",
      "Iteration 35687, loss = 0.00705054\n",
      "Iteration 35688, loss = 0.00705042\n",
      "Iteration 35689, loss = 0.00705029\n",
      "Iteration 35690, loss = 0.00705017\n",
      "Iteration 35691, loss = 0.00705005\n",
      "Iteration 35692, loss = 0.00704993\n",
      "Iteration 35693, loss = 0.00704980\n",
      "Iteration 35694, loss = 0.00704968\n",
      "Iteration 35695, loss = 0.00704956\n",
      "Iteration 35696, loss = 0.00704944\n",
      "Iteration 35697, loss = 0.00704932\n",
      "Iteration 35698, loss = 0.00704919\n",
      "Iteration 35699, loss = 0.00704907\n",
      "Iteration 35700, loss = 0.00704895\n",
      "Iteration 35701, loss = 0.00704883\n",
      "Iteration 35702, loss = 0.00704870\n",
      "Iteration 35703, loss = 0.00704858\n",
      "Iteration 35704, loss = 0.00704846\n",
      "Iteration 35705, loss = 0.00704834\n",
      "Iteration 35706, loss = 0.00704822\n",
      "Iteration 35707, loss = 0.00704809\n",
      "Iteration 35708, loss = 0.00704797\n",
      "Iteration 35709, loss = 0.00704785\n",
      "Iteration 35710, loss = 0.00704773\n",
      "Iteration 35711, loss = 0.00704761\n",
      "Iteration 35712, loss = 0.00704748\n",
      "Iteration 35713, loss = 0.00704736\n",
      "Iteration 35714, loss = 0.00704724\n",
      "Iteration 35715, loss = 0.00704712\n",
      "Iteration 35716, loss = 0.00704700\n",
      "Iteration 35717, loss = 0.00704687\n",
      "Iteration 35718, loss = 0.00704675\n",
      "Iteration 35719, loss = 0.00704663\n",
      "Iteration 35720, loss = 0.00704651\n",
      "Iteration 35721, loss = 0.00704639\n",
      "Iteration 35722, loss = 0.00704626\n",
      "Iteration 35723, loss = 0.00704614\n",
      "Iteration 35724, loss = 0.00704602\n",
      "Iteration 35725, loss = 0.00704590\n",
      "Iteration 35726, loss = 0.00704578\n",
      "Iteration 35727, loss = 0.00704565\n",
      "Iteration 35728, loss = 0.00704553\n",
      "Iteration 35729, loss = 0.00704541\n",
      "Iteration 35730, loss = 0.00704529\n",
      "Iteration 35731, loss = 0.00704517\n",
      "Iteration 35732, loss = 0.00704504\n",
      "Iteration 35733, loss = 0.00704492\n",
      "Iteration 35734, loss = 0.00704480\n",
      "Iteration 35735, loss = 0.00704468\n",
      "Iteration 35736, loss = 0.00704456\n",
      "Iteration 35737, loss = 0.00704443\n",
      "Iteration 35738, loss = 0.00704431\n",
      "Iteration 35739, loss = 0.00704419\n",
      "Iteration 35740, loss = 0.00704407\n",
      "Iteration 35741, loss = 0.00704395\n",
      "Iteration 35742, loss = 0.00704383\n",
      "Iteration 35743, loss = 0.00704370\n",
      "Iteration 35744, loss = 0.00704358\n",
      "Iteration 35745, loss = 0.00704346\n",
      "Iteration 35746, loss = 0.00704334\n",
      "Iteration 35747, loss = 0.00704322\n",
      "Iteration 35748, loss = 0.00704310\n",
      "Iteration 35749, loss = 0.00704297\n",
      "Iteration 35750, loss = 0.00704285\n",
      "Iteration 35751, loss = 0.00704273\n",
      "Iteration 35752, loss = 0.00704261\n",
      "Iteration 35753, loss = 0.00704249\n",
      "Iteration 35754, loss = 0.00704237\n",
      "Iteration 35755, loss = 0.00704224\n",
      "Iteration 35756, loss = 0.00704212\n",
      "Iteration 35757, loss = 0.00704200\n",
      "Iteration 35758, loss = 0.00704188\n",
      "Iteration 35759, loss = 0.00704176\n",
      "Iteration 35760, loss = 0.00704164\n",
      "Iteration 35761, loss = 0.00704151\n",
      "Iteration 35762, loss = 0.00704139\n",
      "Iteration 35763, loss = 0.00704127\n",
      "Iteration 35764, loss = 0.00704115\n",
      "Iteration 35765, loss = 0.00704103\n",
      "Iteration 35766, loss = 0.00704091\n",
      "Iteration 35767, loss = 0.00704078\n",
      "Iteration 35768, loss = 0.00704066\n",
      "Iteration 35769, loss = 0.00704054\n",
      "Iteration 35770, loss = 0.00704042\n",
      "Iteration 35771, loss = 0.00704030\n",
      "Iteration 35772, loss = 0.00704018\n",
      "Iteration 35773, loss = 0.00704006\n",
      "Iteration 35774, loss = 0.00703993\n",
      "Iteration 35775, loss = 0.00703981\n",
      "Iteration 35776, loss = 0.00703969\n",
      "Iteration 35777, loss = 0.00703957\n",
      "Iteration 35778, loss = 0.00703945\n",
      "Iteration 35779, loss = 0.00703933\n",
      "Iteration 35780, loss = 0.00703921\n",
      "Iteration 35781, loss = 0.00703908\n",
      "Iteration 35782, loss = 0.00703896\n",
      "Iteration 35783, loss = 0.00703884\n",
      "Iteration 35784, loss = 0.00703872\n",
      "Iteration 35785, loss = 0.00703860\n",
      "Iteration 35786, loss = 0.00703848\n",
      "Iteration 35787, loss = 0.00703836\n",
      "Iteration 35788, loss = 0.00703823\n",
      "Iteration 35789, loss = 0.00703811\n",
      "Iteration 35790, loss = 0.00703799\n",
      "Iteration 35791, loss = 0.00703787\n",
      "Iteration 35792, loss = 0.00703775\n",
      "Iteration 35793, loss = 0.00703763\n",
      "Iteration 35794, loss = 0.00703751\n",
      "Iteration 35795, loss = 0.00703738\n",
      "Iteration 35796, loss = 0.00703726\n",
      "Iteration 35797, loss = 0.00703714\n",
      "Iteration 35798, loss = 0.00703702\n",
      "Iteration 35799, loss = 0.00703690\n",
      "Iteration 35800, loss = 0.00703678\n",
      "Iteration 35801, loss = 0.00703666\n",
      "Iteration 35802, loss = 0.00703654\n",
      "Iteration 35803, loss = 0.00703641\n",
      "Iteration 35804, loss = 0.00703629\n",
      "Iteration 35805, loss = 0.00703617\n",
      "Iteration 35806, loss = 0.00703605\n",
      "Iteration 35807, loss = 0.00703593\n",
      "Iteration 35808, loss = 0.00703581\n",
      "Iteration 35809, loss = 0.00703569\n",
      "Iteration 35810, loss = 0.00703557\n",
      "Iteration 35811, loss = 0.00703545\n",
      "Iteration 35812, loss = 0.00703532\n",
      "Iteration 35813, loss = 0.00703520\n",
      "Iteration 35814, loss = 0.00703508\n",
      "Iteration 35815, loss = 0.00703496\n",
      "Iteration 35816, loss = 0.00703484\n",
      "Iteration 35817, loss = 0.00703472\n",
      "Iteration 35818, loss = 0.00703460\n",
      "Iteration 35819, loss = 0.00703448\n",
      "Iteration 35820, loss = 0.00703436\n",
      "Iteration 35821, loss = 0.00703423\n",
      "Iteration 35822, loss = 0.00703411\n",
      "Iteration 35823, loss = 0.00703399\n",
      "Iteration 35824, loss = 0.00703387\n",
      "Iteration 35825, loss = 0.00703375\n",
      "Iteration 35826, loss = 0.00703363\n",
      "Iteration 35827, loss = 0.00703351\n",
      "Iteration 35828, loss = 0.00703339\n",
      "Iteration 35829, loss = 0.00703327\n",
      "Iteration 35830, loss = 0.00703315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35831, loss = 0.00703302\n",
      "Iteration 35832, loss = 0.00703290\n",
      "Iteration 35833, loss = 0.00703278\n",
      "Iteration 35834, loss = 0.00703266\n",
      "Iteration 35835, loss = 0.00703254\n",
      "Iteration 35836, loss = 0.00703242\n",
      "Iteration 35837, loss = 0.00703230\n",
      "Iteration 35838, loss = 0.00703218\n",
      "Iteration 35839, loss = 0.00703206\n",
      "Iteration 35840, loss = 0.00703194\n",
      "Iteration 35841, loss = 0.00703182\n",
      "Iteration 35842, loss = 0.00703169\n",
      "Iteration 35843, loss = 0.00703157\n",
      "Iteration 35844, loss = 0.00703145\n",
      "Iteration 35845, loss = 0.00703133\n",
      "Iteration 35846, loss = 0.00703121\n",
      "Iteration 35847, loss = 0.00703109\n",
      "Iteration 35848, loss = 0.00703097\n",
      "Iteration 35849, loss = 0.00703085\n",
      "Iteration 35850, loss = 0.00703073\n",
      "Iteration 35851, loss = 0.00703061\n",
      "Iteration 35852, loss = 0.00703049\n",
      "Iteration 35853, loss = 0.00703037\n",
      "Iteration 35854, loss = 0.00703025\n",
      "Iteration 35855, loss = 0.00703012\n",
      "Iteration 35856, loss = 0.00703000\n",
      "Iteration 35857, loss = 0.00702988\n",
      "Iteration 35858, loss = 0.00702976\n",
      "Iteration 35859, loss = 0.00702964\n",
      "Iteration 35860, loss = 0.00702952\n",
      "Iteration 35861, loss = 0.00702940\n",
      "Iteration 35862, loss = 0.00702928\n",
      "Iteration 35863, loss = 0.00702916\n",
      "Iteration 35864, loss = 0.00702904\n",
      "Iteration 35865, loss = 0.00702892\n",
      "Iteration 35866, loss = 0.00702880\n",
      "Iteration 35867, loss = 0.00702868\n",
      "Iteration 35868, loss = 0.00702856\n",
      "Iteration 35869, loss = 0.00702844\n",
      "Iteration 35870, loss = 0.00702831\n",
      "Iteration 35871, loss = 0.00702819\n",
      "Iteration 35872, loss = 0.00702807\n",
      "Iteration 35873, loss = 0.00702795\n",
      "Iteration 35874, loss = 0.00702783\n",
      "Iteration 35875, loss = 0.00702771\n",
      "Iteration 35876, loss = 0.00702759\n",
      "Iteration 35877, loss = 0.00702747\n",
      "Iteration 35878, loss = 0.00702735\n",
      "Iteration 35879, loss = 0.00702723\n",
      "Iteration 35880, loss = 0.00702711\n",
      "Iteration 35881, loss = 0.00702699\n",
      "Iteration 35882, loss = 0.00702687\n",
      "Iteration 35883, loss = 0.00702675\n",
      "Iteration 35884, loss = 0.00702663\n",
      "Iteration 35885, loss = 0.00702651\n",
      "Iteration 35886, loss = 0.00702639\n",
      "Iteration 35887, loss = 0.00702627\n",
      "Iteration 35888, loss = 0.00702615\n",
      "Iteration 35889, loss = 0.00702603\n",
      "Iteration 35890, loss = 0.00702590\n",
      "Iteration 35891, loss = 0.00702578\n",
      "Iteration 35892, loss = 0.00702566\n",
      "Iteration 35893, loss = 0.00702554\n",
      "Iteration 35894, loss = 0.00702542\n",
      "Iteration 35895, loss = 0.00702530\n",
      "Iteration 35896, loss = 0.00702518\n",
      "Iteration 35897, loss = 0.00702506\n",
      "Iteration 35898, loss = 0.00702494\n",
      "Iteration 35899, loss = 0.00702482\n",
      "Iteration 35900, loss = 0.00702470\n",
      "Iteration 35901, loss = 0.00702458\n",
      "Iteration 35902, loss = 0.00702446\n",
      "Iteration 35903, loss = 0.00702434\n",
      "Iteration 35904, loss = 0.00702422\n",
      "Iteration 35905, loss = 0.00702410\n",
      "Iteration 35906, loss = 0.00702398\n",
      "Iteration 35907, loss = 0.00702386\n",
      "Iteration 35908, loss = 0.00702374\n",
      "Iteration 35909, loss = 0.00702362\n",
      "Iteration 35910, loss = 0.00702350\n",
      "Iteration 35911, loss = 0.00702338\n",
      "Iteration 35912, loss = 0.00702326\n",
      "Iteration 35913, loss = 0.00702314\n",
      "Iteration 35914, loss = 0.00702302\n",
      "Iteration 35915, loss = 0.00702290\n",
      "Iteration 35916, loss = 0.00702278\n",
      "Iteration 35917, loss = 0.00702266\n",
      "Iteration 35918, loss = 0.00702254\n",
      "Iteration 35919, loss = 0.00702242\n",
      "Iteration 35920, loss = 0.00702230\n",
      "Iteration 35921, loss = 0.00702218\n",
      "Iteration 35922, loss = 0.00702206\n",
      "Iteration 35923, loss = 0.00702194\n",
      "Iteration 35924, loss = 0.00702182\n",
      "Iteration 35925, loss = 0.00702170\n",
      "Iteration 35926, loss = 0.00702158\n",
      "Iteration 35927, loss = 0.00702146\n",
      "Iteration 35928, loss = 0.00702134\n",
      "Iteration 35929, loss = 0.00702122\n",
      "Iteration 35930, loss = 0.00702110\n",
      "Iteration 35931, loss = 0.00702098\n",
      "Iteration 35932, loss = 0.00702086\n",
      "Iteration 35933, loss = 0.00702074\n",
      "Iteration 35934, loss = 0.00702062\n",
      "Iteration 35935, loss = 0.00702050\n",
      "Iteration 35936, loss = 0.00702038\n",
      "Iteration 35937, loss = 0.00702026\n",
      "Iteration 35938, loss = 0.00702014\n",
      "Iteration 35939, loss = 0.00702002\n",
      "Iteration 35940, loss = 0.00701990\n",
      "Iteration 35941, loss = 0.00701978\n",
      "Iteration 35942, loss = 0.00701966\n",
      "Iteration 35943, loss = 0.00701954\n",
      "Iteration 35944, loss = 0.00701942\n",
      "Iteration 35945, loss = 0.00701930\n",
      "Iteration 35946, loss = 0.00701918\n",
      "Iteration 35947, loss = 0.00701906\n",
      "Iteration 35948, loss = 0.00701894\n",
      "Iteration 35949, loss = 0.00701882\n",
      "Iteration 35950, loss = 0.00701870\n",
      "Iteration 35951, loss = 0.00701858\n",
      "Iteration 35952, loss = 0.00701846\n",
      "Iteration 35953, loss = 0.00701834\n",
      "Iteration 35954, loss = 0.00701822\n",
      "Iteration 35955, loss = 0.00701810\n",
      "Iteration 35956, loss = 0.00701798\n",
      "Iteration 35957, loss = 0.00701786\n",
      "Iteration 35958, loss = 0.00701774\n",
      "Iteration 35959, loss = 0.00701762\n",
      "Iteration 35960, loss = 0.00701750\n",
      "Iteration 35961, loss = 0.00701738\n",
      "Iteration 35962, loss = 0.00701726\n",
      "Iteration 35963, loss = 0.00701714\n",
      "Iteration 35964, loss = 0.00701702\n",
      "Iteration 35965, loss = 0.00701690\n",
      "Iteration 35966, loss = 0.00701678\n",
      "Iteration 35967, loss = 0.00701666\n",
      "Iteration 35968, loss = 0.00701654\n",
      "Iteration 35969, loss = 0.00701642\n",
      "Iteration 35970, loss = 0.00701630\n",
      "Iteration 35971, loss = 0.00701618\n",
      "Iteration 35972, loss = 0.00701606\n",
      "Iteration 35973, loss = 0.00701594\n",
      "Iteration 35974, loss = 0.00701582\n",
      "Iteration 35975, loss = 0.00701570\n",
      "Iteration 35976, loss = 0.00701558\n",
      "Iteration 35977, loss = 0.00701546\n",
      "Iteration 35978, loss = 0.00701534\n",
      "Iteration 35979, loss = 0.00701523\n",
      "Iteration 35980, loss = 0.00701511\n",
      "Iteration 35981, loss = 0.00701499\n",
      "Iteration 35982, loss = 0.00701487\n",
      "Iteration 35983, loss = 0.00701475\n",
      "Iteration 35984, loss = 0.00701463\n",
      "Iteration 35985, loss = 0.00701451\n",
      "Iteration 35986, loss = 0.00701439\n",
      "Iteration 35987, loss = 0.00701427\n",
      "Iteration 35988, loss = 0.00701415\n",
      "Iteration 35989, loss = 0.00701403\n",
      "Iteration 35990, loss = 0.00701391\n",
      "Iteration 35991, loss = 0.00701379\n",
      "Iteration 35992, loss = 0.00701367\n",
      "Iteration 35993, loss = 0.00701355\n",
      "Iteration 35994, loss = 0.00701343\n",
      "Iteration 35995, loss = 0.00701331\n",
      "Iteration 35996, loss = 0.00701319\n",
      "Iteration 35997, loss = 0.00701307\n",
      "Iteration 35998, loss = 0.00701295\n",
      "Iteration 35999, loss = 0.00701284\n",
      "Iteration 36000, loss = 0.00701272\n",
      "Iteration 36001, loss = 0.00701260\n",
      "Iteration 36002, loss = 0.00701248\n",
      "Iteration 36003, loss = 0.00701236\n",
      "Iteration 36004, loss = 0.00701224\n",
      "Iteration 36005, loss = 0.00701212\n",
      "Iteration 36006, loss = 0.00701200\n",
      "Iteration 36007, loss = 0.00701188\n",
      "Iteration 36008, loss = 0.00701176\n",
      "Iteration 36009, loss = 0.00701164\n",
      "Iteration 36010, loss = 0.00701152\n",
      "Iteration 36011, loss = 0.00701140\n",
      "Iteration 36012, loss = 0.00701128\n",
      "Iteration 36013, loss = 0.00701116\n",
      "Iteration 36014, loss = 0.00701105\n",
      "Iteration 36015, loss = 0.00701093\n",
      "Iteration 36016, loss = 0.00701081\n",
      "Iteration 36017, loss = 0.00701069\n",
      "Iteration 36018, loss = 0.00701057\n",
      "Iteration 36019, loss = 0.00701045\n",
      "Iteration 36020, loss = 0.00701033\n",
      "Iteration 36021, loss = 0.00701021\n",
      "Iteration 36022, loss = 0.00701009\n",
      "Iteration 36023, loss = 0.00700997\n",
      "Iteration 36024, loss = 0.00700985\n",
      "Iteration 36025, loss = 0.00700973\n",
      "Iteration 36026, loss = 0.00700961\n",
      "Iteration 36027, loss = 0.00700950\n",
      "Iteration 36028, loss = 0.00700938\n",
      "Iteration 36029, loss = 0.00700926\n",
      "Iteration 36030, loss = 0.00700914\n",
      "Iteration 36031, loss = 0.00700902\n",
      "Iteration 36032, loss = 0.00700890\n",
      "Iteration 36033, loss = 0.00700878\n",
      "Iteration 36034, loss = 0.00700866\n",
      "Iteration 36035, loss = 0.00700854\n",
      "Iteration 36036, loss = 0.00700842\n",
      "Iteration 36037, loss = 0.00700830\n",
      "Iteration 36038, loss = 0.00700819\n",
      "Iteration 36039, loss = 0.00700807\n",
      "Iteration 36040, loss = 0.00700795\n",
      "Iteration 36041, loss = 0.00700783\n",
      "Iteration 36042, loss = 0.00700771\n",
      "Iteration 36043, loss = 0.00700759\n",
      "Iteration 36044, loss = 0.00700747\n",
      "Iteration 36045, loss = 0.00700735\n",
      "Iteration 36046, loss = 0.00700723\n",
      "Iteration 36047, loss = 0.00700711\n",
      "Iteration 36048, loss = 0.00700699\n",
      "Iteration 36049, loss = 0.00700688\n",
      "Iteration 36050, loss = 0.00700676\n",
      "Iteration 36051, loss = 0.00700664\n",
      "Iteration 36052, loss = 0.00700652\n",
      "Iteration 36053, loss = 0.00700640\n",
      "Iteration 36054, loss = 0.00700628\n",
      "Iteration 36055, loss = 0.00700616\n",
      "Iteration 36056, loss = 0.00700604\n",
      "Iteration 36057, loss = 0.00700592\n",
      "Iteration 36058, loss = 0.00700581\n",
      "Iteration 36059, loss = 0.00700569\n",
      "Iteration 36060, loss = 0.00700557\n",
      "Iteration 36061, loss = 0.00700545\n",
      "Iteration 36062, loss = 0.00700533\n",
      "Iteration 36063, loss = 0.00700521\n",
      "Iteration 36064, loss = 0.00700509\n",
      "Iteration 36065, loss = 0.00700497\n",
      "Iteration 36066, loss = 0.00700486\n",
      "Iteration 36067, loss = 0.00700474\n",
      "Iteration 36068, loss = 0.00700462\n",
      "Iteration 36069, loss = 0.00700450\n",
      "Iteration 36070, loss = 0.00700438\n",
      "Iteration 36071, loss = 0.00700426\n",
      "Iteration 36072, loss = 0.00700414\n",
      "Iteration 36073, loss = 0.00700402\n",
      "Iteration 36074, loss = 0.00700390\n",
      "Iteration 36075, loss = 0.00700379\n",
      "Iteration 36076, loss = 0.00700367\n",
      "Iteration 36077, loss = 0.00700355\n",
      "Iteration 36078, loss = 0.00700343\n",
      "Iteration 36079, loss = 0.00700331\n",
      "Iteration 36080, loss = 0.00700319\n",
      "Iteration 36081, loss = 0.00700307\n",
      "Iteration 36082, loss = 0.00700296\n",
      "Iteration 36083, loss = 0.00700284\n",
      "Iteration 36084, loss = 0.00700272\n",
      "Iteration 36085, loss = 0.00700260\n",
      "Iteration 36086, loss = 0.00700248\n",
      "Iteration 36087, loss = 0.00700236\n",
      "Iteration 36088, loss = 0.00700224\n",
      "Iteration 36089, loss = 0.00700212\n",
      "Iteration 36090, loss = 0.00700201\n",
      "Iteration 36091, loss = 0.00700189\n",
      "Iteration 36092, loss = 0.00700177\n",
      "Iteration 36093, loss = 0.00700165\n",
      "Iteration 36094, loss = 0.00700153\n",
      "Iteration 36095, loss = 0.00700141\n",
      "Iteration 36096, loss = 0.00700129\n",
      "Iteration 36097, loss = 0.00700118\n",
      "Iteration 36098, loss = 0.00700106\n",
      "Iteration 36099, loss = 0.00700094\n",
      "Iteration 36100, loss = 0.00700082\n",
      "Iteration 36101, loss = 0.00700070\n",
      "Iteration 36102, loss = 0.00700058\n",
      "Iteration 36103, loss = 0.00700047\n",
      "Iteration 36104, loss = 0.00700035\n",
      "Iteration 36105, loss = 0.00700023\n",
      "Iteration 36106, loss = 0.00700011\n",
      "Iteration 36107, loss = 0.00699999\n",
      "Iteration 36108, loss = 0.00699987\n",
      "Iteration 36109, loss = 0.00699975\n",
      "Iteration 36110, loss = 0.00699964\n",
      "Iteration 36111, loss = 0.00699952\n",
      "Iteration 36112, loss = 0.00699940\n",
      "Iteration 36113, loss = 0.00699928\n",
      "Iteration 36114, loss = 0.00699916\n",
      "Iteration 36115, loss = 0.00699904\n",
      "Iteration 36116, loss = 0.00699893\n",
      "Iteration 36117, loss = 0.00699881\n",
      "Iteration 36118, loss = 0.00699869\n",
      "Iteration 36119, loss = 0.00699857\n",
      "Iteration 36120, loss = 0.00699845\n",
      "Iteration 36121, loss = 0.00699833\n",
      "Iteration 36122, loss = 0.00699822\n",
      "Iteration 36123, loss = 0.00699810\n",
      "Iteration 36124, loss = 0.00699798\n",
      "Iteration 36125, loss = 0.00699786\n",
      "Iteration 36126, loss = 0.00699774\n",
      "Iteration 36127, loss = 0.00699762\n",
      "Iteration 36128, loss = 0.00699751\n",
      "Iteration 36129, loss = 0.00699739\n",
      "Iteration 36130, loss = 0.00699727\n",
      "Iteration 36131, loss = 0.00699715\n",
      "Iteration 36132, loss = 0.00699703\n",
      "Iteration 36133, loss = 0.00699692\n",
      "Iteration 36134, loss = 0.00699680\n",
      "Iteration 36135, loss = 0.00699668\n",
      "Iteration 36136, loss = 0.00699656\n",
      "Iteration 36137, loss = 0.00699644\n",
      "Iteration 36138, loss = 0.00699632\n",
      "Iteration 36139, loss = 0.00699621\n",
      "Iteration 36140, loss = 0.00699609\n",
      "Iteration 36141, loss = 0.00699597\n",
      "Iteration 36142, loss = 0.00699585\n",
      "Iteration 36143, loss = 0.00699573\n",
      "Iteration 36144, loss = 0.00699562\n",
      "Iteration 36145, loss = 0.00699550\n",
      "Iteration 36146, loss = 0.00699538\n",
      "Iteration 36147, loss = 0.00699526\n",
      "Iteration 36148, loss = 0.00699514\n",
      "Iteration 36149, loss = 0.00699503\n",
      "Iteration 36150, loss = 0.00699491\n",
      "Iteration 36151, loss = 0.00699479\n",
      "Iteration 36152, loss = 0.00699467\n",
      "Iteration 36153, loss = 0.00699455\n",
      "Iteration 36154, loss = 0.00699444\n",
      "Iteration 36155, loss = 0.00699432\n",
      "Iteration 36156, loss = 0.00699420\n",
      "Iteration 36157, loss = 0.00699408\n",
      "Iteration 36158, loss = 0.00699396\n",
      "Iteration 36159, loss = 0.00699385\n",
      "Iteration 36160, loss = 0.00699373\n",
      "Iteration 36161, loss = 0.00699361\n",
      "Iteration 36162, loss = 0.00699349\n",
      "Iteration 36163, loss = 0.00699337\n",
      "Iteration 36164, loss = 0.00699326\n",
      "Iteration 36165, loss = 0.00699314\n",
      "Iteration 36166, loss = 0.00699302\n",
      "Iteration 36167, loss = 0.00699290\n",
      "Iteration 36168, loss = 0.00699278\n",
      "Iteration 36169, loss = 0.00699267\n",
      "Iteration 36170, loss = 0.00699255\n",
      "Iteration 36171, loss = 0.00699243\n",
      "Iteration 36172, loss = 0.00699231\n",
      "Iteration 36173, loss = 0.00699219\n",
      "Iteration 36174, loss = 0.00699208\n",
      "Iteration 36175, loss = 0.00699196\n",
      "Iteration 36176, loss = 0.00699184\n",
      "Iteration 36177, loss = 0.00699172\n",
      "Iteration 36178, loss = 0.00699161\n",
      "Iteration 36179, loss = 0.00699149\n",
      "Iteration 36180, loss = 0.00699137\n",
      "Iteration 36181, loss = 0.00699125\n",
      "Iteration 36182, loss = 0.00699113\n",
      "Iteration 36183, loss = 0.00699102\n",
      "Iteration 36184, loss = 0.00699090\n",
      "Iteration 36185, loss = 0.00699078\n",
      "Iteration 36186, loss = 0.00699066\n",
      "Iteration 36187, loss = 0.00699055\n",
      "Iteration 36188, loss = 0.00699043\n",
      "Iteration 36189, loss = 0.00699031\n",
      "Iteration 36190, loss = 0.00699019\n",
      "Iteration 36191, loss = 0.00699007\n",
      "Iteration 36192, loss = 0.00698996\n",
      "Iteration 36193, loss = 0.00698984\n",
      "Iteration 36194, loss = 0.00698972\n",
      "Iteration 36195, loss = 0.00698960\n",
      "Iteration 36196, loss = 0.00698949\n",
      "Iteration 36197, loss = 0.00698937\n",
      "Iteration 36198, loss = 0.00698925\n",
      "Iteration 36199, loss = 0.00698913\n",
      "Iteration 36200, loss = 0.00698902\n",
      "Iteration 36201, loss = 0.00698890\n",
      "Iteration 36202, loss = 0.00698878\n",
      "Iteration 36203, loss = 0.00698866\n",
      "Iteration 36204, loss = 0.00698855\n",
      "Iteration 36205, loss = 0.00698843\n",
      "Iteration 36206, loss = 0.00698831\n",
      "Iteration 36207, loss = 0.00698819\n",
      "Iteration 36208, loss = 0.00698808\n",
      "Iteration 36209, loss = 0.00698796\n",
      "Iteration 36210, loss = 0.00698784\n",
      "Iteration 36211, loss = 0.00698772\n",
      "Iteration 36212, loss = 0.00698761\n",
      "Iteration 36213, loss = 0.00698749\n",
      "Iteration 36214, loss = 0.00698737\n",
      "Iteration 36215, loss = 0.00698725\n",
      "Iteration 36216, loss = 0.00698714\n",
      "Iteration 36217, loss = 0.00698702\n",
      "Iteration 36218, loss = 0.00698690\n",
      "Iteration 36219, loss = 0.00698678\n",
      "Iteration 36220, loss = 0.00698667\n",
      "Iteration 36221, loss = 0.00698655\n",
      "Iteration 36222, loss = 0.00698643\n",
      "Iteration 36223, loss = 0.00698631\n",
      "Iteration 36224, loss = 0.00698620\n",
      "Iteration 36225, loss = 0.00698608\n",
      "Iteration 36226, loss = 0.00698596\n",
      "Iteration 36227, loss = 0.00698584\n",
      "Iteration 36228, loss = 0.00698573\n",
      "Iteration 36229, loss = 0.00698561\n",
      "Iteration 36230, loss = 0.00698549\n",
      "Iteration 36231, loss = 0.00698537\n",
      "Iteration 36232, loss = 0.00698526\n",
      "Iteration 36233, loss = 0.00698514\n",
      "Iteration 36234, loss = 0.00698502\n",
      "Iteration 36235, loss = 0.00698491\n",
      "Iteration 36236, loss = 0.00698479\n",
      "Iteration 36237, loss = 0.00698467\n",
      "Iteration 36238, loss = 0.00698455\n",
      "Iteration 36239, loss = 0.00698444\n",
      "Iteration 36240, loss = 0.00698432\n",
      "Iteration 36241, loss = 0.00698420\n",
      "Iteration 36242, loss = 0.00698408\n",
      "Iteration 36243, loss = 0.00698397\n",
      "Iteration 36244, loss = 0.00698385\n",
      "Iteration 36245, loss = 0.00698373\n",
      "Iteration 36246, loss = 0.00698362\n",
      "Iteration 36247, loss = 0.00698350\n",
      "Iteration 36248, loss = 0.00698338\n",
      "Iteration 36249, loss = 0.00698326\n",
      "Iteration 36250, loss = 0.00698315\n",
      "Iteration 36251, loss = 0.00698303\n",
      "Iteration 36252, loss = 0.00698291\n",
      "Iteration 36253, loss = 0.00698280\n",
      "Iteration 36254, loss = 0.00698268\n",
      "Iteration 36255, loss = 0.00698256\n",
      "Iteration 36256, loss = 0.00698244\n",
      "Iteration 36257, loss = 0.00698233\n",
      "Iteration 36258, loss = 0.00698221\n",
      "Iteration 36259, loss = 0.00698209\n",
      "Iteration 36260, loss = 0.00698198\n",
      "Iteration 36261, loss = 0.00698186\n",
      "Iteration 36262, loss = 0.00698174\n",
      "Iteration 36263, loss = 0.00698162\n",
      "Iteration 36264, loss = 0.00698151\n",
      "Iteration 36265, loss = 0.00698139\n",
      "Iteration 36266, loss = 0.00698127\n",
      "Iteration 36267, loss = 0.00698116\n",
      "Iteration 36268, loss = 0.00698104\n",
      "Iteration 36269, loss = 0.00698092\n",
      "Iteration 36270, loss = 0.00698081\n",
      "Iteration 36271, loss = 0.00698069\n",
      "Iteration 36272, loss = 0.00698057\n",
      "Iteration 36273, loss = 0.00698045\n",
      "Iteration 36274, loss = 0.00698034\n",
      "Iteration 36275, loss = 0.00698022\n",
      "Iteration 36276, loss = 0.00698010\n",
      "Iteration 36277, loss = 0.00697999\n",
      "Iteration 36278, loss = 0.00697987\n",
      "Iteration 36279, loss = 0.00697975\n",
      "Iteration 36280, loss = 0.00697964\n",
      "Iteration 36281, loss = 0.00697952\n",
      "Iteration 36282, loss = 0.00697940\n",
      "Iteration 36283, loss = 0.00697929\n",
      "Iteration 36284, loss = 0.00697917\n",
      "Iteration 36285, loss = 0.00697905\n",
      "Iteration 36286, loss = 0.00697893\n",
      "Iteration 36287, loss = 0.00697882\n",
      "Iteration 36288, loss = 0.00697870\n",
      "Iteration 36289, loss = 0.00697858\n",
      "Iteration 36290, loss = 0.00697847\n",
      "Iteration 36291, loss = 0.00697835\n",
      "Iteration 36292, loss = 0.00697823\n",
      "Iteration 36293, loss = 0.00697812\n",
      "Iteration 36294, loss = 0.00697800\n",
      "Iteration 36295, loss = 0.00697788\n",
      "Iteration 36296, loss = 0.00697777\n",
      "Iteration 36297, loss = 0.00697765\n",
      "Iteration 36298, loss = 0.00697753\n",
      "Iteration 36299, loss = 0.00697742\n",
      "Iteration 36300, loss = 0.00697730\n",
      "Iteration 36301, loss = 0.00697718\n",
      "Iteration 36302, loss = 0.00697707\n",
      "Iteration 36303, loss = 0.00697695\n",
      "Iteration 36304, loss = 0.00697683\n",
      "Iteration 36305, loss = 0.00697672\n",
      "Iteration 36306, loss = 0.00697660\n",
      "Iteration 36307, loss = 0.00697648\n",
      "Iteration 36308, loss = 0.00697637\n",
      "Iteration 36309, loss = 0.00697625\n",
      "Iteration 36310, loss = 0.00697613\n",
      "Iteration 36311, loss = 0.00697602\n",
      "Iteration 36312, loss = 0.00697590\n",
      "Iteration 36313, loss = 0.00697578\n",
      "Iteration 36314, loss = 0.00697567\n",
      "Iteration 36315, loss = 0.00697555\n",
      "Iteration 36316, loss = 0.00697543\n",
      "Iteration 36317, loss = 0.00697532\n",
      "Iteration 36318, loss = 0.00697520\n",
      "Iteration 36319, loss = 0.00697508\n",
      "Iteration 36320, loss = 0.00697497\n",
      "Iteration 36321, loss = 0.00697485\n",
      "Iteration 36322, loss = 0.00697473\n",
      "Iteration 36323, loss = 0.00697462\n",
      "Iteration 36324, loss = 0.00697450\n",
      "Iteration 36325, loss = 0.00697438\n",
      "Iteration 36326, loss = 0.00697427\n",
      "Iteration 36327, loss = 0.00697415\n",
      "Iteration 36328, loss = 0.00697404\n",
      "Iteration 36329, loss = 0.00697392\n",
      "Iteration 36330, loss = 0.00697380\n",
      "Iteration 36331, loss = 0.00697369\n",
      "Iteration 36332, loss = 0.00697357\n",
      "Iteration 36333, loss = 0.00697345\n",
      "Iteration 36334, loss = 0.00697334\n",
      "Iteration 36335, loss = 0.00697322\n",
      "Iteration 36336, loss = 0.00697310\n",
      "Iteration 36337, loss = 0.00697299\n",
      "Iteration 36338, loss = 0.00697287\n",
      "Iteration 36339, loss = 0.00697275\n",
      "Iteration 36340, loss = 0.00697264\n",
      "Iteration 36341, loss = 0.00697252\n",
      "Iteration 36342, loss = 0.00697241\n",
      "Iteration 36343, loss = 0.00697229\n",
      "Iteration 36344, loss = 0.00697217\n",
      "Iteration 36345, loss = 0.00697206\n",
      "Iteration 36346, loss = 0.00697194\n",
      "Iteration 36347, loss = 0.00697182\n",
      "Iteration 36348, loss = 0.00697171\n",
      "Iteration 36349, loss = 0.00697159\n",
      "Iteration 36350, loss = 0.00697147\n",
      "Iteration 36351, loss = 0.00697136\n",
      "Iteration 36352, loss = 0.00697124\n",
      "Iteration 36353, loss = 0.00697113\n",
      "Iteration 36354, loss = 0.00697101\n",
      "Iteration 36355, loss = 0.00697089\n",
      "Iteration 36356, loss = 0.00697078\n",
      "Iteration 36357, loss = 0.00697066\n",
      "Iteration 36358, loss = 0.00697054\n",
      "Iteration 36359, loss = 0.00697043\n",
      "Iteration 36360, loss = 0.00697031\n",
      "Iteration 36361, loss = 0.00697020\n",
      "Iteration 36362, loss = 0.00697008\n",
      "Iteration 36363, loss = 0.00696996\n",
      "Iteration 36364, loss = 0.00696985\n",
      "Iteration 36365, loss = 0.00696973\n",
      "Iteration 36366, loss = 0.00696962\n",
      "Iteration 36367, loss = 0.00696950\n",
      "Iteration 36368, loss = 0.00696938\n",
      "Iteration 36369, loss = 0.00696927\n",
      "Iteration 36370, loss = 0.00696915\n",
      "Iteration 36371, loss = 0.00696903\n",
      "Iteration 36372, loss = 0.00696892\n",
      "Iteration 36373, loss = 0.00696880\n",
      "Iteration 36374, loss = 0.00696869\n",
      "Iteration 36375, loss = 0.00696857\n",
      "Iteration 36376, loss = 0.00696845\n",
      "Iteration 36377, loss = 0.00696834\n",
      "Iteration 36378, loss = 0.00696822\n",
      "Iteration 36379, loss = 0.00696811\n",
      "Iteration 36380, loss = 0.00696799\n",
      "Iteration 36381, loss = 0.00696787\n",
      "Iteration 36382, loss = 0.00696776\n",
      "Iteration 36383, loss = 0.00696764\n",
      "Iteration 36384, loss = 0.00696753\n",
      "Iteration 36385, loss = 0.00696741\n",
      "Iteration 36386, loss = 0.00696729\n",
      "Iteration 36387, loss = 0.00696718\n",
      "Iteration 36388, loss = 0.00696706\n",
      "Iteration 36389, loss = 0.00696695\n",
      "Iteration 36390, loss = 0.00696683\n",
      "Iteration 36391, loss = 0.00696671\n",
      "Iteration 36392, loss = 0.00696660\n",
      "Iteration 36393, loss = 0.00696648\n",
      "Iteration 36394, loss = 0.00696637\n",
      "Iteration 36395, loss = 0.00696625\n",
      "Iteration 36396, loss = 0.00696614\n",
      "Iteration 36397, loss = 0.00696602\n",
      "Iteration 36398, loss = 0.00696590\n",
      "Iteration 36399, loss = 0.00696579\n",
      "Iteration 36400, loss = 0.00696567\n",
      "Iteration 36401, loss = 0.00696556\n",
      "Iteration 36402, loss = 0.00696544\n",
      "Iteration 36403, loss = 0.00696532\n",
      "Iteration 36404, loss = 0.00696521\n",
      "Iteration 36405, loss = 0.00696509\n",
      "Iteration 36406, loss = 0.00696498\n",
      "Iteration 36407, loss = 0.00696486\n",
      "Iteration 36408, loss = 0.00696475\n",
      "Iteration 36409, loss = 0.00696463\n",
      "Iteration 36410, loss = 0.00696451\n",
      "Iteration 36411, loss = 0.00696440\n",
      "Iteration 36412, loss = 0.00696428\n",
      "Iteration 36413, loss = 0.00696417\n",
      "Iteration 36414, loss = 0.00696405\n",
      "Iteration 36415, loss = 0.00696393\n",
      "Iteration 36416, loss = 0.00696382\n",
      "Iteration 36417, loss = 0.00696370\n",
      "Iteration 36418, loss = 0.00696359\n",
      "Iteration 36419, loss = 0.00696347\n",
      "Iteration 36420, loss = 0.00696336\n",
      "Iteration 36421, loss = 0.00696324\n",
      "Iteration 36422, loss = 0.00696313\n",
      "Iteration 36423, loss = 0.00696301\n",
      "Iteration 36424, loss = 0.00696289\n",
      "Iteration 36425, loss = 0.00696278\n",
      "Iteration 36426, loss = 0.00696266\n",
      "Iteration 36427, loss = 0.00696255\n",
      "Iteration 36428, loss = 0.00696243\n",
      "Iteration 36429, loss = 0.00696232\n",
      "Iteration 36430, loss = 0.00696220\n",
      "Iteration 36431, loss = 0.00696208\n",
      "Iteration 36432, loss = 0.00696197\n",
      "Iteration 36433, loss = 0.00696185\n",
      "Iteration 36434, loss = 0.00696174\n",
      "Iteration 36435, loss = 0.00696162\n",
      "Iteration 36436, loss = 0.00696151\n",
      "Iteration 36437, loss = 0.00696139\n",
      "Iteration 36438, loss = 0.00696128\n",
      "Iteration 36439, loss = 0.00696116\n",
      "Iteration 36440, loss = 0.00696104\n",
      "Iteration 36441, loss = 0.00696093\n",
      "Iteration 36442, loss = 0.00696081\n",
      "Iteration 36443, loss = 0.00696070\n",
      "Iteration 36444, loss = 0.00696058\n",
      "Iteration 36445, loss = 0.00696047\n",
      "Iteration 36446, loss = 0.00696035\n",
      "Iteration 36447, loss = 0.00696024\n",
      "Iteration 36448, loss = 0.00696012\n",
      "Iteration 36449, loss = 0.00696001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36450, loss = 0.00695989\n",
      "Iteration 36451, loss = 0.00695977\n",
      "Iteration 36452, loss = 0.00695966\n",
      "Iteration 36453, loss = 0.00695954\n",
      "Iteration 36454, loss = 0.00695943\n",
      "Iteration 36455, loss = 0.00695931\n",
      "Iteration 36456, loss = 0.00695920\n",
      "Iteration 36457, loss = 0.00695908\n",
      "Iteration 36458, loss = 0.00695897\n",
      "Iteration 36459, loss = 0.00695885\n",
      "Iteration 36460, loss = 0.00695874\n",
      "Iteration 36461, loss = 0.00695862\n",
      "Iteration 36462, loss = 0.00695851\n",
      "Iteration 36463, loss = 0.00695839\n",
      "Iteration 36464, loss = 0.00695828\n",
      "Iteration 36465, loss = 0.00695816\n",
      "Iteration 36466, loss = 0.00695804\n",
      "Iteration 36467, loss = 0.00695793\n",
      "Iteration 36468, loss = 0.00695781\n",
      "Iteration 36469, loss = 0.00695770\n",
      "Iteration 36470, loss = 0.00695758\n",
      "Iteration 36471, loss = 0.00695747\n",
      "Iteration 36472, loss = 0.00695735\n",
      "Iteration 36473, loss = 0.00695724\n",
      "Iteration 36474, loss = 0.00695712\n",
      "Iteration 36475, loss = 0.00695701\n",
      "Iteration 36476, loss = 0.00695689\n",
      "Iteration 36477, loss = 0.00695678\n",
      "Iteration 36478, loss = 0.00695666\n",
      "Iteration 36479, loss = 0.00695655\n",
      "Iteration 36480, loss = 0.00695643\n",
      "Iteration 36481, loss = 0.00695632\n",
      "Iteration 36482, loss = 0.00695620\n",
      "Iteration 36483, loss = 0.00695609\n",
      "Iteration 36484, loss = 0.00695597\n",
      "Iteration 36485, loss = 0.00695586\n",
      "Iteration 36486, loss = 0.00695574\n",
      "Iteration 36487, loss = 0.00695563\n",
      "Iteration 36488, loss = 0.00695551\n",
      "Iteration 36489, loss = 0.00695540\n",
      "Iteration 36490, loss = 0.00695528\n",
      "Iteration 36491, loss = 0.00695517\n",
      "Iteration 36492, loss = 0.00695505\n",
      "Iteration 36493, loss = 0.00695494\n",
      "Iteration 36494, loss = 0.00695482\n",
      "Iteration 36495, loss = 0.00695471\n",
      "Iteration 36496, loss = 0.00695459\n",
      "Iteration 36497, loss = 0.00695448\n",
      "Iteration 36498, loss = 0.00695436\n",
      "Iteration 36499, loss = 0.00695425\n",
      "Iteration 36500, loss = 0.00695413\n",
      "Iteration 36501, loss = 0.00695402\n",
      "Iteration 36502, loss = 0.00695390\n",
      "Iteration 36503, loss = 0.00695379\n",
      "Iteration 36504, loss = 0.00695367\n",
      "Iteration 36505, loss = 0.00695356\n",
      "Iteration 36506, loss = 0.00695344\n",
      "Iteration 36507, loss = 0.00695333\n",
      "Iteration 36508, loss = 0.00695321\n",
      "Iteration 36509, loss = 0.00695310\n",
      "Iteration 36510, loss = 0.00695298\n",
      "Iteration 36511, loss = 0.00695287\n",
      "Iteration 36512, loss = 0.00695275\n",
      "Iteration 36513, loss = 0.00695264\n",
      "Iteration 36514, loss = 0.00695252\n",
      "Iteration 36515, loss = 0.00695241\n",
      "Iteration 36516, loss = 0.00695229\n",
      "Iteration 36517, loss = 0.00695218\n",
      "Iteration 36518, loss = 0.00695206\n",
      "Iteration 36519, loss = 0.00695195\n",
      "Iteration 36520, loss = 0.00695183\n",
      "Iteration 36521, loss = 0.00695172\n",
      "Iteration 36522, loss = 0.00695160\n",
      "Iteration 36523, loss = 0.00695149\n",
      "Iteration 36524, loss = 0.00695137\n",
      "Iteration 36525, loss = 0.00695126\n",
      "Iteration 36526, loss = 0.00695114\n",
      "Iteration 36527, loss = 0.00695103\n",
      "Iteration 36528, loss = 0.00695091\n",
      "Iteration 36529, loss = 0.00695080\n",
      "Iteration 36530, loss = 0.00695069\n",
      "Iteration 36531, loss = 0.00695057\n",
      "Iteration 36532, loss = 0.00695046\n",
      "Iteration 36533, loss = 0.00695034\n",
      "Iteration 36534, loss = 0.00695023\n",
      "Iteration 36535, loss = 0.00695011\n",
      "Iteration 36536, loss = 0.00695000\n",
      "Iteration 36537, loss = 0.00694988\n",
      "Iteration 36538, loss = 0.00694977\n",
      "Iteration 36539, loss = 0.00694965\n",
      "Iteration 36540, loss = 0.00694954\n",
      "Iteration 36541, loss = 0.00694942\n",
      "Iteration 36542, loss = 0.00694931\n",
      "Iteration 36543, loss = 0.00694919\n",
      "Iteration 36544, loss = 0.00694908\n",
      "Iteration 36545, loss = 0.00694897\n",
      "Iteration 36546, loss = 0.00694885\n",
      "Iteration 36547, loss = 0.00694874\n",
      "Iteration 36548, loss = 0.00694862\n",
      "Iteration 36549, loss = 0.00694851\n",
      "Iteration 36550, loss = 0.00694839\n",
      "Iteration 36551, loss = 0.00694828\n",
      "Iteration 36552, loss = 0.00694816\n",
      "Iteration 36553, loss = 0.00694805\n",
      "Iteration 36554, loss = 0.00694793\n",
      "Iteration 36555, loss = 0.00694782\n",
      "Iteration 36556, loss = 0.00694771\n",
      "Iteration 36557, loss = 0.00694759\n",
      "Iteration 36558, loss = 0.00694748\n",
      "Iteration 36559, loss = 0.00694736\n",
      "Iteration 36560, loss = 0.00694725\n",
      "Iteration 36561, loss = 0.00694713\n",
      "Iteration 36562, loss = 0.00694702\n",
      "Iteration 36563, loss = 0.00694690\n",
      "Iteration 36564, loss = 0.00694679\n",
      "Iteration 36565, loss = 0.00694668\n",
      "Iteration 36566, loss = 0.00694656\n",
      "Iteration 36567, loss = 0.00694645\n",
      "Iteration 36568, loss = 0.00694633\n",
      "Iteration 36569, loss = 0.00694622\n",
      "Iteration 36570, loss = 0.00694610\n",
      "Iteration 36571, loss = 0.00694599\n",
      "Iteration 36572, loss = 0.00694587\n",
      "Iteration 36573, loss = 0.00694576\n",
      "Iteration 36574, loss = 0.00694565\n",
      "Iteration 36575, loss = 0.00694553\n",
      "Iteration 36576, loss = 0.00694542\n",
      "Iteration 36577, loss = 0.00694530\n",
      "Iteration 36578, loss = 0.00694519\n",
      "Iteration 36579, loss = 0.00694507\n",
      "Iteration 36580, loss = 0.00694496\n",
      "Iteration 36581, loss = 0.00694485\n",
      "Iteration 36582, loss = 0.00694473\n",
      "Iteration 36583, loss = 0.00694462\n",
      "Iteration 36584, loss = 0.00694450\n",
      "Iteration 36585, loss = 0.00694439\n",
      "Iteration 36586, loss = 0.00694427\n",
      "Iteration 36587, loss = 0.00694416\n",
      "Iteration 36588, loss = 0.00694405\n",
      "Iteration 36589, loss = 0.00694393\n",
      "Iteration 36590, loss = 0.00694382\n",
      "Iteration 36591, loss = 0.00694370\n",
      "Iteration 36592, loss = 0.00694359\n",
      "Iteration 36593, loss = 0.00694347\n",
      "Iteration 36594, loss = 0.00694336\n",
      "Iteration 36595, loss = 0.00694325\n",
      "Iteration 36596, loss = 0.00694313\n",
      "Iteration 36597, loss = 0.00694302\n",
      "Iteration 36598, loss = 0.00694290\n",
      "Iteration 36599, loss = 0.00694279\n",
      "Iteration 36600, loss = 0.00694268\n",
      "Iteration 36601, loss = 0.00694256\n",
      "Iteration 36602, loss = 0.00694245\n",
      "Iteration 36603, loss = 0.00694233\n",
      "Iteration 36604, loss = 0.00694222\n",
      "Iteration 36605, loss = 0.00694211\n",
      "Iteration 36606, loss = 0.00694199\n",
      "Iteration 36607, loss = 0.00694188\n",
      "Iteration 36608, loss = 0.00694176\n",
      "Iteration 36609, loss = 0.00694165\n",
      "Iteration 36610, loss = 0.00694153\n",
      "Iteration 36611, loss = 0.00694142\n",
      "Iteration 36612, loss = 0.00694131\n",
      "Iteration 36613, loss = 0.00694119\n",
      "Iteration 36614, loss = 0.00694108\n",
      "Iteration 36615, loss = 0.00694096\n",
      "Iteration 36616, loss = 0.00694085\n",
      "Iteration 36617, loss = 0.00694074\n",
      "Iteration 36618, loss = 0.00694062\n",
      "Iteration 36619, loss = 0.00694051\n",
      "Iteration 36620, loss = 0.00694039\n",
      "Iteration 36621, loss = 0.00694028\n",
      "Iteration 36622, loss = 0.00694017\n",
      "Iteration 36623, loss = 0.00694005\n",
      "Iteration 36624, loss = 0.00693994\n",
      "Iteration 36625, loss = 0.00693983\n",
      "Iteration 36626, loss = 0.00693971\n",
      "Iteration 36627, loss = 0.00693960\n",
      "Iteration 36628, loss = 0.00693948\n",
      "Iteration 36629, loss = 0.00693937\n",
      "Iteration 36630, loss = 0.00693926\n",
      "Iteration 36631, loss = 0.00693914\n",
      "Iteration 36632, loss = 0.00693903\n",
      "Iteration 36633, loss = 0.00693891\n",
      "Iteration 36634, loss = 0.00693880\n",
      "Iteration 36635, loss = 0.00693869\n",
      "Iteration 36636, loss = 0.00693857\n",
      "Iteration 36637, loss = 0.00693846\n",
      "Iteration 36638, loss = 0.00693835\n",
      "Iteration 36639, loss = 0.00693823\n",
      "Iteration 36640, loss = 0.00693812\n",
      "Iteration 36641, loss = 0.00693800\n",
      "Iteration 36642, loss = 0.00693789\n",
      "Iteration 36643, loss = 0.00693778\n",
      "Iteration 36644, loss = 0.00693766\n",
      "Iteration 36645, loss = 0.00693755\n",
      "Iteration 36646, loss = 0.00693744\n",
      "Iteration 36647, loss = 0.00693732\n",
      "Iteration 36648, loss = 0.00693721\n",
      "Iteration 36649, loss = 0.00693709\n",
      "Iteration 36650, loss = 0.00693698\n",
      "Iteration 36651, loss = 0.00693687\n",
      "Iteration 36652, loss = 0.00693675\n",
      "Iteration 36653, loss = 0.00693664\n",
      "Iteration 36654, loss = 0.00693653\n",
      "Iteration 36655, loss = 0.00693641\n",
      "Iteration 36656, loss = 0.00693630\n",
      "Iteration 36657, loss = 0.00693618\n",
      "Iteration 36658, loss = 0.00693607\n",
      "Iteration 36659, loss = 0.00693596\n",
      "Iteration 36660, loss = 0.00693584\n",
      "Iteration 36661, loss = 0.00693573\n",
      "Iteration 36662, loss = 0.00693562\n",
      "Iteration 36663, loss = 0.00693550\n",
      "Iteration 36664, loss = 0.00693539\n",
      "Iteration 36665, loss = 0.00693528\n",
      "Iteration 36666, loss = 0.00693516\n",
      "Iteration 36667, loss = 0.00693505\n",
      "Iteration 36668, loss = 0.00693494\n",
      "Iteration 36669, loss = 0.00693482\n",
      "Iteration 36670, loss = 0.00693471\n",
      "Iteration 36671, loss = 0.00693459\n",
      "Iteration 36672, loss = 0.00693448\n",
      "Iteration 36673, loss = 0.00693437\n",
      "Iteration 36674, loss = 0.00693425\n",
      "Iteration 36675, loss = 0.00693414\n",
      "Iteration 36676, loss = 0.00693403\n",
      "Iteration 36677, loss = 0.00693391\n",
      "Iteration 36678, loss = 0.00693380\n",
      "Iteration 36679, loss = 0.00693369\n",
      "Iteration 36680, loss = 0.00693357\n",
      "Iteration 36681, loss = 0.00693346\n",
      "Iteration 36682, loss = 0.00693335\n",
      "Iteration 36683, loss = 0.00693323\n",
      "Iteration 36684, loss = 0.00693312\n",
      "Iteration 36685, loss = 0.00693301\n",
      "Iteration 36686, loss = 0.00693289\n",
      "Iteration 36687, loss = 0.00693278\n",
      "Iteration 36688, loss = 0.00693267\n",
      "Iteration 36689, loss = 0.00693255\n",
      "Iteration 36690, loss = 0.00693244\n",
      "Iteration 36691, loss = 0.00693233\n",
      "Iteration 36692, loss = 0.00693221\n",
      "Iteration 36693, loss = 0.00693210\n",
      "Iteration 36694, loss = 0.00693199\n",
      "Iteration 36695, loss = 0.00693187\n",
      "Iteration 36696, loss = 0.00693176\n",
      "Iteration 36697, loss = 0.00693165\n",
      "Iteration 36698, loss = 0.00693153\n",
      "Iteration 36699, loss = 0.00693142\n",
      "Iteration 36700, loss = 0.00693131\n",
      "Iteration 36701, loss = 0.00693119\n",
      "Iteration 36702, loss = 0.00693108\n",
      "Iteration 36703, loss = 0.00693097\n",
      "Iteration 36704, loss = 0.00693085\n",
      "Iteration 36705, loss = 0.00693074\n",
      "Iteration 36706, loss = 0.00693063\n",
      "Iteration 36707, loss = 0.00693051\n",
      "Iteration 36708, loss = 0.00693040\n",
      "Iteration 36709, loss = 0.00693029\n",
      "Iteration 36710, loss = 0.00693017\n",
      "Iteration 36711, loss = 0.00693006\n",
      "Iteration 36712, loss = 0.00692995\n",
      "Iteration 36713, loss = 0.00692983\n",
      "Iteration 36714, loss = 0.00692972\n",
      "Iteration 36715, loss = 0.00692961\n",
      "Iteration 36716, loss = 0.00692949\n",
      "Iteration 36717, loss = 0.00692938\n",
      "Iteration 36718, loss = 0.00692927\n",
      "Iteration 36719, loss = 0.00692916\n",
      "Iteration 36720, loss = 0.00692904\n",
      "Iteration 36721, loss = 0.00692893\n",
      "Iteration 36722, loss = 0.00692882\n",
      "Iteration 36723, loss = 0.00692870\n",
      "Iteration 36724, loss = 0.00692859\n",
      "Iteration 36725, loss = 0.00692848\n",
      "Iteration 36726, loss = 0.00692836\n",
      "Iteration 36727, loss = 0.00692825\n",
      "Iteration 36728, loss = 0.00692814\n",
      "Iteration 36729, loss = 0.00692802\n",
      "Iteration 36730, loss = 0.00692791\n",
      "Iteration 36731, loss = 0.00692780\n",
      "Iteration 36732, loss = 0.00692769\n",
      "Iteration 36733, loss = 0.00692757\n",
      "Iteration 36734, loss = 0.00692746\n",
      "Iteration 36735, loss = 0.00692735\n",
      "Iteration 36736, loss = 0.00692723\n",
      "Iteration 36737, loss = 0.00692712\n",
      "Iteration 36738, loss = 0.00692701\n",
      "Iteration 36739, loss = 0.00692689\n",
      "Iteration 36740, loss = 0.00692678\n",
      "Iteration 36741, loss = 0.00692667\n",
      "Iteration 36742, loss = 0.00692656\n",
      "Iteration 36743, loss = 0.00692644\n",
      "Iteration 36744, loss = 0.00692633\n",
      "Iteration 36745, loss = 0.00692622\n",
      "Iteration 36746, loss = 0.00692610\n",
      "Iteration 36747, loss = 0.00692599\n",
      "Iteration 36748, loss = 0.00692588\n",
      "Iteration 36749, loss = 0.00692577\n",
      "Iteration 36750, loss = 0.00692565\n",
      "Iteration 36751, loss = 0.00692554\n",
      "Iteration 36752, loss = 0.00692543\n",
      "Iteration 36753, loss = 0.00692531\n",
      "Iteration 36754, loss = 0.00692520\n",
      "Iteration 36755, loss = 0.00692509\n",
      "Iteration 36756, loss = 0.00692498\n",
      "Iteration 36757, loss = 0.00692486\n",
      "Iteration 36758, loss = 0.00692475\n",
      "Iteration 36759, loss = 0.00692464\n",
      "Iteration 36760, loss = 0.00692452\n",
      "Iteration 36761, loss = 0.00692441\n",
      "Iteration 36762, loss = 0.00692430\n",
      "Iteration 36763, loss = 0.00692419\n",
      "Iteration 36764, loss = 0.00692407\n",
      "Iteration 36765, loss = 0.00692396\n",
      "Iteration 36766, loss = 0.00692385\n",
      "Iteration 36767, loss = 0.00692374\n",
      "Iteration 36768, loss = 0.00692362\n",
      "Iteration 36769, loss = 0.00692351\n",
      "Iteration 36770, loss = 0.00692340\n",
      "Iteration 36771, loss = 0.00692328\n",
      "Iteration 36772, loss = 0.00692317\n",
      "Iteration 36773, loss = 0.00692306\n",
      "Iteration 36774, loss = 0.00692295\n",
      "Iteration 36775, loss = 0.00692283\n",
      "Iteration 36776, loss = 0.00692272\n",
      "Iteration 36777, loss = 0.00692261\n",
      "Iteration 36778, loss = 0.00692250\n",
      "Iteration 36779, loss = 0.00692238\n",
      "Iteration 36780, loss = 0.00692227\n",
      "Iteration 36781, loss = 0.00692216\n",
      "Iteration 36782, loss = 0.00692205\n",
      "Iteration 36783, loss = 0.00692193\n",
      "Iteration 36784, loss = 0.00692182\n",
      "Iteration 36785, loss = 0.00692171\n",
      "Iteration 36786, loss = 0.00692160\n",
      "Iteration 36787, loss = 0.00692148\n",
      "Iteration 36788, loss = 0.00692137\n",
      "Iteration 36789, loss = 0.00692126\n",
      "Iteration 36790, loss = 0.00692115\n",
      "Iteration 36791, loss = 0.00692103\n",
      "Iteration 36792, loss = 0.00692092\n",
      "Iteration 36793, loss = 0.00692081\n",
      "Iteration 36794, loss = 0.00692070\n",
      "Iteration 36795, loss = 0.00692058\n",
      "Iteration 36796, loss = 0.00692047\n",
      "Iteration 36797, loss = 0.00692036\n",
      "Iteration 36798, loss = 0.00692025\n",
      "Iteration 36799, loss = 0.00692013\n",
      "Iteration 36800, loss = 0.00692002\n",
      "Iteration 36801, loss = 0.00691991\n",
      "Iteration 36802, loss = 0.00691980\n",
      "Iteration 36803, loss = 0.00691968\n",
      "Iteration 36804, loss = 0.00691957\n",
      "Iteration 36805, loss = 0.00691946\n",
      "Iteration 36806, loss = 0.00691935\n",
      "Iteration 36807, loss = 0.00691923\n",
      "Iteration 36808, loss = 0.00691912\n",
      "Iteration 36809, loss = 0.00691901\n",
      "Iteration 36810, loss = 0.00691890\n",
      "Iteration 36811, loss = 0.00691878\n",
      "Iteration 36812, loss = 0.00691867\n",
      "Iteration 36813, loss = 0.00691856\n",
      "Iteration 36814, loss = 0.00691845\n",
      "Iteration 36815, loss = 0.00691834\n",
      "Iteration 36816, loss = 0.00691822\n",
      "Iteration 36817, loss = 0.00691811\n",
      "Iteration 36818, loss = 0.00691800\n",
      "Iteration 36819, loss = 0.00691789\n",
      "Iteration 36820, loss = 0.00691777\n",
      "Iteration 36821, loss = 0.00691766\n",
      "Iteration 36822, loss = 0.00691755\n",
      "Iteration 36823, loss = 0.00691744\n",
      "Iteration 36824, loss = 0.00691732\n",
      "Iteration 36825, loss = 0.00691721\n",
      "Iteration 36826, loss = 0.00691710\n",
      "Iteration 36827, loss = 0.00691699\n",
      "Iteration 36828, loss = 0.00691688\n",
      "Iteration 36829, loss = 0.00691676\n",
      "Iteration 36830, loss = 0.00691665\n",
      "Iteration 36831, loss = 0.00691654\n",
      "Iteration 36832, loss = 0.00691643\n",
      "Iteration 36833, loss = 0.00691632\n",
      "Iteration 36834, loss = 0.00691620\n",
      "Iteration 36835, loss = 0.00691609\n",
      "Iteration 36836, loss = 0.00691598\n",
      "Iteration 36837, loss = 0.00691587\n",
      "Iteration 36838, loss = 0.00691575\n",
      "Iteration 36839, loss = 0.00691564\n",
      "Iteration 36840, loss = 0.00691553\n",
      "Iteration 36841, loss = 0.00691542\n",
      "Iteration 36842, loss = 0.00691531\n",
      "Iteration 36843, loss = 0.00691519\n",
      "Iteration 36844, loss = 0.00691508\n",
      "Iteration 36845, loss = 0.00691497\n",
      "Iteration 36846, loss = 0.00691486\n",
      "Iteration 36847, loss = 0.00691475\n",
      "Iteration 36848, loss = 0.00691463\n",
      "Iteration 36849, loss = 0.00691452\n",
      "Iteration 36850, loss = 0.00691441\n",
      "Iteration 36851, loss = 0.00691430\n",
      "Iteration 36852, loss = 0.00691419\n",
      "Iteration 36853, loss = 0.00691407\n",
      "Iteration 36854, loss = 0.00691396\n",
      "Iteration 36855, loss = 0.00691385\n",
      "Iteration 36856, loss = 0.00691374\n",
      "Iteration 36857, loss = 0.00691363\n",
      "Iteration 36858, loss = 0.00691351\n",
      "Iteration 36859, loss = 0.00691340\n",
      "Iteration 36860, loss = 0.00691329\n",
      "Iteration 36861, loss = 0.00691318\n",
      "Iteration 36862, loss = 0.00691307\n",
      "Iteration 36863, loss = 0.00691295\n",
      "Iteration 36864, loss = 0.00691284\n",
      "Iteration 36865, loss = 0.00691273\n",
      "Iteration 36866, loss = 0.00691262\n",
      "Iteration 36867, loss = 0.00691251\n",
      "Iteration 36868, loss = 0.00691239\n",
      "Iteration 36869, loss = 0.00691228\n",
      "Iteration 36870, loss = 0.00691217\n",
      "Iteration 36871, loss = 0.00691206\n",
      "Iteration 36872, loss = 0.00691195\n",
      "Iteration 36873, loss = 0.00691184\n",
      "Iteration 36874, loss = 0.00691172\n",
      "Iteration 36875, loss = 0.00691161\n",
      "Iteration 36876, loss = 0.00691150\n",
      "Iteration 36877, loss = 0.00691139\n",
      "Iteration 36878, loss = 0.00691128\n",
      "Iteration 36879, loss = 0.00691116\n",
      "Iteration 36880, loss = 0.00691105\n",
      "Iteration 36881, loss = 0.00691094\n",
      "Iteration 36882, loss = 0.00691083\n",
      "Iteration 36883, loss = 0.00691072\n",
      "Iteration 36884, loss = 0.00691061\n",
      "Iteration 36885, loss = 0.00691049\n",
      "Iteration 36886, loss = 0.00691038\n",
      "Iteration 36887, loss = 0.00691027\n",
      "Iteration 36888, loss = 0.00691016\n",
      "Iteration 36889, loss = 0.00691005\n",
      "Iteration 36890, loss = 0.00690994\n",
      "Iteration 36891, loss = 0.00690982\n",
      "Iteration 36892, loss = 0.00690971\n",
      "Iteration 36893, loss = 0.00690960\n",
      "Iteration 36894, loss = 0.00690949\n",
      "Iteration 36895, loss = 0.00690938\n",
      "Iteration 36896, loss = 0.00690927\n",
      "Iteration 36897, loss = 0.00690915\n",
      "Iteration 36898, loss = 0.00690904\n",
      "Iteration 36899, loss = 0.00690893\n",
      "Iteration 36900, loss = 0.00690882\n",
      "Iteration 36901, loss = 0.00690871\n",
      "Iteration 36902, loss = 0.00690860\n",
      "Iteration 36903, loss = 0.00690848\n",
      "Iteration 36904, loss = 0.00690837\n",
      "Iteration 36905, loss = 0.00690826\n",
      "Iteration 36906, loss = 0.00690815\n",
      "Iteration 36907, loss = 0.00690804\n",
      "Iteration 36908, loss = 0.00690793\n",
      "Iteration 36909, loss = 0.00690782\n",
      "Iteration 36910, loss = 0.00690770\n",
      "Iteration 36911, loss = 0.00690759\n",
      "Iteration 36912, loss = 0.00690748\n",
      "Iteration 36913, loss = 0.00690737\n",
      "Iteration 36914, loss = 0.00690726\n",
      "Iteration 36915, loss = 0.00690715\n",
      "Iteration 36916, loss = 0.00690703\n",
      "Iteration 36917, loss = 0.00690692\n",
      "Iteration 36918, loss = 0.00690681\n",
      "Iteration 36919, loss = 0.00690670\n",
      "Iteration 36920, loss = 0.00690659\n",
      "Iteration 36921, loss = 0.00690648\n",
      "Iteration 36922, loss = 0.00690637\n",
      "Iteration 36923, loss = 0.00690625\n",
      "Iteration 36924, loss = 0.00690614\n",
      "Iteration 36925, loss = 0.00690603\n",
      "Iteration 36926, loss = 0.00690592\n",
      "Iteration 36927, loss = 0.00690581\n",
      "Iteration 36928, loss = 0.00690570\n",
      "Iteration 36929, loss = 0.00690559\n",
      "Iteration 36930, loss = 0.00690547\n",
      "Iteration 36931, loss = 0.00690536\n",
      "Iteration 36932, loss = 0.00690525\n",
      "Iteration 36933, loss = 0.00690514\n",
      "Iteration 36934, loss = 0.00690503\n",
      "Iteration 36935, loss = 0.00690492\n",
      "Iteration 36936, loss = 0.00690481\n",
      "Iteration 36937, loss = 0.00690470\n",
      "Iteration 36938, loss = 0.00690458\n",
      "Iteration 36939, loss = 0.00690447\n",
      "Iteration 36940, loss = 0.00690436\n",
      "Iteration 36941, loss = 0.00690425\n",
      "Iteration 36942, loss = 0.00690414\n",
      "Iteration 36943, loss = 0.00690403\n",
      "Iteration 36944, loss = 0.00690392\n",
      "Iteration 36945, loss = 0.00690381\n",
      "Iteration 36946, loss = 0.00690369\n",
      "Iteration 36947, loss = 0.00690358\n",
      "Iteration 36948, loss = 0.00690347\n",
      "Iteration 36949, loss = 0.00690336\n",
      "Iteration 36950, loss = 0.00690325\n",
      "Iteration 36951, loss = 0.00690314\n",
      "Iteration 36952, loss = 0.00690303\n",
      "Iteration 36953, loss = 0.00690292\n",
      "Iteration 36954, loss = 0.00690280\n",
      "Iteration 36955, loss = 0.00690269\n",
      "Iteration 36956, loss = 0.00690258\n",
      "Iteration 36957, loss = 0.00690247\n",
      "Iteration 36958, loss = 0.00690236\n",
      "Iteration 36959, loss = 0.00690225\n",
      "Iteration 36960, loss = 0.00690214\n",
      "Iteration 36961, loss = 0.00690203\n",
      "Iteration 36962, loss = 0.00690192\n",
      "Iteration 36963, loss = 0.00690180\n",
      "Iteration 36964, loss = 0.00690169\n",
      "Iteration 36965, loss = 0.00690158\n",
      "Iteration 36966, loss = 0.00690147\n",
      "Iteration 36967, loss = 0.00690136\n",
      "Iteration 36968, loss = 0.00690125\n",
      "Iteration 36969, loss = 0.00690114\n",
      "Iteration 36970, loss = 0.00690103\n",
      "Iteration 36971, loss = 0.00690092\n",
      "Iteration 36972, loss = 0.00690081\n",
      "Iteration 36973, loss = 0.00690069\n",
      "Iteration 36974, loss = 0.00690058\n",
      "Iteration 36975, loss = 0.00690047\n",
      "Iteration 36976, loss = 0.00690036\n",
      "Iteration 36977, loss = 0.00690025\n",
      "Iteration 36978, loss = 0.00690014\n",
      "Iteration 36979, loss = 0.00690003\n",
      "Iteration 36980, loss = 0.00689992\n",
      "Iteration 36981, loss = 0.00689981\n",
      "Iteration 36982, loss = 0.00689970\n",
      "Iteration 36983, loss = 0.00689958\n",
      "Iteration 36984, loss = 0.00689947\n",
      "Iteration 36985, loss = 0.00689936\n",
      "Iteration 36986, loss = 0.00689925\n",
      "Iteration 36987, loss = 0.00689914\n",
      "Iteration 36988, loss = 0.00689903\n",
      "Iteration 36989, loss = 0.00689892\n",
      "Iteration 36990, loss = 0.00689881\n",
      "Iteration 36991, loss = 0.00689870\n",
      "Iteration 36992, loss = 0.00689859\n",
      "Iteration 36993, loss = 0.00689848\n",
      "Iteration 36994, loss = 0.00689837\n",
      "Iteration 36995, loss = 0.00689825\n",
      "Iteration 36996, loss = 0.00689814\n",
      "Iteration 36997, loss = 0.00689803\n",
      "Iteration 36998, loss = 0.00689792\n",
      "Iteration 36999, loss = 0.00689781\n",
      "Iteration 37000, loss = 0.00689770\n",
      "Iteration 37001, loss = 0.00689759\n",
      "Iteration 37002, loss = 0.00689748\n",
      "Iteration 37003, loss = 0.00689737\n",
      "Iteration 37004, loss = 0.00689726\n",
      "Iteration 37005, loss = 0.00689715\n",
      "Iteration 37006, loss = 0.00689704\n",
      "Iteration 37007, loss = 0.00689693\n",
      "Iteration 37008, loss = 0.00689681\n",
      "Iteration 37009, loss = 0.00689670\n",
      "Iteration 37010, loss = 0.00689659\n",
      "Iteration 37011, loss = 0.00689648\n",
      "Iteration 37012, loss = 0.00689637\n",
      "Iteration 37013, loss = 0.00689626\n",
      "Iteration 37014, loss = 0.00689615\n",
      "Iteration 37015, loss = 0.00689604\n",
      "Iteration 37016, loss = 0.00689593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37017, loss = 0.00689582\n",
      "Iteration 37018, loss = 0.00689571\n",
      "Iteration 37019, loss = 0.00689560\n",
      "Iteration 37020, loss = 0.00689549\n",
      "Iteration 37021, loss = 0.00689538\n",
      "Iteration 37022, loss = 0.00689527\n",
      "Iteration 37023, loss = 0.00689515\n",
      "Iteration 37024, loss = 0.00689504\n",
      "Iteration 37025, loss = 0.00689493\n",
      "Iteration 37026, loss = 0.00689482\n",
      "Iteration 37027, loss = 0.00689471\n",
      "Iteration 37028, loss = 0.00689460\n",
      "Iteration 37029, loss = 0.00689449\n",
      "Iteration 37030, loss = 0.00689438\n",
      "Iteration 37031, loss = 0.00689427\n",
      "Iteration 37032, loss = 0.00689416\n",
      "Iteration 37033, loss = 0.00689405\n",
      "Iteration 37034, loss = 0.00689394\n",
      "Iteration 37035, loss = 0.00689383\n",
      "Iteration 37036, loss = 0.00689372\n",
      "Iteration 37037, loss = 0.00689361\n",
      "Iteration 37038, loss = 0.00689350\n",
      "Iteration 37039, loss = 0.00689339\n",
      "Iteration 37040, loss = 0.00689328\n",
      "Iteration 37041, loss = 0.00689317\n",
      "Iteration 37042, loss = 0.00689305\n",
      "Iteration 37043, loss = 0.00689294\n",
      "Iteration 37044, loss = 0.00689283\n",
      "Iteration 37045, loss = 0.00689272\n",
      "Iteration 37046, loss = 0.00689261\n",
      "Iteration 37047, loss = 0.00689250\n",
      "Iteration 37048, loss = 0.00689239\n",
      "Iteration 37049, loss = 0.00689228\n",
      "Iteration 37050, loss = 0.00689217\n",
      "Iteration 37051, loss = 0.00689206\n",
      "Iteration 37052, loss = 0.00689195\n",
      "Iteration 37053, loss = 0.00689184\n",
      "Iteration 37054, loss = 0.00689173\n",
      "Iteration 37055, loss = 0.00689162\n",
      "Iteration 37056, loss = 0.00689151\n",
      "Iteration 37057, loss = 0.00689140\n",
      "Iteration 37058, loss = 0.00689129\n",
      "Iteration 37059, loss = 0.00689118\n",
      "Iteration 37060, loss = 0.00689107\n",
      "Iteration 37061, loss = 0.00689096\n",
      "Iteration 37062, loss = 0.00689085\n",
      "Iteration 37063, loss = 0.00689074\n",
      "Iteration 37064, loss = 0.00689063\n",
      "Iteration 37065, loss = 0.00689052\n",
      "Iteration 37066, loss = 0.00689041\n",
      "Iteration 37067, loss = 0.00689030\n",
      "Iteration 37068, loss = 0.00689019\n",
      "Iteration 37069, loss = 0.00689008\n",
      "Iteration 37070, loss = 0.00688997\n",
      "Iteration 37071, loss = 0.00688986\n",
      "Iteration 37072, loss = 0.00688975\n",
      "Iteration 37073, loss = 0.00688964\n",
      "Iteration 37074, loss = 0.00688953\n",
      "Iteration 37075, loss = 0.00688941\n",
      "Iteration 37076, loss = 0.00688930\n",
      "Iteration 37077, loss = 0.00688919\n",
      "Iteration 37078, loss = 0.00688908\n",
      "Iteration 37079, loss = 0.00688897\n",
      "Iteration 37080, loss = 0.00688886\n",
      "Iteration 37081, loss = 0.00688875\n",
      "Iteration 37082, loss = 0.00688864\n",
      "Iteration 37083, loss = 0.00688853\n",
      "Iteration 37084, loss = 0.00688842\n",
      "Iteration 37085, loss = 0.00688831\n",
      "Iteration 37086, loss = 0.00688820\n",
      "Iteration 37087, loss = 0.00688809\n",
      "Iteration 37088, loss = 0.00688798\n",
      "Iteration 37089, loss = 0.00688787\n",
      "Iteration 37090, loss = 0.00688776\n",
      "Iteration 37091, loss = 0.00688765\n",
      "Iteration 37092, loss = 0.00688754\n",
      "Iteration 37093, loss = 0.00688743\n",
      "Iteration 37094, loss = 0.00688732\n",
      "Iteration 37095, loss = 0.00688721\n",
      "Iteration 37096, loss = 0.00688710\n",
      "Iteration 37097, loss = 0.00688699\n",
      "Iteration 37098, loss = 0.00688688\n",
      "Iteration 37099, loss = 0.00688677\n",
      "Iteration 37100, loss = 0.00688666\n",
      "Iteration 37101, loss = 0.00688655\n",
      "Iteration 37102, loss = 0.00688644\n",
      "Iteration 37103, loss = 0.00688633\n",
      "Iteration 37104, loss = 0.00688622\n",
      "Iteration 37105, loss = 0.00688611\n",
      "Iteration 37106, loss = 0.00688600\n",
      "Iteration 37107, loss = 0.00688589\n",
      "Iteration 37108, loss = 0.00688578\n",
      "Iteration 37109, loss = 0.00688567\n",
      "Iteration 37110, loss = 0.00688556\n",
      "Iteration 37111, loss = 0.00688545\n",
      "Iteration 37112, loss = 0.00688534\n",
      "Iteration 37113, loss = 0.00688523\n",
      "Iteration 37114, loss = 0.00688512\n",
      "Iteration 37115, loss = 0.00688501\n",
      "Iteration 37116, loss = 0.00688490\n",
      "Iteration 37117, loss = 0.00688480\n",
      "Iteration 37118, loss = 0.00688469\n",
      "Iteration 37119, loss = 0.00688458\n",
      "Iteration 37120, loss = 0.00688447\n",
      "Iteration 37121, loss = 0.00688436\n",
      "Iteration 37122, loss = 0.00688425\n",
      "Iteration 37123, loss = 0.00688414\n",
      "Iteration 37124, loss = 0.00688403\n",
      "Iteration 37125, loss = 0.00688392\n",
      "Iteration 37126, loss = 0.00688381\n",
      "Iteration 37127, loss = 0.00688370\n",
      "Iteration 37128, loss = 0.00688359\n",
      "Iteration 37129, loss = 0.00688348\n",
      "Iteration 37130, loss = 0.00688337\n",
      "Iteration 37131, loss = 0.00688326\n",
      "Iteration 37132, loss = 0.00688315\n",
      "Iteration 37133, loss = 0.00688304\n",
      "Iteration 37134, loss = 0.00688293\n",
      "Iteration 37135, loss = 0.00688282\n",
      "Iteration 37136, loss = 0.00688271\n",
      "Iteration 37137, loss = 0.00688260\n",
      "Iteration 37138, loss = 0.00688249\n",
      "Iteration 37139, loss = 0.00688238\n",
      "Iteration 37140, loss = 0.00688227\n",
      "Iteration 37141, loss = 0.00688216\n",
      "Iteration 37142, loss = 0.00688205\n",
      "Iteration 37143, loss = 0.00688194\n",
      "Iteration 37144, loss = 0.00688183\n",
      "Iteration 37145, loss = 0.00688172\n",
      "Iteration 37146, loss = 0.00688161\n",
      "Iteration 37147, loss = 0.00688150\n",
      "Iteration 37148, loss = 0.00688139\n",
      "Iteration 37149, loss = 0.00688128\n",
      "Iteration 37150, loss = 0.00688118\n",
      "Iteration 37151, loss = 0.00688107\n",
      "Iteration 37152, loss = 0.00688096\n",
      "Iteration 37153, loss = 0.00688085\n",
      "Iteration 37154, loss = 0.00688074\n",
      "Iteration 37155, loss = 0.00688063\n",
      "Iteration 37156, loss = 0.00688052\n",
      "Iteration 37157, loss = 0.00688041\n",
      "Iteration 37158, loss = 0.00688030\n",
      "Iteration 37159, loss = 0.00688019\n",
      "Iteration 37160, loss = 0.00688008\n",
      "Iteration 37161, loss = 0.00687997\n",
      "Iteration 37162, loss = 0.00687986\n",
      "Iteration 37163, loss = 0.00687975\n",
      "Iteration 37164, loss = 0.00687964\n",
      "Iteration 37165, loss = 0.00687953\n",
      "Iteration 37166, loss = 0.00687942\n",
      "Iteration 37167, loss = 0.00687931\n",
      "Iteration 37168, loss = 0.00687920\n",
      "Iteration 37169, loss = 0.00687910\n",
      "Iteration 37170, loss = 0.00687899\n",
      "Iteration 37171, loss = 0.00687888\n",
      "Iteration 37172, loss = 0.00687877\n",
      "Iteration 37173, loss = 0.00687866\n",
      "Iteration 37174, loss = 0.00687855\n",
      "Iteration 37175, loss = 0.00687844\n",
      "Iteration 37176, loss = 0.00687833\n",
      "Iteration 37177, loss = 0.00687822\n",
      "Iteration 37178, loss = 0.00687811\n",
      "Iteration 37179, loss = 0.00687800\n",
      "Iteration 37180, loss = 0.00687789\n",
      "Iteration 37181, loss = 0.00687778\n",
      "Iteration 37182, loss = 0.00687767\n",
      "Iteration 37183, loss = 0.00687756\n",
      "Iteration 37184, loss = 0.00687745\n",
      "Iteration 37185, loss = 0.00687735\n",
      "Iteration 37186, loss = 0.00687724\n",
      "Iteration 37187, loss = 0.00687713\n",
      "Iteration 37188, loss = 0.00687702\n",
      "Iteration 37189, loss = 0.00687691\n",
      "Iteration 37190, loss = 0.00687680\n",
      "Iteration 37191, loss = 0.00687669\n",
      "Iteration 37192, loss = 0.00687658\n",
      "Iteration 37193, loss = 0.00687647\n",
      "Iteration 37194, loss = 0.00687636\n",
      "Iteration 37195, loss = 0.00687625\n",
      "Iteration 37196, loss = 0.00687614\n",
      "Iteration 37197, loss = 0.00687603\n",
      "Iteration 37198, loss = 0.00687593\n",
      "Iteration 37199, loss = 0.00687582\n",
      "Iteration 37200, loss = 0.00687571\n",
      "Iteration 37201, loss = 0.00687560\n",
      "Iteration 37202, loss = 0.00687549\n",
      "Iteration 37203, loss = 0.00687538\n",
      "Iteration 37204, loss = 0.00687527\n",
      "Iteration 37205, loss = 0.00687516\n",
      "Iteration 37206, loss = 0.00687505\n",
      "Iteration 37207, loss = 0.00687494\n",
      "Iteration 37208, loss = 0.00687483\n",
      "Iteration 37209, loss = 0.00687473\n",
      "Iteration 37210, loss = 0.00687462\n",
      "Iteration 37211, loss = 0.00687451\n",
      "Iteration 37212, loss = 0.00687440\n",
      "Iteration 37213, loss = 0.00687429\n",
      "Iteration 37214, loss = 0.00687418\n",
      "Iteration 37215, loss = 0.00687407\n",
      "Iteration 37216, loss = 0.00687396\n",
      "Iteration 37217, loss = 0.00687385\n",
      "Iteration 37218, loss = 0.00687374\n",
      "Iteration 37219, loss = 0.00687363\n",
      "Iteration 37220, loss = 0.00687353\n",
      "Iteration 37221, loss = 0.00687342\n",
      "Iteration 37222, loss = 0.00687331\n",
      "Iteration 37223, loss = 0.00687320\n",
      "Iteration 37224, loss = 0.00687309\n",
      "Iteration 37225, loss = 0.00687298\n",
      "Iteration 37226, loss = 0.00687287\n",
      "Iteration 37227, loss = 0.00687276\n",
      "Iteration 37228, loss = 0.00687265\n",
      "Iteration 37229, loss = 0.00687255\n",
      "Iteration 37230, loss = 0.00687244\n",
      "Iteration 37231, loss = 0.00687233\n",
      "Iteration 37232, loss = 0.00687222\n",
      "Iteration 37233, loss = 0.00687211\n",
      "Iteration 37234, loss = 0.00687200\n",
      "Iteration 37235, loss = 0.00687189\n",
      "Iteration 37236, loss = 0.00687178\n",
      "Iteration 37237, loss = 0.00687167\n",
      "Iteration 37238, loss = 0.00687157\n",
      "Iteration 37239, loss = 0.00687146\n",
      "Iteration 37240, loss = 0.00687135\n",
      "Iteration 37241, loss = 0.00687124\n",
      "Iteration 37242, loss = 0.00687113\n",
      "Iteration 37243, loss = 0.00687102\n",
      "Iteration 37244, loss = 0.00687091\n",
      "Iteration 37245, loss = 0.00687080\n",
      "Iteration 37246, loss = 0.00687069\n",
      "Iteration 37247, loss = 0.00687059\n",
      "Iteration 37248, loss = 0.00687048\n",
      "Iteration 37249, loss = 0.00687037\n",
      "Iteration 37250, loss = 0.00687026\n",
      "Iteration 37251, loss = 0.00687015\n",
      "Iteration 37252, loss = 0.00687004\n",
      "Iteration 37253, loss = 0.00686993\n",
      "Iteration 37254, loss = 0.00686982\n",
      "Iteration 37255, loss = 0.00686972\n",
      "Iteration 37256, loss = 0.00686961\n",
      "Iteration 37257, loss = 0.00686950\n",
      "Iteration 37258, loss = 0.00686939\n",
      "Iteration 37259, loss = 0.00686928\n",
      "Iteration 37260, loss = 0.00686917\n",
      "Iteration 37261, loss = 0.00686906\n",
      "Iteration 37262, loss = 0.00686896\n",
      "Iteration 37263, loss = 0.00686885\n",
      "Iteration 37264, loss = 0.00686874\n",
      "Iteration 37265, loss = 0.00686863\n",
      "Iteration 37266, loss = 0.00686852\n",
      "Iteration 37267, loss = 0.00686841\n",
      "Iteration 37268, loss = 0.00686830\n",
      "Iteration 37269, loss = 0.00686819\n",
      "Iteration 37270, loss = 0.00686809\n",
      "Iteration 37271, loss = 0.00686798\n",
      "Iteration 37272, loss = 0.00686787\n",
      "Iteration 37273, loss = 0.00686776\n",
      "Iteration 37274, loss = 0.00686765\n",
      "Iteration 37275, loss = 0.00686754\n",
      "Iteration 37276, loss = 0.00686743\n",
      "Iteration 37277, loss = 0.00686733\n",
      "Iteration 37278, loss = 0.00686722\n",
      "Iteration 37279, loss = 0.00686711\n",
      "Iteration 37280, loss = 0.00686700\n",
      "Iteration 37281, loss = 0.00686689\n",
      "Iteration 37282, loss = 0.00686678\n",
      "Iteration 37283, loss = 0.00686667\n",
      "Iteration 37284, loss = 0.00686657\n",
      "Iteration 37285, loss = 0.00686646\n",
      "Iteration 37286, loss = 0.00686635\n",
      "Iteration 37287, loss = 0.00686624\n",
      "Iteration 37288, loss = 0.00686613\n",
      "Iteration 37289, loss = 0.00686602\n",
      "Iteration 37290, loss = 0.00686592\n",
      "Iteration 37291, loss = 0.00686581\n",
      "Iteration 37292, loss = 0.00686570\n",
      "Iteration 37293, loss = 0.00686559\n",
      "Iteration 37294, loss = 0.00686548\n",
      "Iteration 37295, loss = 0.00686537\n",
      "Iteration 37296, loss = 0.00686527\n",
      "Iteration 37297, loss = 0.00686516\n",
      "Iteration 37298, loss = 0.00686505\n",
      "Iteration 37299, loss = 0.00686494\n",
      "Iteration 37300, loss = 0.00686483\n",
      "Iteration 37301, loss = 0.00686472\n",
      "Iteration 37302, loss = 0.00686462\n",
      "Iteration 37303, loss = 0.00686451\n",
      "Iteration 37304, loss = 0.00686440\n",
      "Iteration 37305, loss = 0.00686429\n",
      "Iteration 37306, loss = 0.00686418\n",
      "Iteration 37307, loss = 0.00686407\n",
      "Iteration 37308, loss = 0.00686397\n",
      "Iteration 37309, loss = 0.00686386\n",
      "Iteration 37310, loss = 0.00686375\n",
      "Iteration 37311, loss = 0.00686364\n",
      "Iteration 37312, loss = 0.00686353\n",
      "Iteration 37313, loss = 0.00686342\n",
      "Iteration 37314, loss = 0.00686332\n",
      "Iteration 37315, loss = 0.00686321\n",
      "Iteration 37316, loss = 0.00686310\n",
      "Iteration 37317, loss = 0.00686299\n",
      "Iteration 37318, loss = 0.00686288\n",
      "Iteration 37319, loss = 0.00686277\n",
      "Iteration 37320, loss = 0.00686267\n",
      "Iteration 37321, loss = 0.00686256\n",
      "Iteration 37322, loss = 0.00686245\n",
      "Iteration 37323, loss = 0.00686234\n",
      "Iteration 37324, loss = 0.00686223\n",
      "Iteration 37325, loss = 0.00686213\n",
      "Iteration 37326, loss = 0.00686202\n",
      "Iteration 37327, loss = 0.00686191\n",
      "Iteration 37328, loss = 0.00686180\n",
      "Iteration 37329, loss = 0.00686169\n",
      "Iteration 37330, loss = 0.00686158\n",
      "Iteration 37331, loss = 0.00686148\n",
      "Iteration 37332, loss = 0.00686137\n",
      "Iteration 37333, loss = 0.00686126\n",
      "Iteration 37334, loss = 0.00686115\n",
      "Iteration 37335, loss = 0.00686104\n",
      "Iteration 37336, loss = 0.00686094\n",
      "Iteration 37337, loss = 0.00686083\n",
      "Iteration 37338, loss = 0.00686072\n",
      "Iteration 37339, loss = 0.00686061\n",
      "Iteration 37340, loss = 0.00686050\n",
      "Iteration 37341, loss = 0.00686040\n",
      "Iteration 37342, loss = 0.00686029\n",
      "Iteration 37343, loss = 0.00686018\n",
      "Iteration 37344, loss = 0.00686007\n",
      "Iteration 37345, loss = 0.00685996\n",
      "Iteration 37346, loss = 0.00685986\n",
      "Iteration 37347, loss = 0.00685975\n",
      "Iteration 37348, loss = 0.00685964\n",
      "Iteration 37349, loss = 0.00685953\n",
      "Iteration 37350, loss = 0.00685942\n",
      "Iteration 37351, loss = 0.00685932\n",
      "Iteration 37352, loss = 0.00685921\n",
      "Iteration 37353, loss = 0.00685910\n",
      "Iteration 37354, loss = 0.00685899\n",
      "Iteration 37355, loss = 0.00685888\n",
      "Iteration 37356, loss = 0.00685878\n",
      "Iteration 37357, loss = 0.00685867\n",
      "Iteration 37358, loss = 0.00685856\n",
      "Iteration 37359, loss = 0.00685845\n",
      "Iteration 37360, loss = 0.00685834\n",
      "Iteration 37361, loss = 0.00685824\n",
      "Iteration 37362, loss = 0.00685813\n",
      "Iteration 37363, loss = 0.00685802\n",
      "Iteration 37364, loss = 0.00685791\n",
      "Iteration 37365, loss = 0.00685781\n",
      "Iteration 37366, loss = 0.00685770\n",
      "Iteration 37367, loss = 0.00685759\n",
      "Iteration 37368, loss = 0.00685748\n",
      "Iteration 37369, loss = 0.00685737\n",
      "Iteration 37370, loss = 0.00685727\n",
      "Iteration 37371, loss = 0.00685716\n",
      "Iteration 37372, loss = 0.00685705\n",
      "Iteration 37373, loss = 0.00685694\n",
      "Iteration 37374, loss = 0.00685683\n",
      "Iteration 37375, loss = 0.00685673\n",
      "Iteration 37376, loss = 0.00685662\n",
      "Iteration 37377, loss = 0.00685651\n",
      "Iteration 37378, loss = 0.00685640\n",
      "Iteration 37379, loss = 0.00685630\n",
      "Iteration 37380, loss = 0.00685619\n",
      "Iteration 37381, loss = 0.00685608\n",
      "Iteration 37382, loss = 0.00685597\n",
      "Iteration 37383, loss = 0.00685587\n",
      "Iteration 37384, loss = 0.00685576\n",
      "Iteration 37385, loss = 0.00685565\n",
      "Iteration 37386, loss = 0.00685554\n",
      "Iteration 37387, loss = 0.00685543\n",
      "Iteration 37388, loss = 0.00685533\n",
      "Iteration 37389, loss = 0.00685522\n",
      "Iteration 37390, loss = 0.00685511\n",
      "Iteration 37391, loss = 0.00685500\n",
      "Iteration 37392, loss = 0.00685490\n",
      "Iteration 37393, loss = 0.00685479\n",
      "Iteration 37394, loss = 0.00685468\n",
      "Iteration 37395, loss = 0.00685457\n",
      "Iteration 37396, loss = 0.00685447\n",
      "Iteration 37397, loss = 0.00685436\n",
      "Iteration 37398, loss = 0.00685425\n",
      "Iteration 37399, loss = 0.00685414\n",
      "Iteration 37400, loss = 0.00685404\n",
      "Iteration 37401, loss = 0.00685393\n",
      "Iteration 37402, loss = 0.00685382\n",
      "Iteration 37403, loss = 0.00685371\n",
      "Iteration 37404, loss = 0.00685361\n",
      "Iteration 37405, loss = 0.00685350\n",
      "Iteration 37406, loss = 0.00685339\n",
      "Iteration 37407, loss = 0.00685328\n",
      "Iteration 37408, loss = 0.00685318\n",
      "Iteration 37409, loss = 0.00685307\n",
      "Iteration 37410, loss = 0.00685296\n",
      "Iteration 37411, loss = 0.00685285\n",
      "Iteration 37412, loss = 0.00685275\n",
      "Iteration 37413, loss = 0.00685264\n",
      "Iteration 37414, loss = 0.00685253\n",
      "Iteration 37415, loss = 0.00685242\n",
      "Iteration 37416, loss = 0.00685232\n",
      "Iteration 37417, loss = 0.00685221\n",
      "Iteration 37418, loss = 0.00685210\n",
      "Iteration 37419, loss = 0.00685199\n",
      "Iteration 37420, loss = 0.00685189\n",
      "Iteration 37421, loss = 0.00685178\n",
      "Iteration 37422, loss = 0.00685167\n",
      "Iteration 37423, loss = 0.00685156\n",
      "Iteration 37424, loss = 0.00685146\n",
      "Iteration 37425, loss = 0.00685135\n",
      "Iteration 37426, loss = 0.00685124\n",
      "Iteration 37427, loss = 0.00685113\n",
      "Iteration 37428, loss = 0.00685103\n",
      "Iteration 37429, loss = 0.00685092\n",
      "Iteration 37430, loss = 0.00685081\n",
      "Iteration 37431, loss = 0.00685070\n",
      "Iteration 37432, loss = 0.00685060\n",
      "Iteration 37433, loss = 0.00685049\n",
      "Iteration 37434, loss = 0.00685038\n",
      "Iteration 37435, loss = 0.00685028\n",
      "Iteration 37436, loss = 0.00685017\n",
      "Iteration 37437, loss = 0.00685006\n",
      "Iteration 37438, loss = 0.00684995\n",
      "Iteration 37439, loss = 0.00684985\n",
      "Iteration 37440, loss = 0.00684974\n",
      "Iteration 37441, loss = 0.00684963\n",
      "Iteration 37442, loss = 0.00684952\n",
      "Iteration 37443, loss = 0.00684942\n",
      "Iteration 37444, loss = 0.00684931\n",
      "Iteration 37445, loss = 0.00684920\n",
      "Iteration 37446, loss = 0.00684910\n",
      "Iteration 37447, loss = 0.00684899\n",
      "Iteration 37448, loss = 0.00684888\n",
      "Iteration 37449, loss = 0.00684877\n",
      "Iteration 37450, loss = 0.00684867\n",
      "Iteration 37451, loss = 0.00684856\n",
      "Iteration 37452, loss = 0.00684845\n",
      "Iteration 37453, loss = 0.00684835\n",
      "Iteration 37454, loss = 0.00684824\n",
      "Iteration 37455, loss = 0.00684813\n",
      "Iteration 37456, loss = 0.00684802\n",
      "Iteration 37457, loss = 0.00684792\n",
      "Iteration 37458, loss = 0.00684781\n",
      "Iteration 37459, loss = 0.00684770\n",
      "Iteration 37460, loss = 0.00684760\n",
      "Iteration 37461, loss = 0.00684749\n",
      "Iteration 37462, loss = 0.00684738\n",
      "Iteration 37463, loss = 0.00684727\n",
      "Iteration 37464, loss = 0.00684717\n",
      "Iteration 37465, loss = 0.00684706\n",
      "Iteration 37466, loss = 0.00684695\n",
      "Iteration 37467, loss = 0.00684685\n",
      "Iteration 37468, loss = 0.00684674\n",
      "Iteration 37469, loss = 0.00684663\n",
      "Iteration 37470, loss = 0.00684653\n",
      "Iteration 37471, loss = 0.00684642\n",
      "Iteration 37472, loss = 0.00684631\n",
      "Iteration 37473, loss = 0.00684620\n",
      "Iteration 37474, loss = 0.00684610\n",
      "Iteration 37475, loss = 0.00684599\n",
      "Iteration 37476, loss = 0.00684588\n",
      "Iteration 37477, loss = 0.00684578\n",
      "Iteration 37478, loss = 0.00684567\n",
      "Iteration 37479, loss = 0.00684556\n",
      "Iteration 37480, loss = 0.00684546\n",
      "Iteration 37481, loss = 0.00684535\n",
      "Iteration 37482, loss = 0.00684524\n",
      "Iteration 37483, loss = 0.00684513\n",
      "Iteration 37484, loss = 0.00684503\n",
      "Iteration 37485, loss = 0.00684492\n",
      "Iteration 37486, loss = 0.00684481\n",
      "Iteration 37487, loss = 0.00684471\n",
      "Iteration 37488, loss = 0.00684460\n",
      "Iteration 37489, loss = 0.00684449\n",
      "Iteration 37490, loss = 0.00684439\n",
      "Iteration 37491, loss = 0.00684428\n",
      "Iteration 37492, loss = 0.00684417\n",
      "Iteration 37493, loss = 0.00684407\n",
      "Iteration 37494, loss = 0.00684396\n",
      "Iteration 37495, loss = 0.00684385\n",
      "Iteration 37496, loss = 0.00684375\n",
      "Iteration 37497, loss = 0.00684364\n",
      "Iteration 37498, loss = 0.00684353\n",
      "Iteration 37499, loss = 0.00684342\n",
      "Iteration 37500, loss = 0.00684332\n",
      "Iteration 37501, loss = 0.00684321\n",
      "Iteration 37502, loss = 0.00684310\n",
      "Iteration 37503, loss = 0.00684300\n",
      "Iteration 37504, loss = 0.00684289\n",
      "Iteration 37505, loss = 0.00684278\n",
      "Iteration 37506, loss = 0.00684268\n",
      "Iteration 37507, loss = 0.00684257\n",
      "Iteration 37508, loss = 0.00684246\n",
      "Iteration 37509, loss = 0.00684236\n",
      "Iteration 37510, loss = 0.00684225\n",
      "Iteration 37511, loss = 0.00684214\n",
      "Iteration 37512, loss = 0.00684204\n",
      "Iteration 37513, loss = 0.00684193\n",
      "Iteration 37514, loss = 0.00684182\n",
      "Iteration 37515, loss = 0.00684172\n",
      "Iteration 37516, loss = 0.00684161\n",
      "Iteration 37517, loss = 0.00684150\n",
      "Iteration 37518, loss = 0.00684140\n",
      "Iteration 37519, loss = 0.00684129\n",
      "Iteration 37520, loss = 0.00684118\n",
      "Iteration 37521, loss = 0.00684108\n",
      "Iteration 37522, loss = 0.00684097\n",
      "Iteration 37523, loss = 0.00684086\n",
      "Iteration 37524, loss = 0.00684076\n",
      "Iteration 37525, loss = 0.00684065\n",
      "Iteration 37526, loss = 0.00684054\n",
      "Iteration 37527, loss = 0.00684044\n",
      "Iteration 37528, loss = 0.00684033\n",
      "Iteration 37529, loss = 0.00684022\n",
      "Iteration 37530, loss = 0.00684012\n",
      "Iteration 37531, loss = 0.00684001\n",
      "Iteration 37532, loss = 0.00683990\n",
      "Iteration 37533, loss = 0.00683980\n",
      "Iteration 37534, loss = 0.00683969\n",
      "Iteration 37535, loss = 0.00683959\n",
      "Iteration 37536, loss = 0.00683948\n",
      "Iteration 37537, loss = 0.00683937\n",
      "Iteration 37538, loss = 0.00683927\n",
      "Iteration 37539, loss = 0.00683916\n",
      "Iteration 37540, loss = 0.00683905\n",
      "Iteration 37541, loss = 0.00683895\n",
      "Iteration 37542, loss = 0.00683884\n",
      "Iteration 37543, loss = 0.00683873\n",
      "Iteration 37544, loss = 0.00683863\n",
      "Iteration 37545, loss = 0.00683852\n",
      "Iteration 37546, loss = 0.00683841\n",
      "Iteration 37547, loss = 0.00683831\n",
      "Iteration 37548, loss = 0.00683820\n",
      "Iteration 37549, loss = 0.00683809\n",
      "Iteration 37550, loss = 0.00683799\n",
      "Iteration 37551, loss = 0.00683788\n",
      "Iteration 37552, loss = 0.00683778\n",
      "Iteration 37553, loss = 0.00683767\n",
      "Iteration 37554, loss = 0.00683756\n",
      "Iteration 37555, loss = 0.00683746\n",
      "Iteration 37556, loss = 0.00683735\n",
      "Iteration 37557, loss = 0.00683724\n",
      "Iteration 37558, loss = 0.00683714\n",
      "Iteration 37559, loss = 0.00683703\n",
      "Iteration 37560, loss = 0.00683692\n",
      "Iteration 37561, loss = 0.00683682\n",
      "Iteration 37562, loss = 0.00683671\n",
      "Iteration 37563, loss = 0.00683661\n",
      "Iteration 37564, loss = 0.00683650\n",
      "Iteration 37565, loss = 0.00683639\n",
      "Iteration 37566, loss = 0.00683629\n",
      "Iteration 37567, loss = 0.00683618\n",
      "Iteration 37568, loss = 0.00683607\n",
      "Iteration 37569, loss = 0.00683597\n",
      "Iteration 37570, loss = 0.00683586\n",
      "Iteration 37571, loss = 0.00683576\n",
      "Iteration 37572, loss = 0.00683565\n",
      "Iteration 37573, loss = 0.00683554\n",
      "Iteration 37574, loss = 0.00683544\n",
      "Iteration 37575, loss = 0.00683533\n",
      "Iteration 37576, loss = 0.00683522\n",
      "Iteration 37577, loss = 0.00683512\n",
      "Iteration 37578, loss = 0.00683501\n",
      "Iteration 37579, loss = 0.00683491\n",
      "Iteration 37580, loss = 0.00683480\n",
      "Iteration 37581, loss = 0.00683469\n",
      "Iteration 37582, loss = 0.00683459\n",
      "Iteration 37583, loss = 0.00683448\n",
      "Iteration 37584, loss = 0.00683438\n",
      "Iteration 37585, loss = 0.00683427\n",
      "Iteration 37586, loss = 0.00683416\n",
      "Iteration 37587, loss = 0.00683406\n",
      "Iteration 37588, loss = 0.00683395\n",
      "Iteration 37589, loss = 0.00683384\n",
      "Iteration 37590, loss = 0.00683374\n",
      "Iteration 37591, loss = 0.00683363\n",
      "Iteration 37592, loss = 0.00683353\n",
      "Iteration 37593, loss = 0.00683342\n",
      "Iteration 37594, loss = 0.00683331\n",
      "Iteration 37595, loss = 0.00683321\n",
      "Iteration 37596, loss = 0.00683310\n",
      "Iteration 37597, loss = 0.00683300\n",
      "Iteration 37598, loss = 0.00683289\n",
      "Iteration 37599, loss = 0.00683278\n",
      "Iteration 37600, loss = 0.00683268\n",
      "Iteration 37601, loss = 0.00683257\n",
      "Iteration 37602, loss = 0.00683247\n",
      "Iteration 37603, loss = 0.00683236\n",
      "Iteration 37604, loss = 0.00683225\n",
      "Iteration 37605, loss = 0.00683215\n",
      "Iteration 37606, loss = 0.00683204\n",
      "Iteration 37607, loss = 0.00683194\n",
      "Iteration 37608, loss = 0.00683183\n",
      "Iteration 37609, loss = 0.00683172\n",
      "Iteration 37610, loss = 0.00683162\n",
      "Iteration 37611, loss = 0.00683151\n",
      "Iteration 37612, loss = 0.00683141\n",
      "Iteration 37613, loss = 0.00683130\n",
      "Iteration 37614, loss = 0.00683120\n",
      "Iteration 37615, loss = 0.00683109\n",
      "Iteration 37616, loss = 0.00683098\n",
      "Iteration 37617, loss = 0.00683088\n",
      "Iteration 37618, loss = 0.00683077\n",
      "Iteration 37619, loss = 0.00683067\n",
      "Iteration 37620, loss = 0.00683056\n",
      "Iteration 37621, loss = 0.00683045\n",
      "Iteration 37622, loss = 0.00683035\n",
      "Iteration 37623, loss = 0.00683024\n",
      "Iteration 37624, loss = 0.00683014\n",
      "Iteration 37625, loss = 0.00683003\n",
      "Iteration 37626, loss = 0.00682992\n",
      "Iteration 37627, loss = 0.00682982\n",
      "Iteration 37628, loss = 0.00682971\n",
      "Iteration 37629, loss = 0.00682961\n",
      "Iteration 37630, loss = 0.00682950\n",
      "Iteration 37631, loss = 0.00682940\n",
      "Iteration 37632, loss = 0.00682929\n",
      "Iteration 37633, loss = 0.00682918\n",
      "Iteration 37634, loss = 0.00682908\n",
      "Iteration 37635, loss = 0.00682897\n",
      "Iteration 37636, loss = 0.00682887\n",
      "Iteration 37637, loss = 0.00682876\n",
      "Iteration 37638, loss = 0.00682866\n",
      "Iteration 37639, loss = 0.00682855\n",
      "Iteration 37640, loss = 0.00682844\n",
      "Iteration 37641, loss = 0.00682834\n",
      "Iteration 37642, loss = 0.00682823\n",
      "Iteration 37643, loss = 0.00682813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37644, loss = 0.00682802\n",
      "Iteration 37645, loss = 0.00682792\n",
      "Iteration 37646, loss = 0.00682781\n",
      "Iteration 37647, loss = 0.00682770\n",
      "Iteration 37648, loss = 0.00682760\n",
      "Iteration 37649, loss = 0.00682749\n",
      "Iteration 37650, loss = 0.00682739\n",
      "Iteration 37651, loss = 0.00682728\n",
      "Iteration 37652, loss = 0.00682718\n",
      "Iteration 37653, loss = 0.00682707\n",
      "Iteration 37654, loss = 0.00682697\n",
      "Iteration 37655, loss = 0.00682686\n",
      "Iteration 37656, loss = 0.00682675\n",
      "Iteration 37657, loss = 0.00682665\n",
      "Iteration 37658, loss = 0.00682654\n",
      "Iteration 37659, loss = 0.00682644\n",
      "Iteration 37660, loss = 0.00682633\n",
      "Iteration 37661, loss = 0.00682623\n",
      "Iteration 37662, loss = 0.00682612\n",
      "Iteration 37663, loss = 0.00682602\n",
      "Iteration 37664, loss = 0.00682591\n",
      "Iteration 37665, loss = 0.00682580\n",
      "Iteration 37666, loss = 0.00682570\n",
      "Iteration 37667, loss = 0.00682559\n",
      "Iteration 37668, loss = 0.00682549\n",
      "Iteration 37669, loss = 0.00682538\n",
      "Iteration 37670, loss = 0.00682528\n",
      "Iteration 37671, loss = 0.00682517\n",
      "Iteration 37672, loss = 0.00682507\n",
      "Iteration 37673, loss = 0.00682496\n",
      "Iteration 37674, loss = 0.00682485\n",
      "Iteration 37675, loss = 0.00682475\n",
      "Iteration 37676, loss = 0.00682464\n",
      "Iteration 37677, loss = 0.00682454\n",
      "Iteration 37678, loss = 0.00682443\n",
      "Iteration 37679, loss = 0.00682433\n",
      "Iteration 37680, loss = 0.00682422\n",
      "Iteration 37681, loss = 0.00682412\n",
      "Iteration 37682, loss = 0.00682401\n",
      "Iteration 37683, loss = 0.00682391\n",
      "Iteration 37684, loss = 0.00682380\n",
      "Iteration 37685, loss = 0.00682370\n",
      "Iteration 37686, loss = 0.00682359\n",
      "Iteration 37687, loss = 0.00682348\n",
      "Iteration 37688, loss = 0.00682338\n",
      "Iteration 37689, loss = 0.00682327\n",
      "Iteration 37690, loss = 0.00682317\n",
      "Iteration 37691, loss = 0.00682306\n",
      "Iteration 37692, loss = 0.00682296\n",
      "Iteration 37693, loss = 0.00682285\n",
      "Iteration 37694, loss = 0.00682275\n",
      "Iteration 37695, loss = 0.00682264\n",
      "Iteration 37696, loss = 0.00682254\n",
      "Iteration 37697, loss = 0.00682243\n",
      "Iteration 37698, loss = 0.00682233\n",
      "Iteration 37699, loss = 0.00682222\n",
      "Iteration 37700, loss = 0.00682212\n",
      "Iteration 37701, loss = 0.00682201\n",
      "Iteration 37702, loss = 0.00682191\n",
      "Iteration 37703, loss = 0.00682180\n",
      "Iteration 37704, loss = 0.00682169\n",
      "Iteration 37705, loss = 0.00682159\n",
      "Iteration 37706, loss = 0.00682148\n",
      "Iteration 37707, loss = 0.00682138\n",
      "Iteration 37708, loss = 0.00682127\n",
      "Iteration 37709, loss = 0.00682117\n",
      "Iteration 37710, loss = 0.00682106\n",
      "Iteration 37711, loss = 0.00682096\n",
      "Iteration 37712, loss = 0.00682085\n",
      "Iteration 37713, loss = 0.00682075\n",
      "Iteration 37714, loss = 0.00682064\n",
      "Iteration 37715, loss = 0.00682054\n",
      "Iteration 37716, loss = 0.00682043\n",
      "Iteration 37717, loss = 0.00682033\n",
      "Iteration 37718, loss = 0.00682022\n",
      "Iteration 37719, loss = 0.00682012\n",
      "Iteration 37720, loss = 0.00682001\n",
      "Iteration 37721, loss = 0.00681991\n",
      "Iteration 37722, loss = 0.00681980\n",
      "Iteration 37723, loss = 0.00681970\n",
      "Iteration 37724, loss = 0.00681959\n",
      "Iteration 37725, loss = 0.00681949\n",
      "Iteration 37726, loss = 0.00681938\n",
      "Iteration 37727, loss = 0.00681928\n",
      "Iteration 37728, loss = 0.00681917\n",
      "Iteration 37729, loss = 0.00681907\n",
      "Iteration 37730, loss = 0.00681896\n",
      "Iteration 37731, loss = 0.00681886\n",
      "Iteration 37732, loss = 0.00681875\n",
      "Iteration 37733, loss = 0.00681865\n",
      "Iteration 37734, loss = 0.00681854\n",
      "Iteration 37735, loss = 0.00681844\n",
      "Iteration 37736, loss = 0.00681833\n",
      "Iteration 37737, loss = 0.00681823\n",
      "Iteration 37738, loss = 0.00681812\n",
      "Iteration 37739, loss = 0.00681802\n",
      "Iteration 37740, loss = 0.00681791\n",
      "Iteration 37741, loss = 0.00681781\n",
      "Iteration 37742, loss = 0.00681770\n",
      "Iteration 37743, loss = 0.00681760\n",
      "Iteration 37744, loss = 0.00681749\n",
      "Iteration 37745, loss = 0.00681739\n",
      "Iteration 37746, loss = 0.00681728\n",
      "Iteration 37747, loss = 0.00681718\n",
      "Iteration 37748, loss = 0.00681707\n",
      "Iteration 37749, loss = 0.00681697\n",
      "Iteration 37750, loss = 0.00681686\n",
      "Iteration 37751, loss = 0.00681676\n",
      "Iteration 37752, loss = 0.00681665\n",
      "Iteration 37753, loss = 0.00681655\n",
      "Iteration 37754, loss = 0.00681644\n",
      "Iteration 37755, loss = 0.00681634\n",
      "Iteration 37756, loss = 0.00681623\n",
      "Iteration 37757, loss = 0.00681613\n",
      "Iteration 37758, loss = 0.00681602\n",
      "Iteration 37759, loss = 0.00681592\n",
      "Iteration 37760, loss = 0.00681582\n",
      "Iteration 37761, loss = 0.00681571\n",
      "Iteration 37762, loss = 0.00681561\n",
      "Iteration 37763, loss = 0.00681550\n",
      "Iteration 37764, loss = 0.00681540\n",
      "Iteration 37765, loss = 0.00681529\n",
      "Iteration 37766, loss = 0.00681519\n",
      "Iteration 37767, loss = 0.00681508\n",
      "Iteration 37768, loss = 0.00681498\n",
      "Iteration 37769, loss = 0.00681487\n",
      "Iteration 37770, loss = 0.00681477\n",
      "Iteration 37771, loss = 0.00681466\n",
      "Iteration 37772, loss = 0.00681456\n",
      "Iteration 37773, loss = 0.00681445\n",
      "Iteration 37774, loss = 0.00681435\n",
      "Iteration 37775, loss = 0.00681424\n",
      "Iteration 37776, loss = 0.00681414\n",
      "Iteration 37777, loss = 0.00681403\n",
      "Iteration 37778, loss = 0.00681393\n",
      "Iteration 37779, loss = 0.00681383\n",
      "Iteration 37780, loss = 0.00681372\n",
      "Iteration 37781, loss = 0.00681362\n",
      "Iteration 37782, loss = 0.00681351\n",
      "Iteration 37783, loss = 0.00681341\n",
      "Iteration 37784, loss = 0.00681330\n",
      "Iteration 37785, loss = 0.00681320\n",
      "Iteration 37786, loss = 0.00681309\n",
      "Iteration 37787, loss = 0.00681299\n",
      "Iteration 37788, loss = 0.00681288\n",
      "Iteration 37789, loss = 0.00681278\n",
      "Iteration 37790, loss = 0.00681267\n",
      "Iteration 37791, loss = 0.00681257\n",
      "Iteration 37792, loss = 0.00681247\n",
      "Iteration 37793, loss = 0.00681236\n",
      "Iteration 37794, loss = 0.00681226\n",
      "Iteration 37795, loss = 0.00681215\n",
      "Iteration 37796, loss = 0.00681205\n",
      "Iteration 37797, loss = 0.00681194\n",
      "Iteration 37798, loss = 0.00681184\n",
      "Iteration 37799, loss = 0.00681173\n",
      "Iteration 37800, loss = 0.00681163\n",
      "Iteration 37801, loss = 0.00681153\n",
      "Iteration 37802, loss = 0.00681142\n",
      "Iteration 37803, loss = 0.00681132\n",
      "Iteration 37804, loss = 0.00681121\n",
      "Iteration 37805, loss = 0.00681111\n",
      "Iteration 37806, loss = 0.00681100\n",
      "Iteration 37807, loss = 0.00681090\n",
      "Iteration 37808, loss = 0.00681079\n",
      "Iteration 37809, loss = 0.00681069\n",
      "Iteration 37810, loss = 0.00681059\n",
      "Iteration 37811, loss = 0.00681048\n",
      "Iteration 37812, loss = 0.00681038\n",
      "Iteration 37813, loss = 0.00681027\n",
      "Iteration 37814, loss = 0.00681017\n",
      "Iteration 37815, loss = 0.00681006\n",
      "Iteration 37816, loss = 0.00680996\n",
      "Iteration 37817, loss = 0.00680985\n",
      "Iteration 37818, loss = 0.00680975\n",
      "Iteration 37819, loss = 0.00680965\n",
      "Iteration 37820, loss = 0.00680954\n",
      "Iteration 37821, loss = 0.00680944\n",
      "Iteration 37822, loss = 0.00680933\n",
      "Iteration 37823, loss = 0.00680923\n",
      "Iteration 37824, loss = 0.00680912\n",
      "Iteration 37825, loss = 0.00680902\n",
      "Iteration 37826, loss = 0.00680892\n",
      "Iteration 37827, loss = 0.00680881\n",
      "Iteration 37828, loss = 0.00680871\n",
      "Iteration 37829, loss = 0.00680860\n",
      "Iteration 37830, loss = 0.00680850\n",
      "Iteration 37831, loss = 0.00680839\n",
      "Iteration 37832, loss = 0.00680829\n",
      "Iteration 37833, loss = 0.00680819\n",
      "Iteration 37834, loss = 0.00680808\n",
      "Iteration 37835, loss = 0.00680798\n",
      "Iteration 37836, loss = 0.00680787\n",
      "Iteration 37837, loss = 0.00680777\n",
      "Iteration 37838, loss = 0.00680766\n",
      "Iteration 37839, loss = 0.00680756\n",
      "Iteration 37840, loss = 0.00680746\n",
      "Iteration 37841, loss = 0.00680735\n",
      "Iteration 37842, loss = 0.00680725\n",
      "Iteration 37843, loss = 0.00680714\n",
      "Iteration 37844, loss = 0.00680704\n",
      "Iteration 37845, loss = 0.00680694\n",
      "Iteration 37846, loss = 0.00680683\n",
      "Iteration 37847, loss = 0.00680673\n",
      "Iteration 37848, loss = 0.00680662\n",
      "Iteration 37849, loss = 0.00680652\n",
      "Iteration 37850, loss = 0.00680642\n",
      "Iteration 37851, loss = 0.00680631\n",
      "Iteration 37852, loss = 0.00680621\n",
      "Iteration 37853, loss = 0.00680610\n",
      "Iteration 37854, loss = 0.00680600\n",
      "Iteration 37855, loss = 0.00680589\n",
      "Iteration 37856, loss = 0.00680579\n",
      "Iteration 37857, loss = 0.00680569\n",
      "Iteration 37858, loss = 0.00680558\n",
      "Iteration 37859, loss = 0.00680548\n",
      "Iteration 37860, loss = 0.00680537\n",
      "Iteration 37861, loss = 0.00680527\n",
      "Iteration 37862, loss = 0.00680517\n",
      "Iteration 37863, loss = 0.00680506\n",
      "Iteration 37864, loss = 0.00680496\n",
      "Iteration 37865, loss = 0.00680485\n",
      "Iteration 37866, loss = 0.00680475\n",
      "Iteration 37867, loss = 0.00680465\n",
      "Iteration 37868, loss = 0.00680454\n",
      "Iteration 37869, loss = 0.00680444\n",
      "Iteration 37870, loss = 0.00680433\n",
      "Iteration 37871, loss = 0.00680423\n",
      "Iteration 37872, loss = 0.00680413\n",
      "Iteration 37873, loss = 0.00680402\n",
      "Iteration 37874, loss = 0.00680392\n",
      "Iteration 37875, loss = 0.00680381\n",
      "Iteration 37876, loss = 0.00680371\n",
      "Iteration 37877, loss = 0.00680361\n",
      "Iteration 37878, loss = 0.00680350\n",
      "Iteration 37879, loss = 0.00680340\n",
      "Iteration 37880, loss = 0.00680330\n",
      "Iteration 37881, loss = 0.00680319\n",
      "Iteration 37882, loss = 0.00680309\n",
      "Iteration 37883, loss = 0.00680298\n",
      "Iteration 37884, loss = 0.00680288\n",
      "Iteration 37885, loss = 0.00680278\n",
      "Iteration 37886, loss = 0.00680267\n",
      "Iteration 37887, loss = 0.00680257\n",
      "Iteration 37888, loss = 0.00680246\n",
      "Iteration 37889, loss = 0.00680236\n",
      "Iteration 37890, loss = 0.00680226\n",
      "Iteration 37891, loss = 0.00680215\n",
      "Iteration 37892, loss = 0.00680205\n",
      "Iteration 37893, loss = 0.00680195\n",
      "Iteration 37894, loss = 0.00680184\n",
      "Iteration 37895, loss = 0.00680174\n",
      "Iteration 37896, loss = 0.00680163\n",
      "Iteration 37897, loss = 0.00680153\n",
      "Iteration 37898, loss = 0.00680143\n",
      "Iteration 37899, loss = 0.00680132\n",
      "Iteration 37900, loss = 0.00680122\n",
      "Iteration 37901, loss = 0.00680112\n",
      "Iteration 37902, loss = 0.00680101\n",
      "Iteration 37903, loss = 0.00680091\n",
      "Iteration 37904, loss = 0.00680080\n",
      "Iteration 37905, loss = 0.00680070\n",
      "Iteration 37906, loss = 0.00680060\n",
      "Iteration 37907, loss = 0.00680049\n",
      "Iteration 37908, loss = 0.00680039\n",
      "Iteration 37909, loss = 0.00680029\n",
      "Iteration 37910, loss = 0.00680018\n",
      "Iteration 37911, loss = 0.00680008\n",
      "Iteration 37912, loss = 0.00679997\n",
      "Iteration 37913, loss = 0.00679987\n",
      "Iteration 37914, loss = 0.00679977\n",
      "Iteration 37915, loss = 0.00679966\n",
      "Iteration 37916, loss = 0.00679956\n",
      "Iteration 37917, loss = 0.00679946\n",
      "Iteration 37918, loss = 0.00679935\n",
      "Iteration 37919, loss = 0.00679925\n",
      "Iteration 37920, loss = 0.00679915\n",
      "Iteration 37921, loss = 0.00679904\n",
      "Iteration 37922, loss = 0.00679894\n",
      "Iteration 37923, loss = 0.00679884\n",
      "Iteration 37924, loss = 0.00679873\n",
      "Iteration 37925, loss = 0.00679863\n",
      "Iteration 37926, loss = 0.00679852\n",
      "Iteration 37927, loss = 0.00679842\n",
      "Iteration 37928, loss = 0.00679832\n",
      "Iteration 37929, loss = 0.00679821\n",
      "Iteration 37930, loss = 0.00679811\n",
      "Iteration 37931, loss = 0.00679801\n",
      "Iteration 37932, loss = 0.00679790\n",
      "Iteration 37933, loss = 0.00679780\n",
      "Iteration 37934, loss = 0.00679770\n",
      "Iteration 37935, loss = 0.00679759\n",
      "Iteration 37936, loss = 0.00679749\n",
      "Iteration 37937, loss = 0.00679739\n",
      "Iteration 37938, loss = 0.00679728\n",
      "Iteration 37939, loss = 0.00679718\n",
      "Iteration 37940, loss = 0.00679708\n",
      "Iteration 37941, loss = 0.00679697\n",
      "Iteration 37942, loss = 0.00679687\n",
      "Iteration 37943, loss = 0.00679677\n",
      "Iteration 37944, loss = 0.00679666\n",
      "Iteration 37945, loss = 0.00679656\n",
      "Iteration 37946, loss = 0.00679646\n",
      "Iteration 37947, loss = 0.00679635\n",
      "Iteration 37948, loss = 0.00679625\n",
      "Iteration 37949, loss = 0.00679615\n",
      "Iteration 37950, loss = 0.00679604\n",
      "Iteration 37951, loss = 0.00679594\n",
      "Iteration 37952, loss = 0.00679584\n",
      "Iteration 37953, loss = 0.00679573\n",
      "Iteration 37954, loss = 0.00679563\n",
      "Iteration 37955, loss = 0.00679553\n",
      "Iteration 37956, loss = 0.00679542\n",
      "Iteration 37957, loss = 0.00679532\n",
      "Iteration 37958, loss = 0.00679522\n",
      "Iteration 37959, loss = 0.00679511\n",
      "Iteration 37960, loss = 0.00679501\n",
      "Iteration 37961, loss = 0.00679491\n",
      "Iteration 37962, loss = 0.00679480\n",
      "Iteration 37963, loss = 0.00679470\n",
      "Iteration 37964, loss = 0.00679460\n",
      "Iteration 37965, loss = 0.00679449\n",
      "Iteration 37966, loss = 0.00679439\n",
      "Iteration 37967, loss = 0.00679429\n",
      "Iteration 37968, loss = 0.00679418\n",
      "Iteration 37969, loss = 0.00679408\n",
      "Iteration 37970, loss = 0.00679398\n",
      "Iteration 37971, loss = 0.00679387\n",
      "Iteration 37972, loss = 0.00679377\n",
      "Iteration 37973, loss = 0.00679367\n",
      "Iteration 37974, loss = 0.00679356\n",
      "Iteration 37975, loss = 0.00679346\n",
      "Iteration 37976, loss = 0.00679336\n",
      "Iteration 37977, loss = 0.00679325\n",
      "Iteration 37978, loss = 0.00679315\n",
      "Iteration 37979, loss = 0.00679305\n",
      "Iteration 37980, loss = 0.00679294\n",
      "Iteration 37981, loss = 0.00679284\n",
      "Iteration 37982, loss = 0.00679274\n",
      "Iteration 37983, loss = 0.00679264\n",
      "Iteration 37984, loss = 0.00679253\n",
      "Iteration 37985, loss = 0.00679243\n",
      "Iteration 37986, loss = 0.00679233\n",
      "Iteration 37987, loss = 0.00679222\n",
      "Iteration 37988, loss = 0.00679212\n",
      "Iteration 37989, loss = 0.00679202\n",
      "Iteration 37990, loss = 0.00679191\n",
      "Iteration 37991, loss = 0.00679181\n",
      "Iteration 37992, loss = 0.00679171\n",
      "Iteration 37993, loss = 0.00679160\n",
      "Iteration 37994, loss = 0.00679150\n",
      "Iteration 37995, loss = 0.00679140\n",
      "Iteration 37996, loss = 0.00679130\n",
      "Iteration 37997, loss = 0.00679119\n",
      "Iteration 37998, loss = 0.00679109\n",
      "Iteration 37999, loss = 0.00679099\n",
      "Iteration 38000, loss = 0.00679088\n",
      "Iteration 38001, loss = 0.00679078\n",
      "Iteration 38002, loss = 0.00679068\n",
      "Iteration 38003, loss = 0.00679057\n",
      "Iteration 38004, loss = 0.00679047\n",
      "Iteration 38005, loss = 0.00679037\n",
      "Iteration 38006, loss = 0.00679027\n",
      "Iteration 38007, loss = 0.00679016\n",
      "Iteration 38008, loss = 0.00679006\n",
      "Iteration 38009, loss = 0.00678996\n",
      "Iteration 38010, loss = 0.00678985\n",
      "Iteration 38011, loss = 0.00678975\n",
      "Iteration 38012, loss = 0.00678965\n",
      "Iteration 38013, loss = 0.00678955\n",
      "Iteration 38014, loss = 0.00678944\n",
      "Iteration 38015, loss = 0.00678934\n",
      "Iteration 38016, loss = 0.00678924\n",
      "Iteration 38017, loss = 0.00678913\n",
      "Iteration 38018, loss = 0.00678903\n",
      "Iteration 38019, loss = 0.00678893\n",
      "Iteration 38020, loss = 0.00678883\n",
      "Iteration 38021, loss = 0.00678872\n",
      "Iteration 38022, loss = 0.00678862\n",
      "Iteration 38023, loss = 0.00678852\n",
      "Iteration 38024, loss = 0.00678841\n",
      "Iteration 38025, loss = 0.00678831\n",
      "Iteration 38026, loss = 0.00678821\n",
      "Iteration 38027, loss = 0.00678811\n",
      "Iteration 38028, loss = 0.00678800\n",
      "Iteration 38029, loss = 0.00678790\n",
      "Iteration 38030, loss = 0.00678780\n",
      "Iteration 38031, loss = 0.00678769\n",
      "Iteration 38032, loss = 0.00678759\n",
      "Iteration 38033, loss = 0.00678749\n",
      "Iteration 38034, loss = 0.00678739\n",
      "Iteration 38035, loss = 0.00678728\n",
      "Iteration 38036, loss = 0.00678718\n",
      "Iteration 38037, loss = 0.00678708\n",
      "Iteration 38038, loss = 0.00678698\n",
      "Iteration 38039, loss = 0.00678687\n",
      "Iteration 38040, loss = 0.00678677\n",
      "Iteration 38041, loss = 0.00678667\n",
      "Iteration 38042, loss = 0.00678656\n",
      "Iteration 38043, loss = 0.00678646\n",
      "Iteration 38044, loss = 0.00678636\n",
      "Iteration 38045, loss = 0.00678626\n",
      "Iteration 38046, loss = 0.00678615\n",
      "Iteration 38047, loss = 0.00678605\n",
      "Iteration 38048, loss = 0.00678595\n",
      "Iteration 38049, loss = 0.00678585\n",
      "Iteration 38050, loss = 0.00678574\n",
      "Iteration 38051, loss = 0.00678564\n",
      "Iteration 38052, loss = 0.00678554\n",
      "Iteration 38053, loss = 0.00678544\n",
      "Iteration 38054, loss = 0.00678533\n",
      "Iteration 38055, loss = 0.00678523\n",
      "Iteration 38056, loss = 0.00678513\n",
      "Iteration 38057, loss = 0.00678503\n",
      "Iteration 38058, loss = 0.00678492\n",
      "Iteration 38059, loss = 0.00678482\n",
      "Iteration 38060, loss = 0.00678472\n",
      "Iteration 38061, loss = 0.00678462\n",
      "Iteration 38062, loss = 0.00678451\n",
      "Iteration 38063, loss = 0.00678441\n",
      "Iteration 38064, loss = 0.00678431\n",
      "Iteration 38065, loss = 0.00678421\n",
      "Iteration 38066, loss = 0.00678410\n",
      "Iteration 38067, loss = 0.00678400\n",
      "Iteration 38068, loss = 0.00678390\n",
      "Iteration 38069, loss = 0.00678380\n",
      "Iteration 38070, loss = 0.00678369\n",
      "Iteration 38071, loss = 0.00678359\n",
      "Iteration 38072, loss = 0.00678349\n",
      "Iteration 38073, loss = 0.00678339\n",
      "Iteration 38074, loss = 0.00678328\n",
      "Iteration 38075, loss = 0.00678318\n",
      "Iteration 38076, loss = 0.00678308\n",
      "Iteration 38077, loss = 0.00678298\n",
      "Iteration 38078, loss = 0.00678287\n",
      "Iteration 38079, loss = 0.00678277\n",
      "Iteration 38080, loss = 0.00678267\n",
      "Iteration 38081, loss = 0.00678257\n",
      "Iteration 38082, loss = 0.00678246\n",
      "Iteration 38083, loss = 0.00678236\n",
      "Iteration 38084, loss = 0.00678226\n",
      "Iteration 38085, loss = 0.00678216\n",
      "Iteration 38086, loss = 0.00678205\n",
      "Iteration 38087, loss = 0.00678195\n",
      "Iteration 38088, loss = 0.00678185\n",
      "Iteration 38089, loss = 0.00678175\n",
      "Iteration 38090, loss = 0.00678164\n",
      "Iteration 38091, loss = 0.00678154\n",
      "Iteration 38092, loss = 0.00678144\n",
      "Iteration 38093, loss = 0.00678134\n",
      "Iteration 38094, loss = 0.00678124\n",
      "Iteration 38095, loss = 0.00678113\n",
      "Iteration 38096, loss = 0.00678103\n",
      "Iteration 38097, loss = 0.00678093\n",
      "Iteration 38098, loss = 0.00678083\n",
      "Iteration 38099, loss = 0.00678072\n",
      "Iteration 38100, loss = 0.00678062\n",
      "Iteration 38101, loss = 0.00678052\n",
      "Iteration 38102, loss = 0.00678042\n",
      "Iteration 38103, loss = 0.00678032\n",
      "Iteration 38104, loss = 0.00678021\n",
      "Iteration 38105, loss = 0.00678011\n",
      "Iteration 38106, loss = 0.00678001\n",
      "Iteration 38107, loss = 0.00677991\n",
      "Iteration 38108, loss = 0.00677980\n",
      "Iteration 38109, loss = 0.00677970\n",
      "Iteration 38110, loss = 0.00677960\n",
      "Iteration 38111, loss = 0.00677950\n",
      "Iteration 38112, loss = 0.00677940\n",
      "Iteration 38113, loss = 0.00677929\n",
      "Iteration 38114, loss = 0.00677919\n",
      "Iteration 38115, loss = 0.00677909\n",
      "Iteration 38116, loss = 0.00677899\n",
      "Iteration 38117, loss = 0.00677888\n",
      "Iteration 38118, loss = 0.00677878\n",
      "Iteration 38119, loss = 0.00677868\n",
      "Iteration 38120, loss = 0.00677858\n",
      "Iteration 38121, loss = 0.00677848\n",
      "Iteration 38122, loss = 0.00677837\n",
      "Iteration 38123, loss = 0.00677827\n",
      "Iteration 38124, loss = 0.00677817\n",
      "Iteration 38125, loss = 0.00677807\n",
      "Iteration 38126, loss = 0.00677797\n",
      "Iteration 38127, loss = 0.00677786\n",
      "Iteration 38128, loss = 0.00677776\n",
      "Iteration 38129, loss = 0.00677766\n",
      "Iteration 38130, loss = 0.00677756\n",
      "Iteration 38131, loss = 0.00677746\n",
      "Iteration 38132, loss = 0.00677735\n",
      "Iteration 38133, loss = 0.00677725\n",
      "Iteration 38134, loss = 0.00677715\n",
      "Iteration 38135, loss = 0.00677705\n",
      "Iteration 38136, loss = 0.00677695\n",
      "Iteration 38137, loss = 0.00677684\n",
      "Iteration 38138, loss = 0.00677674\n",
      "Iteration 38139, loss = 0.00677664\n",
      "Iteration 38140, loss = 0.00677654\n",
      "Iteration 38141, loss = 0.00677644\n",
      "Iteration 38142, loss = 0.00677633\n",
      "Iteration 38143, loss = 0.00677623\n",
      "Iteration 38144, loss = 0.00677613\n",
      "Iteration 38145, loss = 0.00677603\n",
      "Iteration 38146, loss = 0.00677593\n",
      "Iteration 38147, loss = 0.00677582\n",
      "Iteration 38148, loss = 0.00677572\n",
      "Iteration 38149, loss = 0.00677562\n",
      "Iteration 38150, loss = 0.00677552\n",
      "Iteration 38151, loss = 0.00677542\n",
      "Iteration 38152, loss = 0.00677531\n",
      "Iteration 38153, loss = 0.00677521\n",
      "Iteration 38154, loss = 0.00677511\n",
      "Iteration 38155, loss = 0.00677501\n",
      "Iteration 38156, loss = 0.00677491\n",
      "Iteration 38157, loss = 0.00677481\n",
      "Iteration 38158, loss = 0.00677470\n",
      "Iteration 38159, loss = 0.00677460\n",
      "Iteration 38160, loss = 0.00677450\n",
      "Iteration 38161, loss = 0.00677440\n",
      "Iteration 38162, loss = 0.00677430\n",
      "Iteration 38163, loss = 0.00677419\n",
      "Iteration 38164, loss = 0.00677409\n",
      "Iteration 38165, loss = 0.00677399\n",
      "Iteration 38166, loss = 0.00677389\n",
      "Iteration 38167, loss = 0.00677379\n",
      "Iteration 38168, loss = 0.00677369\n",
      "Iteration 38169, loss = 0.00677358\n",
      "Iteration 38170, loss = 0.00677348\n",
      "Iteration 38171, loss = 0.00677338\n",
      "Iteration 38172, loss = 0.00677328\n",
      "Iteration 38173, loss = 0.00677318\n",
      "Iteration 38174, loss = 0.00677308\n",
      "Iteration 38175, loss = 0.00677297\n",
      "Iteration 38176, loss = 0.00677287\n",
      "Iteration 38177, loss = 0.00677277\n",
      "Iteration 38178, loss = 0.00677267\n",
      "Iteration 38179, loss = 0.00677257\n",
      "Iteration 38180, loss = 0.00677247\n",
      "Iteration 38181, loss = 0.00677236\n",
      "Iteration 38182, loss = 0.00677226\n",
      "Iteration 38183, loss = 0.00677216\n",
      "Iteration 38184, loss = 0.00677206\n",
      "Iteration 38185, loss = 0.00677196\n",
      "Iteration 38186, loss = 0.00677186\n",
      "Iteration 38187, loss = 0.00677175\n",
      "Iteration 38188, loss = 0.00677165\n",
      "Iteration 38189, loss = 0.00677155\n",
      "Iteration 38190, loss = 0.00677145\n",
      "Iteration 38191, loss = 0.00677135\n",
      "Iteration 38192, loss = 0.00677125\n",
      "Iteration 38193, loss = 0.00677114\n",
      "Iteration 38194, loss = 0.00677104\n",
      "Iteration 38195, loss = 0.00677094\n",
      "Iteration 38196, loss = 0.00677084\n",
      "Iteration 38197, loss = 0.00677074\n",
      "Iteration 38198, loss = 0.00677064\n",
      "Iteration 38199, loss = 0.00677054\n",
      "Iteration 38200, loss = 0.00677043\n",
      "Iteration 38201, loss = 0.00677033\n",
      "Iteration 38202, loss = 0.00677023\n",
      "Iteration 38203, loss = 0.00677013\n",
      "Iteration 38204, loss = 0.00677003\n",
      "Iteration 38205, loss = 0.00676993\n",
      "Iteration 38206, loss = 0.00676982\n",
      "Iteration 38207, loss = 0.00676972\n",
      "Iteration 38208, loss = 0.00676962\n",
      "Iteration 38209, loss = 0.00676952\n",
      "Iteration 38210, loss = 0.00676942\n",
      "Iteration 38211, loss = 0.00676932\n",
      "Iteration 38212, loss = 0.00676922\n",
      "Iteration 38213, loss = 0.00676911\n",
      "Iteration 38214, loss = 0.00676901\n",
      "Iteration 38215, loss = 0.00676891\n",
      "Iteration 38216, loss = 0.00676881\n",
      "Iteration 38217, loss = 0.00676871\n",
      "Iteration 38218, loss = 0.00676861\n",
      "Iteration 38219, loss = 0.00676851\n",
      "Iteration 38220, loss = 0.00676840\n",
      "Iteration 38221, loss = 0.00676830\n",
      "Iteration 38222, loss = 0.00676820\n",
      "Iteration 38223, loss = 0.00676810\n",
      "Iteration 38224, loss = 0.00676800\n",
      "Iteration 38225, loss = 0.00676790\n",
      "Iteration 38226, loss = 0.00676780\n",
      "Iteration 38227, loss = 0.00676770\n",
      "Iteration 38228, loss = 0.00676759\n",
      "Iteration 38229, loss = 0.00676749\n",
      "Iteration 38230, loss = 0.00676739\n",
      "Iteration 38231, loss = 0.00676729\n",
      "Iteration 38232, loss = 0.00676719\n",
      "Iteration 38233, loss = 0.00676709\n",
      "Iteration 38234, loss = 0.00676699\n",
      "Iteration 38235, loss = 0.00676689\n",
      "Iteration 38236, loss = 0.00676678\n",
      "Iteration 38237, loss = 0.00676668\n",
      "Iteration 38238, loss = 0.00676658\n",
      "Iteration 38239, loss = 0.00676648\n",
      "Iteration 38240, loss = 0.00676638\n",
      "Iteration 38241, loss = 0.00676628\n",
      "Iteration 38242, loss = 0.00676618\n",
      "Iteration 38243, loss = 0.00676608\n",
      "Iteration 38244, loss = 0.00676597\n",
      "Iteration 38245, loss = 0.00676587\n",
      "Iteration 38246, loss = 0.00676577\n",
      "Iteration 38247, loss = 0.00676567\n",
      "Iteration 38248, loss = 0.00676557\n",
      "Iteration 38249, loss = 0.00676547\n",
      "Iteration 38250, loss = 0.00676537\n",
      "Iteration 38251, loss = 0.00676527\n",
      "Iteration 38252, loss = 0.00676516\n",
      "Iteration 38253, loss = 0.00676506\n",
      "Iteration 38254, loss = 0.00676496\n",
      "Iteration 38255, loss = 0.00676486\n",
      "Iteration 38256, loss = 0.00676476\n",
      "Iteration 38257, loss = 0.00676466\n",
      "Iteration 38258, loss = 0.00676456\n",
      "Iteration 38259, loss = 0.00676446\n",
      "Iteration 38260, loss = 0.00676436\n",
      "Iteration 38261, loss = 0.00676425\n",
      "Iteration 38262, loss = 0.00676415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38263, loss = 0.00676405\n",
      "Iteration 38264, loss = 0.00676395\n",
      "Iteration 38265, loss = 0.00676385\n",
      "Iteration 38266, loss = 0.00676375\n",
      "Iteration 38267, loss = 0.00676365\n",
      "Iteration 38268, loss = 0.00676355\n",
      "Iteration 38269, loss = 0.00676345\n",
      "Iteration 38270, loss = 0.00676335\n",
      "Iteration 38271, loss = 0.00676324\n",
      "Iteration 38272, loss = 0.00676314\n",
      "Iteration 38273, loss = 0.00676304\n",
      "Iteration 38274, loss = 0.00676294\n",
      "Iteration 38275, loss = 0.00676284\n",
      "Iteration 38276, loss = 0.00676274\n",
      "Iteration 38277, loss = 0.00676264\n",
      "Iteration 38278, loss = 0.00676254\n",
      "Iteration 38279, loss = 0.00676244\n",
      "Iteration 38280, loss = 0.00676234\n",
      "Iteration 38281, loss = 0.00676223\n",
      "Iteration 38282, loss = 0.00676213\n",
      "Iteration 38283, loss = 0.00676203\n",
      "Iteration 38284, loss = 0.00676193\n",
      "Iteration 38285, loss = 0.00676183\n",
      "Iteration 38286, loss = 0.00676173\n",
      "Iteration 38287, loss = 0.00676163\n",
      "Iteration 38288, loss = 0.00676153\n",
      "Iteration 38289, loss = 0.00676143\n",
      "Iteration 38290, loss = 0.00676133\n",
      "Iteration 38291, loss = 0.00676123\n",
      "Iteration 38292, loss = 0.00676112\n",
      "Iteration 38293, loss = 0.00676102\n",
      "Iteration 38294, loss = 0.00676092\n",
      "Iteration 38295, loss = 0.00676082\n",
      "Iteration 38296, loss = 0.00676072\n",
      "Iteration 38297, loss = 0.00676062\n",
      "Iteration 38298, loss = 0.00676052\n",
      "Iteration 38299, loss = 0.00676042\n",
      "Iteration 38300, loss = 0.00676032\n",
      "Iteration 38301, loss = 0.00676022\n",
      "Iteration 38302, loss = 0.00676012\n",
      "Iteration 38303, loss = 0.00676002\n",
      "Iteration 38304, loss = 0.00675992\n",
      "Iteration 38305, loss = 0.00675981\n",
      "Iteration 38306, loss = 0.00675971\n",
      "Iteration 38307, loss = 0.00675961\n",
      "Iteration 38308, loss = 0.00675951\n",
      "Iteration 38309, loss = 0.00675941\n",
      "Iteration 38310, loss = 0.00675931\n",
      "Iteration 38311, loss = 0.00675921\n",
      "Iteration 38312, loss = 0.00675911\n",
      "Iteration 38313, loss = 0.00675901\n",
      "Iteration 38314, loss = 0.00675891\n",
      "Iteration 38315, loss = 0.00675881\n",
      "Iteration 38316, loss = 0.00675871\n",
      "Iteration 38317, loss = 0.00675861\n",
      "Iteration 38318, loss = 0.00675851\n",
      "Iteration 38319, loss = 0.00675840\n",
      "Iteration 38320, loss = 0.00675830\n",
      "Iteration 38321, loss = 0.00675820\n",
      "Iteration 38322, loss = 0.00675810\n",
      "Iteration 38323, loss = 0.00675800\n",
      "Iteration 38324, loss = 0.00675790\n",
      "Iteration 38325, loss = 0.00675780\n",
      "Iteration 38326, loss = 0.00675770\n",
      "Iteration 38327, loss = 0.00675760\n",
      "Iteration 38328, loss = 0.00675750\n",
      "Iteration 38329, loss = 0.00675740\n",
      "Iteration 38330, loss = 0.00675730\n",
      "Iteration 38331, loss = 0.00675720\n",
      "Iteration 38332, loss = 0.00675710\n",
      "Iteration 38333, loss = 0.00675700\n",
      "Iteration 38334, loss = 0.00675690\n",
      "Iteration 38335, loss = 0.00675679\n",
      "Iteration 38336, loss = 0.00675669\n",
      "Iteration 38337, loss = 0.00675659\n",
      "Iteration 38338, loss = 0.00675649\n",
      "Iteration 38339, loss = 0.00675639\n",
      "Iteration 38340, loss = 0.00675629\n",
      "Iteration 38341, loss = 0.00675619\n",
      "Iteration 38342, loss = 0.00675609\n",
      "Iteration 38343, loss = 0.00675599\n",
      "Iteration 38344, loss = 0.00675589\n",
      "Iteration 38345, loss = 0.00675579\n",
      "Iteration 38346, loss = 0.00675569\n",
      "Iteration 38347, loss = 0.00675559\n",
      "Iteration 38348, loss = 0.00675549\n",
      "Iteration 38349, loss = 0.00675539\n",
      "Iteration 38350, loss = 0.00675529\n",
      "Iteration 38351, loss = 0.00675519\n",
      "Iteration 38352, loss = 0.00675509\n",
      "Iteration 38353, loss = 0.00675499\n",
      "Iteration 38354, loss = 0.00675489\n",
      "Iteration 38355, loss = 0.00675479\n",
      "Iteration 38356, loss = 0.00675469\n",
      "Iteration 38357, loss = 0.00675458\n",
      "Iteration 38358, loss = 0.00675448\n",
      "Iteration 38359, loss = 0.00675438\n",
      "Iteration 38360, loss = 0.00675428\n",
      "Iteration 38361, loss = 0.00675418\n",
      "Iteration 38362, loss = 0.00675408\n",
      "Iteration 38363, loss = 0.00675398\n",
      "Iteration 38364, loss = 0.00675388\n",
      "Iteration 38365, loss = 0.00675378\n",
      "Iteration 38366, loss = 0.00675368\n",
      "Iteration 38367, loss = 0.00675358\n",
      "Iteration 38368, loss = 0.00675348\n",
      "Iteration 38369, loss = 0.00675338\n",
      "Iteration 38370, loss = 0.00675328\n",
      "Iteration 38371, loss = 0.00675318\n",
      "Iteration 38372, loss = 0.00675308\n",
      "Iteration 38373, loss = 0.00675298\n",
      "Iteration 38374, loss = 0.00675288\n",
      "Iteration 38375, loss = 0.00675278\n",
      "Iteration 38376, loss = 0.00675268\n",
      "Iteration 38377, loss = 0.00675258\n",
      "Iteration 38378, loss = 0.00675248\n",
      "Iteration 38379, loss = 0.00675238\n",
      "Iteration 38380, loss = 0.00675228\n",
      "Iteration 38381, loss = 0.00675218\n",
      "Iteration 38382, loss = 0.00675208\n",
      "Iteration 38383, loss = 0.00675198\n",
      "Iteration 38384, loss = 0.00675188\n",
      "Iteration 38385, loss = 0.00675178\n",
      "Iteration 38386, loss = 0.00675168\n",
      "Iteration 38387, loss = 0.00675158\n",
      "Iteration 38388, loss = 0.00675148\n",
      "Iteration 38389, loss = 0.00675138\n",
      "Iteration 38390, loss = 0.00675128\n",
      "Iteration 38391, loss = 0.00675118\n",
      "Iteration 38392, loss = 0.00675108\n",
      "Iteration 38393, loss = 0.00675098\n",
      "Iteration 38394, loss = 0.00675088\n",
      "Iteration 38395, loss = 0.00675078\n",
      "Iteration 38396, loss = 0.00675068\n",
      "Iteration 38397, loss = 0.00675058\n",
      "Iteration 38398, loss = 0.00675047\n",
      "Iteration 38399, loss = 0.00675037\n",
      "Iteration 38400, loss = 0.00675027\n",
      "Iteration 38401, loss = 0.00675017\n",
      "Iteration 38402, loss = 0.00675007\n",
      "Iteration 38403, loss = 0.00674997\n",
      "Iteration 38404, loss = 0.00674987\n",
      "Iteration 38405, loss = 0.00674977\n",
      "Iteration 38406, loss = 0.00674967\n",
      "Iteration 38407, loss = 0.00674957\n",
      "Iteration 38408, loss = 0.00674947\n",
      "Iteration 38409, loss = 0.00674937\n",
      "Iteration 38410, loss = 0.00674927\n",
      "Iteration 38411, loss = 0.00674917\n",
      "Iteration 38412, loss = 0.00674907\n",
      "Iteration 38413, loss = 0.00674897\n",
      "Iteration 38414, loss = 0.00674887\n",
      "Iteration 38415, loss = 0.00674877\n",
      "Iteration 38416, loss = 0.00674867\n",
      "Iteration 38417, loss = 0.00674857\n",
      "Iteration 38418, loss = 0.00674847\n",
      "Iteration 38419, loss = 0.00674837\n",
      "Iteration 38420, loss = 0.00674827\n",
      "Iteration 38421, loss = 0.00674817\n",
      "Iteration 38422, loss = 0.00674807\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf2 = MLPClassifier(hidden_layer_sizes=(1,),activation='logistic', solver='sgd', alpha=0.0001,tol=0.0000001,\n",
    "                    learning_rate='constant', learning_rate_init=0.01,verbose=True,\n",
    "                    random_state=1, max_iter=500000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decision_boundary(lambda x: clf2.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
